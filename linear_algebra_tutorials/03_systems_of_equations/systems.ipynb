{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 03: Systems of Linear Equations\n",
    "\n",
    "Solving Ax = b - the foundation of ML optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Visualizing Systems of Equations (2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_system(A, b, title=\"System of 2 Equations\"):\n",
    "    \"\"\"Visualize a 2x2 system as intersecting lines.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    x_range = np.linspace(-5, 5, 100)\n",
    "    \n",
    "    # Each row of A gives a line: a1*x + a2*y = b\n",
    "    # Rearrange: y = (b - a1*x) / a2\n",
    "    \n",
    "    colors = ['blue', 'red', 'green', 'orange']\n",
    "    for i in range(A.shape[0]):\n",
    "        a1, a2 = A[i, 0], A[i, 1]\n",
    "        if abs(a2) > 1e-10:\n",
    "            y = (b[i] - a1 * x_range) / a2\n",
    "            ax.plot(x_range, y, colors[i % len(colors)], linewidth=2,\n",
    "                   label=f'{a1}x + {a2}y = {b[i]}')\n",
    "        else:\n",
    "            # Vertical line\n",
    "            ax.axvline(x=b[i]/a1, color=colors[i % len(colors)], linewidth=2,\n",
    "                      label=f'{a1}x = {b[i]}')\n",
    "    \n",
    "    # Try to find and plot solution\n",
    "    try:\n",
    "        x_sol = np.linalg.solve(A, b)\n",
    "        ax.plot(x_sol[0], x_sol[1], 'ko', markersize=15, zorder=5)\n",
    "        ax.annotate(f'Solution: ({x_sol[0]:.2f}, {x_sol[1]:.2f})',\n",
    "                   xy=(x_sol[0], x_sol[1]), xytext=(10, 10),\n",
    "                   textcoords='offset points', fontsize=12)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    ax.set_xlim(-5, 5)\n",
    "    ax.set_ylim(-5, 5)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "    plt.show()\n",
    "\n",
    "# Unique solution\n",
    "A1 = np.array([[1, 1], [1, -1]])\n",
    "b1 = np.array([4, 2])\n",
    "plot_2d_system(A1, b1, \"Unique Solution: Lines Intersect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No solution (parallel lines)\n",
    "A2 = np.array([[1, 2], [2, 4]])\n",
    "b2 = np.array([3, 8])\n",
    "plot_2d_system(A2, b2, \"No Solution: Parallel Lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infinitely many solutions (same line)\n",
    "A3 = np.array([[1, 2], [2, 4]])\n",
    "b3 = np.array([3, 6])\n",
    "plot_2d_system(A3, b3, \"Infinite Solutions: Same Line\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gaussian Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_elimination(A, b, verbose=True):\n",
    "    \"\"\"Solve Ax = b using Gaussian elimination with back substitution.\"\"\"\n",
    "    n = len(b)\n",
    "    # Create augmented matrix\n",
    "    M = np.hstack([A.astype(float), b.reshape(-1, 1).astype(float)])\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Initial augmented matrix:\")\n",
    "        print(M)\n",
    "        print()\n",
    "    \n",
    "    # Forward elimination\n",
    "    for i in range(n):\n",
    "        # Find pivot\n",
    "        max_row = i + np.argmax(np.abs(M[i:, i]))\n",
    "        M[[i, max_row]] = M[[max_row, i]]\n",
    "        \n",
    "        if np.abs(M[i, i]) < 1e-10:\n",
    "            print(\"Matrix is singular!\")\n",
    "            return None\n",
    "        \n",
    "        # Eliminate below\n",
    "        for j in range(i + 1, n):\n",
    "            factor = M[j, i] / M[i, i]\n",
    "            M[j, i:] -= factor * M[i, i:]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"After eliminating column {i}:\")\n",
    "            print(M)\n",
    "            print()\n",
    "    \n",
    "    # Back substitution\n",
    "    x = np.zeros(n)\n",
    "    for i in range(n - 1, -1, -1):\n",
    "        x[i] = (M[i, -1] - np.dot(M[i, i+1:n], x[i+1:])) / M[i, i]\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Test\n",
    "A = np.array([[1, 2, 1], [2, 1, 1], [3, 1, 2]])\n",
    "b = np.array([9, 8, 13])\n",
    "\n",
    "x = gaussian_elimination(A, b)\n",
    "print(f\"Solution: x = {x}\")\n",
    "print(f\"Verification Ax = {A @ x}\")\n",
    "print(f\"Expected b = {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LU Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lu_decomposition(A):\n",
    "    \"\"\"Compute LU decomposition (without pivoting).\"\"\"\n",
    "    n = A.shape[0]\n",
    "    L = np.eye(n)\n",
    "    U = A.astype(float).copy()\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            factor = U[j, i] / U[i, i]\n",
    "            L[j, i] = factor\n",
    "            U[j, i:] -= factor * U[i, i:]\n",
    "    \n",
    "    return L, U\n",
    "\n",
    "def solve_lu(L, U, b):\n",
    "    \"\"\"Solve Ax = b using LU decomposition.\"\"\"\n",
    "    n = len(b)\n",
    "    \n",
    "    # Forward substitution: Ly = b\n",
    "    y = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        y[i] = b[i] - np.dot(L[i, :i], y[:i])\n",
    "    \n",
    "    # Back substitution: Ux = y\n",
    "    x = np.zeros(n)\n",
    "    for i in range(n - 1, -1, -1):\n",
    "        x[i] = (y[i] - np.dot(U[i, i+1:], x[i+1:])) / U[i, i]\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Test\n",
    "A = np.array([[4, 3], [6, 3]], dtype=float)\n",
    "b = np.array([10, 12], dtype=float)\n",
    "\n",
    "L, U = lu_decomposition(A)\n",
    "print(\"L =\")\n",
    "print(L)\n",
    "print(\"\\nU =\")\n",
    "print(U)\n",
    "print(\"\\nL @ U =\")\n",
    "print(L @ U)\n",
    "print(\"\\nOriginal A =\")\n",
    "print(A)\n",
    "\n",
    "x = solve_lu(L, U, b)\n",
    "print(f\"\\nSolution: x = {x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Least Squares Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_least_squares():\n",
    "    \"\"\"Show overdetermined system and least squares solution.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate noisy linear data\n",
    "    n_points = 50\n",
    "    x = np.linspace(0, 10, n_points)\n",
    "    true_slope, true_intercept = 2.5, 1\n",
    "    y = true_slope * x + true_intercept + np.random.randn(n_points) * 2\n",
    "    \n",
    "    # Set up overdetermined system: A @ [w0, w1]^T = y\n",
    "    A = np.column_stack([np.ones(n_points), x])\n",
    "    \n",
    "    # Solve via normal equations\n",
    "    ATA = A.T @ A\n",
    "    ATy = A.T @ y\n",
    "    w = np.linalg.solve(ATA, ATy)\n",
    "    \n",
    "    print(f\"Normal equations solution:\")\n",
    "    print(f\"  Intercept: {w[0]:.4f}\")\n",
    "    print(f\"  Slope: {w[1]:.4f}\")\n",
    "    \n",
    "    # Compare with numpy's least squares\n",
    "    w_numpy, residuals, rank, s = np.linalg.lstsq(A, y, rcond=None)\n",
    "    print(f\"\\nNumPy lstsq solution: {w_numpy}\")\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.scatter(x, y, alpha=0.6, label='Data points')\n",
    "    \n",
    "    x_line = np.linspace(0, 10, 100)\n",
    "    y_line = w[0] + w[1] * x_line\n",
    "    ax.plot(x_line, y_line, 'r-', linewidth=2, label=f'Least squares: y = {w[0]:.2f} + {w[1]:.2f}x')\n",
    "    \n",
    "    # Show residuals\n",
    "    y_pred = A @ w\n",
    "    for i in range(0, n_points, 5):\n",
    "        ax.plot([x[i], x[i]], [y[i], y_pred[i]], 'g-', alpha=0.5)\n",
    "    \n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title('Least Squares: Minimizing Sum of Squared Residuals')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return A, y, w\n",
    "\n",
    "A, y, w = visualize_least_squares()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Geometric View: Projection onto Column Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_projection_3d():\n",
    "    \"\"\"Show least squares as projection onto column space.\"\"\"\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    \n",
    "    # Simple example: 3 equations, 2 unknowns\n",
    "    A = np.array([[1, 0], [0, 1], [1, 1]])\n",
    "    b = np.array([1, 1, 1])\n",
    "    \n",
    "    # Least squares solution\n",
    "    x_ls = np.linalg.lstsq(A, b, rcond=None)[0]\n",
    "    projection = A @ x_ls\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # 3D plot\n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    \n",
    "    # Column space (plane spanned by columns of A)\n",
    "    col1, col2 = A[:, 0], A[:, 1]\n",
    "    \n",
    "    # Create mesh for plane\n",
    "    s_range = np.linspace(-1, 2, 10)\n",
    "    t_range = np.linspace(-1, 2, 10)\n",
    "    S, T = np.meshgrid(s_range, t_range)\n",
    "    \n",
    "    X = S * col1[0] + T * col2[0]\n",
    "    Y = S * col1[1] + T * col2[1]\n",
    "    Z = S * col1[2] + T * col2[2]\n",
    "    \n",
    "    ax1.plot_surface(X, Y, Z, alpha=0.3, color='blue')\n",
    "    \n",
    "    # Plot b\n",
    "    ax1.scatter([b[0]], [b[1]], [b[2]], color='red', s=100, label='b')\n",
    "    \n",
    "    # Plot projection\n",
    "    ax1.scatter([projection[0]], [projection[1]], [projection[2]], \n",
    "                color='green', s=100, label='Ax (projection)')\n",
    "    \n",
    "    # Plot error vector\n",
    "    ax1.plot([b[0], projection[0]], [b[1], projection[1]], [b[2], projection[2]],\n",
    "            'r--', linewidth=2, label='Error (perpendicular)')\n",
    "    \n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('y')\n",
    "    ax1.set_zlabel('z')\n",
    "    ax1.set_title('Least Squares = Projection\\nonto Column Space of A')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # 2D view showing residual is perpendicular\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    \n",
    "    error = b - projection\n",
    "    \n",
    "    # Verify error is perpendicular to column space\n",
    "    print(f\"Error · col1 = {np.dot(error, col1):.6f} (should be ~0)\")\n",
    "    print(f\"Error · col2 = {np.dot(error, col2):.6f} (should be ~0)\")\n",
    "    \n",
    "    ax2.bar(['Error · col1', 'Error · col2'], [np.dot(error, col1), np.dot(error, col2)])\n",
    "    ax2.axhline(y=0, color='k', linestyle='--')\n",
    "    ax2.set_ylabel('Dot Product')\n",
    "    ax2.set_title('Error is Perpendicular to Column Space\\n(Dot products ≈ 0)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_projection_3d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Condition Number and Numerical Stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_condition_number():\n",
    "    \"\"\"Show how condition number affects solution stability.\"\"\"\n",
    "    \n",
    "    # Well-conditioned matrix\n",
    "    A_good = np.array([[1, 0], [0, 1]])\n",
    "    \n",
    "    # Ill-conditioned matrix (nearly singular)\n",
    "    A_bad = np.array([[1, 1], [1, 1.0001]])\n",
    "    \n",
    "    b = np.array([1, 2])\n",
    "    \n",
    "    # Solve both\n",
    "    x_good = np.linalg.solve(A_good, b)\n",
    "    x_bad = np.linalg.solve(A_bad, b)\n",
    "    \n",
    "    print(\"Well-conditioned matrix:\")\n",
    "    print(f\"  Condition number: {np.linalg.cond(A_good):.2f}\")\n",
    "    print(f\"  Solution: {x_good}\")\n",
    "    \n",
    "    print(\"\\nIll-conditioned matrix:\")\n",
    "    print(f\"  Condition number: {np.linalg.cond(A_bad):.2f}\")\n",
    "    print(f\"  Solution: {x_bad}\")\n",
    "    \n",
    "    # Show sensitivity to perturbation\n",
    "    print(\"\\n--- Sensitivity to small perturbation in b ---\")\n",
    "    \n",
    "    delta_b = np.array([0.001, 0])\n",
    "    \n",
    "    x_good_perturbed = np.linalg.solve(A_good, b + delta_b)\n",
    "    x_bad_perturbed = np.linalg.solve(A_bad, b + delta_b)\n",
    "    \n",
    "    print(f\"\\nPerturbation in b: {delta_b}\")\n",
    "    print(f\"\\nWell-conditioned:\")\n",
    "    print(f\"  Change in x: {np.linalg.norm(x_good_perturbed - x_good):.6f}\")\n",
    "    print(f\"\\nIll-conditioned:\")\n",
    "    print(f\"  Change in x: {np.linalg.norm(x_bad_perturbed - x_bad):.6f}\")\n",
    "\n",
    "demonstrate_condition_number()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Ridge Regression (Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_ols_ridge():\n",
    "    \"\"\"Compare ordinary least squares with ridge regression.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate data with collinear features (ill-conditioned)\n",
    "    n_samples = 100\n",
    "    x1 = np.random.randn(n_samples)\n",
    "    x2 = x1 + np.random.randn(n_samples) * 0.01  # Almost same as x1\n",
    "    X = np.column_stack([np.ones(n_samples), x1, x2])\n",
    "    \n",
    "    # True relationship only uses x1\n",
    "    y = 1 + 2 * x1 + np.random.randn(n_samples) * 0.5\n",
    "    \n",
    "    print(f\"Condition number of X^T X: {np.linalg.cond(X.T @ X):.2f}\")\n",
    "    \n",
    "    # OLS solution (can be unstable)\n",
    "    try:\n",
    "        w_ols = np.linalg.solve(X.T @ X, X.T @ y)\n",
    "        print(f\"\\nOLS weights: {w_ols}\")\n",
    "    except:\n",
    "        print(\"OLS failed due to singular matrix\")\n",
    "        w_ols = None\n",
    "    \n",
    "    # Ridge regression for different lambda values\n",
    "    lambdas = [0.001, 0.01, 0.1, 1, 10]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    w_history = []\n",
    "    cond_history = []\n",
    "    \n",
    "    for lam in lambdas:\n",
    "        # Ridge: (X^T X + λI) w = X^T y\n",
    "        XTX_reg = X.T @ X + lam * np.eye(X.shape[1])\n",
    "        w_ridge = np.linalg.solve(XTX_reg, X.T @ y)\n",
    "        w_history.append(w_ridge)\n",
    "        cond_history.append(np.linalg.cond(XTX_reg))\n",
    "        print(f\"\\nλ = {lam}: weights = {w_ridge}, cond = {cond_history[-1]:.2f}\")\n",
    "    \n",
    "    w_history = np.array(w_history)\n",
    "    \n",
    "    # Plot weight paths\n",
    "    for i in range(w_history.shape[1]):\n",
    "        ax.semilogx(lambdas, w_history[:, i], 'o-', label=f'w{i}')\n",
    "    \n",
    "    ax.set_xlabel('λ (regularization)')\n",
    "    ax.set_ylabel('Weight value')\n",
    "    ax.set_title('Ridge Regression: Weight Paths\\n(Regularization stabilizes ill-conditioned problem)')\n",
    "    ax.legend()\n",
    "    ax.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "compare_ols_ridge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Iterative Method: Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_for_linear_system(A, b, lr=0.01, max_iter=1000, tol=1e-6):\n",
    "    \"\"\"Solve Ax = b using gradient descent on ||Ax - b||^2.\"\"\"\n",
    "    n = A.shape[1]\n",
    "    x = np.zeros(n)\n",
    "    \n",
    "    history = {'x': [x.copy()], 'loss': []}\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        # Loss = ||Ax - b||^2\n",
    "        residual = A @ x - b\n",
    "        loss = np.sum(residual ** 2)\n",
    "        history['loss'].append(loss)\n",
    "        \n",
    "        # Gradient = 2 * A^T (Ax - b)\n",
    "        gradient = 2 * A.T @ residual\n",
    "        \n",
    "        # Update\n",
    "        x = x - lr * gradient\n",
    "        history['x'].append(x.copy())\n",
    "        \n",
    "        if loss < tol:\n",
    "            print(f\"Converged at iteration {i}\")\n",
    "            break\n",
    "    \n",
    "    return x, history\n",
    "\n",
    "# Test\n",
    "A = np.array([[4, 1], [1, 3]], dtype=float)\n",
    "b = np.array([1, 2], dtype=float)\n",
    "\n",
    "x_gd, history = gradient_descent_for_linear_system(A, b, lr=0.1, max_iter=100)\n",
    "x_exact = np.linalg.solve(A, b)\n",
    "\n",
    "print(f\"\\nGradient descent solution: {x_gd}\")\n",
    "print(f\"Exact solution: {x_exact}\")\n",
    "\n",
    "# Plot convergence\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].semilogy(history['loss'])\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[0].set_ylabel('Loss (log scale)')\n",
    "axes[0].set_title('Convergence of Gradient Descent')\n",
    "\n",
    "# Path in solution space\n",
    "x_history = np.array(history['x'])\n",
    "axes[1].plot(x_history[:, 0], x_history[:, 1], 'b.-', alpha=0.5, label='GD path')\n",
    "axes[1].plot(x_history[0, 0], x_history[0, 1], 'go', markersize=10, label='Start')\n",
    "axes[1].plot(x_exact[0], x_exact[1], 'r*', markersize=15, label='Exact solution')\n",
    "axes[1].set_xlabel('x1')\n",
    "axes[1].set_ylabel('x2')\n",
    "axes[1].set_title('Gradient Descent Path')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "SOLVING LINEAR SYSTEMS: SUMMARY\n",
    "================================\n",
    "\n",
    "1. EXISTENCE OF SOLUTIONS\n",
    "   - Unique: rank(A) = rank([A|b]) = n\n",
    "   - None: rank(A) < rank([A|b])\n",
    "   - Infinite: rank(A) = rank([A|b]) < n\n",
    "\n",
    "2. DIRECT METHODS\n",
    "   - Gaussian elimination: O(n³), general purpose\n",
    "   - LU decomposition: efficient for same A, different b\n",
    "\n",
    "3. LEAST SQUARES\n",
    "   - When no exact solution exists (overdetermined)\n",
    "   - Normal equations: A^T A x = A^T b\n",
    "   - Geometric: projection onto column space\n",
    "\n",
    "4. NUMERICAL CONSIDERATIONS\n",
    "   - Condition number: measures sensitivity\n",
    "   - Ridge regression: adds λI to stabilize\n",
    "\n",
    "5. ITERATIVE METHODS\n",
    "   - Gradient descent: simple but can be slow\n",
    "   - Conjugate gradient: optimal for symmetric positive definite\n",
    "   - Essential for very large systems\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
