{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 08: Positive Definite Matrices\n",
    "\n",
    "Interactive visualizations for understanding positive definite matrices in ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What Makes a Matrix Positive Definite?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_positive_definiteness(A, name=\"A\"):\n",
    "    \"\"\"\n",
    "    Check if a matrix is positive definite using multiple tests.\n",
    "    \"\"\"\n",
    "    print(f\"Matrix {name}:\")\n",
    "    print(A)\n",
    "    print()\n",
    "    \n",
    "    # Test 1: Eigenvalues\n",
    "    eigenvalues = np.linalg.eigvalsh(A)\n",
    "    print(f\"Eigenvalues: {eigenvalues}\")\n",
    "    \n",
    "    all_positive = np.all(eigenvalues > 0)\n",
    "    all_nonneg = np.all(eigenvalues >= 0)\n",
    "    \n",
    "    if all_positive:\n",
    "        status = \"POSITIVE DEFINITE\"\n",
    "    elif all_nonneg:\n",
    "        status = \"POSITIVE SEMI-DEFINITE\"\n",
    "    else:\n",
    "        status = \"INDEFINITE\"\n",
    "    \n",
    "    print(f\"Status: {status}\")\n",
    "    \n",
    "    # Test 2: Try Cholesky\n",
    "    try:\n",
    "        L = np.linalg.cholesky(A)\n",
    "        print(f\"Cholesky exists: Yes\")\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(f\"Cholesky exists: No\")\n",
    "    \n",
    "    # Test 3: Leading principal minors (Sylvester)\n",
    "    n = A.shape[0]\n",
    "    minors = [np.linalg.det(A[:k, :k]) for k in range(1, n+1)]\n",
    "    print(f\"Leading principal minors: {minors}\")\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "    return status\n",
    "\n",
    "# Test different matrices\n",
    "A_pd = np.array([[2, 1], [1, 2]])  # Positive definite\n",
    "A_psd = np.array([[1, 1], [1, 1]])  # Positive semi-definite\n",
    "A_indef = np.array([[1, 2], [2, 1]])  # Indefinite\n",
    "\n",
    "check_positive_definiteness(A_pd, \"A (should be PD)\")\n",
    "check_positive_definiteness(A_psd, \"B (should be PSD)\")\n",
    "check_positive_definiteness(A_indef, \"C (should be indefinite)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Geometric Visualization: Quadratic Forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_quadratic_form(A, title=\"Quadratic Form\"):\n",
    "    \"\"\"\n",
    "    Visualize f(x) = x^T A x for a 2x2 symmetric matrix.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Create grid\n",
    "    x = np.linspace(-2, 2, 100)\n",
    "    y = np.linspace(-2, 2, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    # Compute quadratic form\n",
    "    Z = A[0, 0] * X**2 + (A[0, 1] + A[1, 0]) * X * Y + A[1, 1] * Y**2\n",
    "    \n",
    "    # Eigendecomposition\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(A)\n",
    "    \n",
    "    # 3D Surface\n",
    "    ax1 = fig.add_subplot(131, projection='3d')\n",
    "    \n",
    "    # Color based on definiteness\n",
    "    if np.all(eigenvalues > 0):\n",
    "        cmap = 'Greens'\n",
    "        status = 'PD (bowl up)'\n",
    "    elif np.all(eigenvalues >= 0):\n",
    "        cmap = 'Blues'\n",
    "        status = 'PSD'\n",
    "    elif np.all(eigenvalues < 0):\n",
    "        cmap = 'Reds'\n",
    "        status = 'ND (bowl down)'\n",
    "    else:\n",
    "        cmap = 'RdYlBu'\n",
    "        status = 'Indefinite (saddle)'\n",
    "    \n",
    "    ax1.plot_surface(X, Y, Z, cmap=cmap, alpha=0.8)\n",
    "    ax1.set_xlabel('x₁')\n",
    "    ax1.set_ylabel('x₂')\n",
    "    ax1.set_zlabel('f(x)')\n",
    "    ax1.set_title(f'Surface\\n{status}')\n",
    "    \n",
    "    # Contour plot\n",
    "    ax2 = fig.add_subplot(132)\n",
    "    levels = np.linspace(Z.min(), Z.max(), 20)\n",
    "    contour = ax2.contour(X, Y, Z, levels=levels, cmap=cmap)\n",
    "    ax2.clabel(contour, inline=True, fontsize=8)\n",
    "    \n",
    "    # Draw eigenvectors\n",
    "    scale = 1.5\n",
    "    colors = ['red', 'blue']\n",
    "    for i, (val, vec) in enumerate(zip(eigenvalues, eigenvectors.T)):\n",
    "        ax2.arrow(0, 0, scale*vec[0], scale*vec[1], head_width=0.1,\n",
    "                  head_length=0.05, fc=colors[i], ec=colors[i], linewidth=2)\n",
    "        ax2.text(scale*vec[0]*1.15, scale*vec[1]*1.15, f'λ={val:.2f}',\n",
    "                fontsize=10, color=colors[i], fontweight='bold')\n",
    "    \n",
    "    ax2.set_xlim(-2, 2)\n",
    "    ax2.set_ylim(-2, 2)\n",
    "    ax2.set_aspect('equal')\n",
    "    ax2.set_xlabel('x₁')\n",
    "    ax2.set_ylabel('x₂')\n",
    "    ax2.set_title('Contours with Eigenvectors')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Energy along eigenvector directions\n",
    "    ax3 = fig.add_subplot(133)\n",
    "    t = np.linspace(-2, 2, 100)\n",
    "    \n",
    "    for i, (val, vec) in enumerate(zip(eigenvalues, eigenvectors.T)):\n",
    "        # f along eigenvector direction\n",
    "        f_along = val * t**2\n",
    "        ax3.plot(t, f_along, colors[i], linewidth=2, label=f'Along v{i+1} (λ={val:.2f})')\n",
    "    \n",
    "    ax3.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax3.set_xlabel('t (position along eigenvector)')\n",
    "    ax3.set_ylabel('f(t·v)')\n",
    "    ax3.set_title('Quadratic Form Along Eigenvectors')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'{title}\\nA = {A.tolist()}\\nEigenvalues: {eigenvalues}', fontsize=12, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Positive definite (bowl shape)\n",
    "print(\"POSITIVE DEFINITE: All eigenvalues > 0\")\n",
    "visualize_quadratic_form(np.array([[3, 1], [1, 2]]), \"Positive Definite Matrix\")\n",
    "\n",
    "# Indefinite (saddle shape)\n",
    "print(\"\\nINDEFINITE: Mixed sign eigenvalues\")\n",
    "visualize_quadratic_form(np.array([[1, 2], [2, 1]]), \"Indefinite Matrix\")\n",
    "\n",
    "# Effect of condition number\n",
    "print(\"\\nEFFECT OF CONDITION NUMBER:\")\n",
    "visualize_quadratic_form(np.array([[4, 0], [0, 1]]), \"Well-conditioned (κ=4)\")\n",
    "visualize_quadratic_form(np.array([[10, 0], [0, 1]]), \"Ill-conditioned (κ=10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cholesky Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cholesky_step_by_step(A):\n",
    "    \"\"\"\n",
    "    Compute Cholesky decomposition with detailed steps.\n",
    "    \"\"\"\n",
    "    print(\"Cholesky Decomposition: A = L @ L.T\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Input A:\\n{A}\\n\")\n",
    "    \n",
    "    n = A.shape[0]\n",
    "    L = np.zeros((n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i + 1):\n",
    "            if i == j:\n",
    "                # Diagonal element\n",
    "                sum_sq = sum(L[i, k]**2 for k in range(j))\n",
    "                val = A[i, i] - sum_sq\n",
    "                L[i, j] = np.sqrt(val)\n",
    "                print(f\"L[{i},{j}] = sqrt(A[{i},{i}] - sum(L[{i},0:{j}]²))\")\n",
    "                print(f\"       = sqrt({A[i,i]} - {sum_sq}) = sqrt({val}) = {L[i,j]:.4f}\")\n",
    "            else:\n",
    "                # Off-diagonal element\n",
    "                sum_prod = sum(L[i, k] * L[j, k] for k in range(j))\n",
    "                L[i, j] = (A[i, j] - sum_prod) / L[j, j]\n",
    "                print(f\"L[{i},{j}] = (A[{i},{j}] - sum(L[{i},k]*L[{j},k])) / L[{j},{j}]\")\n",
    "                print(f\"       = ({A[i,j]} - {sum_prod}) / {L[j,j]:.4f} = {L[i,j]:.4f}\")\n",
    "        print()\n",
    "    \n",
    "    print(f\"Result L:\\n{L}\\n\")\n",
    "    print(f\"Verification L @ L.T:\\n{L @ L.T}\")\n",
    "    \n",
    "    return L\n",
    "\n",
    "# Example\n",
    "A = np.array([[4, 2, -2],\n",
    "              [2, 5, -4],\n",
    "              [-2, -4, 14]], dtype=float)\n",
    "\n",
    "L = cholesky_step_by_step(A)\n",
    "\n",
    "# Compare with numpy\n",
    "print(\"\\nNumPy Cholesky:\")\n",
    "print(np.linalg.cholesky(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Application: Sampling from Multivariate Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_and_visualize_gaussian(mu, Sigma, n_samples=1000):\n",
    "    \"\"\"\n",
    "    Sample from N(mu, Sigma) using Cholesky and visualize.\n",
    "    \"\"\"\n",
    "    # Cholesky decomposition\n",
    "    L = np.linalg.cholesky(Sigma)\n",
    "    \n",
    "    # Sample from standard normal\n",
    "    z = np.random.randn(n_samples, 2)\n",
    "    \n",
    "    # Transform: x = mu + L @ z\n",
    "    samples = mu + z @ L.T\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Plot standard normal samples\n",
    "    ax1 = axes[0]\n",
    "    ax1.scatter(z[:, 0], z[:, 1], alpha=0.3, s=10)\n",
    "    ax1.set_xlim(-4, 4)\n",
    "    ax1.set_ylim(-4, 4)\n",
    "    ax1.set_aspect('equal')\n",
    "    ax1.set_title('Standard Normal N(0, I)\\n(Before transformation)')\n",
    "    ax1.set_xlabel('z₁')\n",
    "    ax1.set_ylabel('z₂')\n",
    "    circle = plt.Circle((0, 0), 2, fill=False, color='red', linewidth=2)\n",
    "    ax1.add_patch(circle)\n",
    "    \n",
    "    # Plot transformation matrix effect\n",
    "    ax2 = axes[1]\n",
    "    ax2.text(0.1, 0.9, f'L (Cholesky):', transform=ax2.transAxes, fontsize=12)\n",
    "    ax2.text(0.1, 0.7, f'{L}', transform=ax2.transAxes, fontsize=10, family='monospace')\n",
    "    ax2.text(0.1, 0.5, f'\\nΣ = L @ L.T:', transform=ax2.transAxes, fontsize=12)\n",
    "    ax2.text(0.1, 0.3, f'{Sigma}', transform=ax2.transAxes, fontsize=10, family='monospace')\n",
    "    ax2.text(0.1, 0.1, f'\\nμ = {mu}', transform=ax2.transAxes, fontsize=12)\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title('Transformation Parameters')\n",
    "    \n",
    "    # Plot transformed samples\n",
    "    ax3 = axes[2]\n",
    "    ax3.scatter(samples[:, 0], samples[:, 1], alpha=0.3, s=10, color='green')\n",
    "    \n",
    "    # Draw covariance ellipse\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(Sigma)\n",
    "    theta = np.linspace(0, 2*np.pi, 100)\n",
    "    \n",
    "    for n_std in [1, 2, 3]:\n",
    "        ellipse = np.array([np.cos(theta), np.sin(theta)])\n",
    "        scaled = np.diag(n_std * np.sqrt(eigenvalues)) @ ellipse\n",
    "        rotated = eigenvectors @ scaled\n",
    "        shifted = rotated + np.array(mu).reshape(-1, 1)\n",
    "        ax3.plot(shifted[0], shifted[1], 'r-', linewidth=2, \n",
    "                label=f'{n_std}σ' if n_std == 1 else None)\n",
    "    \n",
    "    ax3.scatter([mu[0]], [mu[1]], color='red', s=100, marker='x', linewidths=3)\n",
    "    ax3.set_aspect('equal')\n",
    "    ax3.set_title(f'Transformed N(μ, Σ)\\n(x = μ + L @ z)')\n",
    "    ax3.set_xlabel('x₁')\n",
    "    ax3.set_ylabel('x₂')\n",
    "    \n",
    "    plt.suptitle('Sampling from Multivariate Gaussian via Cholesky', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Verify sample statistics\n",
    "    print(f\"Target mean: {mu}\")\n",
    "    print(f\"Sample mean: {samples.mean(axis=0)}\")\n",
    "    print(f\"\\nTarget covariance:\\n{Sigma}\")\n",
    "    print(f\"Sample covariance:\\n{np.cov(samples.T)}\")\n",
    "\n",
    "# Example with correlated Gaussian\n",
    "mu = np.array([1, 2])\n",
    "Sigma = np.array([[2.0, 0.8],\n",
    "                  [0.8, 1.0]])\n",
    "\n",
    "sample_and_visualize_gaussian(mu, Sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Application: Covariance Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_covariance_matrix():\n",
    "    \"\"\"\n",
    "    Show why covariance matrices are positive semi-definite.\n",
    "    \"\"\"\n",
    "    # Generate data\n",
    "    np.random.seed(42)\n",
    "    n, d = 100, 2\n",
    "    \n",
    "    # Correlated data\n",
    "    true_cov = np.array([[2, 1], [1, 1.5]])\n",
    "    L = np.linalg.cholesky(true_cov)\n",
    "    X = np.random.randn(n, d) @ L.T\n",
    "    \n",
    "    # Center data\n",
    "    X_centered = X - X.mean(axis=0)\n",
    "    \n",
    "    # Sample covariance\n",
    "    Sigma = X_centered.T @ X_centered / (n - 1)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Plot data\n",
    "    ax1 = axes[0]\n",
    "    ax1.scatter(X_centered[:, 0], X_centered[:, 1], alpha=0.5)\n",
    "    ax1.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax1.axvline(x=0, color='k', linewidth=0.5)\n",
    "    ax1.set_aspect('equal')\n",
    "    ax1.set_title('Centered Data')\n",
    "    ax1.set_xlabel('x₁')\n",
    "    ax1.set_ylabel('x₂')\n",
    "    \n",
    "    # Show covariance matrix\n",
    "    ax2 = axes[1]\n",
    "    im = ax2.imshow(Sigma, cmap='RdBu_r', vmin=-2, vmax=2)\n",
    "    ax2.set_title(f'Sample Covariance Σ\\n(det = {np.linalg.det(Sigma):.3f})')\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax2.text(j, i, f'{Sigma[i,j]:.2f}', ha='center', va='center', fontsize=14)\n",
    "    ax2.set_xticks([0, 1])\n",
    "    ax2.set_yticks([0, 1])\n",
    "    plt.colorbar(im, ax=ax2)\n",
    "    \n",
    "    # Eigenvalues\n",
    "    eigenvalues = np.linalg.eigvalsh(Sigma)\n",
    "    ax3 = axes[2]\n",
    "    ax3.bar([0, 1], eigenvalues, color=['blue', 'orange'])\n",
    "    ax3.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax3.set_xticks([0, 1])\n",
    "    ax3.set_xticklabels(['λ₁', 'λ₂'])\n",
    "    ax3.set_title(f'Eigenvalues\\nAll ≥ 0 → PSD')\n",
    "    ax3.set_ylabel('Eigenvalue')\n",
    "    \n",
    "    for i, ev in enumerate(eigenvalues):\n",
    "        ax3.text(i, ev + 0.1, f'{ev:.3f}', ha='center', fontsize=12)\n",
    "    \n",
    "    plt.suptitle('Covariance Matrix is Always Positive Semi-Definite\\n'\n",
    "                 'Σ = XᵀX/(n-1), so vᵀΣv = ||Xv||²/(n-1) ≥ 0', fontsize=12, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Sample covariance:\\n{Sigma}\")\n",
    "    print(f\"\\nEigenvalues: {eigenvalues}\")\n",
    "    print(f\"All eigenvalues ≥ 0: {np.all(eigenvalues >= -1e-10)}\")\n",
    "\n",
    "visualize_covariance_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Application: Optimization and Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_optimization_landscape(A, b):\n",
    "    \"\"\"\n",
    "    Visualize optimization of f(x) = 1/2 x^T A x - b^T x.\n",
    "    Show how positive definiteness guarantees unique minimum.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Create grid\n",
    "    x = np.linspace(-3, 5, 100)\n",
    "    y = np.linspace(-3, 5, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    # Compute objective function\n",
    "    Z = np.zeros_like(X)\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            pt = np.array([X[i,j], Y[i,j]])\n",
    "            Z[i,j] = 0.5 * pt @ A @ pt - b @ pt\n",
    "    \n",
    "    # Optimal solution\n",
    "    x_opt = np.linalg.solve(A, b)\n",
    "    f_opt = 0.5 * x_opt @ A @ x_opt - b @ x_opt\n",
    "    \n",
    "    # Eigenvalues of Hessian (= A)\n",
    "    eigenvalues = np.linalg.eigvalsh(A)\n",
    "    \n",
    "    # 3D surface\n",
    "    ax1 = fig.add_subplot(131, projection='3d')\n",
    "    ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\n",
    "    ax1.scatter([x_opt[0]], [x_opt[1]], [f_opt], color='red', s=100, marker='*')\n",
    "    ax1.set_xlabel('x₁')\n",
    "    ax1.set_ylabel('x₂')\n",
    "    ax1.set_zlabel('f(x)')\n",
    "    ax1.set_title('Objective Function')\n",
    "    \n",
    "    # Contours with gradient descent\n",
    "    ax2 = fig.add_subplot(132)\n",
    "    ax2.contour(X, Y, Z, levels=30, cmap='viridis')\n",
    "    \n",
    "    # Gradient descent\n",
    "    x_gd = np.array([4.0, 4.0])\n",
    "    lr = 0.1\n",
    "    path = [x_gd.copy()]\n",
    "    \n",
    "    for _ in range(50):\n",
    "        grad = A @ x_gd - b\n",
    "        x_gd = x_gd - lr * grad\n",
    "        path.append(x_gd.copy())\n",
    "        \n",
    "        if np.linalg.norm(grad) < 1e-6:\n",
    "            break\n",
    "    \n",
    "    path = np.array(path)\n",
    "    ax2.plot(path[:, 0], path[:, 1], 'r.-', markersize=8, label='GD path')\n",
    "    ax2.scatter([x_opt[0]], [x_opt[1]], color='green', s=100, marker='*', label='Optimal')\n",
    "    ax2.set_xlabel('x₁')\n",
    "    ax2.set_ylabel('x₂')\n",
    "    ax2.set_title(f'Gradient Descent\\n(converged in {len(path)-1} steps)')\n",
    "    ax2.legend()\n",
    "    ax2.set_aspect('equal')\n",
    "    \n",
    "    # Eigenvalue analysis\n",
    "    ax3 = fig.add_subplot(133)\n",
    "    ax3.bar([0, 1], eigenvalues, color=['blue', 'orange'])\n",
    "    ax3.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax3.set_xticks([0, 1])\n",
    "    ax3.set_xticklabels(['λ₁', 'λ₂'])\n",
    "    ax3.set_title(f'Hessian Eigenvalues\\nκ = {max(eigenvalues)/min(eigenvalues):.1f}')\n",
    "    ax3.set_ylabel('Eigenvalue')\n",
    "    \n",
    "    for i, ev in enumerate(eigenvalues):\n",
    "        ax3.text(i, ev + 0.1, f'{ev:.2f}', ha='center', fontsize=12)\n",
    "    \n",
    "    status = \"UNIQUE MINIMUM\" if np.all(eigenvalues > 0) else \"NO UNIQUE MIN\"\n",
    "    ax3.text(0.5, 0.1, f'All λ > 0 → {status}', transform=ax3.transAxes, \n",
    "             ha='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle(f'Optimization: f(x) = ½xᵀAx - bᵀx\\n'\n",
    "                 f'A = {A.tolist()}, b = {b.tolist()}\\n'\n",
    "                 f'Optimal x* = A⁻¹b = {x_opt}', fontsize=11, y=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Well-conditioned problem\n",
    "print(\"Well-conditioned problem (κ ≈ 2):\")\n",
    "A_good = np.array([[2, 0.5], [0.5, 1]])\n",
    "b = np.array([1, 1])\n",
    "visualize_optimization_landscape(A_good, b)\n",
    "\n",
    "# Ill-conditioned problem\n",
    "print(\"\\nIll-conditioned problem (κ = 10):\")\n",
    "A_bad = np.array([[10, 0], [0, 1]])\n",
    "visualize_optimization_landscape(A_bad, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Regularization: Making Matrices Positive Definite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_regularization():\n",
    "    \"\"\"\n",
    "    Show how regularization fixes non-positive-definite matrices.\n",
    "    \"\"\"\n",
    "    # Create a nearly singular matrix\n",
    "    np.random.seed(42)\n",
    "    n = 50\n",
    "    k = 5  # True rank\n",
    "    \n",
    "    # Low-rank matrix + small noise\n",
    "    U = np.random.randn(n, k)\n",
    "    A = U @ U.T / k + 0.01 * np.eye(n)\n",
    "    \n",
    "    # Make it nearly singular by subtracting small amount\n",
    "    eigenvalues_orig = np.linalg.eigvalsh(A)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Original eigenvalue spectrum\n",
    "    ax1 = axes[0]\n",
    "    ax1.semilogy(range(1, n+1), np.sort(eigenvalues_orig)[::-1], 'b.-', markersize=5)\n",
    "    ax1.axhline(y=0, color='r', linewidth=0.5, linestyle='--')\n",
    "    ax1.set_xlabel('Index')\n",
    "    ax1.set_ylabel('Eigenvalue (log scale)')\n",
    "    ax1.set_title(f'Original Eigenvalues\\nMin = {eigenvalues_orig.min():.2e}\\n'\n",
    "                 f'Condition = {eigenvalues_orig.max()/eigenvalues_orig.min():.2e}')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Ridge regularization\n",
    "    lambda_reg = 0.1\n",
    "    A_ridge = A + lambda_reg * np.eye(n)\n",
    "    eigenvalues_ridge = np.linalg.eigvalsh(A_ridge)\n",
    "    \n",
    "    ax2 = axes[1]\n",
    "    ax2.semilogy(range(1, n+1), np.sort(eigenvalues_orig)[::-1], 'b.-', \n",
    "                markersize=5, alpha=0.5, label='Original')\n",
    "    ax2.semilogy(range(1, n+1), np.sort(eigenvalues_ridge)[::-1], 'g.-', \n",
    "                markersize=5, label=f'Ridge (λ={lambda_reg})')\n",
    "    ax2.set_xlabel('Index')\n",
    "    ax2.set_ylabel('Eigenvalue (log scale)')\n",
    "    ax2.set_title(f'After Ridge Regularization\\nMin = {eigenvalues_ridge.min():.2e}\\n'\n",
    "                 f'Condition = {eigenvalues_ridge.max()/eigenvalues_ridge.min():.2e}')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Effect of regularization strength\n",
    "    ax3 = axes[2]\n",
    "    lambdas = np.logspace(-4, 0, 50)\n",
    "    conditions = []\n",
    "    min_eigs = []\n",
    "    \n",
    "    for lam in lambdas:\n",
    "        A_reg = A + lam * np.eye(n)\n",
    "        eigs = np.linalg.eigvalsh(A_reg)\n",
    "        conditions.append(eigs.max() / eigs.min())\n",
    "        min_eigs.append(eigs.min())\n",
    "    \n",
    "    ax3.loglog(lambdas, conditions, 'b-', linewidth=2, label='Condition Number')\n",
    "    ax3.set_xlabel('Regularization λ')\n",
    "    ax3.set_ylabel('Condition Number', color='blue')\n",
    "    ax3.tick_params(axis='y', labelcolor='blue')\n",
    "    \n",
    "    ax3_twin = ax3.twinx()\n",
    "    ax3_twin.loglog(lambdas, min_eigs, 'r-', linewidth=2, label='Min Eigenvalue')\n",
    "    ax3_twin.set_ylabel('Min Eigenvalue', color='red')\n",
    "    ax3_twin.tick_params(axis='y', labelcolor='red')\n",
    "    \n",
    "    ax3.set_title('Effect of Regularization Strength')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Regularization: A + λI\\n'\n",
    "                 'Shifts all eigenvalues by λ, improving conditioning', fontsize=12, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "demonstrate_regularization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Mahalanobis Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mahalanobis_distance():\n",
    "    \"\"\"\n",
    "    Compare Euclidean and Mahalanobis distances.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate correlated data\n",
    "    mu = np.array([0, 0])\n",
    "    Sigma = np.array([[2, 1.5], [1.5, 2]])\n",
    "    L = np.linalg.cholesky(Sigma)\n",
    "    \n",
    "    n_samples = 300\n",
    "    samples = np.random.randn(n_samples, 2) @ L.T\n",
    "    \n",
    "    # Test points\n",
    "    test_points = np.array([\n",
    "        [2, 0],      # Along minor axis\n",
    "        [0, 2],      # Along minor axis (different)\n",
    "        [1.5, 1.5],  # Along major axis\n",
    "        [-1.5, -1.5] # Along major axis\n",
    "    ])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Euclidean distance\n",
    "    ax1 = axes[0]\n",
    "    ax1.scatter(samples[:, 0], samples[:, 1], alpha=0.3, s=20)\n",
    "    \n",
    "    # Euclidean distance circles\n",
    "    for r in [1, 2, 3]:\n",
    "        circle = plt.Circle((0, 0), r, fill=False, color='red', linewidth=2, linestyle='--')\n",
    "        ax1.add_patch(circle)\n",
    "    \n",
    "    # Mark test points with Euclidean distances\n",
    "    for pt in test_points:\n",
    "        eucl_dist = np.linalg.norm(pt)\n",
    "        ax1.scatter([pt[0]], [pt[1]], s=100, marker='*', c='green', edgecolors='black', linewidths=1)\n",
    "        ax1.annotate(f'd_E={eucl_dist:.2f}', xy=pt, xytext=(pt[0]+0.3, pt[1]+0.3),\n",
    "                    fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax1.set_xlim(-5, 5)\n",
    "    ax1.set_ylim(-5, 5)\n",
    "    ax1.set_aspect('equal')\n",
    "    ax1.set_title('Euclidean Distance\\n(Ignores correlation structure)')\n",
    "    ax1.set_xlabel('x₁')\n",
    "    ax1.set_ylabel('x₂')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Mahalanobis distance\n",
    "    ax2 = axes[1]\n",
    "    ax2.scatter(samples[:, 0], samples[:, 1], alpha=0.3, s=20)\n",
    "    \n",
    "    # Mahalanobis distance ellipses\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(Sigma)\n",
    "    theta = np.linspace(0, 2*np.pi, 100)\n",
    "    \n",
    "    for d_M in [1, 2, 3]:\n",
    "        ellipse = np.array([np.cos(theta), np.sin(theta)])\n",
    "        scaled = np.diag(d_M * np.sqrt(eigenvalues)) @ ellipse\n",
    "        rotated = eigenvectors @ scaled\n",
    "        ax2.plot(rotated[0], rotated[1], 'r-', linewidth=2)\n",
    "    \n",
    "    # Compute and show Mahalanobis distances\n",
    "    Sigma_inv = np.linalg.inv(Sigma)\n",
    "    for pt in test_points:\n",
    "        mahal_dist = np.sqrt(pt @ Sigma_inv @ pt)\n",
    "        ax2.scatter([pt[0]], [pt[1]], s=100, marker='*', c='green', edgecolors='black', linewidths=1)\n",
    "        ax2.annotate(f'd_M={mahal_dist:.2f}', xy=pt, xytext=(pt[0]+0.3, pt[1]+0.3),\n",
    "                    fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax2.set_xlim(-5, 5)\n",
    "    ax2.set_ylim(-5, 5)\n",
    "    ax2.set_aspect('equal')\n",
    "    ax2.set_title('Mahalanobis Distance\\n(Accounts for correlation)')\n",
    "    ax2.set_xlabel('x₁')\n",
    "    ax2.set_ylabel('x₂')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Mahalanobis vs Euclidean Distance\\n'\n",
    "                 'd_M(x) = √(xᵀΣ⁻¹x) — uses covariance structure', fontsize=12, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Note: Points equidistant in Euclidean space have different Mahalanobis distances.\")\n",
    "    print(\"Points along the major axis of the ellipse (high variance direction) have smaller d_M.\")\n",
    "\n",
    "visualize_mahalanobis_distance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "KEY CONCEPTS: POSITIVE DEFINITE MATRICES\n",
    "=========================================\n",
    "\n",
    "1. DEFINITION AND TESTS\n",
    "   - x^T A x > 0 for all x ≠ 0\n",
    "   - All eigenvalues > 0\n",
    "   - All leading principal minors > 0 (Sylvester)\n",
    "   - Cholesky decomposition exists (A = LL^T)\n",
    "\n",
    "2. GEOMETRIC MEANING\n",
    "   - Quadratic form x^T A x is bowl-shaped\n",
    "   - Level sets are ellipsoids\n",
    "   - Unique minimum at origin\n",
    "\n",
    "3. CHOLESKY DECOMPOSITION\n",
    "   - A = L L^T (L lower triangular, positive diagonal)\n",
    "   - \"Square root\" of a matrix\n",
    "   - 2× faster than LU, no pivoting needed\n",
    "   - Use for: solving systems, sampling Gaussians\n",
    "\n",
    "4. ML APPLICATIONS\n",
    "   - Covariance matrices: always PSD (Σ = X^T X / n)\n",
    "   - Optimization: PD Hessian → unique minimum\n",
    "   - Gaussian sampling: x = μ + L z where z ~ N(0,I)\n",
    "   - Ridge regression: X^T X + λI is always PD\n",
    "\n",
    "5. CONDITION NUMBER\n",
    "   - κ(A) = λ_max / λ_min\n",
    "   - Large κ → ill-conditioned → slow optimization\n",
    "   - Regularization (A + λI) improves conditioning\n",
    "\n",
    "6. MAHALANOBIS DISTANCE\n",
    "   - d_M(x) = √(x^T Σ^{-1} x)\n",
    "   - Accounts for covariance structure\n",
    "   - Standard measure for Gaussian distributions\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
