{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 06: Singular Value Decomposition (SVD)\n",
    "\n",
    "Interactive visualizations for understanding SVD in ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Geometric Interpretation: Rotate → Scale → Rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_svd_geometry(A):\n",
    "    \"\"\"\n",
    "    Visualize SVD as: Rotate (V^T) → Scale (Σ) → Rotate (U)\n",
    "    \"\"\"\n",
    "    U, S, Vt = np.linalg.svd(A)\n",
    "    \n",
    "    # Unit circle\n",
    "    theta = np.linspace(0, 2*np.pi, 100)\n",
    "    circle = np.array([np.cos(theta), np.sin(theta)])\n",
    "    \n",
    "    # Transformations step by step\n",
    "    step1 = Vt @ circle           # After V^T (rotation in input space)\n",
    "    step2 = np.diag(S) @ step1    # After Σ (scaling)\n",
    "    step3 = U @ step2             # After U (rotation in output space)\n",
    "    \n",
    "    # Also: full transformation\n",
    "    final = A @ circle\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    \n",
    "    titles = [\n",
    "        'Original Unit Circle',\n",
    "        'After V^T (rotate/reflect)',\n",
    "        'After Σ (scale)',\n",
    "        'After U (rotate/reflect)\\n= A @ circle'\n",
    "    ]\n",
    "    data = [circle, step1, step2, step3]\n",
    "    \n",
    "    for ax, d, title in zip(axes, data, titles):\n",
    "        ax.plot(d[0], d[1], 'b-', linewidth=2)\n",
    "        ax.scatter([d[0, 0]], [d[1, 0]], c='red', s=50, zorder=5)  # Mark starting point\n",
    "        ax.set_aspect('equal')\n",
    "        ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "        ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "        ax.set_title(title)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Set same scale for all\n",
    "        max_val = max(3, np.abs(d).max() * 1.2)\n",
    "        ax.set_xlim(-max_val, max_val)\n",
    "        ax.set_ylim(-max_val, max_val)\n",
    "    \n",
    "    plt.suptitle(f'SVD Decomposition: A = UΣV^T\\nSingular values: σ₁={S[0]:.2f}, σ₂={S[1]:.2f}', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return U, S, Vt\n",
    "\n",
    "# Example 1: Simple scaling\n",
    "print(\"Example 1: Scaling matrix\")\n",
    "A = np.array([[2, 0], [0, 1]])\n",
    "visualize_svd_geometry(A)\n",
    "\n",
    "# Example 2: Rotation + scaling\n",
    "print(\"\\nExample 2: General matrix\")\n",
    "A = np.array([[2, 1], [1, 2]])\n",
    "visualize_svd_geometry(A)\n",
    "\n",
    "# Example 3: Shear\n",
    "print(\"\\nExample 3: Shear matrix\")\n",
    "A = np.array([[1, 1], [0, 1]])\n",
    "visualize_svd_geometry(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SVD for Non-Square Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_nonsquare_demo():\n",
    "    \"\"\"\n",
    "    Show SVD works for any matrix, not just square.\n",
    "    \"\"\"\n",
    "    # Tall matrix (m > n): maps R^2 to R^3\n",
    "    A_tall = np.array([[1, 0], [0, 1], [1, 1]])\n",
    "    \n",
    "    # Wide matrix (m < n): maps R^3 to R^2\n",
    "    A_wide = np.array([[1, 0, 1], [0, 1, 1]])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    for ax, A, name in [(axes[0], A_tall, 'Tall (3×2)'), (axes[1], A_wide, 'Wide (2×3)')]:\n",
    "        U, S, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "        \n",
    "        ax.text(0.5, 0.9, f'{name} Matrix', ha='center', va='top', fontsize=14, \n",
    "                transform=ax.transAxes, fontweight='bold')\n",
    "        ax.text(0.5, 0.75, f'A = \\n{A}', ha='center', va='top', fontsize=10,\n",
    "                transform=ax.transAxes, family='monospace')\n",
    "        ax.text(0.5, 0.45, f'Singular values: {S}', ha='center', va='top', fontsize=11,\n",
    "                transform=ax.transAxes)\n",
    "        ax.text(0.5, 0.3, f'Rank: {np.sum(S > 1e-10)}', ha='center', va='top', fontsize=11,\n",
    "                transform=ax.transAxes)\n",
    "        ax.text(0.5, 0.15, f'U shape: {U.shape}, Σ shape: {len(S)}, V^T shape: {Vt.shape}',\n",
    "                ha='center', va='top', fontsize=10, transform=ax.transAxes)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "svd_nonsquare_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Low-Rank Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_low_rank_approximation():\n",
    "    \"\"\"\n",
    "    Show how SVD provides the best low-rank approximation.\n",
    "    \"\"\"\n",
    "    # Create a matrix with clear low-rank structure\n",
    "    np.random.seed(42)\n",
    "    m, n = 50, 40\n",
    "    \n",
    "    # True low-rank matrix plus noise\n",
    "    true_rank = 5\n",
    "    U_true = np.random.randn(m, true_rank)\n",
    "    V_true = np.random.randn(n, true_rank)\n",
    "    A_true = U_true @ V_true.T\n",
    "    noise = 0.5 * np.random.randn(m, n)\n",
    "    A = A_true + noise\n",
    "    \n",
    "    # Compute SVD\n",
    "    U, S, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "    \n",
    "    # Different rank approximations\n",
    "    ranks = [1, 3, 5, 10, 20]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, len(ranks) + 1, figsize=(18, 8))\n",
    "    \n",
    "    # Original matrix\n",
    "    axes[0, 0].imshow(A, cmap='viridis', aspect='auto')\n",
    "    axes[0, 0].set_title('Original A')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # Singular values\n",
    "    axes[1, 0].bar(range(len(S[:20])), S[:20])\n",
    "    axes[1, 0].set_xlabel('Index')\n",
    "    axes[1, 0].set_ylabel('Singular Value')\n",
    "    axes[1, 0].set_title('Singular Values')\n",
    "    \n",
    "    for i, k in enumerate(ranks):\n",
    "        # Rank-k approximation\n",
    "        A_k = U[:, :k] @ np.diag(S[:k]) @ Vt[:k, :]\n",
    "        error = np.linalg.norm(A - A_k, 'fro') / np.linalg.norm(A, 'fro')\n",
    "        energy = np.sum(S[:k]**2) / np.sum(S**2)\n",
    "        \n",
    "        axes[0, i+1].imshow(A_k, cmap='viridis', aspect='auto')\n",
    "        axes[0, i+1].set_title(f'Rank {k}\\nError: {error:.2%}')\n",
    "        axes[0, i+1].axis('off')\n",
    "        \n",
    "        axes[1, i+1].imshow(A - A_k, cmap='RdBu', aspect='auto', vmin=-2, vmax=2)\n",
    "        axes[1, i+1].set_title(f'Difference\\nEnergy: {energy:.1%}')\n",
    "        axes[1, i+1].axis('off')\n",
    "    \n",
    "    plt.suptitle('Low-Rank Approximation via SVD (Eckart-Young Theorem)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_low_rank_approximation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Image Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_compression_demo():\n",
    "    \"\"\"\n",
    "    Demonstrate SVD image compression.\n",
    "    \"\"\"\n",
    "    # Create a sample image with structure\n",
    "    np.random.seed(42)\n",
    "    m, n = 100, 120\n",
    "    \n",
    "    # Create image with patterns\n",
    "    x, y = np.meshgrid(np.linspace(0, 1, n), np.linspace(0, 1, m))\n",
    "    image = (np.sin(4*np.pi*x) * np.sin(4*np.pi*y) + \n",
    "             np.sin(8*np.pi*x) * 0.5 +\n",
    "             np.sin(12*np.pi*y) * 0.3 +\n",
    "             0.1 * np.random.randn(m, n))\n",
    "    \n",
    "    # SVD\n",
    "    U, S, Vt = np.linalg.svd(image, full_matrices=False)\n",
    "    \n",
    "    # Different compression levels\n",
    "    k_values = [1, 5, 10, 20, 50]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, len(k_values) + 1, figsize=(18, 8))\n",
    "    \n",
    "    # Original\n",
    "    axes[0, 0].imshow(image, cmap='gray')\n",
    "    axes[0, 0].set_title(f'Original\\n{m}×{n} = {m*n} values')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # Singular value spectrum\n",
    "    axes[1, 0].semilogy(S, 'b.-')\n",
    "    axes[1, 0].set_xlabel('Index')\n",
    "    axes[1, 0].set_ylabel('Singular Value (log)')\n",
    "    axes[1, 0].set_title('Singular Value Spectrum')\n",
    "    \n",
    "    for i, k in enumerate(k_values):\n",
    "        compressed = U[:, :k] @ np.diag(S[:k]) @ Vt[:k, :]\n",
    "        storage = k * (m + n + 1)\n",
    "        ratio = (m * n) / storage\n",
    "        error = np.linalg.norm(image - compressed, 'fro') / np.linalg.norm(image, 'fro')\n",
    "        \n",
    "        axes[0, i+1].imshow(compressed, cmap='gray')\n",
    "        axes[0, i+1].set_title(f'k={k}\\n{storage} values ({ratio:.1f}× compression)')\n",
    "        axes[0, i+1].axis('off')\n",
    "        \n",
    "        # Cumulative energy\n",
    "        cumsum = np.cumsum(S[:k]**2) / np.sum(S**2)\n",
    "        axes[1, i+1].fill_between(range(k), cumsum, alpha=0.3)\n",
    "        axes[1, i+1].plot(range(k), cumsum, 'b-')\n",
    "        axes[1, i+1].axhline(y=1, color='r', linestyle='--', alpha=0.5)\n",
    "        axes[1, i+1].set_ylim(0, 1.1)\n",
    "        axes[1, i+1].set_xlabel('# Components')\n",
    "        axes[1, i+1].set_title(f'Energy: {cumsum[-1]:.1%}\\nError: {error:.1%}')\n",
    "    \n",
    "    plt.suptitle('Image Compression with SVD', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "image_compression_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SVD Components as Rank-1 Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_svd_components():\n",
    "    \"\"\"\n",
    "    Show SVD as sum of rank-1 matrices: A = Σ σ_i u_i v_i^T\n",
    "    \"\"\"\n",
    "    # Create a simple image\n",
    "    np.random.seed(42)\n",
    "    m, n = 30, 40\n",
    "    x, y = np.meshgrid(np.linspace(0, 1, n), np.linspace(0, 1, m))\n",
    "    image = np.sin(2*np.pi*x) * np.sin(2*np.pi*y) + 0.5 * np.cos(4*np.pi*x)\n",
    "    \n",
    "    U, S, Vt = np.linalg.svd(image, full_matrices=False)\n",
    "    \n",
    "    n_components = 5\n",
    "    fig, axes = plt.subplots(3, n_components + 1, figsize=(18, 10))\n",
    "    \n",
    "    cumulative = np.zeros_like(image)\n",
    "    \n",
    "    # Original\n",
    "    axes[0, 0].imshow(image, cmap='RdBu')\n",
    "    axes[0, 0].set_title('Original A')\n",
    "    axes[0, 0].axis('off')\n",
    "    axes[1, 0].axis('off')\n",
    "    axes[2, 0].text(0.5, 0.5, f'σ values:\\n' + '\\n'.join([f'σ{i+1}={S[i]:.2f}' for i in range(n_components)]),\n",
    "                   ha='center', va='center', fontsize=10, transform=axes[2, 0].transAxes)\n",
    "    axes[2, 0].axis('off')\n",
    "    \n",
    "    for i in range(n_components):\n",
    "        # Rank-1 component\n",
    "        component = S[i] * np.outer(U[:, i], Vt[i, :])\n",
    "        cumulative += component\n",
    "        \n",
    "        # Show component\n",
    "        vmax = max(np.abs(component).max(), 0.1)\n",
    "        axes[0, i+1].imshow(component, cmap='RdBu', vmin=-vmax, vmax=vmax)\n",
    "        axes[0, i+1].set_title(f'σ{i+1}·u{i+1}·v{i+1}ᵀ\\nσ={S[i]:.2f}')\n",
    "        axes[0, i+1].axis('off')\n",
    "        \n",
    "        # Show cumulative reconstruction\n",
    "        axes[1, i+1].imshow(cumulative, cmap='RdBu')\n",
    "        error = np.linalg.norm(image - cumulative, 'fro') / np.linalg.norm(image, 'fro')\n",
    "        axes[1, i+1].set_title(f'Sum of 1..{i+1}\\nError: {error:.2%}')\n",
    "        axes[1, i+1].axis('off')\n",
    "        \n",
    "        # Show u and v vectors\n",
    "        axes[2, i+1].plot(U[:, i], 'b-', label=f'u{i+1}')\n",
    "        axes[2, i+1].plot(Vt[i, :], 'r-', label=f'v{i+1}')\n",
    "        axes[2, i+1].legend(fontsize=8)\n",
    "        axes[2, i+1].set_title(f'Singular vectors')\n",
    "    \n",
    "    plt.suptitle('SVD as Sum of Rank-1 Matrices: A = Σ σᵢ uᵢ vᵢᵀ', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_svd_components()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. PCA via SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_via_svd_demo():\n",
    "    \"\"\"\n",
    "    Show the connection between PCA and SVD.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate 2D data with correlation\n",
    "    n_samples = 200\n",
    "    mean = [0, 0]\n",
    "    cov = [[3, 2], [2, 2]]\n",
    "    X = np.random.multivariate_normal(mean, cov, n_samples)\n",
    "    \n",
    "    # Center the data\n",
    "    X_centered = X - X.mean(axis=0)\n",
    "    \n",
    "    # Method 1: SVD of data matrix\n",
    "    U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)\n",
    "    \n",
    "    # Method 2: Eigendecomposition of covariance matrix\n",
    "    cov_matrix = X_centered.T @ X_centered / (n_samples - 1)\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "    idx = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[idx]\n",
    "    eigenvectors = eigenvectors[:, idx]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Original data with PCs from SVD\n",
    "    ax1 = axes[0]\n",
    "    ax1.scatter(X_centered[:, 0], X_centered[:, 1], alpha=0.5, s=20)\n",
    "    \n",
    "    # PCs from SVD (rows of Vt)\n",
    "    for i in range(2):\n",
    "        pc = Vt[i] * S[i] / np.sqrt(n_samples - 1) * 2  # Scale for visualization\n",
    "        ax1.arrow(0, 0, pc[0], pc[1], head_width=0.1, head_length=0.05,\n",
    "                 fc=f'C{i}', ec=f'C{i}', linewidth=2, label=f'PC{i+1} (SVD)')\n",
    "    \n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('y')\n",
    "    ax1.set_title('Data with Principal Components from SVD')\n",
    "    ax1.legend()\n",
    "    ax1.set_aspect('equal')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Compare SVD and eigendecomposition\n",
    "    ax2 = axes[1]\n",
    "    ax2.text(0.5, 0.9, 'SVD of X:', ha='center', fontsize=12, fontweight='bold',\n",
    "            transform=ax2.transAxes)\n",
    "    ax2.text(0.5, 0.75, f'Singular values: {S}', ha='center', fontsize=10,\n",
    "            transform=ax2.transAxes)\n",
    "    ax2.text(0.5, 0.6, f'σ²/(n-1) = {S**2 / (n_samples-1)}', ha='center', fontsize=10,\n",
    "            transform=ax2.transAxes)\n",
    "    ax2.text(0.5, 0.45, 'Eigendecomp of XᵀX/(n-1):', ha='center', fontsize=12, fontweight='bold',\n",
    "            transform=ax2.transAxes)\n",
    "    ax2.text(0.5, 0.3, f'Eigenvalues: {eigenvalues}', ha='center', fontsize=10,\n",
    "            transform=ax2.transAxes)\n",
    "    ax2.text(0.5, 0.1, 'They match! σ² = (n-1) × eigenvalue', ha='center', fontsize=11,\n",
    "            transform=ax2.transAxes, color='green', fontweight='bold')\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title('SVD vs Eigendecomposition')\n",
    "    \n",
    "    # Projected data\n",
    "    ax3 = axes[2]\n",
    "    projected = X_centered @ Vt.T  # = U @ diag(S)\n",
    "    ax3.scatter(projected[:, 0], projected[:, 1], alpha=0.5, s=20)\n",
    "    ax3.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "    ax3.axvline(x=0, color='r', linestyle='--', alpha=0.5)\n",
    "    ax3.set_xlabel('PC1 score')\n",
    "    ax3.set_ylabel('PC2 score')\n",
    "    ax3.set_title('Data in PC Space\\n(Decorrelated)')\n",
    "    ax3.set_aspect('equal')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "pca_via_svd_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pseudoinverse and Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudoinverse_demo():\n",
    "    \"\"\"\n",
    "    Show how SVD provides the pseudoinverse for least squares.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Overdetermined system: Ax = b (more equations than unknowns)\n",
    "    A = np.array([[1, 1], [1, 2], [1, 3], [1, 4]])\n",
    "    b = np.array([2.1, 3.9, 6.2, 7.8])  # Noisy linear relationship\n",
    "    \n",
    "    # SVD of A\n",
    "    U, S, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "    \n",
    "    # Pseudoinverse\n",
    "    S_inv = np.diag(1/S)\n",
    "    A_pinv = Vt.T @ S_inv @ U.T\n",
    "    \n",
    "    # Least squares solution\n",
    "    x_svd = A_pinv @ b\n",
    "    x_numpy = np.linalg.lstsq(A, b, rcond=None)[0]\n",
    "    \n",
    "    # Residual\n",
    "    residual = b - A @ x_svd\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Data and fit\n",
    "    ax1 = axes[0]\n",
    "    x_data = A[:, 1]  # Second column is the x values\n",
    "    ax1.scatter(x_data, b, s=100, label='Data points')\n",
    "    x_line = np.linspace(0, 5, 100)\n",
    "    y_line = x_svd[0] + x_svd[1] * x_line\n",
    "    ax1.plot(x_line, y_line, 'r-', linewidth=2, label=f'Fit: y = {x_svd[0]:.2f} + {x_svd[1]:.2f}x')\n",
    "    \n",
    "    # Show residuals\n",
    "    for xi, bi, ri in zip(x_data, b, residual):\n",
    "        yi_fit = x_svd[0] + x_svd[1] * xi\n",
    "        ax1.plot([xi, xi], [bi, yi_fit], 'g--', alpha=0.7)\n",
    "    \n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('y')\n",
    "    ax1.set_title('Least Squares Fit via SVD')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # SVD components\n",
    "    ax2 = axes[1]\n",
    "    ax2.text(0.5, 0.9, 'SVD Pseudoinverse:', ha='center', fontsize=12, fontweight='bold',\n",
    "            transform=ax2.transAxes)\n",
    "    ax2.text(0.5, 0.75, f'A = U Σ Vᵀ', ha='center', fontsize=11, transform=ax2.transAxes)\n",
    "    ax2.text(0.5, 0.6, f'A⁺ = V Σ⁻¹ Uᵀ', ha='center', fontsize=11, transform=ax2.transAxes)\n",
    "    ax2.text(0.5, 0.45, f'x = A⁺b minimizes ||Ax - b||²', ha='center', fontsize=11, transform=ax2.transAxes)\n",
    "    ax2.text(0.5, 0.25, f'Singular values: σ = {S}', ha='center', fontsize=10, transform=ax2.transAxes)\n",
    "    ax2.text(0.5, 0.1, f'Solution: x = {x_svd}', ha='center', fontsize=10, transform=ax2.transAxes)\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title('SVD Method')\n",
    "    \n",
    "    # Verify orthogonality of residual\n",
    "    ax3 = axes[2]\n",
    "    residual_dot_A = A.T @ residual  # Should be zero if residual ⊥ column space\n",
    "    ax3.bar(['Aᵀ @ residual [0]', 'Aᵀ @ residual [1]'], residual_dot_A)\n",
    "    ax3.axhline(y=0, color='r', linestyle='--')\n",
    "    ax3.set_ylabel('Value')\n",
    "    ax3.set_title('Residual is ⊥ to Column Space\\n(Aᵀr ≈ 0)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"SVD solution: {x_svd}\")\n",
    "    print(f\"NumPy lstsq: {x_numpy}\")\n",
    "    print(f\"Residual norm: {np.linalg.norm(residual):.4f}\")\n",
    "\n",
    "pseudoinverse_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Matrix Completion (Recommender Systems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommender_demo():\n",
    "    \"\"\"\n",
    "    Simple demonstration of matrix completion for recommendations.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create a simple user-item rating matrix with low-rank structure\n",
    "    n_users, n_items = 20, 15\n",
    "    true_rank = 3\n",
    "    \n",
    "    # User and item factors\n",
    "    user_factors = np.random.rand(n_users, true_rank)\n",
    "    item_factors = np.random.rand(n_items, true_rank)\n",
    "    \n",
    "    # True rating matrix (low-rank)\n",
    "    R_true = user_factors @ item_factors.T\n",
    "    R_true = (R_true - R_true.min()) / (R_true.max() - R_true.min()) * 4 + 1  # Scale to 1-5\n",
    "    \n",
    "    # Observe only 40% of entries\n",
    "    mask = np.random.rand(n_users, n_items) < 0.4\n",
    "    R_observed = np.where(mask, R_true, np.nan)\n",
    "    \n",
    "    # Matrix completion via SVD\n",
    "    R_filled = np.where(mask, R_observed, 0)\n",
    "    \n",
    "    # Iterative SVD completion\n",
    "    for _ in range(10):\n",
    "        U, S, Vt = np.linalg.svd(R_filled, full_matrices=False)\n",
    "        R_approx = U[:, :true_rank] @ np.diag(S[:true_rank]) @ Vt[:true_rank, :]\n",
    "        R_filled = np.where(mask, R_observed, R_approx)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    \n",
    "    axes[0].imshow(R_true, cmap='YlOrRd', vmin=1, vmax=5)\n",
    "    axes[0].set_title('True Ratings\\n(Unknown)')\n",
    "    axes[0].set_xlabel('Items')\n",
    "    axes[0].set_ylabel('Users')\n",
    "    \n",
    "    R_display = np.where(mask, R_observed, np.nan)\n",
    "    axes[1].imshow(R_display, cmap='YlOrRd', vmin=1, vmax=5)\n",
    "    axes[1].set_title(f'Observed Ratings\\n({mask.sum()} / {mask.size} = {mask.mean():.0%})')\n",
    "    axes[1].set_xlabel('Items')\n",
    "    \n",
    "    axes[2].imshow(R_approx, cmap='YlOrRd', vmin=1, vmax=5)\n",
    "    axes[2].set_title('SVD Completion\\n(Rank-3 approx)')\n",
    "    axes[2].set_xlabel('Items')\n",
    "    \n",
    "    error = np.sqrt(np.nanmean((R_approx[~mask] - R_true[~mask])**2))\n",
    "    axes[3].imshow(np.abs(R_true - R_approx), cmap='Reds', vmin=0, vmax=1)\n",
    "    axes[3].set_title(f'|Error|\\nRMSE: {error:.3f}')\n",
    "    axes[3].set_xlabel('Items')\n",
    "    \n",
    "    for ax in axes:\n",
    "        ax.set_aspect('auto')\n",
    "    \n",
    "    plt.suptitle('Matrix Completion for Recommendations using SVD', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "recommender_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Truncated SVD for Large Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_svd_comparison():\n",
    "    \"\"\"\n",
    "    Compare full SVD vs truncated/randomized SVD.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    from scipy.sparse.linalg import svds\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create large matrix\n",
    "    m, n = 1000, 800\n",
    "    A = np.random.randn(m, n)\n",
    "    \n",
    "    k = 20  # Number of components\n",
    "    \n",
    "    # Full SVD\n",
    "    start = time.time()\n",
    "    U_full, S_full, Vt_full = np.linalg.svd(A, full_matrices=False)\n",
    "    time_full = time.time() - start\n",
    "    \n",
    "    # Truncated SVD (scipy.sparse.linalg.svds)\n",
    "    start = time.time()\n",
    "    U_trunc, S_trunc, Vt_trunc = svds(A, k=k)\n",
    "    time_trunc = time.time() - start\n",
    "    \n",
    "    # Sort truncated results (svds returns in ascending order)\n",
    "    idx = np.argsort(S_trunc)[::-1]\n",
    "    S_trunc = S_trunc[idx]\n",
    "    U_trunc = U_trunc[:, idx]\n",
    "    Vt_trunc = Vt_trunc[idx, :]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Timing comparison\n",
    "    ax1 = axes[0]\n",
    "    ax1.bar(['Full SVD\\n(all singular values)', f'Truncated SVD\\n(top {k} only)'], \n",
    "           [time_full, time_trunc], color=['steelblue', 'coral'])\n",
    "    ax1.set_ylabel('Time (seconds)')\n",
    "    ax1.set_title(f'Computation Time\\n({m}×{n} matrix)')\n",
    "    for i, t in enumerate([time_full, time_trunc]):\n",
    "        ax1.text(i, t + 0.01, f'{t:.3f}s', ha='center', fontsize=12)\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    ax2 = axes[1]\n",
    "    ax2.plot(range(1, k+1), S_full[:k], 'b.-', markersize=10, label='Full SVD')\n",
    "    ax2.plot(range(1, k+1), S_trunc, 'rx', markersize=10, label='Truncated SVD')\n",
    "    ax2.set_xlabel('Component index')\n",
    "    ax2.set_ylabel('Singular value')\n",
    "    ax2.set_title(f'Top {k} Singular Values\\n(Should match!)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Full SVD time: {time_full:.3f}s\")\n",
    "    print(f\"Truncated SVD time: {time_trunc:.3f}s\")\n",
    "    print(f\"Speedup: {time_full / time_trunc:.1f}x\")\n",
    "    print(f\"Max singular value difference: {np.max(np.abs(S_full[:k] - S_trunc)):.2e}\")\n",
    "\n",
    "truncated_svd_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "KEY CONCEPTS SUMMARY\n",
    "====================\n",
    "\n",
    "1. SVD DEFINITION\n",
    "   - A = UΣVᵀ for ANY matrix (not just square!)\n",
    "   - U, V orthogonal; Σ diagonal with σ₁ ≥ σ₂ ≥ ... ≥ 0\n",
    "\n",
    "2. GEOMETRIC MEANING\n",
    "   - Any linear transform = rotate → scale → rotate\n",
    "   - σᵢ are the scaling factors along principal axes\n",
    "   - Vᵀ gives input directions, U gives output directions\n",
    "\n",
    "3. ECKART-YOUNG THEOREM\n",
    "   - Truncated SVD gives BEST low-rank approximation\n",
    "   - A_k = Σᵢ₌₁ᵏ σᵢ uᵢ vᵢᵀ minimizes ||A - A_k||\n",
    "\n",
    "4. CONNECTION TO EIGENDECOMPOSITION\n",
    "   - σᵢ² = eigenvalues of AᵀA (and AAᵀ)\n",
    "   - V = eigenvectors of AᵀA\n",
    "   - U = eigenvectors of AAᵀ\n",
    "\n",
    "5. ML APPLICATIONS\n",
    "   - PCA: X = UΣVᵀ, PCs are rows of Vᵀ\n",
    "   - Image compression: Keep top k components\n",
    "   - Recommender systems: Matrix completion\n",
    "   - Pseudoinverse: A⁺ = VΣ⁺Uᵀ for least squares\n",
    "   - Latent semantic analysis: Document-term matrices\n",
    "\n",
    "6. PRACTICAL TIPS\n",
    "   - Use truncated SVD for large matrices (scipy.sparse.linalg.svds)\n",
    "   - Randomized SVD for even larger matrices\n",
    "   - SVD is numerically more stable than forming AᵀA\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
