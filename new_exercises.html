            <!-- Exercises Section -->
            <section id="exercises" style="padding-top: 2rem; margin-top: 2rem; border-top: 1px solid #e5e7eb;">
                <h2 style="margin-bottom: 1.5rem;">30 Practice Exercises</h2>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>E1. Entropy of Uniform Distribution</h3>
                        <span class="difficulty-badge diff-easy">Easy</span>
                    </div>
                    <p>Calculate the entropy (in bits) of a uniform distribution over 16 outcomes.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>$H(X) = \log_2(N) = \log_2(16) = 4$ bits.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>E2. Surprise Calculation</h3>
                        <span class="difficulty-badge diff-easy">Easy</span>
                    </div>
                    <p>An event has a probability of $p=0.01$. What is the 'surprise' or information content (in bits) of observing this event?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>$I(x) = -\log_2(0.01) \approx 6.64$ bits.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>E3. Deterministic Entropy</h3>
                        <span class="difficulty-badge diff-easy">Easy</span>
                    </div>
                    <p>What is the entropy of a random variable that always takes the value 5?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>$H(X) = 0$. There is no uncertainty.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>E4. Bits vs Nats</h3>
                        <span class="difficulty-badge diff-easy">Easy</span>
                    </div>
                    <p>If an event has 1 bit of information, how many 'nats' of information does it have? (Hint: $\ln 2 \approx 0.693$)</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>1 bit = $\ln 2$ nats $\approx 0.693$ nats.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>E5. Independent Events</h3>
                        <span class="difficulty-badge diff-easy">Easy</span>
                    </div>
                    <p>You flip a fair coin 3 times. What is the total entropy of the sequence of outcomes?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>Since flips are independent, $H(X_1, X_2, X_3) = H(X_1) + H(X_2) + H(X_3) = 1 + 1 + 1 = 3$ bits.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>E6. Specific Distribution</h3>
                        <span class="difficulty-badge diff-easy">Easy</span>
                    </div>
                    <p>Calculate the entropy of $P = [0.5, 0.25, 0.25]$.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>
    $$H(P) = -(0.5 \log_2 0.5 + 0.25 \log_2 0.25 + 0.25 \log_2 0.25)$$
    $$= - (0.5(-1) + 0.25(-2) + 0.25(-2))$$
    $$= 0.5 + 0.5 + 0.5 = 1.5 \text{ bits}$$
    </p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>E7. Max Entropy Binary</h3>
                        <span class="difficulty-badge diff-easy">Easy</span>
                    </div>
                    <p>For a binary random variable with $P(X=1) = p$, for what value of $p$ is entropy maximized?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>$p=0.5$. The uniform distribution maximizes entropy.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>E8. Cross-Entropy Identity</h3>
                        <span class="difficulty-badge diff-easy">Easy</span>
                    </div>
                    <p>If the predicted distribution $Q$ is exactly equal to the true distribution $P$, what is the cross-entropy $H(P, Q)$ equal to?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>It is equal to the entropy $H(P)$. Since $D_{KL}(P||Q) = 0$.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>E9. KL Divergence Minimum</h3>
                        <span class="difficulty-badge diff-easy">Easy</span>
                    </div>
                    <p>What is the minimum possible value for KL Divergence $D_{KL}(P||Q)$?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>0. It is always non-negative (Gibbs' inequality).</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>E10. Negative Entropy?</h3>
                        <span class="difficulty-badge diff-easy">Easy</span>
                    </div>
                    <p>True or False: The entropy of a discrete random variable can be negative.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>False. Since $0 \le p \le 1$, $\log p \le 0$, so $-\sum p \log p \ge 0$. (Note: Differential entropy for continuous variables <em>can</em> be negative).</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>M1. Joint Entropy</h3>
                        <span class="difficulty-badge diff-medium">Medium</span>
                    </div>
                    <p>Let $X$ be the outcome of a fair coin flip (H/T). Let $Y=X$ (copy of X). What is the joint entropy $H(X, Y)$?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>Since $X$ and $Y$ are perfectly correlated (dependent):</p>
    <p>$H(X, Y) = H(X) + H(Y|X) = H(X) + 0 = 1$ bit.</p>
    <p>Alternatively, there are only 2 possible joint outcomes: (H,H) and (T,T), each with prob 0.5.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>M2. Conditional Entropy</h3>
                        <span class="difficulty-badge diff-medium">Medium</span>
                    </div>
                    <p>If $Y = f(X)$ is a deterministic function of $X$, what is $H(Y|X)$?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>$H(Y|X) = 0$. If you know $X$, you know $Y$ exactly, so there is no remaining uncertainty.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>M3. Huffman Lower Bound</h3>
                        <span class="difficulty-badge diff-medium">Medium</span>
                    </div>
                    <p>Can the average length of a Huffman code be strictly less than the entropy $H(X)$?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>No. Shannon's Source Coding Theorem states $L \ge H(X)$. It can only be equal if all probabilities are powers of 2.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>M4. KL Asymmetry</h3>
                        <span class="difficulty-badge diff-medium">Medium</span>
                    </div>
                    <p>Let $P=[1, 0]$ and $Q=[0.5, 0.5]$. Calculate $D_{KL}(P||Q)$ and $D_{KL}(Q||P)$. What does this show?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>
    $D_{KL}(P||Q) = 1 \log_2(1/0.5) + 0 = 1$ bit.<br>
    $D_{KL}(Q||P) = 0.5 \log_2(0.5/1) + 0.5 \log_2(0.5/0) = \infty$ (undefined/infinite).<br>
    This shows KL Divergence is <strong>not symmetric</strong>.
    </p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>M5. Cross-Entropy vs Entropy</h3>
                        <span class="difficulty-badge diff-medium">Medium</span>
                    </div>
                    <p>Prove that Cross-Entropy $H(P, Q)$ is always greater than or equal to Entropy $H(P)$.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>$H(P, Q) = H(P) + D_{KL}(P||Q)$. Since $D_{KL} \ge 0$, it follows that $H(P, Q) \ge H(P)$.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>M6. Entropy of Sum</h3>
                        <span class="difficulty-badge diff-medium">Medium</span>
                    </div>
                    <p>Let $X_1, X_2$ be independent fair binary variables (0 or 1). Let $Y = X_1 + X_2$. What is $H(Y)$?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>Possible values for Y: 0, 1, 2.<br>
    P(0) = 0.25 (0+0)<br>
    P(1) = 0.50 (0+1 or 1+0)<br>
    P(2) = 0.25 (1+1)<br>
    $H(Y) = -(0.25 \log 0.25 + 0.5 \log 0.5 + 0.25 \log 0.25) = -(-0.5 - 0.5 - 0.5) = 1.5$ bits.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>M7. Mutual Information</h3>
                        <span class="difficulty-badge diff-medium">Medium</span>
                    </div>
                    <p>Define Mutual Information $I(X; Y)$ in terms of Entropy $H(X)$ and Conditional Entropy $H(X|Y)$. Explain intuitively.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>$I(X; Y) = H(X) - H(X|Y)$. It represents the reduction in uncertainty about $X$ gained by observing $Y$.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>M8. Perplexity</h3>
                        <span class="difficulty-badge diff-medium">Medium</span>
                    </div>
                    <p>In NLP, models are evaluated on 'Perplexity', defined as $2^{H(P)}$. If a model has a perplexity of 8, what does this intuitively mean?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>It means the model is as confused as if it were choosing uniformly at random from 8 equally likely words.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>M9. Chain Rule</h3>
                        <span class="difficulty-badge diff-medium">Medium</span>
                    </div>
                    <p>Write the Chain Rule for Entropy for three variables $H(X, Y, Z)$.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>$H(X, Y, Z) = H(X) + H(Y|X) + H(Z|X, Y)$. Uncertainty sums up: uncertainty of X, plus uncertainty of Y given X, etc.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>M10. Concavity</h3>
                        <span class="difficulty-badge diff-medium">Medium</span>
                    </div>
                    <p>Is the entropy function $H(p)$ concave or convex? Why is this important for optimization?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>It is <strong>concave</strong>. This guarantees that a local maximum (uniform distribution) is the global maximum.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>H1. Geometric Distribution</h3>
                        <span class="difficulty-badge diff-hard">Hard</span>
                    </div>
                    <p>Derive the entropy of a geometric distribution $P(k) = (1-p)^{k-1}p$ for $k=1, 2, \dots$.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>
    $H(X) = -\sum p(k) \log p(k) = -\sum (1-p)^{k-1}p [\log p + (k-1)\log(1-p)]$
    After simplifications using expected value $E[X] = 1/p$:
    $$H(X) = \frac{-(1-p)\log_2(1-p) - p\log_2 p}{p} \text{ bits}$$
    </p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>H2. Independence Bound</h3>
                        <span class="difficulty-badge diff-hard">Hard</span>
                    </div>
                    <p>Prove that $H(X, Y) \le H(X) + H(Y)$. When does equality hold?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>This follows from $I(X; Y) \ge 0$. Since $I(X; Y) = H(X) + H(Y) - H(X, Y)$, non-negativity implies $H(X, Y) \le H(X) + H(Y)$. Equality holds iff X and Y are independent.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>H3. Differential Entropy Uniform</h3>
                        <span class="difficulty-badge diff-hard">Hard</span>
                    </div>
                    <p>Calculate the differential entropy of a Continuous Uniform distribution on $[0, a]$.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>
    PDF $p(x) = 1/a$ for $0 \le x \le a$.
    $$h(X) = -\int_0^a \frac{1}{a} \log \frac{1}{a} dx = - \log \frac{1}{a} \int_0^a \frac{1}{a} dx = \log a$$
    Note: If $a < 1$, entropy is negative!
    </p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>H4. Exponential Entropy</h3>
                        <span class="difficulty-badge diff-hard">Hard</span>
                    </div>
                    <p>Calculate the differential entropy of an Exponential distribution $\lambda e^{-\lambda x}$.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>
    $h(X) = 1 - \ln \lambda$ (nats).
    Derivation involves $\ln p(x) = \ln \lambda - \lambda x$ and taking expectation.
    </p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>H5. Gaussian KL</h3>
                        <span class="difficulty-badge diff-hard">Hard</span>
                    </div>
                    <p>Write the formula for KL Divergence between two univariate Gaussians $P \sim \mathcal{N}(\mu_1, \sigma_1^2)$ and $Q \sim \mathcal{N}(\mu_2, \sigma_2^2)$.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>
    $$D_{KL}(P||Q) = \ln\frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2 + (\mu_1 - \mu_2)^2}{2\sigma_2^2} - \frac{1}{2}$$
    This is crucial for Variational Autoencoders (VAEs).
    </p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>H6. Max Entropy Variance</h3>
                        <span class="difficulty-badge diff-hard">Hard</span>
                    </div>
                    <p>Why does the Gaussian distribution have the maximum entropy among all distributions with a fixed mean and variance?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>Using Lagrange multipliers with constraints $\int p(x)dx=1$, $\int xp(x)dx=\mu$, $\int (x-\mu)^2 p(x)dx=\sigma^2$ yields the form of the Gaussian PDF.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>H7. MLE vs KL</h3>
                        <span class="difficulty-badge diff-hard">Hard</span>
                    </div>
                    <p>Formal proof: Show that maximizing Likelihood is equivalent to minimizing KL divergence from the empirical distribution to the model.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>
    Let $P_{data}$ be the empirical distribution. Max $\sum \log Q(x_i)$ is equivalent to Max $E_{P_{data}}[\log Q(x)]$.
    Min $D_{KL}(P_{data}||Q) = \sum P_{data} \log P_{data} - \sum P_{data} \log Q$.
    The first term is constant wrt Q. So Min KL $\iff$ Max $\sum P_{data} \log Q$.
    </p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>H8. Entropy Rate</h3>
                        <span class="difficulty-badge diff-hard">Hard</span>
                    </div>
                    <p>For a stochastic process (like a Markov chain), what is the Entropy Rate?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>$\lim_{n \to \infty} \frac{1}{n} H(X_1, \dots, X_n)$. It measures the average new information per step in the long run.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>H9. Data Processing Inequality</h3>
                        <span class="difficulty-badge diff-hard">Hard</span>
                    </div>
                    <p>State the Data Processing Inequality for a Markov chain $X \to Y \to Z$.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>$I(X; Z) \le I(X; Y)$. Processing data (Y to Z) cannot create new information about the source X.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>H10. Fano's Inequality</h3>
                        <span class="difficulty-badge diff-hard">Hard</span>
                    </div>
                    <p>What does Fano's Inequality relate?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>It relates the probability of error $P_e$ in estimating $X$ from $Y$ to the conditional entropy $H(X|Y)$. It sets a lower bound on error probability based on uncertainty.</p>
                        </div>
                    </details>
                </div>
            </section>
