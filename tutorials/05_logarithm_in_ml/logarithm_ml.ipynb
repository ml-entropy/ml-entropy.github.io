{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 05: Why Logarithm is Fundamental in ML\n",
    "\n",
    "This notebook demonstrates **experimentally** why logarithm is not just a computational convenience, but mathematically fundamental to how neural networks learn.\n",
    "\n",
    "**Central Question**: If we had perfect computers with infinite precision, would we still need log?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from decimal import Decimal, getcontext\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set high precision for Decimal experiments\n",
    "getcontext().prec = 100\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Uniqueness of Logarithm\n",
    "\n",
    "**Theorem**: Log is the ONLY continuous function satisfying $f(xy) = f(x) + f(y)$\n",
    "\n",
    "Let's verify this numerically and see why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test various functions for the multiplicative-to-additive property\n",
    "def test_additivity(f, name, x_vals):\n",
    "    \"\"\"Test if f(xy) = f(x) + f(y)\"\"\"\n",
    "    errors = []\n",
    "    for x in x_vals:\n",
    "        for y in x_vals:\n",
    "            if x > 0 and y > 0:  # Avoid domain issues\n",
    "                lhs = f(x * y)\n",
    "                rhs = f(x) + f(y)\n",
    "                error = abs(lhs - rhs)\n",
    "                errors.append(error)\n",
    "    return np.mean(errors)\n",
    "\n",
    "x_vals = np.linspace(0.1, 5, 20)\n",
    "\n",
    "functions = {\n",
    "    'log(x)': np.log,\n",
    "    'sqrt(x)': np.sqrt,\n",
    "    'x^2': lambda x: x**2,\n",
    "    '1/x': lambda x: 1/x,\n",
    "    'x': lambda x: x,\n",
    "    'exp(x)': np.exp,\n",
    "}\n",
    "\n",
    "print(\"Testing f(xy) = f(x) + f(y):\")\n",
    "print(\"-\" * 40)\n",
    "for name, f in functions.items():\n",
    "    error = test_additivity(f, name, x_vals)\n",
    "    status = \"✓ SATISFIES\" if error < 1e-10 else \"✗ Fails\"\n",
    "    print(f\"{name:12} | Mean error: {error:.2e} | {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Only logarithm** converts multiplication to addition. This is the key property we need for probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Gradient Scaling Problem\n",
    "\n",
    "### The Core Issue: Gradients Scale with Likelihood\n",
    "\n",
    "Let's see what happens to gradients when we DON'T use log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients_comparison(n_samples, prob_per_sample):\n",
    "    \"\"\"\n",
    "    Compare gradients for direct likelihood vs log-likelihood.\n",
    "    \n",
    "    Simplified model: L = prod(p_i) where p_i = prob_per_sample\n",
    "    We compute dL/dp for direct and d(log L)/dp for log version.\n",
    "    \"\"\"\n",
    "    # Direct likelihood\n",
    "    L = prob_per_sample ** n_samples\n",
    "    \n",
    "    # Gradient of direct likelihood: dL/dp = n * p^(n-1)\n",
    "    grad_direct = n_samples * (prob_per_sample ** (n_samples - 1))\n",
    "    \n",
    "    # Log-likelihood: log L = n * log(p)\n",
    "    log_L = n_samples * np.log(prob_per_sample)\n",
    "    \n",
    "    # Gradient of log-likelihood: d(log L)/dp = n/p\n",
    "    grad_log = n_samples / prob_per_sample\n",
    "    \n",
    "    return {\n",
    "        'L': L,\n",
    "        'log_L': log_L,\n",
    "        'grad_direct': grad_direct,\n",
    "        'grad_log': grad_log,\n",
    "        'grad_ratio': grad_direct / grad_log if grad_log != 0 else np.inf\n",
    "    }\n",
    "\n",
    "# Test with different number of samples\n",
    "print(\"Gradient comparison (probability per sample = 0.9):\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'n_samples':>10} | {'L':>15} | {'grad_direct':>15} | {'grad_log':>12} | {'ratio':>12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for n in [10, 100, 1000, 10000]:\n",
    "    result = compute_gradients_comparison(n, 0.9)\n",
    "    print(f\"{n:>10} | {result['L']:>15.2e} | {result['grad_direct']:>15.2e} | {result['grad_log']:>12.2f} | {result['grad_ratio']:>12.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observation\n",
    "\n",
    "- **Direct gradient** shrinks exponentially with more samples\n",
    "- **Log gradient** stays O(n) — perfectly usable!\n",
    "- This happens even with very confident predictions (p=0.9)\n",
    "\n",
    "This is NOT a numerical precision issue — it's a **mathematical scaling problem**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the gradient scaling\n",
    "n_samples_range = np.arange(1, 501)\n",
    "prob = 0.9\n",
    "\n",
    "grad_direct = [n * (prob ** (n-1)) for n in n_samples_range]\n",
    "grad_log = [n / prob for n in n_samples_range]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Linear scale (log gradient)\n",
    "axes[0].plot(n_samples_range, grad_log, 'b-', linewidth=2, label='∇ log L')\n",
    "axes[0].set_xlabel('Number of samples', fontsize=12)\n",
    "axes[0].set_ylabel('Gradient magnitude', fontsize=12)\n",
    "axes[0].set_title('Log-Likelihood Gradient (Linear Scale)', fontsize=14)\n",
    "axes[0].legend(fontsize=12)\n",
    "\n",
    "# Log scale comparison\n",
    "axes[1].semilogy(n_samples_range, grad_direct, 'r-', linewidth=2, label='∇ L (direct)')\n",
    "axes[1].semilogy(n_samples_range, grad_log, 'b-', linewidth=2, label='∇ log L')\n",
    "axes[1].set_xlabel('Number of samples', fontsize=12)\n",
    "axes[1].set_ylabel('Gradient magnitude (log scale)', fontsize=12)\n",
    "axes[1].set_title('Gradient Comparison (Log Scale)', fontsize=14)\n",
    "axes[1].legend(fontsize=12)\n",
    "axes[1].axhline(y=1e-300, color='gray', linestyle='--', alpha=0.5, label='Float64 limit')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAt n=500 samples with p=0.9:\")\n",
    "print(f\"  Direct gradient: {500 * (0.9 ** 499):.2e}\")\n",
    "print(f\"  Log gradient: {500 / 0.9:.2f}\")\n",
    "print(f\"  Ratio: {(500 * (0.9 ** 499)) / (500 / 0.9):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: High-Precision Experiment\n",
    "\n",
    "Let's use Python's `Decimal` for arbitrary precision to show the problem persists even with perfect arithmetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 100 digits of precision\n",
    "getcontext().prec = 100\n",
    "\n",
    "def high_precision_gradient_comparison(n_samples, prob):\n",
    "    \"\"\"Compute gradients with arbitrary precision.\"\"\"\n",
    "    p = Decimal(str(prob))\n",
    "    n = Decimal(str(n_samples))\n",
    "    \n",
    "    # Direct likelihood: L = p^n\n",
    "    L = p ** int(n_samples)\n",
    "    \n",
    "    # Direct gradient: dL/dp = n * p^(n-1)\n",
    "    grad_direct = n * (p ** (int(n_samples) - 1))\n",
    "    \n",
    "    # Log gradient: d(log L)/dp = n/p\n",
    "    grad_log = n / p\n",
    "    \n",
    "    return L, grad_direct, grad_log\n",
    "\n",
    "print(\"High-Precision Gradient Comparison (100 decimal digits):\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "for n in [100, 1000, 10000]:\n",
    "    L, grad_d, grad_l = high_precision_gradient_comparison(n, 0.9)\n",
    "    \n",
    "    # Convert to float for display (may lose precision for very small numbers)\n",
    "    print(f\"\\nn = {n}:\")\n",
    "    print(f\"  L = {float(L):.6e}\")\n",
    "    print(f\"  ∇L (direct) = {float(grad_d):.6e}\")\n",
    "    print(f\"  ∇log(L) = {float(grad_l):.2f}\")\n",
    "    \n",
    "    # The key insight: gradient ratio\n",
    "    if grad_l != 0:\n",
    "        ratio = grad_d / grad_l\n",
    "        print(f\"  Ratio ∇L/∇log(L) = {float(ratio):.6e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem ISN'T Precision\n",
    "\n",
    "Even with 100 decimal digits:\n",
    "- Direct gradients still vanish exponentially\n",
    "- The RELATIVE magnitude between good and bad models is astronomically different\n",
    "\n",
    "**This breaks learning**, not because we can't represent the numbers, but because the gradient landscape is pathological."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Actual Training Comparison\n",
    "\n",
    "Let's train a simple model using:\n",
    "1. Cross-entropy loss (uses log)\n",
    "2. Direct probability product (no log)\n",
    "\n",
    "We'll use PyTorch's autograd for exact gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple binary classification problem\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Generate linearly separable data\n",
    "n_samples = 100\n",
    "X = torch.randn(n_samples, 2)\n",
    "y = (X[:, 0] + X[:, 1] > 0).float()\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], c='blue', label='Class 0', alpha=0.6)\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], c='red', label='Class 1', alpha=0.6)\n",
    "plt.xlabel('$x_1$', fontsize=12)\n",
    "plt.ylabel('$x_2$', fontsize=12)\n",
    "plt.title('Binary Classification Dataset', fontsize=14)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 1)\n",
    "        # Initialize with small weights\n",
    "        nn.init.normal_(self.linear.weight, std=0.01)\n",
    "        nn.init.zeros_(self.linear.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x)).squeeze()\n",
    "\n",
    "def train_with_log_loss(X, y, epochs=100, lr=0.1):\n",
    "    \"\"\"Train using cross-entropy (log-based) loss.\"\"\"\n",
    "    model = SimpleLogisticRegression()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    losses = []\n",
    "    grad_norms = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        probs = model(X)\n",
    "        # Cross-entropy loss (uses log internally)\n",
    "        loss = -torch.mean(y * torch.log(probs + 1e-10) + (1-y) * torch.log(1 - probs + 1e-10))\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Record gradient norm\n",
    "        grad_norm = model.linear.weight.grad.norm().item()\n",
    "        grad_norms.append(grad_norm)\n",
    "        \n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    return model, losses, grad_norms\n",
    "\n",
    "def train_with_direct_likelihood(X, y, epochs=100, lr=1e10):\n",
    "    \"\"\"Train using direct likelihood (no log) - maximize product of probabilities.\"\"\"\n",
    "    model = SimpleLogisticRegression()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    losses = []  # We'll track negative log-likelihood for comparison\n",
    "    grad_norms = []\n",
    "    likelihoods = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        probs = model(X)\n",
    "        \n",
    "        # Direct likelihood: product of P(correct class)\n",
    "        correct_probs = y * probs + (1-y) * (1 - probs)\n",
    "        likelihood = torch.prod(correct_probs)  # This is what we'd maximize\n",
    "        \n",
    "        # Negative likelihood for minimization\n",
    "        neg_likelihood = -likelihood\n",
    "        neg_likelihood.backward()\n",
    "        \n",
    "        # Record gradient norm\n",
    "        grad_norm = model.linear.weight.grad.norm().item()\n",
    "        grad_norms.append(grad_norm)\n",
    "        likelihoods.append(likelihood.item())\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track equivalent cross-entropy for comparison\n",
    "        with torch.no_grad():\n",
    "            ce_loss = -torch.mean(y * torch.log(probs + 1e-10) + (1-y) * torch.log(1 - probs + 1e-10))\n",
    "            losses.append(ce_loss.item())\n",
    "    \n",
    "    return model, losses, grad_norms, likelihoods\n",
    "\n",
    "# Train both\n",
    "model_log, losses_log, grads_log = train_with_log_loss(X, y, epochs=200, lr=1.0)\n",
    "model_direct, losses_direct, grads_direct, likelihoods = train_with_direct_likelihood(X, y, epochs=200, lr=1e20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare training\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Loss comparison\n",
    "axes[0, 0].plot(losses_log, 'b-', linewidth=2, label='Log loss (cross-entropy)')\n",
    "axes[0, 0].plot(losses_direct, 'r-', linewidth=2, label='Direct likelihood', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "axes[0, 0].set_title('Training Loss Comparison', fontsize=14)\n",
    "axes[0, 0].legend(fontsize=11)\n",
    "axes[0, 0].set_ylim(0, max(losses_log[0], losses_direct[0]) * 1.1)\n",
    "\n",
    "# Gradient norms\n",
    "axes[0, 1].semilogy(grads_log, 'b-', linewidth=2, label='Log loss gradients')\n",
    "axes[0, 1].semilogy(grads_direct, 'r-', linewidth=2, label='Direct likelihood gradients', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Gradient Norm (log scale)', fontsize=12)\n",
    "axes[0, 1].set_title('Gradient Magnitude Comparison', fontsize=14)\n",
    "axes[0, 1].legend(fontsize=11)\n",
    "\n",
    "# Likelihood values\n",
    "axes[1, 0].semilogy(likelihoods, 'r-', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Likelihood (log scale)', fontsize=12)\n",
    "axes[1, 0].set_title('Direct Likelihood Values', fontsize=14)\n",
    "axes[1, 0].axhline(y=1e-300, color='gray', linestyle='--', label='Float64 underflow')\n",
    "axes[1, 0].legend(fontsize=11)\n",
    "\n",
    "# Final decision boundaries\n",
    "def plot_decision_boundary(model, ax, title):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        Z = model(torch.FloatTensor(np.c_[xx.ravel(), yy.ravel()]))\n",
    "        Z = Z.numpy().reshape(xx.shape)\n",
    "    \n",
    "    ax.contourf(xx, yy, Z, levels=20, cmap='RdBu', alpha=0.6)\n",
    "    ax.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "    ax.scatter(X[y==0, 0], X[y==0, 1], c='blue', edgecolors='white', s=50)\n",
    "    ax.scatter(X[y==1, 0], X[y==1, 1], c='red', edgecolors='white', s=50)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "\n",
    "plot_decision_boundary(model_log, axes[1, 1], 'Log Loss - Decision Boundary')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final accuracies\n",
    "with torch.no_grad():\n",
    "    acc_log = ((model_log(X) > 0.5) == y).float().mean().item()\n",
    "    acc_direct = ((model_direct(X) > 0.5) == y).float().mean().item()\n",
    "\n",
    "print(f\"\\nFinal Accuracies:\")\n",
    "print(f\"  Log loss model: {acc_log:.1%}\")\n",
    "print(f\"  Direct likelihood model: {acc_direct:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Notice:\n",
    "1. **Direct likelihood gradients** are astronomically small (often underflow to 0)\n",
    "2. **Log loss gradients** stay in a reasonable range throughout training\n",
    "3. Even with a huge learning rate (1e20!), direct likelihood barely learns\n",
    "4. The problem isn't precision — it's that gradients scale with likelihood magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: The Additivity Requirement\n",
    "\n",
    "Independent samples MUST contribute additively to learning. Let's demonstrate why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate that log makes contributions additive\n",
    "\n",
    "# Three independent samples with probabilities\n",
    "p1, p2, p3 = 0.8, 0.6, 0.9\n",
    "\n",
    "# Direct likelihood\n",
    "L = p1 * p2 * p3\n",
    "print(\"Direct Likelihood (multiplicative):\")\n",
    "print(f\"  L = p1 × p2 × p3 = {p1} × {p2} × {p3} = {L:.4f}\")\n",
    "print(f\"  Contribution of sample 1: p1 = {p1}\")\n",
    "print(f\"  Contribution of sample 2: p2 = {p2}\")\n",
    "print(f\"  How do they combine? Multiplication (not additive!)\\n\")\n",
    "\n",
    "# Log-likelihood\n",
    "log_L = np.log(p1) + np.log(p2) + np.log(p3)\n",
    "print(\"Log-Likelihood (additive):\")\n",
    "print(f\"  log L = log(p1) + log(p2) + log(p3)\")\n",
    "print(f\"        = {np.log(p1):.4f} + {np.log(p2):.4f} + {np.log(p3):.4f} = {log_L:.4f}\")\n",
    "print(f\"  Contribution of sample 1: log(p1) = {np.log(p1):.4f}\")\n",
    "print(f\"  Contribution of sample 2: log(p2) = {np.log(p2):.4f}\")\n",
    "print(f\"  How do they combine? Addition (additive!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: per-sample contributions\n",
    "np.random.seed(42)\n",
    "n_samples = 20\n",
    "probs = np.random.uniform(0.5, 0.95, n_samples)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Direct: cumulative product\n",
    "cumulative_L = np.cumprod(probs)\n",
    "axes[0].bar(range(n_samples), probs, alpha=0.5, label='Individual p_i')\n",
    "axes[0].plot(range(n_samples), cumulative_L, 'r-o', linewidth=2, markersize=6, label='Cumulative L')\n",
    "axes[0].set_xlabel('Sample index', fontsize=12)\n",
    "axes[0].set_ylabel('Value', fontsize=12)\n",
    "axes[0].set_title('Direct Likelihood: Products → Vanishing', fontsize=14)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# Log: cumulative sum\n",
    "log_probs = np.log(probs)\n",
    "cumulative_log_L = np.cumsum(log_probs)\n",
    "axes[1].bar(range(n_samples), log_probs, alpha=0.5, label='Individual log(p_i)')\n",
    "axes[1].plot(range(n_samples), cumulative_log_L, 'b-o', linewidth=2, markersize=6, label='Cumulative log L')\n",
    "axes[1].set_xlabel('Sample index', fontsize=12)\n",
    "axes[1].set_ylabel('Value', fontsize=12)\n",
    "axes[1].set_title('Log-Likelihood: Sums → Well-behaved', fontsize=14)\n",
    "axes[1].legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"After {n_samples} samples:\")\n",
    "print(f\"  Direct likelihood: {cumulative_L[-1]:.2e}\")\n",
    "print(f\"  Log-likelihood: {cumulative_log_L[-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Information Theory Perspective\n",
    "\n",
    "Shannon PROVED that information MUST be logarithmic. Let's see why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shannon's requirements for information measure I(p):\n",
    "# 1. I(p) ≥ 0 (non-negative)\n",
    "# 2. I(1) = 0 (certain events have no information)\n",
    "# 3. I(p) decreasing in p (rarer events have more information)\n",
    "# 4. I(p·q) = I(p) + I(q) for independent events (ADDITIVITY)\n",
    "\n",
    "# Let's test candidate functions\n",
    "def test_information_axioms(f, name, p_vals):\n",
    "    \"\"\"Test if f(p) satisfies Shannon's axioms.\"\"\"\n",
    "    results = {\n",
    "        'non_negative': True,\n",
    "        'I(1)=0': abs(f(1.0)) < 1e-10,\n",
    "        'decreasing': True,\n",
    "        'additive': True,\n",
    "    }\n",
    "    \n",
    "    for p in p_vals:\n",
    "        if p > 0:\n",
    "            if f(p) < -1e-10:\n",
    "                results['non_negative'] = False\n",
    "    \n",
    "    # Check decreasing\n",
    "    for i in range(len(p_vals) - 1):\n",
    "        if p_vals[i] < p_vals[i+1] and p_vals[i] > 0 and p_vals[i+1] > 0:\n",
    "            if f(p_vals[i]) < f(p_vals[i+1]):\n",
    "                results['decreasing'] = False\n",
    "    \n",
    "    # Check additivity: I(p·q) = I(p) + I(q)\n",
    "    for p in [0.2, 0.5, 0.8]:\n",
    "        for q in [0.3, 0.6, 0.9]:\n",
    "            if abs(f(p*q) - f(p) - f(q)) > 1e-10:\n",
    "                results['additive'] = False\n",
    "    \n",
    "    return results\n",
    "\n",
    "p_vals = np.linspace(0.01, 1.0, 100)\n",
    "\n",
    "# Test different functions\n",
    "candidates = {\n",
    "    '-log(p)': lambda p: -np.log(p),\n",
    "    '1-p': lambda p: 1 - p,\n",
    "    '1/p - 1': lambda p: 1/p - 1,\n",
    "    '-p·log(p)': lambda p: -p * np.log(p) if p > 0 else 0,\n",
    "    'sqrt(1-p)': lambda p: np.sqrt(1 - p),\n",
    "}\n",
    "\n",
    "print(\"Testing Shannon's Axioms for Information Measure:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Function':>15} | {'I≥0':>6} | {'I(1)=0':>7} | {'Decreasing':>10} | {'Additive':>10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, f in candidates.items():\n",
    "    results = test_information_axioms(f, name, p_vals)\n",
    "    print(f\"{name:>15} | {'✓' if results['non_negative'] else '✗':>6} | {'✓' if results['I(1)=0'] else '✗':>7} | {'✓' if results['decreasing'] else '✗':>10} | {'✓' if results['additive'] else '✗':>10}\")\n",
    "\n",
    "print(\"\\n→ Only -log(p) satisfies ALL axioms!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the candidates\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "p_vals = np.linspace(0.01, 1.0, 100)\n",
    "\n",
    "for name, f in candidates.items():\n",
    "    y_vals = [f(p) for p in p_vals]\n",
    "    axes[0].plot(p_vals, y_vals, linewidth=2, label=name)\n",
    "\n",
    "axes[0].set_xlabel('Probability p', fontsize=12)\n",
    "axes[0].set_ylabel('Information I(p)', fontsize=12)\n",
    "axes[0].set_title('Candidate Information Functions', fontsize=14)\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].set_ylim(-0.5, 5)\n",
    "\n",
    "# Additivity test visualization\n",
    "p_range = np.linspace(0.1, 0.9, 50)\n",
    "q = 0.5\n",
    "\n",
    "for name, f in [('-log(p)', lambda p: -np.log(p)), ('1-p', lambda p: 1-p)]:\n",
    "    # I(p·q) vs I(p) + I(q)\n",
    "    lhs = [f(p * q) for p in p_range]  # I(p·q)\n",
    "    rhs = [f(p) + f(q) for p in p_range]  # I(p) + I(q)\n",
    "    error = [abs(l - r) for l, r in zip(lhs, rhs)]\n",
    "    axes[1].plot(p_range, error, linewidth=2, label=f'{name}: |I(pq) - I(p) - I(q)|')\n",
    "\n",
    "axes[1].set_xlabel('Probability p (with q=0.5 fixed)', fontsize=12)\n",
    "axes[1].set_ylabel('Additivity Error', fontsize=12)\n",
    "axes[1].set_title('Additivity Test: I(p·q) = I(p) + I(q)?', fontsize=14)\n",
    "axes[1].legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: The Fisher Information Connection\n",
    "\n",
    "The logarithm has a special property: the **Fisher Information** only exists because of log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fisher Information: I(θ) = Var[∇_θ log p(x|θ)] = -E[∇²_θ log p(x|θ)]\n",
    "# This fundamental quantity in statistics ONLY works with log!\n",
    "\n",
    "# Example: Bernoulli distribution p(x|θ) = θ^x (1-θ)^(1-x)\n",
    "# log p = x log(θ) + (1-x) log(1-θ)\n",
    "# ∇_θ log p = x/θ - (1-x)/(1-θ)\n",
    "# Fisher Info = 1/(θ(1-θ))\n",
    "\n",
    "theta_vals = np.linspace(0.01, 0.99, 100)\n",
    "fisher_info = 1 / (theta_vals * (1 - theta_vals))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Fisher Information\n",
    "axes[0].plot(theta_vals, fisher_info, 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('θ (probability parameter)', fontsize=12)\n",
    "axes[0].set_ylabel('Fisher Information I(θ)', fontsize=12)\n",
    "axes[0].set_title('Fisher Information for Bernoulli', fontsize=14)\n",
    "axes[0].fill_between(theta_vals, fisher_info, alpha=0.3)\n",
    "\n",
    "# Score function variance = Fisher Information\n",
    "# Simulate: compute variance of ∇log p for samples from Bernoulli(θ)\n",
    "n_simulations = 10000\n",
    "theta_test = [0.2, 0.5, 0.8]\n",
    "colors = ['red', 'green', 'blue']\n",
    "\n",
    "for theta, color in zip(theta_test, colors):\n",
    "    # Generate samples\n",
    "    samples = np.random.binomial(1, theta, n_simulations)\n",
    "    \n",
    "    # Compute score (gradient of log-likelihood)\n",
    "    scores = samples / theta - (1 - samples) / (1 - theta)\n",
    "    \n",
    "    # Variance should equal Fisher Information\n",
    "    empirical_var = np.var(scores)\n",
    "    theoretical_fi = 1 / (theta * (1 - theta))\n",
    "    \n",
    "    axes[1].hist(scores, bins=50, alpha=0.5, density=True, color=color,\n",
    "                 label=f'θ={theta}: Var={empirical_var:.2f}, I(θ)={theoretical_fi:.2f}')\n",
    "\n",
    "axes[1].set_xlabel('Score: ∇_θ log p(x|θ)', fontsize=12)\n",
    "axes[1].set_ylabel('Density', fontsize=12)\n",
    "axes[1].set_title('Score Function Distribution\\n(Variance = Fisher Information)', fontsize=14)\n",
    "axes[1].legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: The Fisher Information defines the FUNDAMENTAL LIMIT\")\n",
    "print(\"on how well we can estimate θ. This only works because of log!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: The Softmax + Cross-Entropy Magic\n",
    "\n",
    "The log in cross-entropy exactly cancels the exp in softmax, giving beautiful linear gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax: p_i = exp(z_i) / Σ exp(z_j)\n",
    "# Cross-entropy: L = -Σ y_i log(p_i)\n",
    "# For one-hot y with class k: L = -log(p_k) = -z_k + log(Σ exp(z_j))\n",
    "\n",
    "# Gradient: ∂L/∂z_i = p_i - y_i\n",
    "# This is BEAUTIFUL: gradient = predicted - actual\n",
    "\n",
    "# Let's verify this\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z))  # Numerical stability\n",
    "    return exp_z / exp_z.sum()\n",
    "\n",
    "def cross_entropy_loss(z, y_true_idx):\n",
    "    p = softmax(z)\n",
    "    return -np.log(p[y_true_idx])\n",
    "\n",
    "def gradient_numerical(z, y_true_idx, eps=1e-5):\n",
    "    \"\"\"Compute gradient numerically.\"\"\"\n",
    "    grad = np.zeros_like(z)\n",
    "    for i in range(len(z)):\n",
    "        z_plus = z.copy()\n",
    "        z_plus[i] += eps\n",
    "        z_minus = z.copy()\n",
    "        z_minus[i] -= eps\n",
    "        grad[i] = (cross_entropy_loss(z_plus, y_true_idx) - cross_entropy_loss(z_minus, y_true_idx)) / (2 * eps)\n",
    "    return grad\n",
    "\n",
    "def gradient_analytical(z, y_true_idx):\n",
    "    \"\"\"Compute gradient analytically: p - y\"\"\"\n",
    "    p = softmax(z)\n",
    "    y = np.zeros_like(z)\n",
    "    y[y_true_idx] = 1\n",
    "    return p - y\n",
    "\n",
    "# Test\n",
    "z = np.array([2.0, 1.0, 0.5, -1.0])  # Logits\n",
    "y_true = 2  # True class\n",
    "\n",
    "grad_num = gradient_numerical(z, y_true)\n",
    "grad_ana = gradient_analytical(z, y_true)\n",
    "\n",
    "print(\"Softmax + Cross-Entropy Gradient:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Logits z: {z}\")\n",
    "print(f\"True class: {y_true}\")\n",
    "print(f\"Softmax probabilities: {softmax(z).round(4)}\")\n",
    "print(f\"\")\n",
    "print(f\"Numerical gradient:  {grad_num.round(6)}\")\n",
    "print(f\"Analytical (p - y):  {grad_ana.round(6)}\")\n",
    "print(f\"Match: {np.allclose(grad_num, grad_ana)}\")\n",
    "print(f\"\")\n",
    "print(\"The gradient is simply: (predicted probability) - (true label)\")\n",
    "print(\"This elegant formula exists ONLY because of log!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the gradient\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart\n",
    "x_pos = np.arange(len(z))\n",
    "width = 0.35\n",
    "\n",
    "p = softmax(z)\n",
    "y_onehot = np.zeros_like(z)\n",
    "y_onehot[y_true] = 1\n",
    "\n",
    "axes[0].bar(x_pos - width/2, p, width, label='Predicted p', color='steelblue', alpha=0.7)\n",
    "axes[0].bar(x_pos + width/2, y_onehot, width, label='True y', color='coral', alpha=0.7)\n",
    "axes[0].set_xlabel('Class', fontsize=12)\n",
    "axes[0].set_ylabel('Probability', fontsize=12)\n",
    "axes[0].set_title('Predicted vs True', fontsize=14)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].set_xticks(x_pos)\n",
    "\n",
    "# Gradient = p - y\n",
    "axes[1].bar(x_pos, grad_ana, color='green', alpha=0.7)\n",
    "axes[1].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[1].set_xlabel('Class', fontsize=12)\n",
    "axes[1].set_ylabel('Gradient (∂L/∂z)', fontsize=12)\n",
    "axes[1].set_title('Gradient = p - y\\n(Decrease logits for overconfident classes)', fontsize=14)\n",
    "axes[1].set_xticks(x_pos)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "for i in range(len(z)):\n",
    "    direction = \"↑ increase\" if grad_ana[i] < 0 else \"↓ decrease\"\n",
    "    print(f\"  Class {i}: gradient = {grad_ana[i]:.4f} → {direction} logit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### The Answer to Our Central Question\n",
    "\n",
    "> **\"Would MLE still work without logarithm if we had perfect computers?\"**\n",
    "\n",
    "**NO.** The logarithm is **mathematically fundamental**, not just a computational convenience.\n",
    "\n",
    "### Why Logarithm is Fundamental:\n",
    "\n",
    "| Reason | Why It Matters |\n",
    "|--------|----------------|\n",
    "| **Gradient scaling** | Without log, gradients scale with $L$ → bad models get no learning signal |\n",
    "| **Additivity** | Independent samples must contribute additively; only log converts × to + |\n",
    "| **Shannon's proof** | Information MUST be logarithmic (uniqueness theorem) |\n",
    "| **Fisher Information** | Fundamental statistics rely on log-likelihood |\n",
    "| **Softmax gradient** | Log cancels exp → beautiful p - y gradients |\n",
    "\n",
    "### The Deep Insight\n",
    "\n",
    "The logarithm is the **unique mathematical bridge** between:\n",
    "- The **multiplicative** world of probabilities\n",
    "- The **additive** world of learning and information\n",
    "\n",
    "Without this bridge, neural networks as we know them would not work — regardless of computational precision."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
