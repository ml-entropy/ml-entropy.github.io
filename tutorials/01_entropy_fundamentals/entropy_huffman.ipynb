{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé≤ Entropy Fundamentals & Huffman Coding\n",
    "\n",
    "This notebook provides hands-on exploration of:\n",
    "1. Information content and surprise\n",
    "2. Entropy computation and visualization\n",
    "3. Huffman coding implementation\n",
    "4. Cross-entropy and its relationship to ML loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import heapq\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Information Content: Measuring Surprise\n",
    "\n",
    "$$I(x) = -\\log_2 P(x) = \\log_2 \\frac{1}{P(x)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_content(p: float) -> float:\n",
    "    \"\"\"Calculate information content in bits.\"\"\"\n",
    "    if p <= 0:\n",
    "        return float('inf')\n",
    "    if p >= 1:\n",
    "        return 0\n",
    "    return -np.log2(p)\n",
    "\n",
    "# Visualize information content vs probability\n",
    "probabilities = np.linspace(0.001, 1, 1000)\n",
    "information = -np.log2(probabilities)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(probabilities, information, 'b-', linewidth=2)\n",
    "ax.set_xlabel('Probability P(x)')\n",
    "ax.set_ylabel('Information I(x) = -log‚ÇÇ(P(x)) [bits]')\n",
    "ax.set_title('Information Content: Rare Events are Surprising')\n",
    "\n",
    "# Add annotations for specific points\n",
    "examples = [(0.5, 'Fair coin flip'), (0.1, 'Rare event'), (0.01, 'Very rare')]\n",
    "for p, label in examples:\n",
    "    info = information_content(p)\n",
    "    ax.annotate(f'{label}\\nP={p}, I={info:.2f} bits', \n",
    "                xy=(p, info), xytext=(p+0.15, info+0.5),\n",
    "                arrowprops=dict(arrowstyle='->', color='red'),\n",
    "                fontsize=10)\n",
    "    ax.plot(p, info, 'ro', markersize=8)\n",
    "\n",
    "ax.set_xlim(0, 1.1)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Entropy: Expected Surprise\n",
    "\n",
    "$$H(X) = -\\sum_x P(x) \\log_2 P(x)$$\n",
    "\n",
    "Let's explore how entropy changes with distribution shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(probs: np.ndarray) -> float:\n",
    "    \"\"\"Calculate entropy of a discrete distribution.\"\"\"\n",
    "    probs = np.array(probs)\n",
    "    probs = probs[probs > 0]  # Avoid log(0)\n",
    "    return -np.sum(probs * np.log2(probs))\n",
    "\n",
    "# Binary entropy function H(p) for a coin with P(heads) = p\n",
    "def binary_entropy(p: float) -> float:\n",
    "    if p <= 0 or p >= 1:\n",
    "        return 0\n",
    "    return -p * np.log2(p) - (1-p) * np.log2(1-p)\n",
    "\n",
    "# Plot binary entropy\n",
    "p_values = np.linspace(0.001, 0.999, 1000)\n",
    "h_values = [binary_entropy(p) for p in p_values]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(p_values, h_values, 'b-', linewidth=2)\n",
    "ax.set_xlabel('P(heads)')\n",
    "ax.set_ylabel('Entropy H(X) [bits]')\n",
    "ax.set_title('Binary Entropy: Maximum Uncertainty at p=0.5')\n",
    "\n",
    "# Mark maximum\n",
    "ax.axvline(x=0.5, color='r', linestyle='--', alpha=0.5)\n",
    "ax.plot(0.5, 1.0, 'ro', markersize=10)\n",
    "ax.annotate('Maximum entropy\\nH=1 bit at p=0.5', xy=(0.5, 1), xytext=(0.65, 0.85),\n",
    "            arrowprops=dict(arrowstyle='->', color='red'), fontsize=11)\n",
    "\n",
    "# Add interpretation regions\n",
    "ax.fill_between(p_values[:250], h_values[:250], alpha=0.3, color='green', label='Predictable (biased towards tails)')\n",
    "ax.fill_between(p_values[750:], h_values[750:], alpha=0.3, color='green', label='Predictable (biased towards heads)')\n",
    "ax.fill_between(p_values[400:600], h_values[400:600], alpha=0.3, color='red', label='High uncertainty')\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare entropy of different distributions\n",
    "distributions = {\n",
    "    'Uniform (4 outcomes)': [0.25, 0.25, 0.25, 0.25],\n",
    "    'Slightly biased': [0.4, 0.3, 0.2, 0.1],\n",
    "    'Very biased': [0.7, 0.1, 0.1, 0.1],\n",
    "    'Almost deterministic': [0.97, 0.01, 0.01, 0.01]\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, (name, probs) in zip(axes, distributions.items()):\n",
    "    h = entropy(probs)\n",
    "    colors = plt.cm.Blues(np.array(probs) / max(probs))\n",
    "    bars = ax.bar(range(len(probs)), probs, color=colors, edgecolor='black')\n",
    "    ax.set_title(f'{name}\\nH = {h:.3f} bits', fontsize=12)\n",
    "    ax.set_xlabel('Outcome')\n",
    "    ax.set_ylabel('Probability')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xticks(range(len(probs)))\n",
    "    \n",
    "    # Add probability labels on bars\n",
    "    for i, (bar, p) in enumerate(zip(bars, probs)):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                f'{p:.2f}', ha='center', fontsize=10)\n",
    "\n",
    "plt.suptitle('Entropy Decreases as Distribution Becomes More Predictable', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEntropy comparison:\")\n",
    "print(f\"Maximum possible (uniform over 4): {np.log2(4):.3f} bits\")\n",
    "for name, probs in distributions.items():\n",
    "    print(f\"{name}: {entropy(probs):.3f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Huffman Coding: Achieving Entropy Bounds\n",
    "\n",
    "Huffman coding creates an optimal prefix-free code that approaches the entropy bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(order=True)\n",
    "class HuffmanNode:\n",
    "    \"\"\"Node in a Huffman tree.\"\"\"\n",
    "    prob: float\n",
    "    symbol: Optional[str] = field(compare=False, default=None)\n",
    "    left: Optional['HuffmanNode'] = field(compare=False, default=None)\n",
    "    right: Optional['HuffmanNode'] = field(compare=False, default=None)\n",
    "    \n",
    "    def is_leaf(self) -> bool:\n",
    "        return self.symbol is not None\n",
    "\n",
    "\n",
    "def build_huffman_tree(symbol_probs: Dict[str, float]) -> HuffmanNode:\n",
    "    \"\"\"Build Huffman tree from symbol probabilities.\"\"\"\n",
    "    # Create leaf nodes\n",
    "    heap = [HuffmanNode(prob=p, symbol=s) for s, p in symbol_probs.items()]\n",
    "    heapq.heapify(heap)\n",
    "    \n",
    "    # Build tree\n",
    "    while len(heap) > 1:\n",
    "        left = heapq.heappop(heap)\n",
    "        right = heapq.heappop(heap)\n",
    "        merged = HuffmanNode(\n",
    "            prob=left.prob + right.prob,\n",
    "            left=left,\n",
    "            right=right\n",
    "        )\n",
    "        heapq.heappush(heap, merged)\n",
    "    \n",
    "    return heap[0]\n",
    "\n",
    "\n",
    "def get_huffman_codes(root: HuffmanNode, prefix: str = '') -> Dict[str, str]:\n",
    "    \"\"\"Extract codes from Huffman tree.\"\"\"\n",
    "    if root.is_leaf():\n",
    "        return {root.symbol: prefix or '0'}  # Handle single symbol case\n",
    "    \n",
    "    codes = {}\n",
    "    if root.left:\n",
    "        codes.update(get_huffman_codes(root.left, prefix + '0'))\n",
    "    if root.right:\n",
    "        codes.update(get_huffman_codes(root.right, prefix + '1'))\n",
    "    return codes\n",
    "\n",
    "\n",
    "def average_code_length(codes: Dict[str, str], probs: Dict[str, float]) -> float:\n",
    "    \"\"\"Calculate average code length.\"\"\"\n",
    "    return sum(len(codes[s]) * p for s, p in probs.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Weather forecasts\n",
    "weather_probs = {\n",
    "    '‚òÄÔ∏è Sunny': 0.5,\n",
    "    '‚òÅÔ∏è Cloudy': 0.25,\n",
    "    'üåßÔ∏è Rainy': 0.125,\n",
    "    '‚ùÑÔ∏è Snowy': 0.125\n",
    "}\n",
    "\n",
    "# Build Huffman tree and get codes\n",
    "tree = build_huffman_tree(weather_probs)\n",
    "codes = get_huffman_codes(tree)\n",
    "\n",
    "# Calculate metrics\n",
    "h = entropy(list(weather_probs.values()))\n",
    "avg_len = average_code_length(codes, weather_probs)\n",
    "\n",
    "print(\"Weather Forecast Huffman Coding\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Symbol':<15} {'Probability':<12} {'Code':<10} {'Code Length'}\")\n",
    "print(\"-\" * 50)\n",
    "for symbol, prob in sorted(weather_probs.items(), key=lambda x: -x[1]):\n",
    "    code = codes[symbol]\n",
    "    print(f\"{symbol:<15} {prob:<12.3f} {code:<10} {len(code)}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"\\nEntropy H(X): {h:.4f} bits\")\n",
    "print(f\"Average code length: {avg_len:.4f} bits\")\n",
    "print(f\"Efficiency: {h/avg_len*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Huffman coding efficiency across different distributions\n",
    "def analyze_huffman_efficiency(n_symbols: int, n_trials: int = 100):\n",
    "    \"\"\"Compare Huffman code length to entropy for random distributions.\"\"\"\n",
    "    entropies = []\n",
    "    avg_lengths = []\n",
    "    \n",
    "    for _ in range(n_trials):\n",
    "        # Generate random probability distribution\n",
    "        raw = np.random.exponential(1, n_symbols)\n",
    "        probs = raw / raw.sum()\n",
    "        \n",
    "        symbol_probs = {str(i): p for i, p in enumerate(probs)}\n",
    "        tree = build_huffman_tree(symbol_probs)\n",
    "        codes = get_huffman_codes(tree)\n",
    "        \n",
    "        entropies.append(entropy(probs))\n",
    "        avg_lengths.append(average_code_length(codes, symbol_probs))\n",
    "    \n",
    "    return np.array(entropies), np.array(avg_lengths)\n",
    "\n",
    "# Run analysis\n",
    "entropies, avg_lengths = analyze_huffman_efficiency(8, 500)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scatter plot\n",
    "ax = axes[0]\n",
    "ax.scatter(entropies, avg_lengths, alpha=0.5, s=30)\n",
    "ax.plot([0, 3.5], [0, 3.5], 'r--', linewidth=2, label='H(X) = Avg Length (ideal)')\n",
    "ax.plot([0, 3.5], [1, 4.5], 'g--', linewidth=2, label='H(X) + 1 (upper bound)')\n",
    "ax.set_xlabel('Entropy H(X) [bits]')\n",
    "ax.set_ylabel('Average Huffman Code Length [bits]')\n",
    "ax.set_title('Huffman Coding Achieves Near-Entropy Code Length')\n",
    "ax.legend()\n",
    "ax.set_xlim(0, 3.5)\n",
    "ax.set_ylim(0, 4.5)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Efficiency histogram\n",
    "ax = axes[1]\n",
    "overhead = avg_lengths - entropies\n",
    "ax.hist(overhead, bins=30, edgecolor='black', alpha=0.7)\n",
    "ax.axvline(x=0, color='r', linestyle='--', linewidth=2, label='Perfect efficiency')\n",
    "ax.axvline(x=overhead.mean(), color='g', linestyle='-', linewidth=2, label=f'Mean overhead: {overhead.mean():.3f}')\n",
    "ax.set_xlabel('Overhead: Avg Code Length - Entropy [bits]')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Huffman Coding Overhead Distribution')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nHuffman coding overhead statistics:\")\n",
    "print(f\"Mean overhead: {overhead.mean():.4f} bits\")\n",
    "print(f\"Max overhead: {overhead.max():.4f} bits\")\n",
    "print(f\"Shannon's theorem guarantees overhead < 1 bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross-Entropy: Using the Wrong Model\n",
    "\n",
    "$$H(P, Q) = -\\sum_x P(x) \\log_2 Q(x)$$\n",
    "\n",
    "Cross-entropy measures the cost of encoding data from $P$ using a code designed for $Q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(p: np.ndarray, q: np.ndarray) -> float:\n",
    "    \"\"\"Calculate cross-entropy H(P, Q).\"\"\"\n",
    "    p, q = np.array(p), np.array(q)\n",
    "    # Add small epsilon to avoid log(0)\n",
    "    q = np.clip(q, 1e-10, 1)\n",
    "    return -np.sum(p * np.log2(q))\n",
    "\n",
    "def kl_divergence(p: np.ndarray, q: np.ndarray) -> float:\n",
    "    \"\"\"Calculate KL divergence D_KL(P || Q).\"\"\"\n",
    "    return cross_entropy(p, q) - entropy(p)\n",
    "\n",
    "# True distribution (what nature generates)\n",
    "p_true = np.array([0.5, 0.3, 0.15, 0.05])\n",
    "\n",
    "# Model distributions (what we predict)\n",
    "models = {\n",
    "    'Perfect model (Q = P)': p_true.copy(),\n",
    "    'Uniform model': np.array([0.25, 0.25, 0.25, 0.25]),\n",
    "    'Overconfident wrong': np.array([0.1, 0.1, 0.1, 0.7]),\n",
    "    'Close but not perfect': np.array([0.45, 0.35, 0.12, 0.08])\n",
    "}\n",
    "\n",
    "print(\"Cross-Entropy Analysis\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"True distribution P: {p_true}\")\n",
    "print(f\"Entropy H(P): {entropy(p_true):.4f} bits\")\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "\n",
    "results = []\n",
    "for name, q in models.items():\n",
    "    h_pq = cross_entropy(p_true, q)\n",
    "    kl = kl_divergence(p_true, q)\n",
    "    results.append((name, h_pq, kl))\n",
    "    print(f\"\\n{name}\")\n",
    "    print(f\"  Q: {q}\")\n",
    "    print(f\"  Cross-entropy H(P,Q): {h_pq:.4f} bits\")\n",
    "    print(f\"  KL divergence D_KL(P||Q): {kl:.4f} bits\")\n",
    "    print(f\"  Extra bits wasted: {kl:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-entropy decomposition\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "names = [r[0] for r in results]\n",
    "cross_entropies = [r[1] for r in results]\n",
    "kl_divs = [r[2] for r in results]\n",
    "h_p = entropy(p_true)\n",
    "\n",
    "x = np.arange(len(names))\n",
    "width = 0.6\n",
    "\n",
    "# Stacked bar chart\n",
    "bars1 = ax.bar(x, [h_p]*len(names), width, label=f'Entropy H(P) = {h_p:.3f}', color='steelblue')\n",
    "bars2 = ax.bar(x, kl_divs, width, bottom=[h_p]*len(names), label='KL Divergence (waste)', color='coral')\n",
    "\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Bits')\n",
    "ax.set_title('Cross-Entropy = Entropy + KL Divergence\\n(Minimum achievable + Waste from wrong model)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(names, rotation=15, ha='right')\n",
    "ax.legend()\n",
    "\n",
    "# Add value labels\n",
    "for i, (ce, kl) in enumerate(zip(cross_entropies, kl_divs)):\n",
    "    ax.text(i, ce + 0.05, f'H(P,Q)={ce:.2f}', ha='center', fontsize=10)\n",
    "\n",
    "ax.set_ylim(0, max(cross_entropies) * 1.2)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ML Loss as Cross-Entropy: A Classification Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute softmax probabilities.\"\"\"\n",
    "    exp_logits = np.exp(logits - np.max(logits))  # Numerical stability\n",
    "    return exp_logits / exp_logits.sum()\n",
    "\n",
    "def cross_entropy_loss(y_true: int, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Cross-entropy loss for classification.\"\"\"\n",
    "    return -np.log(y_pred[y_true] + 1e-10)\n",
    "\n",
    "# Simulate a classifier's predictions during training\n",
    "n_classes = 4\n",
    "true_label = 0  # The correct class\n",
    "\n",
    "# Simulate logits at different training stages\n",
    "training_stages = {\n",
    "    'Random init': np.random.randn(n_classes),\n",
    "    'Early training': np.array([1.0, 0.5, 0.3, 0.2]),\n",
    "    'Mid training': np.array([2.0, 0.5, 0.3, 0.2]),\n",
    "    'Late training': np.array([4.0, 0.5, 0.3, 0.2]),\n",
    "    'Converged': np.array([8.0, 0.5, 0.3, 0.2])\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left plot: probability distributions\n",
    "ax = axes[0]\n",
    "x = np.arange(n_classes)\n",
    "width = 0.15\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(training_stages)))\n",
    "losses = []\n",
    "\n",
    "for i, (stage, logits) in enumerate(training_stages.items()):\n",
    "    probs = softmax(logits)\n",
    "    loss = cross_entropy_loss(true_label, probs)\n",
    "    losses.append(loss)\n",
    "    ax.bar(x + i*width, probs, width, label=f'{stage}', color=colors[i], alpha=0.8)\n",
    "\n",
    "ax.axhline(y=1/n_classes, color='red', linestyle='--', label='Uniform (random guess)')\n",
    "ax.set_xlabel('Class')\n",
    "ax.set_ylabel('Predicted Probability')\n",
    "ax.set_title('Classifier Confidence Over Training\\n(True class = 0)')\n",
    "ax.set_xticks(x + width*2)\n",
    "ax.set_xticklabels(['Class 0\\n(TRUE)', 'Class 1', 'Class 2', 'Class 3'])\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "# Right plot: loss curve\n",
    "ax = axes[1]\n",
    "ax.plot(range(len(losses)), losses, 'bo-', linewidth=2, markersize=10)\n",
    "ax.set_xlabel('Training Stage')\n",
    "ax.set_ylabel('Cross-Entropy Loss')\n",
    "ax.set_title('Loss Decreases as Model Improves')\n",
    "ax.set_xticks(range(len(losses)))\n",
    "ax.set_xticklabels(list(training_stages.keys()), rotation=15, ha='right')\n",
    "\n",
    "# Add annotations\n",
    "for i, (stage, loss) in enumerate(zip(training_stages.keys(), losses)):\n",
    "    ax.annotate(f'{loss:.2f} bits', (i, loss), textcoords=\"offset points\", \n",
    "                xytext=(0, 10), ha='center')\n",
    "\n",
    "ax.axhline(y=0, color='green', linestyle='--', label='Perfect prediction (0 bits)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîë Key Insight:\")\n",
    "print(\"Cross-entropy loss = bits needed to encode the true label using our model's distribution\")\n",
    "print(\"Perfect model ‚Üí 0 bits (we're certain of the right answer)\")\n",
    "print(\"Uniform model ‚Üí log‚ÇÇ(n_classes) bits (maximum uncertainty)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Real Text Example: Compression = Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze entropy of real text\n",
    "sample_text = \"\"\"\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn \n",
    "and improve from experience without being explicitly programmed. Machine learning \n",
    "focuses on the development of computer programs that can access data and use it to \n",
    "learn for themselves. The process of learning begins with observations or data, such \n",
    "as examples, direct experience, or instruction, in order to look for patterns in data \n",
    "and make better decisions in the future based on the examples that we provide.\n",
    "\"\"\"\n",
    "\n",
    "# Character-level analysis\n",
    "chars = [c.lower() for c in sample_text if c.isalpha() or c == ' ']\n",
    "char_counts = Counter(chars)\n",
    "total_chars = len(chars)\n",
    "char_probs = {c: count/total_chars for c, count in char_counts.items()}\n",
    "\n",
    "# Calculate entropy\n",
    "h_chars = entropy(list(char_probs.values()))\n",
    "\n",
    "# Build Huffman codes\n",
    "tree = build_huffman_tree(char_probs)\n",
    "codes = get_huffman_codes(tree)\n",
    "avg_len = average_code_length(codes, char_probs)\n",
    "\n",
    "# Compare to fixed-length encoding\n",
    "n_symbols = len(char_probs)\n",
    "fixed_len = np.ceil(np.log2(n_symbols))\n",
    "\n",
    "print(\"Text Compression Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Text length: {total_chars} characters\")\n",
    "print(f\"Unique characters: {n_symbols}\")\n",
    "print(f\"\\nEntropy: {h_chars:.3f} bits/character\")\n",
    "print(f\"Huffman average: {avg_len:.3f} bits/character\")\n",
    "print(f\"Fixed-length: {fixed_len:.0f} bits/character\")\n",
    "print(f\"\\nCompression ratio (vs fixed): {fixed_len/avg_len:.2f}x\")\n",
    "print(f\"\\nTotal bits needed:\")\n",
    "print(f\"  Fixed-length: {total_chars * fixed_len:.0f} bits = {total_chars * fixed_len / 8:.0f} bytes\")\n",
    "print(f\"  Huffman: {total_chars * avg_len:.0f} bits = {total_chars * avg_len / 8:.0f} bytes\")\n",
    "print(f\"  Savings: {(1 - avg_len/fixed_len)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize character frequency distribution\n",
    "sorted_chars = sorted(char_probs.items(), key=lambda x: -x[1])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Character frequencies\n",
    "ax = axes[0]\n",
    "chars_list = [c if c != ' ' else '‚ê£' for c, _ in sorted_chars]\n",
    "probs_list = [p for _, p in sorted_chars]\n",
    "ax.bar(range(len(chars_list)), probs_list, color='steelblue', edgecolor='black')\n",
    "ax.set_xlabel('Character')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_title('Character Frequency Distribution in English Text')\n",
    "ax.set_xticks(range(len(chars_list)))\n",
    "ax.set_xticklabels(chars_list, fontsize=9)\n",
    "\n",
    "# Huffman codes\n",
    "ax = axes[1]\n",
    "code_lengths = [len(codes[c]) for c, _ in sorted_chars[:15]]\n",
    "ax.bar(range(len(code_lengths)), code_lengths, color='coral', edgecolor='black')\n",
    "ax.set_xlabel('Character (sorted by frequency)')\n",
    "ax.set_ylabel('Huffman Code Length (bits)')\n",
    "ax.set_title('Huffman Code Lengths\\n(Common chars ‚Üí Short codes)')\n",
    "ax.set_xticks(range(len(code_lengths)))\n",
    "ax.set_xticklabels([c if c != ' ' else '‚ê£' for c, _ in sorted_chars[:15]])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîë Key Insight:\")\n",
    "print(\"Common characters (space, 'e', 't', 'a') get short codes\")\n",
    "print(\"Rare characters get longer codes\")\n",
    "print(\"This is exactly what a good language model learns!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Formulas\n",
    "\n",
    "| Concept | Formula | Meaning |\n",
    "|---------|---------|--------|\n",
    "| Information | $I(x) = -\\log_2 P(x)$ | Surprise of event $x$ |\n",
    "| Entropy | $H(X) = -\\sum_x P(x) \\log_2 P(x)$ | Expected surprise |\n",
    "| Cross-Entropy | $H(P,Q) = -\\sum_x P(x) \\log_2 Q(x)$ | Bits using wrong model |\n",
    "| KL Divergence | $D_{KL}(P\\|Q) = H(P,Q) - H(P)$ | Extra bits from wrong model |\n",
    "\n",
    "### The ML Connection\n",
    "\n",
    "1. **Training** = Minimizing cross-entropy = Finding best compression\n",
    "2. **Better model** = Assigns higher probability to real data = Needs fewer bits\n",
    "3. **Overfitting** = Memorizing noise = Wasting bits on incompressible patterns\n",
    "4. **Regularization** = Limiting model complexity = Limiting description length"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
