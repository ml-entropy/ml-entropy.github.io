{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 11: Learning Rate\n",
    "\n",
    "Exploring the most important hyperparameter in deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Visualizing Learning Rate Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple 2D optimization problem\n",
    "def loss_fn(x, y):\n",
    "    \"\"\"Rosenbrock-like function with global minimum at (1, 1)\"\"\"\n",
    "    return (1 - x)**2 + 10 * (y - x**2)**2\n",
    "\n",
    "def grad_fn(x, y):\n",
    "    \"\"\"Gradient of loss\"\"\"\n",
    "    dx = -2*(1-x) - 40*x*(y - x**2)\n",
    "    dy = 20*(y - x**2)\n",
    "    return np.array([dx, dy])\n",
    "\n",
    "def gradient_descent(lr, start=(-1, 1), n_steps=100):\n",
    "    \"\"\"Run gradient descent and return trajectory\"\"\"\n",
    "    pos = np.array(start, dtype=float)\n",
    "    trajectory = [pos.copy()]\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        grad = grad_fn(pos[0], pos[1])\n",
    "        pos = pos - lr * grad\n",
    "        trajectory.append(pos.copy())\n",
    "        \n",
    "        # Stop if diverging\n",
    "        if np.any(np.abs(pos) > 10):\n",
    "            break\n",
    "    \n",
    "    return np.array(trajectory)\n",
    "\n",
    "# Different learning rates\n",
    "lrs = [0.001, 0.01, 0.03, 0.05]\n",
    "\n",
    "# Plot loss landscape\n",
    "x_range = np.linspace(-2, 2, 100)\n",
    "y_range = np.linspace(-1, 3, 100)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = loss_fn(X, Y)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "for ax, lr in zip(axes.flatten(), lrs):\n",
    "    # Contour plot\n",
    "    ax.contour(X, Y, Z, levels=np.logspace(-1, 3, 20), cmap='viridis', alpha=0.7)\n",
    "    \n",
    "    # Trajectory\n",
    "    traj = gradient_descent(lr)\n",
    "    ax.plot(traj[:, 0], traj[:, 1], 'r.-', linewidth=1, markersize=3)\n",
    "    ax.plot(traj[0, 0], traj[0, 1], 'go', markersize=10, label='Start')\n",
    "    ax.plot(1, 1, 'r*', markersize=15, label='Minimum')\n",
    "    \n",
    "    final_loss = loss_fn(traj[-1, 0], traj[-1, 1])\n",
    "    ax.set_title(f'LR = {lr}\\nFinal loss: {final_loss:.4f}')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Too small LR: Slow convergence\")\n",
    "print(\"Too large LR: Oscillation or divergence\")\n",
    "print(\"Just right: Fast and stable convergence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implementing Optimizers from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def step(self, params, grads):\n",
    "        return params - self.lr * grads\n",
    "\n",
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, beta=0.9):\n",
    "        self.lr = lr\n",
    "        self.beta = beta\n",
    "        self.v = None\n",
    "    \n",
    "    def step(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = np.zeros_like(params)\n",
    "        self.v = self.beta * self.v + grads\n",
    "        return params - self.lr * self.v\n",
    "\n",
    "class RMSprop:\n",
    "    def __init__(self, lr=0.01, beta=0.9, eps=1e-8):\n",
    "        self.lr = lr\n",
    "        self.beta = beta\n",
    "        self.eps = eps\n",
    "        self.v = None\n",
    "    \n",
    "    def step(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = np.zeros_like(params)\n",
    "        self.v = self.beta * self.v + (1 - self.beta) * grads**2\n",
    "        return params - self.lr * grads / (np.sqrt(self.v) + self.eps)\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, lr=0.01, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.t = 0\n",
    "    \n",
    "    def step(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m = np.zeros_like(params)\n",
    "            self.v = np.zeros_like(params)\n",
    "        \n",
    "        self.t += 1\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * grads\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * grads**2\n",
    "        \n",
    "        # Bias correction\n",
    "        m_hat = self.m / (1 - self.beta1**self.t)\n",
    "        v_hat = self.v / (1 - self.beta2**self.t)\n",
    "        \n",
    "        return params - self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n",
    "\n",
    "# Compare optimizers\n",
    "def optimize(optimizer, start=(-1, 1), n_steps=100):\n",
    "    pos = np.array(start, dtype=float)\n",
    "    trajectory = [pos.copy()]\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        grad = grad_fn(pos[0], pos[1])\n",
    "        pos = optimizer.step(pos, grad)\n",
    "        trajectory.append(pos.copy())\n",
    "    \n",
    "    return np.array(trajectory)\n",
    "\n",
    "# Run all optimizers\n",
    "optimizers = {\n",
    "    'SGD (lr=0.001)': SGD(lr=0.001),\n",
    "    'Momentum': Momentum(lr=0.001),\n",
    "    'RMSprop': RMSprop(lr=0.01),\n",
    "    'Adam': Adam(lr=0.1)\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.contour(X, Y, Z, levels=np.logspace(-1, 3, 20), cmap='viridis', alpha=0.5)\n",
    "\n",
    "colors = ['blue', 'green', 'red', 'purple']\n",
    "for (name, opt), color in zip(optimizers.items(), colors):\n",
    "    traj = optimize(opt)\n",
    "    ax.plot(traj[:, 0], traj[:, 1], '.-', linewidth=1, markersize=3, \n",
    "            color=color, label=name, alpha=0.8)\n",
    "\n",
    "ax.plot(1, 1, 'r*', markersize=20)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('Optimizer Comparison on Rosenbrock-like Function')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Learning Rate Schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "base_lr = 0.1\n",
    "\n",
    "# Different schedules\n",
    "def constant(epoch):\n",
    "    return base_lr\n",
    "\n",
    "def step_decay(epoch, drop=0.5, epochs_drop=30):\n",
    "    return base_lr * (drop ** (epoch // epochs_drop))\n",
    "\n",
    "def exponential_decay(epoch, k=0.05):\n",
    "    return base_lr * np.exp(-k * epoch)\n",
    "\n",
    "def cosine_annealing(epoch, T_max=100, eta_min=0.001):\n",
    "    return eta_min + (base_lr - eta_min) * (1 + np.cos(np.pi * epoch / T_max)) / 2\n",
    "\n",
    "def warmup_cosine(epoch, warmup=10, T_max=100):\n",
    "    if epoch < warmup:\n",
    "        return base_lr * epoch / warmup\n",
    "    else:\n",
    "        return cosine_annealing(epoch - warmup, T_max - warmup)\n",
    "\n",
    "# Plot schedules\n",
    "epochs_range = np.arange(epochs)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "schedules = {\n",
    "    'Constant': [constant(e) for e in epochs_range],\n",
    "    'Step Decay': [step_decay(e) for e in epochs_range],\n",
    "    'Exponential': [exponential_decay(e) for e in epochs_range],\n",
    "    'Cosine': [cosine_annealing(e) for e in epochs_range],\n",
    "    'Warmup + Cosine': [warmup_cosine(e) for e in epochs_range]\n",
    "}\n",
    "\n",
    "for name, lrs in schedules.items():\n",
    "    plt.plot(epochs_range, lrs, linewidth=2, label=name)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedules')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Learning Rate Range Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load MNIST\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Simple model\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(784, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "def lr_range_test(model, loader, lr_min=1e-7, lr_max=10, num_iters=100):\n",
    "    \"\"\"Find optimal learning rate by gradually increasing LR\"\"\"\n",
    "    # Exponential LR increase\n",
    "    lr_mult = (lr_max / lr_min) ** (1 / num_iters)\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr_min)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    lrs = []\n",
    "    losses = []\n",
    "    lr = lr_min\n",
    "    \n",
    "    model.train()\n",
    "    data_iter = iter(loader)\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        try:\n",
    "            x, y = next(data_iter)\n",
    "        except StopIteration:\n",
    "            data_iter = iter(loader)\n",
    "            x, y = next(data_iter)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        lrs.append(lr)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Increase LR\n",
    "        lr *= lr_mult\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        # Stop if loss explodes\n",
    "        if loss.item() > 100:\n",
    "            break\n",
    "    \n",
    "    return lrs, losses\n",
    "\n",
    "# Run test\n",
    "model = SimpleNet()\n",
    "lrs, losses = lr_range_test(model, train_loader)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(lrs, losses)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Learning Rate (log scale)')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Learning Rate Range Test')\n",
    "plt.axvline(lrs[np.argmin(losses)], color='r', linestyle='--', label=f'Min loss at LR={lrs[np.argmin(losses)]:.4f}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Find suggested LR (where loss decreases fastest)\n",
    "grad = np.gradient(losses)\n",
    "suggested_idx = np.argmin(grad[:len(grad)//2])  # Before loss starts increasing\n",
    "print(f\"\\nSuggested learning rate: {lrs[suggested_idx]:.4f}\")\n",
    "print(\"(Choose LR where loss decreases steepest, before it diverges)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Key insights:**\n",
    "1. **Learning rate** controls step size in parameter space\n",
    "2. **Too small** → slow convergence, **too large** → divergence\n",
    "3. **Adaptive optimizers** (Adam, RMSprop) adjust LR per parameter\n",
    "4. **Schedules** (cosine, warmup) improve final performance\n",
    "5. **LR range test** helps find good starting LR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
