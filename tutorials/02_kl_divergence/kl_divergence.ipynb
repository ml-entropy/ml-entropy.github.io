{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š KL Divergence: Theory and Practice\n",
    "\n",
    "This notebook explores:\n",
    "1. KL divergence for discrete distributions\n",
    "2. KL divergence for continuous distributions\n",
    "3. The critical asymmetry (forward vs reverse KL)\n",
    "4. Closed-form KL for Gaussians\n",
    "5. Why KL matters for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.special import kl_div as scipy_kl\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. KL Divergence: Discrete Distributions\n",
    "\n",
    "$$D_{KL}(P \\| Q) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence_discrete(p: np.ndarray, q: np.ndarray, eps: float = 1e-10) -> float:\n",
    "    \"\"\"\n",
    "    Calculate KL divergence D_KL(P || Q) for discrete distributions.\n",
    "    \n",
    "    Args:\n",
    "        p: True distribution\n",
    "        q: Approximate distribution\n",
    "        eps: Small constant to avoid log(0)\n",
    "    \n",
    "    Returns:\n",
    "        KL divergence in nats (natural log)\n",
    "    \"\"\"\n",
    "    p = np.array(p, dtype=np.float64)\n",
    "    q = np.array(q, dtype=np.float64)\n",
    "    \n",
    "    # Ensure normalized\n",
    "    p = p / p.sum()\n",
    "    q = q / q.sum()\n",
    "    \n",
    "    # Add epsilon to avoid log(0)\n",
    "    q = np.clip(q, eps, 1)\n",
    "    \n",
    "    # Only sum where p > 0\n",
    "    mask = p > eps\n",
    "    return np.sum(p[mask] * np.log(p[mask] / q[mask]))\n",
    "\n",
    "\n",
    "def kl_divergence_discrete_bits(p: np.ndarray, q: np.ndarray) -> float:\n",
    "    \"\"\"KL divergence in bits (log base 2).\"\"\"\n",
    "    return kl_divergence_discrete(p, q) / np.log(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compare different distributions\n",
    "n_outcomes = 4\n",
    "\n",
    "distributions = {\n",
    "    'P (true)': np.array([0.5, 0.3, 0.15, 0.05]),\n",
    "    'Q1 (uniform)': np.array([0.25, 0.25, 0.25, 0.25]),\n",
    "    'Q2 (close)': np.array([0.45, 0.32, 0.15, 0.08]),\n",
    "    'Q3 (wrong mode)': np.array([0.05, 0.1, 0.35, 0.5])\n",
    "}\n",
    "\n",
    "P = distributions['P (true)']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "for ax, (name, dist) in zip(axes.flatten(), distributions.items()):\n",
    "    colors = ['steelblue' if name == 'P (true)' else 'coral'] * n_outcomes\n",
    "    bars = ax.bar(range(n_outcomes), dist, color=colors[0], edgecolor='black', alpha=0.7)\n",
    "    \n",
    "    if name != 'P (true)':\n",
    "        # Overlay P as reference\n",
    "        ax.bar(range(n_outcomes), P, color='steelblue', edgecolor='black', alpha=0.3, label='P (true)')\n",
    "        kl = kl_divergence_discrete_bits(P, dist)\n",
    "        ax.set_title(f'{name}\\n$D_{{KL}}(P \\\\| Q)$ = {kl:.4f} bits', fontsize=12)\n",
    "    else:\n",
    "        ax.set_title(f'{name}\\n(Reference distribution)', fontsize=12)\n",
    "    \n",
    "    ax.set_xlabel('Outcome')\n",
    "    ax.set_ylabel('Probability')\n",
    "    ax.set_ylim(0, 0.7)\n",
    "    ax.set_xticks(range(n_outcomes))\n",
    "    \n",
    "    # Add probability labels\n",
    "    for i, p in enumerate(dist):\n",
    "        ax.text(i, p + 0.02, f'{p:.2f}', ha='center', fontsize=10)\n",
    "\n",
    "plt.suptitle('KL Divergence: Extra Bits from Using Wrong Distribution', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"KL Divergence Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "for name, dist in distributions.items():\n",
    "    if name != 'P (true)':\n",
    "        kl_nats = kl_divergence_discrete(P, dist)\n",
    "        kl_bits = kl_divergence_discrete_bits(P, dist)\n",
    "        print(f\"{name}: {kl_nats:.4f} nats = {kl_bits:.4f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Asymmetry: Forward vs Reverse KL\n",
    "\n",
    "This is **crucial** to understand! The direction matters enormously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bimodal distribution\n",
    "def bimodal_pdf(x, mu1=-2, mu2=2, sigma=0.8, weight=0.5):\n",
    "    \"\"\"Mixture of two Gaussians.\"\"\"\n",
    "    return weight * stats.norm.pdf(x, mu1, sigma) + (1-weight) * stats.norm.pdf(x, mu2, sigma)\n",
    "\n",
    "x = np.linspace(-6, 6, 1000)\n",
    "p_bimodal = bimodal_pdf(x)\n",
    "p_bimodal = p_bimodal / np.trapz(p_bimodal, x)  # Normalize\n",
    "\n",
    "# Fit single Gaussians with different KL objectives\n",
    "# Forward KL: mean-seeking (covers both modes)\n",
    "# Reverse KL: mode-seeking (focuses on one mode)\n",
    "\n",
    "# Forward KL solution (approximately): matches moments\n",
    "mu_forward = 0  # Mean of the mixture\n",
    "sigma_forward = 2.5  # Large variance to cover both modes\n",
    "\n",
    "# Reverse KL solutions (mode-seeking): focuses on one mode\n",
    "mu_reverse = -2  # Focuses on left mode\n",
    "sigma_reverse = 0.8\n",
    "\n",
    "q_forward = stats.norm.pdf(x, mu_forward, sigma_forward)\n",
    "q_reverse_left = stats.norm.pdf(x, mu_reverse, sigma_reverse)\n",
    "q_reverse_right = stats.norm.pdf(x, -mu_reverse, sigma_reverse)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Forward KL solution\n",
    "ax = axes[0]\n",
    "ax.fill_between(x, p_bimodal, alpha=0.3, color='blue', label='P (true bimodal)')\n",
    "ax.plot(x, p_bimodal, 'b-', linewidth=2)\n",
    "ax.plot(x, q_forward, 'r--', linewidth=2, label='Q (Gaussian fit)')\n",
    "ax.set_title('Forward KL: $D_{KL}(P \\\\| Q)$\\n\"Mean-seeking\" - covers both modes', fontsize=11)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('Density')\n",
    "ax.legend()\n",
    "ax.set_xlim(-6, 6)\n",
    "\n",
    "# Plot 2: Reverse KL solution (left mode)\n",
    "ax = axes[1]\n",
    "ax.fill_between(x, p_bimodal, alpha=0.3, color='blue', label='P (true bimodal)')\n",
    "ax.plot(x, p_bimodal, 'b-', linewidth=2)\n",
    "ax.plot(x, q_reverse_left, 'r--', linewidth=2, label='Q (Gaussian fit)')\n",
    "ax.set_title('Reverse KL: $D_{KL}(Q \\\\| P)$\\n\"Mode-seeking\" - locks onto left mode', fontsize=11)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('Density')\n",
    "ax.legend()\n",
    "ax.set_xlim(-6, 6)\n",
    "\n",
    "# Plot 3: Reverse KL solution (right mode)\n",
    "ax = axes[2]\n",
    "ax.fill_between(x, p_bimodal, alpha=0.3, color='blue', label='P (true bimodal)')\n",
    "ax.plot(x, p_bimodal, 'b-', linewidth=2)\n",
    "ax.plot(x, q_reverse_right, 'r--', linewidth=2, label='Q (Gaussian fit)')\n",
    "ax.set_title('Reverse KL: $D_{KL}(Q \\\\| P)$\\n\"Mode-seeking\" - locks onto right mode', fontsize=11)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('Density')\n",
    "ax.legend()\n",
    "ax.set_xlim(-6, 6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ”‘ Key Insight:\")\n",
    "print(\"Forward KL (used in ML): Q must cover all of P â†’ overdispersed\")\n",
    "print(\"Reverse KL (used in VI): Q can ignore parts of P â†’ underdispersed, mode-seeking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical demonstration of asymmetry\n",
    "def compute_kl_continuous(p_samples, q_samples, x_range):\n",
    "    \"\"\"Approximate KL divergence from samples using KDE.\"\"\"\n",
    "    p_kde = stats.gaussian_kde(p_samples)\n",
    "    q_kde = stats.gaussian_kde(q_samples)\n",
    "    \n",
    "    x = np.linspace(x_range[0], x_range[1], 1000)\n",
    "    p_vals = p_kde(x)\n",
    "    q_vals = q_kde(x)\n",
    "    \n",
    "    # Add small epsilon to avoid log(0)\n",
    "    eps = 1e-10\n",
    "    p_vals = np.clip(p_vals, eps, None)\n",
    "    q_vals = np.clip(q_vals, eps, None)\n",
    "    \n",
    "    # Numerical integration\n",
    "    dx = x[1] - x[0]\n",
    "    kl = np.sum(p_vals * np.log(p_vals / q_vals)) * dx\n",
    "    return kl\n",
    "\n",
    "# Generate samples from bimodal P\n",
    "n_samples = 10000\n",
    "p_samples = np.concatenate([\n",
    "    np.random.normal(-2, 0.8, n_samples//2),\n",
    "    np.random.normal(2, 0.8, n_samples//2)\n",
    "])\n",
    "\n",
    "# Different Q distributions\n",
    "q_wide = np.random.normal(0, 2.5, n_samples)  # Wide, covers both\n",
    "q_left = np.random.normal(-2, 0.8, n_samples)  # Left mode\n",
    "q_right = np.random.normal(2, 0.8, n_samples)  # Right mode\n",
    "\n",
    "print(\"KL Divergence Asymmetry Demonstration\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nP = Bimodal (modes at -2 and +2)\")\n",
    "print(\"\\n1. Q = Wide Gaussian (Î¼=0, Ïƒ=2.5):\")\n",
    "print(f\"   D_KL(P || Q) â‰ˆ {compute_kl_continuous(p_samples, q_wide, (-8, 8)):.4f} nats\")\n",
    "print(f\"   D_KL(Q || P) â‰ˆ {compute_kl_continuous(q_wide, p_samples, (-8, 8)):.4f} nats\")\n",
    "\n",
    "print(\"\\n2. Q = Left mode (Î¼=-2, Ïƒ=0.8):\")\n",
    "print(f\"   D_KL(P || Q) â‰ˆ {compute_kl_continuous(p_samples, q_left, (-8, 8)):.4f} nats\")\n",
    "print(f\"   D_KL(Q || P) â‰ˆ {compute_kl_continuous(q_left, p_samples, (-8, 8)):.4f} nats\")\n",
    "\n",
    "print(\"\\nðŸ”‘ Notice:\")\n",
    "print(\"- Forward KL penalizes missing modes heavily (Q_left has high D_KL(P||Q))\")\n",
    "print(\"- Reverse KL allows ignoring modes (Q_left has low D_KL(Q||P))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. KL Divergence for Gaussians: Closed Form\n",
    "\n",
    "For two Gaussians $P = \\mathcal{N}(\\mu_1, \\sigma_1^2)$ and $Q = \\mathcal{N}(\\mu_2, \\sigma_2^2)$:\n",
    "\n",
    "$$D_{KL}(P \\| Q) = \\log\\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_gaussian(mu1: float, sigma1: float, mu2: float, sigma2: float) -> float:\n",
    "    \"\"\"\n",
    "    KL divergence between two univariate Gaussians.\n",
    "    D_KL(N(mu1, sigma1^2) || N(mu2, sigma2^2))\n",
    "    \"\"\"\n",
    "    return (np.log(sigma2/sigma1) + \n",
    "            (sigma1**2 + (mu1 - mu2)**2) / (2 * sigma2**2) - \n",
    "            0.5)\n",
    "\n",
    "\n",
    "def kl_to_standard_normal(mu: float, sigma: float) -> float:\n",
    "    \"\"\"\n",
    "    KL divergence from N(mu, sigma^2) to N(0, 1).\n",
    "    This is the VAE latent regularization term!\n",
    "    \"\"\"\n",
    "    return 0.5 * (mu**2 + sigma**2 - 1 - np.log(sigma**2))\n",
    "\n",
    "\n",
    "# Verify formula matches numerical integration\n",
    "mu1, sigma1 = 1.0, 1.5\n",
    "mu2, sigma2 = 0.0, 1.0\n",
    "\n",
    "# Analytical\n",
    "kl_analytical = kl_gaussian(mu1, sigma1, mu2, sigma2)\n",
    "\n",
    "# Numerical\n",
    "x = np.linspace(-10, 10, 10000)\n",
    "p = stats.norm.pdf(x, mu1, sigma1)\n",
    "q = stats.norm.pdf(x, mu2, sigma2)\n",
    "dx = x[1] - x[0]\n",
    "kl_numerical = np.sum(p * np.log(p / (q + 1e-10))) * dx\n",
    "\n",
    "print(\"Verifying Gaussian KL Formula\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"P = N({mu1}, {sigma1}Â²), Q = N({mu2}, {sigma2}Â²)\")\n",
    "print(f\"Analytical KL: {kl_analytical:.6f} nats\")\n",
    "print(f\"Numerical KL:  {kl_numerical:.6f} nats\")\n",
    "print(f\"Difference:    {abs(kl_analytical - kl_numerical):.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how KL changes with parameters\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Reference: Q = N(0, 1)\n",
    "mu_q, sigma_q = 0, 1\n",
    "\n",
    "# Plot 1: KL vs mean (fixed variance)\n",
    "ax = axes[0]\n",
    "mus = np.linspace(-4, 4, 100)\n",
    "sigma_p = 1.0\n",
    "kls = [kl_gaussian(mu, sigma_p, mu_q, sigma_q) for mu in mus]\n",
    "\n",
    "ax.plot(mus, kls, 'b-', linewidth=2)\n",
    "ax.set_xlabel('Î¼ of P')\n",
    "ax.set_ylabel('$D_{KL}(P \\\\| Q)$ [nats]')\n",
    "ax.set_title(f'KL vs Mean\\n(P = N(Î¼, 1), Q = N(0, 1))')\n",
    "ax.axvline(x=0, color='r', linestyle='--', alpha=0.5, label='Minimum at Î¼=0')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: KL vs variance (fixed mean)\n",
    "ax = axes[1]\n",
    "sigmas = np.linspace(0.1, 3, 100)\n",
    "mu_p = 0\n",
    "kls = [kl_gaussian(mu_p, sigma, mu_q, sigma_q) for sigma in sigmas]\n",
    "\n",
    "ax.plot(sigmas, kls, 'b-', linewidth=2)\n",
    "ax.set_xlabel('Ïƒ of P')\n",
    "ax.set_ylabel('$D_{KL}(P \\\\| Q)$ [nats]')\n",
    "ax.set_title(f'KL vs Std Dev\\n(P = N(0, ÏƒÂ²), Q = N(0, 1))')\n",
    "ax.axvline(x=1, color='r', linestyle='--', alpha=0.5, label='Minimum at Ïƒ=1')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: 2D heatmap\n",
    "ax = axes[2]\n",
    "mu_range = np.linspace(-3, 3, 50)\n",
    "sigma_range = np.linspace(0.2, 2.5, 50)\n",
    "MU, SIGMA = np.meshgrid(mu_range, sigma_range)\n",
    "KL = np.array([[kl_gaussian(mu, sigma, 0, 1) for mu in mu_range] for sigma in sigma_range])\n",
    "\n",
    "contour = ax.contourf(MU, SIGMA, KL, levels=20, cmap='viridis')\n",
    "plt.colorbar(contour, ax=ax, label='$D_{KL}$ [nats]')\n",
    "ax.plot(0, 1, 'r*', markersize=15, label='Minimum (Î¼=0, Ïƒ=1)')\n",
    "ax.set_xlabel('Î¼ of P')\n",
    "ax.set_ylabel('Ïƒ of P')\n",
    "ax.set_title('$D_{KL}(\\mathcal{N}(\\mu, \\sigma^2) \\\\| \\mathcal{N}(0, 1))$')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. VAE Latent Space KL: The Regularization Term\n",
    "\n",
    "In VAEs, we compute:\n",
    "$$D_{KL}(q(z|x) \\| p(z)) = D_{KL}(\\mathcal{N}(\\mu, \\sigma^2) \\| \\mathcal{N}(0, 1))$$\n",
    "\n",
    "$$= \\frac{1}{2}\\sum_j \\left[\\mu_j^2 + \\sigma_j^2 - 1 - \\log(\\sigma_j^2)\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_kl_loss(mu: np.ndarray, log_var: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    VAE KL loss: D_KL(N(mu, exp(log_var)) || N(0, I))\n",
    "    \n",
    "    This is the closed-form KL divergence used in VAEs.\n",
    "    \n",
    "    Args:\n",
    "        mu: Mean vector of shape (batch, latent_dim) or (latent_dim,)\n",
    "        log_var: Log variance vector of same shape\n",
    "    \n",
    "    Returns:\n",
    "        KL divergence (scalar or per-sample)\n",
    "    \"\"\"\n",
    "    # KL = 0.5 * sum(mu^2 + sigma^2 - 1 - log(sigma^2))\n",
    "    #    = 0.5 * sum(mu^2 + exp(log_var) - 1 - log_var)\n",
    "    return 0.5 * np.sum(mu**2 + np.exp(log_var) - 1 - log_var, axis=-1)\n",
    "\n",
    "\n",
    "# Visualize the components of VAE KL loss\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Component 1: mu^2 (penalizes moving mean away from 0)\n",
    "ax = axes[0]\n",
    "mu_range = np.linspace(-3, 3, 100)\n",
    "ax.plot(mu_range, 0.5 * mu_range**2, 'b-', linewidth=2)\n",
    "ax.set_xlabel('Î¼')\n",
    "ax.set_ylabel('$0.5 \\cdot \\mu^2$')\n",
    "ax.set_title('KL Component 1: Mean Penalty\\n\"Stay near origin\"')\n",
    "ax.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Component 2: sigma^2 - 1 - log(sigma^2) (penalizes variance != 1)\n",
    "ax = axes[1]\n",
    "log_var_range = np.linspace(-4, 2, 100)\n",
    "sigma_sq = np.exp(log_var_range)\n",
    "penalty = 0.5 * (sigma_sq - 1 - log_var_range)\n",
    "ax.plot(sigma_sq, penalty, 'b-', linewidth=2)\n",
    "ax.set_xlabel('ÏƒÂ²')\n",
    "ax.set_ylabel('$0.5 \\cdot (\\sigma^2 - 1 - \\log \\sigma^2)$')\n",
    "ax.set_title('KL Component 2: Variance Penalty\\n\"Match unit variance\"')\n",
    "ax.axvline(x=1, color='r', linestyle='--', alpha=0.5, label='Minimum at ÏƒÂ²=1')\n",
    "ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "ax.set_xlim(0, 5)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Combined effect in 2D\n",
    "ax = axes[2]\n",
    "mu_grid = np.linspace(-3, 3, 50)\n",
    "logvar_grid = np.linspace(-2, 2, 50)\n",
    "MU, LOGVAR = np.meshgrid(mu_grid, logvar_grid)\n",
    "KL = 0.5 * (MU**2 + np.exp(LOGVAR) - 1 - LOGVAR)\n",
    "\n",
    "contour = ax.contourf(MU, np.exp(LOGVAR/2), KL, levels=20, cmap='viridis')\n",
    "plt.colorbar(contour, ax=ax, label='KL [nats]')\n",
    "ax.plot(0, 1, 'r*', markersize=15, label='Minimum (Î¼=0, Ïƒ=1)')\n",
    "ax.set_xlabel('Î¼')\n",
    "ax.set_ylabel('Ïƒ')\n",
    "ax.set_title('Total VAE KL Loss\\n$D_{KL}(q(z|x) \\\\| p(z))$')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ”‘ VAE KL Intuition:\")\n",
    "print(\"- Î¼Â² term: Pulls latent means toward 0 (centers the distribution)\")\n",
    "print(\"- ÏƒÂ² - 1 - log(ÏƒÂ²) term: Pulls latent variance toward 1\")\n",
    "print(\"- Together: Forces q(z|x) to stay close to standard normal p(z)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate VAE latent distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Different encoder outputs (mu, log_var)\n",
    "encoder_outputs = [\n",
    "    (0, 0, 'Perfect match'),           # mu=0, sigma=1 â†’ KL=0\n",
    "    (2, 0, 'Shifted mean'),            # mu=2, sigma=1\n",
    "    (0, 1, 'Large variance'),          # mu=0, sigma=e^0.5â‰ˆ1.65\n",
    "    (0, -1, 'Small variance'),         # mu=0, sigma=e^-0.5â‰ˆ0.61\n",
    "    (1, 0.5, 'Both shifted'),          # mu=1, sigmaâ‰ˆ1.28\n",
    "    (-1, -0.5, 'Both shifted (neg)')   # mu=-1, sigmaâ‰ˆ0.78\n",
    "]\n",
    "\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "prior = stats.norm.pdf(x, 0, 1)\n",
    "\n",
    "for ax, (mu, log_var, title) in zip(axes.flatten(), encoder_outputs):\n",
    "    sigma = np.exp(0.5 * log_var)\n",
    "    posterior = stats.norm.pdf(x, mu, sigma)\n",
    "    kl = vae_kl_loss(np.array([mu]), np.array([log_var]))\n",
    "    \n",
    "    ax.fill_between(x, prior, alpha=0.3, color='blue', label='p(z) = N(0,1)')\n",
    "    ax.plot(x, prior, 'b-', linewidth=2)\n",
    "    ax.fill_between(x, posterior, alpha=0.3, color='red', label=f'q(z|x) = N({mu},{sigma:.2f}Â²)')\n",
    "    ax.plot(x, posterior, 'r--', linewidth=2)\n",
    "    \n",
    "    ax.set_title(f'{title}\\nÎ¼={mu}, Ïƒ={sigma:.2f}, KL={kl:.3f} nats')\n",
    "    ax.set_xlabel('z')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.legend(loc='upper right', fontsize=9)\n",
    "    ax.set_xlim(-5, 5)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('VAE: Posterior q(z|x) vs Prior p(z)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Information-Theoretic View: KL as Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian update example: KL measures information gained from data\n",
    "\n",
    "# Prior: broad uncertainty\n",
    "mu_prior, sigma_prior = 0, 3\n",
    "\n",
    "# Posteriors after seeing different amounts of data\n",
    "posteriors = [\n",
    "    (0.5, 2.5, '1 data point'),\n",
    "    (0.8, 1.5, '10 data points'),\n",
    "    (1.0, 0.8, '100 data points'),\n",
    "    (1.0, 0.3, '1000 data points')\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.linspace(-8, 8, 1000)\n",
    "prior_pdf = stats.norm.pdf(x, mu_prior, sigma_prior)\n",
    "ax.fill_between(x, prior_pdf, alpha=0.3, color='gray', label='Prior N(0, 9)')\n",
    "ax.plot(x, prior_pdf, 'k--', linewidth=2)\n",
    "\n",
    "colors = plt.cm.Reds(np.linspace(0.3, 0.9, len(posteriors)))\n",
    "\n",
    "print(\"Information Gained from Data (KL from Prior to Posterior)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for (mu, sigma, label), color in zip(posteriors, colors):\n",
    "    posterior_pdf = stats.norm.pdf(x, mu, sigma)\n",
    "    kl = kl_gaussian(mu, sigma, mu_prior, sigma_prior)\n",
    "    \n",
    "    ax.plot(x, posterior_pdf, linewidth=2, color=color, \n",
    "            label=f'{label}: KL = {kl:.2f} nats')\n",
    "    \n",
    "    print(f\"{label}: Posterior N({mu}, {sigma}Â²) â†’ KL = {kl:.3f} nats = {kl/np.log(2):.3f} bits\")\n",
    "\n",
    "ax.axvline(x=1, color='green', linestyle=':', alpha=0.7, label='True value = 1')\n",
    "ax.set_xlabel('Parameter value')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Bayesian Learning: KL Divergence = Information Gained\\n(More data â†’ more information â†’ higher KL from prior)')\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_xlim(-8, 8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ”‘ Key Insight:\")\n",
    "print(\"KL divergence from prior to posterior = information gained from data\")\n",
    "print(\"More data â†’ narrower posterior â†’ larger KL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Maximum Likelihood = Minimizing Forward KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate that ML = minimizing D_KL(P_data || Q_model)\n",
    "\n",
    "# True data distribution (unknown to model)\n",
    "true_mu, true_sigma = 2.0, 1.5\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(42)\n",
    "data = np.random.normal(true_mu, true_sigma, 1000)\n",
    "\n",
    "# Fit Gaussian by maximum likelihood\n",
    "ml_mu = np.mean(data)\n",
    "ml_sigma = np.std(data, ddof=0)  # ML estimate (not unbiased)\n",
    "\n",
    "# Compare different model parameters\n",
    "mu_range = np.linspace(0, 4, 50)\n",
    "sigma_range = np.linspace(0.5, 3, 50)\n",
    "\n",
    "def neg_log_likelihood(mu, sigma, data):\n",
    "    \"\"\"Negative log-likelihood (equivalent to cross-entropy).\"\"\"\n",
    "    return -np.mean(stats.norm.logpdf(data, mu, sigma))\n",
    "\n",
    "# Compute NLL surface\n",
    "NLL = np.zeros((len(sigma_range), len(mu_range)))\n",
    "for i, sigma in enumerate(sigma_range):\n",
    "    for j, mu in enumerate(mu_range):\n",
    "        NLL[i, j] = neg_log_likelihood(mu, sigma, data)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# NLL surface\n",
    "ax = axes[0]\n",
    "MU, SIGMA = np.meshgrid(mu_range, sigma_range)\n",
    "contour = ax.contourf(MU, SIGMA, NLL, levels=30, cmap='viridis')\n",
    "plt.colorbar(contour, ax=ax, label='Negative Log-Likelihood')\n",
    "ax.plot(ml_mu, ml_sigma, 'r*', markersize=15, label=f'ML estimate ({ml_mu:.2f}, {ml_sigma:.2f})')\n",
    "ax.plot(true_mu, true_sigma, 'g^', markersize=12, label=f'True params ({true_mu}, {true_sigma})')\n",
    "ax.set_xlabel('Î¼')\n",
    "ax.set_ylabel('Ïƒ')\n",
    "ax.set_title('Negative Log-Likelihood Surface\\n(Minimizing NLL = Minimizing KL)')\n",
    "ax.legend()\n",
    "\n",
    "# Show distributions\n",
    "ax = axes[1]\n",
    "x = np.linspace(-3, 7, 200)\n",
    "\n",
    "ax.hist(data, bins=50, density=True, alpha=0.5, color='gray', label='Data')\n",
    "ax.plot(x, stats.norm.pdf(x, true_mu, true_sigma), 'g-', linewidth=2, label=f'True: N({true_mu}, {true_sigma}Â²)')\n",
    "ax.plot(x, stats.norm.pdf(x, ml_mu, ml_sigma), 'r--', linewidth=2, label=f'ML fit: N({ml_mu:.2f}, {ml_sigma:.2f}Â²)')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Maximum Likelihood Fit')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTrue parameters: Î¼ = {true_mu}, Ïƒ = {true_sigma}\")\n",
    "print(f\"ML estimates:    Î¼ = {ml_mu:.4f}, Ïƒ = {ml_sigma:.4f}\")\n",
    "print(f\"\\nMaximum Likelihood finds parameters that minimize KL(P_data || Q_model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Formulas\n",
    "\n",
    "| Distribution | KL Divergence Formula |\n",
    "|-------------|----------------------|\n",
    "| Discrete | $D_{KL}(P \\| Q) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)}$ |\n",
    "| Continuous | $D_{KL}(P \\| Q) = \\int p(x) \\log \\frac{p(x)}{q(x)} dx$ |\n",
    "| Gaussians | $\\log\\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}$ |\n",
    "| VAE (to N(0,1)) | $\\frac{1}{2}\\sum_j (\\mu_j^2 + \\sigma_j^2 - 1 - \\log\\sigma_j^2)$ |\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **KL is asymmetric**: Forward vs reverse KL give very different results\n",
    "2. **Forward KL** (ML): Mean-seeking, covers all modes\n",
    "3. **Reverse KL** (VI): Mode-seeking, can ignore modes\n",
    "4. **VAE KL term**: Regularizes latent space toward standard normal\n",
    "5. **Information view**: KL = information gained when updating beliefs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
