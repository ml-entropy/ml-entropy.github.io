{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 15: Cross-Entropy â€” Code & Analysis\n",
    "\n",
    "This notebook provides an in-depth exploration of Cross-Entropy, including:\n",
    "1.  **Numerical Stability**: Why naive implementations fail and how to fix them (LogSumExp).\n",
    "2.  **Gradient Analysis**: Why Cross-Entropy beats Mean Squared Error (MSE) for classification.\n",
    "3.  **Focal Loss**: How to modify Cross-Entropy to handle class imbalance.\n",
    "4.  **Softmax Temperature**: How temperature affects prediction confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Numerical Stability (LogSumExp)\n",
    "\n",
    "A naive implementation of Softmax + Cross-Entropy often leads to numerical instability (overflow/underflow). \n",
    "\n",
    "$$ \\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}} $$\n",
    "\n",
    "If $x_i$ is large (e.g., 1000), $e^{1000}$ overflows. If $x_i$ is small negative (e.g., -1000), it underflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_softmax(logits):\n",
    "    return np.exp(logits) / np.sum(np.exp(logits))\n",
    "\n",
    "def stable_softmax(logits):\n",
    "    # LogSumExp trick: shift logits by max value\n",
    "    # e^(x-c) / sum(e^(x-c)) == e^x / sum(e^x)\n",
    "    c = np.max(logits)\n",
    "    exp_logits = np.exp(logits - c)\n",
    "    return exp_logits / np.sum(exp_logits)\n",
    "\n",
    "# Test with large values\n",
    "logits_large = np.array([1000.0, 1001.0, 1002.0])\n",
    "\n",
    "print(\"Naive Softmax (Large Input):\")\n",
    "try:\n",
    "    print(naive_softmax(logits_large))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "print(\"\\nStable Softmax (Large Input):\")\n",
    "print(stable_softmax(logits_large))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualizing BCE vs. MSE Loss\n",
    "\n",
    "Why don't we use MSE for classification? Let's visualize the loss surface and the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probabilities for the correct class (where true label y=1)\n",
    "p_hat = np.linspace(0.001, 0.999, 500)\n",
    "\n",
    "# Binary Cross-Entropy Loss for y=1 is -log(p_hat)\n",
    "bce_loss = -np.log(p_hat)\n",
    "\n",
    "# Mean Squared Error Loss for y=1 is (1-p_hat)^2\n",
    "mse_loss = (1 - p_hat)**2\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(p_hat, bce_loss, label='Binary Cross-Entropy Loss', color='#d62728', lw=3)\n",
    "plt.plot(p_hat, mse_loss, label='Mean Squared Error Loss', color='#1f77b4', lw=3, linestyle='--')\n",
    "\n",
    "plt.xlabel(\"Predicted Probability for Correct Class (p\u0302)\", fontsize=12)\n",
    "plt.ylabel(\"Loss Value\", fontsize=12)\n",
    "plt.title(\"BCE vs. MSE Loss (When True Label is 1)\", fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.ylim(0, 5) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Magnitude Analysis\n",
    "\n",
    "Models learn via gradients. A vanishing gradient means no learning. \n",
    "\n",
    "Let $p = \\sigma(z)$. \n",
    "-   Gradient of BCE w.r.t logits $z$: $|p - y|$\n",
    "-   Gradient of MSE w.r.t logits $z$: $|(p - y)p(1-p)|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "z = np.linspace(-10, 10, 500)\n",
    "p = sigmoid(z)\n",
    "\n",
    "# Gradient of BCE w.r.t z (target y=1)\n",
    "# dL/dz = p - 1\n",
    "grad_bce = np.abs(p - 1)\n",
    "\n",
    "# Gradient of MSE w.r.t z (target y=1)\n",
    "# dL/dz = -2(1-p) * p(1-p)\n",
    "grad_mse = np.abs(-2 * (1 - p) * p * (1 - p))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(z, grad_bce, label='Gradient of BCE', color='#d62728', lw=3)\n",
    "plt.plot(z, grad_mse, label='Gradient of MSE', color='#1f77b4', lw=3, linestyle='--')\n",
    "\n",
    "plt.xlabel(\"Logit input (z)\", fontsize=12)\n",
    "plt.ylabel(\"Gradient Magnitude |dL/dz|\", fontsize=12)\n",
    "plt.title(\"Gradient Strength: BCE vs MSE\", fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Highlight the \"Confidently Wrong\" area\n",
    "plt.axvspan(-10, -5, color='gray', alpha=0.15)\n",
    "plt.text(-7.5, 0.5, \"Confidently Wrong\\n(Vanishing MSE Grad)\", fontsize=11, ha='center', fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Focal Loss: Handling Imbalance\n",
    "\n",
    "When you have many easy examples (background) and few hard examples (objects), standard Cross-Entropy can be overwhelmed by the easy negatives.\n",
    "\n",
    "**Focal Loss** adds a factor $(1 - p_t)^\\gamma$ to down-weight easy examples.\n",
    "\n",
    "$$ FL(p_t) = -(1 - p_t)^\\gamma \\log(p_t) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_t = np.linspace(0.001, 0.999, 500)\n",
    "ce_loss = -np.log(p_t)\n",
    "\n",
    "# Focal Loss with different gammas\n",
    "gamma_0 = ce_loss # Gamma=0 is just CE\n",
    "gamma_1 = -((1 - p_t)**1) * np.log(p_t)\n",
    "gamma_2 = -((1 - p_t)**2) * np.log(p_t)\n",
    "gamma_5 = -((1 - p_t)**5) * np.log(p_t)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(p_t, gamma_0, label='Cross Entropy (Gamma=0)', color='black', linestyle='--', lw=2)\n",
    "plt.plot(p_t, gamma_1, label='Focal Loss (Gamma=1)', color='#fdae61', lw=2.5)\n",
    "plt.plot(p_t, gamma_2, label='Focal Loss (Gamma=2)', color='#d7191c', lw=2.5)\n",
    "plt.plot(p_t, gamma_5, label='Focal Loss (Gamma=5)', color='#2c7bb6', lw=2.5)\n",
    "\n",
    "plt.xlabel(\"Probability of Ground Truth Class (p_t)\", fontsize=12)\n",
    "plt.ylabel(\"Loss\", fontsize=12)\n",
    "plt.title(\"Focal Loss vs Cross Entropy\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.5)\n",
    "plt.ylim(0, 5)\n",
    "\n",
    "plt.text(0.6, 2.0, \"Easy Examples (High p_t)\\nare down-weighted\", fontsize=11, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PyTorch: Categorical vs Sparse\n",
    "\n",
    "A common source of confusion is the difference between `CrossEntropyLoss` (which expects class indices, essentially \"Sparse\") and custom implementations needing one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Standard PyTorch CrossEntropyLoss\n",
    "# Expects: Logits (N, C) and Target Indices (N)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "logits = torch.tensor([[2.0, 1.0, 0.1], [0.5, 2.5, 0.3]]) # 2 samples, 3 classes\n",
    "targets = torch.tensor([0, 1]) # Class 0 for first, Class 1 for second\n",
    "\n",
    "loss = criterion(logits, targets)\n",
    "print(f\"Standard Loss (indices): {loss.item():.4f}\")\n",
    "\n",
    "# 2. If you have One-Hot vectors (e.g. from Mixup augmentation)\n",
    "# You strictly speaking need to implement it manually or use BCEWithLogitsLoss if binary.\n",
    "# But usually, just use indices.\n",
    "\n",
    "# Let's verify manually for the first sample:\n",
    "# Logits: [2.0, 1.0, 0.1], True: 0\n",
    "probs = F.softmax(logits[0], dim=0)\n",
    "manual_loss = -torch.log(probs[0])\n",
    "print(f\"Manual Check: {manual_loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
