{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 12: Convolutional Neural Networks\n",
    "\n",
    "Implementing convolutions from scratch and understanding what CNNs learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Convolution from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_naive(image, kernel, stride=1, padding=0):\n",
    "    \"\"\"\n",
    "    2D convolution (naive implementation for understanding).\n",
    "    \n",
    "    Args:\n",
    "        image: Input image, shape (H, W)\n",
    "        kernel: Convolution kernel, shape (K, K)\n",
    "        stride: Step size\n",
    "        padding: Zero-padding size\n",
    "    \n",
    "    Returns:\n",
    "        Output feature map\n",
    "    \"\"\"\n",
    "    # Pad image\n",
    "    if padding > 0:\n",
    "        image = np.pad(image, padding, mode='constant', constant_values=0)\n",
    "    \n",
    "    H, W = image.shape\n",
    "    K = kernel.shape[0]\n",
    "    \n",
    "    # Output size\n",
    "    H_out = (H - K) // stride + 1\n",
    "    W_out = (W - K) // stride + 1\n",
    "    \n",
    "    output = np.zeros((H_out, W_out))\n",
    "    \n",
    "    for i in range(H_out):\n",
    "        for j in range(W_out):\n",
    "            # Extract patch\n",
    "            patch = image[i*stride:i*stride+K, j*stride:j*stride+K]\n",
    "            # Convolve (element-wise multiply and sum)\n",
    "            output[i, j] = np.sum(patch * kernel)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Test with edge detection kernels\n",
    "# Create a simple image with edges\n",
    "image = np.zeros((10, 10))\n",
    "image[2:8, 2:8] = 1  # White square in center\n",
    "\n",
    "# Common edge detection kernels\n",
    "kernels = {\n",
    "    'Horizontal Edge': np.array([[-1, -1, -1],\n",
    "                                  [ 0,  0,  0],\n",
    "                                  [ 1,  1,  1]]),\n",
    "    'Vertical Edge': np.array([[-1, 0, 1],\n",
    "                                [-1, 0, 1],\n",
    "                                [-1, 0, 1]]),\n",
    "    'Sobel X': np.array([[-1, 0, 1],\n",
    "                         [-2, 0, 2],\n",
    "                         [-1, 0, 1]]),\n",
    "    'Laplacian': np.array([[ 0, -1,  0],\n",
    "                           [-1,  4, -1],\n",
    "                           [ 0, -1,  0]])\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "\n",
    "# Original image\n",
    "axes[0, 0].imshow(image, cmap='gray')\n",
    "axes[0, 0].set_title('Original Image')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Apply kernels\n",
    "for ax, (name, kernel) in zip(axes.flatten()[1:], kernels.items()):\n",
    "    output = conv2d_naive(image, kernel, padding=1)\n",
    "    ax.imshow(output, cmap='gray')\n",
    "    ax.set_title(f'{name}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Each kernel detects different features (edges, corners, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Verify Against PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare our implementation with PyTorch\n",
    "image_np = np.random.randn(5, 5)\n",
    "kernel_np = np.random.randn(3, 3)\n",
    "\n",
    "# Our implementation\n",
    "our_output = conv2d_naive(image_np, kernel_np, padding=1)\n",
    "\n",
    "# PyTorch\n",
    "image_torch = torch.tensor(image_np).float().unsqueeze(0).unsqueeze(0)  # (1, 1, H, W)\n",
    "kernel_torch = torch.tensor(kernel_np).float().unsqueeze(0).unsqueeze(0)  # (1, 1, K, K)\n",
    "pytorch_output = F.conv2d(image_torch, kernel_torch, padding=1).squeeze().numpy()\n",
    "\n",
    "print(\"Our output shape:\", our_output.shape)\n",
    "print(\"PyTorch output shape:\", pytorch_output.shape)\n",
    "print(f\"Max difference: {np.abs(our_output - pytorch_output).max():.2e}\")\n",
    "print(\"✓ Implementations match!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Receptive Field Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_receptive_field(layers):\n",
    "    \"\"\"\n",
    "    Compute receptive field size for stacked conv layers.\n",
    "    \n",
    "    layers: list of (kernel_size, stride) tuples\n",
    "    \"\"\"\n",
    "    rf = 1  # Start with single pixel\n",
    "    stride_product = 1\n",
    "    \n",
    "    for k, s in layers:\n",
    "        rf = rf + (k - 1) * stride_product\n",
    "        stride_product *= s\n",
    "    \n",
    "    return rf\n",
    "\n",
    "# Compare different architectures\n",
    "architectures = {\n",
    "    'One 7x7': [(7, 1)],\n",
    "    'Three 3x3': [(3, 1), (3, 1), (3, 1)],\n",
    "    'Two 5x5': [(5, 1), (5, 1)],\n",
    "    'VGG-style (3x3s + pool)': [(3, 1), (3, 1), (2, 2), (3, 1), (3, 1), (2, 2)],\n",
    "}\n",
    "\n",
    "print(\"Receptive Field Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "for name, layers in architectures.items():\n",
    "    rf = compute_receptive_field(layers)\n",
    "    params = sum(k*k for k, s in layers if s == 1)  # Ignoring pooling\n",
    "    print(f\"{name:30s}: RF = {rf:3d}, params ∝ {params}\")\n",
    "\n",
    "print(\"\\nInsight: Three 3x3 convs have same RF as one 7x7, but fewer parameters!\")\n",
    "print(\"7x7 = 49 params, 3×(3×3) = 27 params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Build and Train a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Conv layers\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Pooling\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Fully connected\n",
    "        self.fc1 = nn.Linear(64 * 3 * 3, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Conv blocks\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # 28 -> 14\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # 14 -> 7\n",
    "        x = self.pool(F.relu(self.conv3(x)))  # 7 -> 3\n",
    "        \n",
    "        # Flatten and FC\n",
    "        x = x.view(-1, 64 * 3 * 3)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Load MNIST\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000)\n",
    "\n",
    "# Train\n",
    "model = SimpleCNN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "train_losses = []\n",
    "test_accs = []\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    # Test accuracy\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            out = model(x)\n",
    "            correct += (out.argmax(1) == y).sum().item()\n",
    "            total += len(y)\n",
    "    \n",
    "    train_losses.append(epoch_loss / len(train_loader))\n",
    "    test_accs.append(correct / total)\n",
    "    print(f\"Epoch {epoch+1}: Loss={train_losses[-1]:.4f}, Test Acc={test_accs[-1]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Visualize Learned Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first layer filters\n",
    "filters = model.conv1.weight.detach().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < filters.shape[0]:\n",
    "        ax.imshow(filters[i, 0], cmap='gray')\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.suptitle('Learned Conv1 Filters (3x3)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"First layer learns edge-like and blob-like patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature maps for a sample\n",
    "sample_image, label = test_dataset[0]\n",
    "\n",
    "# Get activations\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x = sample_image.unsqueeze(0)\n",
    "    act1 = F.relu(model.conv1(x))\n",
    "    act2 = F.relu(model.conv2(model.pool(act1)))\n",
    "    act3 = F.relu(model.conv3(model.pool(act2)))\n",
    "\n",
    "fig, axes = plt.subplots(3, 6, figsize=(15, 8))\n",
    "\n",
    "# Original\n",
    "axes[0, 0].imshow(sample_image.squeeze(), cmap='gray')\n",
    "axes[0, 0].set_title(f'Input (label={label})')\n",
    "axes[0, 0].axis('off')\n",
    "for ax in axes[0, 1:]:\n",
    "    ax.axis('off')\n",
    "\n",
    "# Conv1 activations\n",
    "for i in range(5):\n",
    "    axes[1, i].imshow(act1[0, i].numpy(), cmap='viridis')\n",
    "    axes[1, i].set_title(f'Conv1 ch{i}')\n",
    "    axes[1, i].axis('off')\n",
    "axes[1, 5].axis('off')\n",
    "\n",
    "# Conv2 activations\n",
    "for i in range(5):\n",
    "    axes[2, i].imshow(act2[0, i].numpy(), cmap='viridis')\n",
    "    axes[2, i].set_title(f'Conv2 ch{i}')\n",
    "    axes[2, i].axis('off')\n",
    "axes[2, 5].axis('off')\n",
    "\n",
    "plt.suptitle('Feature Maps Through the Network', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Deeper layers have smaller spatial size but detect more complex patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Key insights:**\n",
    "1. **Convolution** = sliding dot product, detects local patterns\n",
    "2. **Weight sharing** = same filter everywhere = translation invariance\n",
    "3. **Pooling** = downsampling + invariance\n",
    "4. **Hierarchy** = edges → textures → parts → objects\n",
    "5. **Receptive field** grows with depth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
