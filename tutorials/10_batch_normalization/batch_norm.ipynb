{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 10: Batch Normalization\n",
    "\n",
    "This notebook implements BatchNorm from scratch and visualizes its effects on training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: BatchNorm from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm1D:\n",
    "    \"\"\"Batch Normalization implemented from scratch\"\"\"\n",
    "    \n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        # Learnable parameters\n",
    "        self.gamma = np.ones(num_features)\n",
    "        self.beta = np.zeros(num_features)\n",
    "        \n",
    "        # Running statistics for inference\n",
    "        self.running_mean = np.zeros(num_features)\n",
    "        self.running_var = np.ones(num_features)\n",
    "        \n",
    "        # Cache for backward pass\n",
    "        self.cache = None\n",
    "        self.training = True\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        x: input, shape (batch_size, num_features)\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            # Batch statistics\n",
    "            mu = np.mean(x, axis=0)\n",
    "            var = np.var(x, axis=0)\n",
    "            \n",
    "            # Update running statistics\n",
    "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mu\n",
    "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var\n",
    "        else:\n",
    "            mu = self.running_mean\n",
    "            var = self.running_var\n",
    "        \n",
    "        # Normalize\n",
    "        std = np.sqrt(var + self.eps)\n",
    "        x_norm = (x - mu) / std\n",
    "        \n",
    "        # Scale and shift\n",
    "        out = self.gamma * x_norm + self.beta\n",
    "        \n",
    "        # Cache for backward\n",
    "        self.cache = (x, x_norm, mu, var, std)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        Backward pass.\n",
    "        dout: upstream gradient, shape (batch_size, num_features)\n",
    "        \"\"\"\n",
    "        x, x_norm, mu, var, std = self.cache\n",
    "        m = x.shape[0]\n",
    "        \n",
    "        # Gradients for gamma and beta\n",
    "        self.dgamma = np.sum(dout * x_norm, axis=0)\n",
    "        self.dbeta = np.sum(dout, axis=0)\n",
    "        \n",
    "        # Gradient for x (the complex part!)\n",
    "        dx_norm = dout * self.gamma\n",
    "        \n",
    "        # Using the derived formula from theory\n",
    "        dx = (1 / (m * std)) * (m * dx_norm - np.sum(dx_norm, axis=0) \n",
    "                                 - x_norm * np.sum(dx_norm * x_norm, axis=0))\n",
    "        \n",
    "        return dx\n",
    "\n",
    "# Test our implementation\n",
    "bn = BatchNorm1D(4)\n",
    "x = np.random.randn(32, 4) * 5 + 3  # Non-zero mean, non-unit variance\n",
    "out = bn.forward(x)\n",
    "\n",
    "print(\"Input statistics:\")\n",
    "print(f\"  Mean: {x.mean(axis=0)}\")\n",
    "print(f\"  Var:  {x.var(axis=0)}\")\n",
    "print(\"\\nOutput statistics (should be ~0 mean, ~1 var):\")\n",
    "print(f\"  Mean: {out.mean(axis=0)}\")\n",
    "print(f\"  Var:  {out.var(axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Effect on Activation Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create networks with and without BatchNorm\n",
    "class DeepNetNoBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(784, 256), nn.Sigmoid(),\n",
    "            nn.Linear(256, 256), nn.Sigmoid(),\n",
    "            nn.Linear(256, 256), nn.Sigmoid(),\n",
    "            nn.Linear(256, 256), nn.Sigmoid(),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class DeepNetWithBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(784, 256), nn.BatchNorm1d(256), nn.Sigmoid(),\n",
    "            nn.Linear(256, 256), nn.BatchNorm1d(256), nn.Sigmoid(),\n",
    "            nn.Linear(256, 256), nn.BatchNorm1d(256), nn.Sigmoid(),\n",
    "            nn.Linear(256, 256), nn.BatchNorm1d(256), nn.Sigmoid(),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Visualize activations\n",
    "def get_activations(model, x):\n",
    "    \"\"\"Get activations at each layer\"\"\"\n",
    "    activations = []\n",
    "    for layer in model.layers:\n",
    "        x = layer(x)\n",
    "        if isinstance(layer, nn.Sigmoid):\n",
    "            activations.append(x.detach().numpy().flatten())\n",
    "    return activations\n",
    "\n",
    "# Random input\n",
    "x = torch.randn(100, 784)\n",
    "\n",
    "model_no_bn = DeepNetNoBN()\n",
    "model_with_bn = DeepNetWithBN()\n",
    "\n",
    "act_no_bn = get_activations(model_no_bn, x)\n",
    "model_with_bn.eval()  # Use running stats\n",
    "act_with_bn = get_activations(model_with_bn, x)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for i, (ax, act) in enumerate(zip(axes[0], act_no_bn)):\n",
    "    ax.hist(act, bins=50, alpha=0.7)\n",
    "    ax.set_title(f'Layer {i+1} (No BN)\\nmean={act.mean():.2f}, std={act.std():.2f}')\n",
    "    ax.set_xlim(0, 1)\n",
    "\n",
    "for i, (ax, act) in enumerate(zip(axes[1], act_with_bn)):\n",
    "    ax.hist(act, bins=50, alpha=0.7, color='green')\n",
    "    ax.set_title(f'Layer {i+1} (With BN)\\nmean={act.mean():.2f}, std={act.std():.2f}')\n",
    "    ax.set_xlim(0, 1)\n",
    "\n",
    "plt.suptitle('Activation Distributions: Without vs With BatchNorm', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Without BN: Activations saturate (all near 0 or 1) → vanishing gradients!\")\n",
    "print(\"With BN: Activations stay in useful range → healthy gradients!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Training Speed Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for x, y in loader:\n",
    "        x = x.view(-1, 784)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        correct += (out.argmax(1) == y).sum().item()\n",
    "        total += len(y)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "# Train both models\n",
    "epochs = 10\n",
    "lr = 0.1  # Larger LR to show BN's benefit\n",
    "\n",
    "model_no_bn = DeepNetNoBN()\n",
    "model_with_bn = DeepNetWithBN()\n",
    "\n",
    "opt_no_bn = optim.SGD(model_no_bn.parameters(), lr=lr)\n",
    "opt_with_bn = optim.SGD(model_with_bn.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "history_no_bn = {'loss': [], 'acc': []}\n",
    "history_with_bn = {'loss': [], 'acc': []}\n",
    "\n",
    "print(\"Training with LR=0.1 (large!)...\")\n",
    "for epoch in range(epochs):\n",
    "    loss1, acc1 = train_epoch(model_no_bn, train_loader, opt_no_bn, criterion)\n",
    "    loss2, acc2 = train_epoch(model_with_bn, train_loader, opt_with_bn, criterion)\n",
    "    \n",
    "    history_no_bn['loss'].append(loss1)\n",
    "    history_no_bn['acc'].append(acc1)\n",
    "    history_with_bn['loss'].append(loss2)\n",
    "    history_with_bn['acc'].append(acc2)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: No BN acc={acc1:.2%}, With BN acc={acc2:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(history_no_bn['loss'], 'b-', label='Without BN', linewidth=2)\n",
    "axes[0].plot(history_with_bn['loss'], 'g-', label='With BN', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history_no_bn['acc'], 'b-', label='Without BN', linewidth=2)\n",
    "axes[1].plot(history_with_bn['acc'], 'g-', label='With BN', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBatchNorm enables training with much larger learning rates!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Normalization Variants Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize different normalization methods\n",
    "# Create a fake activation tensor: (batch=4, channels=3, height=2, width=2)\n",
    "x = torch.randn(4, 3, 2, 2) * 5 + 3\n",
    "\n",
    "# Different normalizations\n",
    "bn = nn.BatchNorm2d(3)(x)   # Normalize over batch, H, W\n",
    "ln = nn.LayerNorm([3, 2, 2])(x)  # Normalize over C, H, W\n",
    "In = nn.InstanceNorm2d(3)(x)  # Normalize over H, W per sample/channel\n",
    "gn = nn.GroupNorm(1, 3)(x)  # Normalize over groups of channels\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "titles = ['BatchNorm', 'LayerNorm', 'InstanceNorm', 'GroupNorm']\n",
    "outputs = [bn, ln, In, gn]\n",
    "\n",
    "for ax, title, out in zip(axes, titles, outputs):\n",
    "    ax.hist(out.detach().numpy().flatten(), bins=30, alpha=0.7)\n",
    "    ax.set_title(f'{title}\\nmean={out.mean():.3f}, std={out.std():.3f}')\n",
    "    ax.set_xlabel('Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**BatchNorm normalizes activations:**\n",
    "$$y = \\gamma \\cdot \\frac{x - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} + \\beta$$\n",
    "\n",
    "**Benefits:**\n",
    "1. Enables larger learning rates\n",
    "2. Reduces sensitivity to initialization\n",
    "3. Acts as regularization\n",
    "4. Keeps activations in high-gradient regime"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
