{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 06: Probability Concepts in ML\n",
    "\n",
    "Interactive visualizations for:\n",
    "- Joint, marginal, conditional probability\n",
    "- Bayes' theorem in action\n",
    "- Probability vs likelihood\n",
    "- Prior → Posterior updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.special import comb\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Joint, Marginal, and Conditional Probability\n",
    "\n",
    "Let's create a joint distribution and visualize all the relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a joint probability table P(X, Y)\n",
    "# X: 0 or 1 (e.g., has disease or not)\n",
    "# Y: 0 or 1 (e.g., tests negative or positive)\n",
    "\n",
    "# Joint probabilities (must sum to 1)\n",
    "P_joint = np.array([\n",
    "    [0.85, 0.05],   # X=0: P(X=0,Y=0)=0.85, P(X=0,Y=1)=0.05\n",
    "    [0.02, 0.08]    # X=1: P(X=1,Y=0)=0.02, P(X=1,Y=1)=0.08\n",
    "])\n",
    "\n",
    "print(\"Joint Probability Table P(X, Y):\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"              Y=0      Y=1     | P(X)\")\n",
    "print(f\"X=0          {P_joint[0,0]:.2f}     {P_joint[0,1]:.2f}    | {P_joint[0,:].sum():.2f}\")\n",
    "print(f\"X=1          {P_joint[1,0]:.2f}     {P_joint[1,1]:.2f}    | {P_joint[1,:].sum():.2f}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"P(Y)         {P_joint[:,0].sum():.2f}     {P_joint[:,1].sum():.2f}    | {P_joint.sum():.2f}\")\n",
    "\n",
    "# Compute marginals\n",
    "P_X = P_joint.sum(axis=1)  # Sum over Y\n",
    "P_Y = P_joint.sum(axis=0)  # Sum over X\n",
    "\n",
    "print(f\"\\nMarginal P(X): {P_X}\")\n",
    "print(f\"Marginal P(Y): {P_Y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute conditional probabilities\n",
    "# P(Y|X) = P(X,Y) / P(X)\n",
    "P_Y_given_X = P_joint / P_X[:, np.newaxis]\n",
    "\n",
    "# P(X|Y) = P(X,Y) / P(Y)\n",
    "P_X_given_Y = P_joint / P_Y[np.newaxis, :]\n",
    "\n",
    "print(\"Conditional P(Y|X):\")\n",
    "print(f\"P(Y=0|X=0) = {P_Y_given_X[0,0]:.4f}    P(Y=1|X=0) = {P_Y_given_X[0,1]:.4f}  (sums to {P_Y_given_X[0,:].sum():.1f})\")\n",
    "print(f\"P(Y=0|X=1) = {P_Y_given_X[1,0]:.4f}    P(Y=1|X=1) = {P_Y_given_X[1,1]:.4f}  (sums to {P_Y_given_X[1,:].sum():.1f})\")\n",
    "\n",
    "print(\"\\nConditional P(X|Y):\")\n",
    "print(f\"P(X=0|Y=0) = {P_X_given_Y[0,0]:.4f}    P(X=1|Y=0) = {P_X_given_Y[1,0]:.4f}  (sums to {P_X_given_Y[:,0].sum():.1f})\")\n",
    "print(f\"P(X=0|Y=1) = {P_X_given_Y[0,1]:.4f}    P(X=1|Y=1) = {P_X_given_Y[1,1]:.4f}  (sums to {P_X_given_Y[:,1].sum():.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize joint, marginal, conditional\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Joint probability\n",
    "im = axes[0, 0].imshow(P_joint, cmap='Blues', vmin=0, vmax=1)\n",
    "axes[0, 0].set_xticks([0, 1])\n",
    "axes[0, 0].set_xticklabels(['Y=0', 'Y=1'])\n",
    "axes[0, 0].set_yticks([0, 1])\n",
    "axes[0, 0].set_yticklabels(['X=0', 'X=1'])\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[0, 0].text(j, i, f'{P_joint[i,j]:.2f}', ha='center', va='center', fontsize=14)\n",
    "axes[0, 0].set_title('Joint P(X, Y)', fontsize=14)\n",
    "plt.colorbar(im, ax=axes[0, 0])\n",
    "\n",
    "# Marginals\n",
    "x_pos = np.array([0, 1])\n",
    "width = 0.35\n",
    "axes[0, 1].bar(x_pos - width/2, P_X, width, label='P(X)', color='steelblue')\n",
    "axes[0, 1].bar(x_pos + width/2, P_Y, width, label='P(Y)', color='coral')\n",
    "axes[0, 1].set_xticks([0, 1])\n",
    "axes[0, 1].set_xticklabels(['0', '1'])\n",
    "axes[0, 1].set_ylabel('Probability', fontsize=12)\n",
    "axes[0, 1].set_title('Marginal Probabilities', fontsize=14)\n",
    "axes[0, 1].legend(fontsize=12)\n",
    "axes[0, 1].set_ylim(0, 1)\n",
    "\n",
    "# P(Y|X)\n",
    "im2 = axes[1, 0].imshow(P_Y_given_X, cmap='Greens', vmin=0, vmax=1)\n",
    "axes[1, 0].set_xticks([0, 1])\n",
    "axes[1, 0].set_xticklabels(['Y=0', 'Y=1'])\n",
    "axes[1, 0].set_yticks([0, 1])\n",
    "axes[1, 0].set_yticklabels(['X=0', 'X=1'])\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[1, 0].text(j, i, f'{P_Y_given_X[i,j]:.2f}', ha='center', va='center', fontsize=14)\n",
    "axes[1, 0].set_title('Conditional P(Y|X)\\n(each row sums to 1)', fontsize=14)\n",
    "plt.colorbar(im2, ax=axes[1, 0])\n",
    "\n",
    "# P(X|Y)\n",
    "im3 = axes[1, 1].imshow(P_X_given_Y, cmap='Oranges', vmin=0, vmax=1)\n",
    "axes[1, 1].set_xticks([0, 1])\n",
    "axes[1, 1].set_xticklabels(['Y=0', 'Y=1'])\n",
    "axes[1, 1].set_yticks([0, 1])\n",
    "axes[1, 1].set_yticklabels(['X=0', 'X=1'])\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[1, 1].text(j, i, f'{P_X_given_Y[i,j]:.2f}', ha='center', va='center', fontsize=14)\n",
    "axes[1, 1].set_title('Conditional P(X|Y)\\n(each column sums to 1)', fontsize=14)\n",
    "plt.colorbar(im3, ax=axes[1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Bayes' Theorem in Action\n",
    "\n",
    "The classic medical diagnosis example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_medical_diagnosis(prevalence, sensitivity, specificity):\n",
    "    \"\"\"\n",
    "    Compute P(Disease | Positive Test) using Bayes' theorem.\n",
    "    \n",
    "    Args:\n",
    "        prevalence: P(D) - prior probability of disease\n",
    "        sensitivity: P(T+|D) - true positive rate\n",
    "        specificity: P(T-|not D) - true negative rate\n",
    "    \"\"\"\n",
    "    P_D = prevalence\n",
    "    P_not_D = 1 - prevalence\n",
    "    P_pos_given_D = sensitivity\n",
    "    P_pos_given_not_D = 1 - specificity  # False positive rate\n",
    "    \n",
    "    # Law of total probability: P(T+)\n",
    "    P_pos = P_pos_given_D * P_D + P_pos_given_not_D * P_not_D\n",
    "    \n",
    "    # Bayes' theorem: P(D|T+)\n",
    "    P_D_given_pos = (P_pos_given_D * P_D) / P_pos\n",
    "    \n",
    "    return P_D_given_pos, P_pos\n",
    "\n",
    "# Example: Rare disease\n",
    "prevalence = 0.01  # 1% have disease\n",
    "sensitivity = 0.99  # 99% true positive rate\n",
    "specificity = 0.95  # 95% true negative rate (5% false positive)\n",
    "\n",
    "P_D_given_pos, P_pos = bayes_medical_diagnosis(prevalence, sensitivity, specificity)\n",
    "\n",
    "print(\"Medical Diagnosis with Bayes' Theorem\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Prior P(Disease) = {prevalence:.2%}\")\n",
    "print(f\"Sensitivity P(T+|D) = {sensitivity:.2%}\")\n",
    "print(f\"Specificity P(T-|¬D) = {specificity:.2%}\")\n",
    "print(f\"\\nP(Positive Test) = {P_pos:.2%}\")\n",
    "print(f\"\\n>>> P(Disease | Positive Test) = {P_D_given_pos:.2%} <<<\")\n",
    "print(f\"\\nSurprising! Despite 99% sensitivity, only {P_D_given_pos:.1%} chance of disease!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how posterior varies with prevalence\n",
    "prevalences = np.linspace(0.001, 0.5, 100)\n",
    "posteriors = [bayes_medical_diagnosis(p, 0.99, 0.95)[0] for p in prevalences]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Posterior vs prevalence\n",
    "axes[0].plot(prevalences * 100, np.array(posteriors) * 100, 'b-', linewidth=2)\n",
    "axes[0].axhline(y=50, color='gray', linestyle='--', alpha=0.5, label='50% threshold')\n",
    "axes[0].axvline(x=1, color='red', linestyle='--', alpha=0.5, label='1% prevalence')\n",
    "axes[0].set_xlabel('Disease Prevalence (%)', fontsize=12)\n",
    "axes[0].set_ylabel('P(Disease | Positive Test) (%)', fontsize=12)\n",
    "axes[0].set_title('Posterior Probability vs Disease Prevalence\\n(Sensitivity=99%, Specificity=95%)', fontsize=14)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Population breakdown\n",
    "N = 10000\n",
    "n_disease = int(N * prevalence)\n",
    "n_healthy = N - n_disease\n",
    "\n",
    "true_positive = int(n_disease * sensitivity)\n",
    "false_negative = n_disease - true_positive\n",
    "false_positive = int(n_healthy * (1 - specificity))\n",
    "true_negative = n_healthy - false_positive\n",
    "\n",
    "categories = ['True Positive\\n(Disease, T+)', 'False Negative\\n(Disease, T-)', \n",
    "              'False Positive\\n(Healthy, T+)', 'True Negative\\n(Healthy, T-)']\n",
    "counts = [true_positive, false_negative, false_positive, true_negative]\n",
    "colors = ['darkred', 'salmon', 'orange', 'lightgreen']\n",
    "\n",
    "axes[1].bar(categories, counts, color=colors)\n",
    "axes[1].set_ylabel('Number of People', fontsize=12)\n",
    "axes[1].set_title(f'Population of {N:,} (Prevalence = {prevalence:.1%})', fontsize=14)\n",
    "\n",
    "# Add count labels\n",
    "for i, (cat, count) in enumerate(zip(categories, counts)):\n",
    "    axes[1].text(i, count + 100, f'{count:,}', ha='center', fontsize=11)\n",
    "\n",
    "# Highlight the positive tests\n",
    "total_positive = true_positive + false_positive\n",
    "axes[1].axhline(y=0, color='black', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOf {total_positive} positive tests:\")\n",
    "print(f\"  {true_positive} actually have disease ({true_positive/total_positive:.1%})\")\n",
    "print(f\"  {false_positive} are false positives ({false_positive/total_positive:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Probability vs Likelihood\n",
    "\n",
    "The same formula, viewed two different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binomial_prob(k, n, theta):\n",
    "    \"\"\"P(X=k | n, theta) - probability of k successes in n trials.\"\"\"\n",
    "    return comb(n, k, exact=True) * (theta ** k) * ((1-theta) ** (n-k))\n",
    "\n",
    "n = 10  # number of trials\n",
    "\n",
    "# PROBABILITY view: fixed theta, varying k\n",
    "theta_fixed = 0.7\n",
    "k_values = np.arange(0, n+1)\n",
    "probs = [binomial_prob(k, n, theta_fixed) for k in k_values]\n",
    "\n",
    "# LIKELIHOOD view: fixed k (observed data), varying theta\n",
    "k_observed = 7\n",
    "theta_values = np.linspace(0.01, 0.99, 100)\n",
    "likelihoods = [binomial_prob(k_observed, n, theta) for theta in theta_values]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Probability view\n",
    "axes[0].bar(k_values, probs, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].axvline(x=k_observed, color='red', linestyle='--', linewidth=2, label=f'Observed k={k_observed}')\n",
    "axes[0].set_xlabel('k (number of successes)', fontsize=12)\n",
    "axes[0].set_ylabel('P(X=k | θ=0.7)', fontsize=12)\n",
    "axes[0].set_title(f'PROBABILITY View\\nθ={theta_fixed} fixed, k varies\\n(sums to {sum(probs):.1f})', fontsize=14)\n",
    "axes[0].legend(fontsize=11)\n",
    "\n",
    "# Likelihood view\n",
    "axes[1].plot(theta_values, likelihoods, 'b-', linewidth=2)\n",
    "axes[1].fill_between(theta_values, likelihoods, alpha=0.3)\n",
    "axes[1].axvline(x=theta_fixed, color='red', linestyle='--', linewidth=2, label=f'MLE θ={k_observed/n}')\n",
    "axes[1].set_xlabel('θ (probability parameter)', fontsize=12)\n",
    "axes[1].set_ylabel(f'L(θ | k={k_observed})', fontsize=12)\n",
    "axes[1].set_title(f'LIKELIHOOD View\\nk={k_observed} fixed (observed), θ varies\\n(does NOT sum to 1!)', fontsize=14)\n",
    "axes[1].legend(fontsize=11)\n",
    "\n",
    "# Mark MLE\n",
    "mle_theta = k_observed / n\n",
    "mle_likelihood = binomial_prob(k_observed, n, mle_theta)\n",
    "axes[1].plot(mle_theta, mle_likelihood, 'ro', markersize=10)\n",
    "axes[1].annotate(f'MLE: θ={mle_theta}', xy=(mle_theta, mle_likelihood), \n",
    "                 xytext=(mle_theta+0.1, mle_likelihood+0.02), fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show they DON'T integrate to 1\n",
    "approx_integral = np.trapz(likelihoods, theta_values)\n",
    "print(f\"\\nKey difference:\")\n",
    "print(f\"  Probability (over k): sums to {sum(probs):.4f}\")\n",
    "print(f\"  Likelihood (over θ): integrates to {approx_integral:.4f} (NOT 1!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Prior → Posterior Updates (Bayesian Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beta-Binomial conjugate prior example\n",
    "# Prior: Beta(a, b)\n",
    "# Likelihood: Binomial(n, theta)\n",
    "# Posterior: Beta(a + k, b + n - k)\n",
    "\n",
    "def plot_bayesian_update(prior_a, prior_b, n_trials, n_successes):\n",
    "    \"\"\"Visualize Bayesian update from prior to posterior.\"\"\"\n",
    "    theta = np.linspace(0, 1, 1000)\n",
    "    \n",
    "    # Prior\n",
    "    prior = stats.beta.pdf(theta, prior_a, prior_b)\n",
    "    \n",
    "    # Likelihood (unnormalized)\n",
    "    likelihood = theta**n_successes * (1-theta)**(n_trials-n_successes)\n",
    "    likelihood = likelihood / likelihood.max()  # Normalize for visualization\n",
    "    \n",
    "    # Posterior\n",
    "    post_a = prior_a + n_successes\n",
    "    post_b = prior_b + n_trials - n_successes\n",
    "    posterior = stats.beta.pdf(theta, post_a, post_b)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    ax.plot(theta, prior, 'b-', linewidth=2, label=f'Prior: Beta({prior_a}, {prior_b})')\n",
    "    ax.plot(theta, likelihood * prior.max(), 'g--', linewidth=2, \n",
    "            label=f'Likelihood (scaled): {n_successes}/{n_trials} successes')\n",
    "    ax.plot(theta, posterior, 'r-', linewidth=3, label=f'Posterior: Beta({post_a}, {post_b})')\n",
    "    \n",
    "    ax.fill_between(theta, prior, alpha=0.2, color='blue')\n",
    "    ax.fill_between(theta, posterior, alpha=0.2, color='red')\n",
    "    \n",
    "    ax.set_xlabel('θ (probability parameter)', fontsize=12)\n",
    "    ax.set_ylabel('Density', fontsize=12)\n",
    "    ax.set_title('Bayesian Update: Prior × Likelihood ∝ Posterior', fontsize=14)\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.set_xlim(0, 1)\n",
    "    \n",
    "    # Add MLE and MAP estimates\n",
    "    mle = n_successes / n_trials if n_trials > 0 else 0.5\n",
    "    map_estimate = (post_a - 1) / (post_a + post_b - 2) if post_a > 1 and post_b > 1 else 0.5\n",
    "    \n",
    "    ax.axvline(x=mle, color='green', linestyle=':', linewidth=2, alpha=0.7)\n",
    "    ax.axvline(x=map_estimate, color='red', linestyle=':', linewidth=2, alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"MLE estimate: {mle:.3f}\")\n",
    "    print(f\"MAP estimate: {map_estimate:.3f}\")\n",
    "    print(f\"Posterior mean: {post_a / (post_a + post_b):.3f}\")\n",
    "\n",
    "# Example 1: Uniform prior, 7/10 successes\n",
    "print(\"Example 1: Uniform Prior (no prior knowledge)\")\n",
    "print(\"=\" * 50)\n",
    "plot_bayesian_update(prior_a=1, prior_b=1, n_trials=10, n_successes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Strong prior that coin is fair\n",
    "print(\"Example 2: Strong Prior (believe coin is fair)\")\n",
    "print(\"=\" * 50)\n",
    "plot_bayesian_update(prior_a=20, prior_b=20, n_trials=10, n_successes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: More data overwhelms prior\n",
    "print(\"Example 3: More Data (prior gets overwhelmed)\")\n",
    "print(\"=\" * 50)\n",
    "plot_bayesian_update(prior_a=20, prior_b=20, n_trials=100, n_successes=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Bayesian updates\n",
    "print(\"Sequential Bayesian Updates\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Start with uniform prior\n",
    "a, b = 1, 1\n",
    "theta = np.linspace(0, 1, 1000)\n",
    "\n",
    "# Simulate coin flips from a coin with true θ=0.65\n",
    "np.random.seed(42)\n",
    "true_theta = 0.65\n",
    "flips = np.random.binomial(1, true_theta, 50)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "checkpoints = [0, 1, 5, 10, 25, 50]  # After how many flips\n",
    "\n",
    "for idx, n in enumerate(checkpoints):\n",
    "    if n == 0:\n",
    "        current_a, current_b = 1, 1\n",
    "    else:\n",
    "        current_a = 1 + flips[:n].sum()\n",
    "        current_b = 1 + n - flips[:n].sum()\n",
    "    \n",
    "    posterior = stats.beta.pdf(theta, current_a, current_b)\n",
    "    \n",
    "    axes[idx].plot(theta, posterior, 'b-', linewidth=2)\n",
    "    axes[idx].fill_between(theta, posterior, alpha=0.3)\n",
    "    axes[idx].axvline(x=true_theta, color='red', linestyle='--', linewidth=2, label=f'True θ={true_theta}')\n",
    "    \n",
    "    if n > 0:\n",
    "        mean = current_a / (current_a + current_b)\n",
    "        axes[idx].axvline(x=mean, color='green', linestyle=':', linewidth=2, label=f'Mean={mean:.2f}')\n",
    "    \n",
    "    axes[idx].set_xlabel('θ', fontsize=11)\n",
    "    axes[idx].set_ylabel('Density', fontsize=11)\n",
    "    heads = flips[:n].sum() if n > 0 else 0\n",
    "    axes[idx].set_title(f'After {n} flips ({heads}H, {n-heads}T)\\nBeta({current_a}, {current_b})', fontsize=12)\n",
    "    axes[idx].legend(fontsize=9)\n",
    "    axes[idx].set_xlim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nWith more data, posterior concentrates around true θ={true_theta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: ML Losses as Negative Log-Likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show that MSE = NLL for Gaussian\n",
    "# If y|x ~ N(f(x), σ²), then -log p(y|x) ∝ (y - f(x))²\n",
    "\n",
    "y_true = 3.0\n",
    "predictions = np.linspace(0, 6, 100)\n",
    "sigma = 1.0\n",
    "\n",
    "# MSE loss\n",
    "mse = (predictions - y_true) ** 2\n",
    "\n",
    "# Negative log-likelihood (Gaussian)\n",
    "nll = 0.5 * (predictions - y_true)**2 / sigma**2 + 0.5 * np.log(2 * np.pi * sigma**2)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(predictions, mse, 'b-', linewidth=2, label='MSE Loss')\n",
    "axes[0].plot(predictions, nll, 'r--', linewidth=2, label='NLL (Gaussian)')\n",
    "axes[0].axvline(x=y_true, color='green', linestyle=':', linewidth=2, label=f'y_true={y_true}')\n",
    "axes[0].set_xlabel('Prediction', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('MSE = Negative Log-Likelihood (Gaussian noise)', fontsize=14)\n",
    "axes[0].legend(fontsize=11)\n",
    "\n",
    "# Cross-entropy for classification\n",
    "y_true_class = 1  # True class is 1\n",
    "p_pred = np.linspace(0.01, 0.99, 100)  # Predicted P(class=1)\n",
    "\n",
    "# Cross-entropy: -y*log(p) - (1-y)*log(1-p)\n",
    "# For y=1: -log(p)\n",
    "ce_loss = -np.log(p_pred)\n",
    "\n",
    "axes[1].plot(p_pred, ce_loss, 'b-', linewidth=2)\n",
    "axes[1].set_xlabel('Predicted P(y=1)', fontsize=12)\n",
    "axes[1].set_ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "axes[1].set_title('Cross-Entropy = NLL for Bernoulli\\n(True class = 1)', fontsize=14)\n",
    "axes[1].axvline(x=1, color='green', linestyle=':', linewidth=2, label='Optimal: predict p=1')\n",
    "axes[1].legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: Common loss functions ARE negative log-likelihoods!\")\n",
    "print(\"  • MSE Loss ↔ Gaussian noise assumption\")\n",
    "print(\"  • Cross-Entropy ↔ Bernoulli/Categorical assumption\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Concept | Formula | Interpretation |\n",
    "|---------|---------|----------------|\n",
    "| **Joint** | $P(x,y)$ | Probability of x AND y |\n",
    "| **Marginal** | $P(x) = \\sum_y P(x,y)$ | Sum out other variables |\n",
    "| **Conditional** | $P(x|y) = P(x,y)/P(y)$ | Restrict to world where y happened |\n",
    "| **Bayes** | $P(A|B) = P(B|A)P(A)/P(B)$ | Flip the conditioning |\n",
    "| **Probability** | $P(x|\\theta)$ as function of $x$ | What data might we see? |\n",
    "| **Likelihood** | $P(x|\\theta)$ as function of $\\theta$ | What parameters explain data? |\n",
    "| **Prior** | $P(\\theta)$ | Belief before seeing data |\n",
    "| **Posterior** | $P(\\theta|x) \\propto P(x|\\theta)P(\\theta)$ | Belief after seeing data |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
