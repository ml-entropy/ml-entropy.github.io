{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 13: Recurrent Neural Networks\n",
    "\n",
    "Implementing RNNs and LSTMs from scratch, understanding vanishing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Basic RNN from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN:\n",
    "    \"\"\"\n",
    "    Vanilla RNN implemented from scratch.\n",
    "    h_t = tanh(W_hh @ h_{t-1} + W_xh @ x_t + b_h)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Initialize weights (Xavier)\n",
    "        scale = np.sqrt(2.0 / (input_size + hidden_size))\n",
    "        self.W_xh = np.random.randn(hidden_size, input_size) * scale\n",
    "        self.W_hh = np.random.randn(hidden_size, hidden_size) * scale\n",
    "        self.b_h = np.zeros((hidden_size, 1))\n",
    "        \n",
    "    def forward(self, x, h_prev):\n",
    "        \"\"\"\n",
    "        Single step forward.\n",
    "        x: (input_size, 1)\n",
    "        h_prev: (hidden_size, 1)\n",
    "        \"\"\"\n",
    "        z = self.W_hh @ h_prev + self.W_xh @ x + self.b_h\n",
    "        h = np.tanh(z)\n",
    "        return h, z  # Return z for backward pass\n",
    "    \n",
    "    def forward_sequence(self, xs, h0=None):\n",
    "        \"\"\"\n",
    "        Process entire sequence.\n",
    "        xs: list of (input_size, 1) arrays\n",
    "        \"\"\"\n",
    "        if h0 is None:\n",
    "            h0 = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        hs = [h0]\n",
    "        zs = []\n",
    "        \n",
    "        h = h0\n",
    "        for x in xs:\n",
    "            h, z = self.forward(x, h)\n",
    "            hs.append(h)\n",
    "            zs.append(z)\n",
    "        \n",
    "        return hs, zs\n",
    "\n",
    "# Test\n",
    "rnn = SimpleRNN(input_size=4, hidden_size=8)\n",
    "\n",
    "# Random sequence of length 5\n",
    "sequence = [np.random.randn(4, 1) for _ in range(5)]\n",
    "hs, zs = rnn.forward_sequence(sequence)\n",
    "\n",
    "print(f\"Input sequence length: {len(sequence)}\")\n",
    "print(f\"Hidden states: {len(hs)} (including h0)\")\n",
    "print(f\"Final hidden state shape: {hs[-1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Visualize Vanishing Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_magnitude(W_hh, sequence_length):\n",
    "    \"\"\"\n",
    "    Compute how gradient magnitude changes over timesteps.\n",
    "    \n",
    "    For vanilla RNN, gradient at step t w.r.t. step 0 involves:\n",
    "    ∂h_t/∂h_0 = ∏_{i=1}^{t} W_hh * diag(1 - h_i²)\n",
    "    \n",
    "    Simplified: assume tanh'(z) ≈ 1 (near zero), so gradient ∝ W_hh^t\n",
    "    \"\"\"\n",
    "    eigenvalues = np.linalg.eigvals(W_hh)\n",
    "    max_eigenvalue = np.max(np.abs(eigenvalues))\n",
    "    \n",
    "    # Gradient magnitude over time\n",
    "    magnitudes = [max_eigenvalue ** t for t in range(sequence_length)]\n",
    "    return magnitudes, max_eigenvalue\n",
    "\n",
    "# Test with different weight initializations\n",
    "hidden_size = 100\n",
    "sequence_length = 50\n",
    "\n",
    "# Small weights (vanishing)\n",
    "W_small = np.random.randn(hidden_size, hidden_size) * 0.5 / np.sqrt(hidden_size)\n",
    "mag_small, eig_small = compute_gradient_magnitude(W_small, sequence_length)\n",
    "\n",
    "# Identity-like (stable)\n",
    "W_stable = np.eye(hidden_size) + np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "mag_stable, eig_stable = compute_gradient_magnitude(W_stable, sequence_length)\n",
    "\n",
    "# Large weights (exploding)\n",
    "W_large = np.random.randn(hidden_size, hidden_size) * 1.5 / np.sqrt(hidden_size)\n",
    "mag_large, eig_large = compute_gradient_magnitude(W_large, sequence_length)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.semilogy(mag_small, label=f'Small init (λ_max={eig_small:.2f})')\n",
    "plt.semilogy(mag_stable, label=f'Stable init (λ_max={eig_stable:.2f})')\n",
    "plt.semilogy(mag_large, label=f'Large init (λ_max={eig_large:.2f})')\n",
    "plt.xlabel('Timestep')\n",
    "plt.ylabel('Gradient magnitude (log scale)')\n",
    "plt.title('Gradient Flow Through Time')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Show eigenvalue distribution\n",
    "for W, name in [(W_small, 'Small'), (W_stable, 'Stable'), (W_large, 'Large')]:\n",
    "    eigs = np.linalg.eigvals(W)\n",
    "    plt.scatter(eigs.real, eigs.imag, alpha=0.5, label=name, s=10)\n",
    "circle = plt.Circle((0, 0), 1, fill=False, color='red', linestyle='--', label='Unit circle')\n",
    "plt.gca().add_patch(circle)\n",
    "plt.xlabel('Real')\n",
    "plt.ylabel('Imaginary')\n",
    "plt.title('Eigenvalue Distribution of W_hh')\n",
    "plt.legend()\n",
    "plt.axis('equal')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Eigenvalues inside unit circle → vanishing gradients\")\n",
    "print(\"Eigenvalues outside unit circle → exploding gradients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: LSTM from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "class SimpleLSTM:\n",
    "    \"\"\"\n",
    "    LSTM implemented from scratch.\n",
    "    \n",
    "    f_t = σ(W_f [h_{t-1}, x_t] + b_f)  # Forget gate\n",
    "    i_t = σ(W_i [h_{t-1}, x_t] + b_i)  # Input gate\n",
    "    c̃_t = tanh(W_c [h_{t-1}, x_t] + b_c)  # Candidate\n",
    "    c_t = f_t * c_{t-1} + i_t * c̃_t  # Cell state\n",
    "    o_t = σ(W_o [h_{t-1}, x_t] + b_o)  # Output gate\n",
    "    h_t = o_t * tanh(c_t)  # Hidden state\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        concat_size = hidden_size + input_size\n",
    "        \n",
    "        # Initialize weights for all gates\n",
    "        scale = np.sqrt(2.0 / concat_size)\n",
    "        self.W_f = np.random.randn(hidden_size, concat_size) * scale\n",
    "        self.W_i = np.random.randn(hidden_size, concat_size) * scale\n",
    "        self.W_c = np.random.randn(hidden_size, concat_size) * scale\n",
    "        self.W_o = np.random.randn(hidden_size, concat_size) * scale\n",
    "        \n",
    "        # Biases (forget gate bias often initialized to 1)\n",
    "        self.b_f = np.ones((hidden_size, 1))  # Important!\n",
    "        self.b_i = np.zeros((hidden_size, 1))\n",
    "        self.b_c = np.zeros((hidden_size, 1))\n",
    "        self.b_o = np.zeros((hidden_size, 1))\n",
    "    \n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        \"\"\"\n",
    "        Single step forward.\n",
    "        Returns: h_t, c_t, and all gate values for analysis\n",
    "        \"\"\"\n",
    "        # Concatenate h and x\n",
    "        concat = np.vstack([h_prev, x])\n",
    "        \n",
    "        # Gates\n",
    "        f = sigmoid(self.W_f @ concat + self.b_f)  # Forget\n",
    "        i = sigmoid(self.W_i @ concat + self.b_i)  # Input\n",
    "        c_tilde = np.tanh(self.W_c @ concat + self.b_c)  # Candidate\n",
    "        o = sigmoid(self.W_o @ concat + self.b_o)  # Output\n",
    "        \n",
    "        # Cell and hidden state\n",
    "        c = f * c_prev + i * c_tilde\n",
    "        h = o * np.tanh(c)\n",
    "        \n",
    "        return h, c, {'f': f, 'i': i, 'o': o, 'c_tilde': c_tilde}\n",
    "    \n",
    "    def forward_sequence(self, xs, h0=None, c0=None):\n",
    "        \"\"\"Process entire sequence.\"\"\"\n",
    "        if h0 is None:\n",
    "            h0 = np.zeros((self.hidden_size, 1))\n",
    "        if c0 is None:\n",
    "            c0 = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        hs, cs, gates = [h0], [c0], []\n",
    "        h, c = h0, c0\n",
    "        \n",
    "        for x in xs:\n",
    "            h, c, g = self.forward(x, h, c)\n",
    "            hs.append(h)\n",
    "            cs.append(c)\n",
    "            gates.append(g)\n",
    "        \n",
    "        return hs, cs, gates\n",
    "\n",
    "# Test\n",
    "lstm = SimpleLSTM(input_size=4, hidden_size=8)\n",
    "sequence = [np.random.randn(4, 1) for _ in range(10)]\n",
    "hs, cs, gates = lstm.forward_sequence(sequence)\n",
    "\n",
    "print(f\"Sequence length: {len(sequence)}\")\n",
    "print(f\"Final hidden state shape: {hs[-1].shape}\")\n",
    "print(f\"Final cell state shape: {cs[-1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: LSTM Gradient Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare gradient flow: RNN vs LSTM\n",
    "# For LSTM, gradient through cell state: ∂c_t/∂c_{t-1} = f_t\n",
    "# If f_t ≈ 1, gradients flow unchanged!\n",
    "\n",
    "# Simulate gradient flow\n",
    "sequence_length = 50\n",
    "\n",
    "# RNN: gradient ∝ λ^t where λ is max eigenvalue\n",
    "rnn_gradient = [0.9 ** t for t in range(sequence_length)]  # Typical vanishing\n",
    "\n",
    "# LSTM: gradient ∝ ∏ f_t where f_t can be close to 1\n",
    "forget_gate_values = np.random.uniform(0.9, 1.0, sequence_length)  # High forget gates\n",
    "lstm_gradient = np.cumprod(forget_gate_values)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.semilogy(rnn_gradient, 'r-', linewidth=2, label='Vanilla RNN (λ=0.9)')\n",
    "plt.semilogy(lstm_gradient, 'b-', linewidth=2, label='LSTM (forget gate ≈ 0.95)')\n",
    "plt.xlabel('Timestep')\n",
    "plt.ylabel('Gradient magnitude (log scale)')\n",
    "plt.title('Gradient Flow: RNN vs LSTM')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"RNN gradient at t=50: {rnn_gradient[-1]:.2e}\")\n",
    "print(f\"LSTM gradient at t=50: {lstm_gradient[-1]:.2e}\")\n",
    "print(\"\\nLSTM maintains gradient flow through the cell state 'highway'!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Train on Sequence Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Remember the first element of a sequence\n",
    "# Input: [x1, x2, ..., xT] where xi ∈ {0, 1}\n",
    "# Output: x1 (requires long-term memory)\n",
    "\n",
    "def generate_data(n_samples, sequence_length):\n",
    "    \"\"\"Generate sequences where we need to remember the first element.\"\"\"\n",
    "    X = torch.zeros(n_samples, sequence_length, 1)\n",
    "    y = torch.zeros(n_samples, dtype=torch.long)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        first = np.random.randint(0, 2)\n",
    "        X[i, 0, 0] = first\n",
    "        # Fill rest with noise (but target is still the first element)\n",
    "        X[i, 1:, 0] = torch.rand(sequence_length - 1) * 0.1\n",
    "        y[i] = first\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Models\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size=32):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(1, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, h_n = self.rnn(x)\n",
    "        return self.fc(h_n.squeeze(0))\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size=32):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(1, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        return self.fc(h_n.squeeze(0))\n",
    "\n",
    "# Train and compare\n",
    "def train_model(model, sequence_length, epochs=100):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        X, y = generate_data(100, sequence_length)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out = model(X)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        acc = (out.argmax(1) == y).float().mean().item()\n",
    "        accuracies.append(acc)\n",
    "    \n",
    "    return accuracies\n",
    "\n",
    "# Compare on different sequence lengths\n",
    "sequence_lengths = [10, 25, 50, 100]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "for ax, seq_len in zip(axes.flatten(), sequence_lengths):\n",
    "    rnn_model = RNNClassifier()\n",
    "    lstm_model = LSTMClassifier()\n",
    "    \n",
    "    rnn_acc = train_model(rnn_model, seq_len)\n",
    "    lstm_acc = train_model(lstm_model, seq_len)\n",
    "    \n",
    "    ax.plot(rnn_acc, 'r-', alpha=0.7, label='RNN')\n",
    "    ax.plot(lstm_acc, 'b-', alpha=0.7, label='LSTM')\n",
    "    ax.axhline(0.5, color='gray', linestyle='--', label='Random')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title(f'Sequence Length = {seq_len}')\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0.4, 1.05)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('RNN vs LSTM: Remembering First Element', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"As sequence length increases, RNN fails but LSTM succeeds!\")\n",
    "print(\"This demonstrates LSTM's ability to capture long-range dependencies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Key insights:**\n",
    "1. **RNNs** process sequences with shared weights across time\n",
    "2. **Vanishing gradients** prevent learning long-range dependencies\n",
    "3. **LSTM** solves this with a cell state that can carry information unchanged\n",
    "4. **Gates** (forget, input, output) control information flow\n",
    "5. For very long sequences, consider **Transformers** (no recurrence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
