{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 08: Backpropagation — Implementation from Scratch\n",
    "\n",
    "This notebook implements backpropagation step by step, verifies against PyTorch, and visualizes the entropy connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Building Blocks — Activation Functions and Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions and their derivatives\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"σ(z) = 1 / (1 + e^(-z))\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    \"\"\"σ'(z) = σ(z)(1 - σ(z))\"\"\"\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"ReLU(z) = max(0, z)\"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    \"\"\"ReLU'(z) = 1 if z > 0, else 0\"\"\"\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def tanh_derivative(z):\n",
    "    \"\"\"tanh'(z) = 1 - tanh²(z)\"\"\"\n",
    "    return 1 - np.tanh(z)**2\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"Numerically stable softmax\"\"\"\n",
    "    z_shifted = z - np.max(z, axis=0, keepdims=True)\n",
    "    exp_z = np.exp(z_shifted)\n",
    "    return exp_z / np.sum(exp_z, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize activation functions and their derivatives\n",
    "z = np.linspace(-5, 5, 100)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "\n",
    "# Sigmoid\n",
    "axes[0, 0].plot(z, sigmoid(z), 'b-', linewidth=2)\n",
    "axes[0, 0].set_title('Sigmoid')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(z, sigmoid_derivative(z), 'r-', linewidth=2)\n",
    "axes[1, 0].set_title('Sigmoid Derivative')\n",
    "axes[1, 0].axhline(0.25, color='gray', linestyle='--', label='max=0.25')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# ReLU\n",
    "axes[0, 1].plot(z, relu(z), 'b-', linewidth=2)\n",
    "axes[0, 1].set_title('ReLU')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(z, relu_derivative(z), 'r-', linewidth=2)\n",
    "axes[1, 1].set_title('ReLU Derivative')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Tanh\n",
    "axes[0, 2].plot(z, tanh(z), 'b-', linewidth=2)\n",
    "axes[0, 2].set_title('Tanh')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 2].plot(z, tanh_derivative(z), 'r-', linewidth=2)\n",
    "axes[1, 2].set_title('Tanh Derivative')\n",
    "axes[1, 2].axhline(1, color='gray', linestyle='--', label='max=1')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Activation Functions and Their Derivatives', y=1.02, fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: Sigmoid derivative is maximized at z=0 (maximum uncertainty)\")\n",
    "print(\"This is the ENTROPY connection - uncertain outputs have high gradient flow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Manual Backprop for a Single Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single neuron: z = w·x + b, a = σ(z), L = (a - y)²\n",
    "\n",
    "# Forward pass\n",
    "x = np.array([1.0, 2.0, 3.0])  # Input\n",
    "w = np.array([0.5, -0.5, 0.2])  # Weights\n",
    "b = 0.1  # Bias\n",
    "y = 0.8  # Target\n",
    "\n",
    "z = np.dot(w, x) + b\n",
    "a = sigmoid(z)\n",
    "L = (a - y) ** 2\n",
    "\n",
    "print(\"=== Forward Pass ===\")\n",
    "print(f\"z = w·x + b = {z:.4f}\")\n",
    "print(f\"a = σ(z) = {a:.4f}\")\n",
    "print(f\"L = (a - y)² = {L:.4f}\")\n",
    "\n",
    "# Backward pass (manually applying chain rule)\n",
    "# dL/da = 2(a - y)\n",
    "dL_da = 2 * (a - y)\n",
    "\n",
    "# da/dz = σ'(z) = σ(z)(1 - σ(z))\n",
    "da_dz = sigmoid_derivative(z)\n",
    "\n",
    "# dL/dz = dL/da · da/dz\n",
    "dL_dz = dL_da * da_dz  # This is δ (delta)\n",
    "\n",
    "# dz/dw = x, dz/db = 1\n",
    "dL_dw = dL_dz * x\n",
    "dL_db = dL_dz * 1\n",
    "\n",
    "print(\"\\n=== Backward Pass ===\")\n",
    "print(f\"dL/da = 2(a-y) = {dL_da:.4f}\")\n",
    "print(f\"da/dz = σ'(z) = {da_dz:.4f}\")\n",
    "print(f\"δ = dL/dz = {dL_dz:.4f}\")\n",
    "print(f\"dL/dw = {dL_dw}\")\n",
    "print(f\"dL/db = {dL_db:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify with numerical gradient\n",
    "def numerical_gradient(f, x, eps=1e-5):\n",
    "    \"\"\"Compute numerical gradient using central differences\"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    for i in range(len(x)):\n",
    "        x_plus = x.copy()\n",
    "        x_plus[i] += eps\n",
    "        x_minus = x.copy()\n",
    "        x_minus[i] -= eps\n",
    "        grad[i] = (f(x_plus) - f(x_minus)) / (2 * eps)\n",
    "    return grad\n",
    "\n",
    "# Define loss as function of w\n",
    "def loss_fn(w_test):\n",
    "    z_test = np.dot(w_test, x) + b\n",
    "    a_test = sigmoid(z_test)\n",
    "    return (a_test - y) ** 2\n",
    "\n",
    "numerical_dw = numerical_gradient(loss_fn, w)\n",
    "\n",
    "print(\"=== Gradient Verification ===\")\n",
    "print(f\"Analytical dL/dw: {dL_dw}\")\n",
    "print(f\"Numerical dL/dw:  {numerical_dw}\")\n",
    "print(f\"Difference: {np.abs(dL_dw - numerical_dw).max():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Full Neural Network from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    A neural network implemented from scratch with backpropagation.\n",
    "    Architecture: Input → Hidden (ReLU) → Output (Softmax)\n",
    "    Loss: Cross-Entropy\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes):\n",
    "        \"\"\"\n",
    "        layer_sizes: list of ints, e.g., [784, 128, 10]\n",
    "        \"\"\"\n",
    "        self.num_layers = len(layer_sizes)\n",
    "        self.layer_sizes = layer_sizes\n",
    "        \n",
    "        # Initialize weights (He initialization)\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            w = np.random.randn(layer_sizes[i+1], layer_sizes[i]) * np.sqrt(2 / layer_sizes[i])\n",
    "            b = np.zeros((layer_sizes[i+1], 1))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass. Store intermediate values for backprop.\n",
    "        X: input, shape (n_features, n_samples)\n",
    "        \"\"\"\n",
    "        self.activations = [X]  # a[0] = input\n",
    "        self.z_values = []  # pre-activation values\n",
    "        \n",
    "        a = X\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = self.weights[i] @ a + self.biases[i]\n",
    "            self.z_values.append(z)\n",
    "            a = relu(z)  # Hidden layers use ReLU\n",
    "            self.activations.append(a)\n",
    "        \n",
    "        # Output layer (softmax)\n",
    "        z = self.weights[-1] @ a + self.biases[-1]\n",
    "        self.z_values.append(z)\n",
    "        a = softmax(z)\n",
    "        self.activations.append(a)\n",
    "        \n",
    "        return a\n",
    "    \n",
    "    def cross_entropy_loss(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Cross-entropy loss: L = -Σ y_true * log(y_pred)\n",
    "        y_true: one-hot encoded, shape (n_classes, n_samples)\n",
    "        \"\"\"\n",
    "        eps = 1e-10\n",
    "        m = y_true.shape[1]\n",
    "        return -np.sum(y_true * np.log(y_pred + eps)) / m\n",
    "    \n",
    "    def backward(self, y_true):\n",
    "        \"\"\"\n",
    "        Backward pass. Compute gradients using backpropagation.\n",
    "        \"\"\"\n",
    "        m = y_true.shape[1]  # Number of samples\n",
    "        \n",
    "        self.dW = []\n",
    "        self.db = []\n",
    "        \n",
    "        # Output layer: softmax + cross-entropy has simple gradient\n",
    "        # dL/dz = y_pred - y_true\n",
    "        dz = self.activations[-1] - y_true  # Shape: (n_classes, m)\n",
    "        \n",
    "        # Backpropagate through layers\n",
    "        for i in range(len(self.weights) - 1, -1, -1):\n",
    "            # Gradient for weights and biases\n",
    "            dW = (1/m) * dz @ self.activations[i].T\n",
    "            db = (1/m) * np.sum(dz, axis=1, keepdims=True)\n",
    "            \n",
    "            self.dW.insert(0, dW)\n",
    "            self.db.insert(0, db)\n",
    "            \n",
    "            if i > 0:  # Don't need to compute for input layer\n",
    "                # Propagate error to previous layer\n",
    "                da = self.weights[i].T @ dz\n",
    "                # Apply activation derivative (ReLU for hidden layers)\n",
    "                dz = da * relu_derivative(self.z_values[i-1])\n",
    "    \n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"Update weights using computed gradients\"\"\"\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= learning_rate * self.dW[i]\n",
    "            self.biases[i] -= learning_rate * self.db[i]\n",
    "    \n",
    "    def train_step(self, X, y, learning_rate):\n",
    "        \"\"\"One training step: forward, backward, update\"\"\"\n",
    "        y_pred = self.forward(X)\n",
    "        loss = self.cross_entropy_loss(y_pred, y)\n",
    "        self.backward(y)\n",
    "        self.update(learning_rate)\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        y_pred = self.forward(X)\n",
    "        return np.argmax(y_pred, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Train on XOR Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR problem - not linearly separable!\n",
    "X_xor = np.array([[0, 0, 1, 1],\n",
    "                  [0, 1, 0, 1]])  # Shape: (2, 4)\n",
    "\n",
    "y_xor = np.array([[1, 0, 0, 1],   # Class 0: XOR = 0\n",
    "                  [0, 1, 1, 0]])  # Class 1: XOR = 1 (one-hot)\n",
    "\n",
    "# Create network: 2 inputs → 4 hidden → 2 outputs\n",
    "nn = NeuralNetwork([2, 4, 2])\n",
    "\n",
    "# Train\n",
    "losses = []\n",
    "for epoch in range(5000):\n",
    "    loss = nn.train_step(X_xor, y_xor, learning_rate=0.5)\n",
    "    losses.append(loss)\n",
    "    if epoch % 1000 == 0:\n",
    "        preds = nn.predict(X_xor)\n",
    "        acc = np.mean(preds == np.argmax(y_xor, axis=0))\n",
    "        print(f\"Epoch {epoch}: Loss = {loss:.4f}, Accuracy = {acc:.2%}\")\n",
    "\n",
    "# Final predictions\n",
    "print(\"\\n=== Final Predictions ===\")\n",
    "for i in range(4):\n",
    "    pred = nn.predict(X_xor[:, i:i+1])[0]\n",
    "    true = np.argmax(y_xor[:, i])\n",
    "    print(f\"Input: {X_xor[:, i]} → Predicted: {pred}, True: {true}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.title('Training Loss (XOR Problem)')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Verify Against PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create identical network in PyTorch\n",
    "class PyTorchNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2, 4)\n",
    "        self.fc2 = nn.Linear(4, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Copy weights from our implementation\n",
    "torch_nn = PyTorchNN()\n",
    "with torch.no_grad():\n",
    "    torch_nn.fc1.weight.copy_(torch.tensor(nn.weights[0], dtype=torch.float32))\n",
    "    torch_nn.fc1.bias.copy_(torch.tensor(nn.biases[0].flatten(), dtype=torch.float32))\n",
    "    torch_nn.fc2.weight.copy_(torch.tensor(nn.weights[1], dtype=torch.float32))\n",
    "    torch_nn.fc2.bias.copy_(torch.tensor(nn.biases[1].flatten(), dtype=torch.float32))\n",
    "\n",
    "# Forward pass comparison\n",
    "X_torch = torch.tensor(X_xor.T, dtype=torch.float32)  # PyTorch: (batch, features)\n",
    "y_torch = torch.tensor(np.argmax(y_xor, axis=0), dtype=torch.long)\n",
    "\n",
    "# Our implementation\n",
    "our_output = nn.forward(X_xor)\n",
    "\n",
    "# PyTorch\n",
    "torch_output = torch.softmax(torch_nn(X_torch), dim=1)\n",
    "\n",
    "print(\"=== Output Comparison ===\")\n",
    "print(f\"Our implementation:\\n{our_output.T}\")\n",
    "print(f\"\\nPyTorch:\\n{torch_output.detach().numpy()}\")\n",
    "print(f\"\\nMax difference: {np.abs(our_output.T - torch_output.detach().numpy()).max():.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient comparison\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "torch_logits = torch_nn(X_torch)\n",
    "loss_torch = criterion(torch_logits, y_torch)\n",
    "loss_torch.backward()\n",
    "\n",
    "# Our gradients\n",
    "nn.forward(X_xor)\n",
    "nn.backward(y_xor)\n",
    "\n",
    "print(\"=== Gradient Comparison (Layer 1 Weights) ===\")\n",
    "print(f\"Our dW1:\\n{nn.dW[0]}\")\n",
    "print(f\"\\nPyTorch dW1:\\n{torch_nn.fc1.weight.grad.numpy()}\")\n",
    "print(f\"\\nMax difference: {np.abs(nn.dW[0] - torch_nn.fc1.weight.grad.numpy()).max():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: The Entropy Connection — Visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate: Cross-entropy loss = entropy + KL divergence\n",
    "\n",
    "def entropy(p):\n",
    "    \"\"\"Shannon entropy: H(p) = -Σ p log p\"\"\"\n",
    "    p = np.clip(p, 1e-10, 1)\n",
    "    return -np.sum(p * np.log(p))\n",
    "\n",
    "def cross_entropy(p, q):\n",
    "    \"\"\"Cross-entropy: H(p, q) = -Σ p log q\"\"\"\n",
    "    q = np.clip(q, 1e-10, 1)\n",
    "    return -np.sum(p * np.log(q))\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    \"\"\"KL divergence: D_KL(p || q) = Σ p log(p/q)\"\"\"\n",
    "    p = np.clip(p, 1e-10, 1)\n",
    "    q = np.clip(q, 1e-10, 1)\n",
    "    return np.sum(p * np.log(p / q))\n",
    "\n",
    "# Example: true distribution vs predicted\n",
    "p_true = np.array([1.0, 0.0, 0.0])  # One-hot (class 0)\n",
    "\n",
    "# Different predicted distributions\n",
    "predictions = [\n",
    "    np.array([0.9, 0.05, 0.05]),   # Good prediction\n",
    "    np.array([0.6, 0.2, 0.2]),     # Medium\n",
    "    np.array([0.33, 0.33, 0.34]),  # Bad (near uniform)\n",
    "    np.array([0.1, 0.8, 0.1]),     # Wrong class\n",
    "]\n",
    "\n",
    "print(\"=== Cross-Entropy Decomposition ===\")\n",
    "print(f\"True distribution: {p_true}\")\n",
    "print(f\"H(true) = {entropy(p_true):.4f} (always 0 for one-hot)\\n\")\n",
    "\n",
    "for q in predictions:\n",
    "    H_true = entropy(p_true)\n",
    "    H_cross = cross_entropy(p_true, q)\n",
    "    D_KL = kl_divergence(p_true, q)\n",
    "    \n",
    "    print(f\"Predicted: {q}\")\n",
    "    print(f\"  Cross-entropy H(p,q) = {H_cross:.4f}\")\n",
    "    print(f\"  = H(p) + D_KL(p||q) = {H_true:.4f} + {D_KL:.4f} = {H_true + D_KL:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: Gradient magnitude vs entropy of prediction\n",
    "\n",
    "# For softmax + cross-entropy, gradient = y_pred - y_true\n",
    "# The magnitude depends on how \"wrong\" the prediction is\n",
    "\n",
    "p_range = np.linspace(0.01, 0.99, 100)\n",
    "\n",
    "# True label is class 0\n",
    "y_true_binary = np.array([1, 0])\n",
    "\n",
    "gradients = []\n",
    "entropies = []\n",
    "losses = []\n",
    "\n",
    "for p in p_range:\n",
    "    y_pred = np.array([p, 1-p])\n",
    "    \n",
    "    # Gradient: y_pred - y_true\n",
    "    grad = y_pred - y_true_binary\n",
    "    grad_magnitude = np.linalg.norm(grad)\n",
    "    \n",
    "    # Entropy of prediction\n",
    "    H = entropy(y_pred)\n",
    "    \n",
    "    # Cross-entropy loss\n",
    "    L = cross_entropy(y_true_binary, y_pred)\n",
    "    \n",
    "    gradients.append(grad_magnitude)\n",
    "    entropies.append(H)\n",
    "    losses.append(L)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "axes[0].plot(p_range, losses, 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('P(class 0)')\n",
    "axes[0].set_ylabel('Cross-Entropy Loss')\n",
    "axes[0].set_title('Loss vs Prediction Confidence')\n",
    "axes[0].axvline(1, color='g', linestyle='--', label='True class')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(p_range, gradients, 'r-', linewidth=2)\n",
    "axes[1].set_xlabel('P(class 0)')\n",
    "axes[1].set_ylabel('Gradient Magnitude |∂L/∂z|')\n",
    "axes[1].set_title('Gradient vs Prediction')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(p_range, entropies, 'purple', linewidth=2)\n",
    "axes[2].set_xlabel('P(class 0)')\n",
    "axes[2].set_ylabel('Entropy H(y_pred)')\n",
    "axes[2].set_title('Prediction Entropy (Uncertainty)')\n",
    "axes[2].axhline(np.log(2), color='gray', linestyle='--', label='Max entropy')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: Large gradients when prediction is WRONG (far from true class)\")\n",
    "print(\"Maximum uncertainty (entropy) is at p=0.5 — the network is most 'confused'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final visualization: Gradient flow and entropy\n",
    "\n",
    "# For sigmoid activation: σ'(z) = σ(z)(1 - σ(z))\n",
    "# This is maximized at z=0 where σ(z) = 0.5 (maximum entropy!)\n",
    "\n",
    "z = np.linspace(-6, 6, 100)\n",
    "sigmoid_out = sigmoid(z)\n",
    "sigmoid_grad = sigmoid_derivative(z)\n",
    "\n",
    "# Entropy of sigmoid output (treating it as Bernoulli probability)\n",
    "def binary_entropy(p):\n",
    "    p = np.clip(p, 1e-10, 1-1e-10)\n",
    "    return -p * np.log(p) - (1-p) * np.log(1-p)\n",
    "\n",
    "entropy_sigmoid = binary_entropy(sigmoid_out)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(z, sigmoid_out, 'b-', linewidth=2, label='σ(z) - Output')\n",
    "ax.plot(z, sigmoid_grad, 'r-', linewidth=2, label=\"σ'(z) - Gradient\")\n",
    "ax.plot(z, entropy_sigmoid / np.log(2), 'g-', linewidth=2, label='H(σ(z)) / ln(2) - Entropy')\n",
    "\n",
    "ax.axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.axhline(0.5, color='gray', linestyle=':', alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('Pre-activation z', fontsize=12)\n",
    "ax.set_ylabel('Value', fontsize=12)\n",
    "ax.set_title('The Entropy-Gradient Connection in Sigmoid Activation', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Annotate\n",
    "ax.annotate('Max gradient & entropy\\nat z=0', xy=(0, 0.25), xytext=(2, 0.4),\n",
    "            arrowprops=dict(arrowstyle='->', color='black'),\n",
    "            fontsize=11)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"THE FUNDAMENTAL CONNECTION:\")\n",
    "print(\"=\"*60)\n",
    "print(\"• High entropy (uncertainty) → High gradient flow\")\n",
    "print(\"• Low entropy (saturation) → Vanishing gradients\")\n",
    "print(\"• Backpropagation naturally learns through uncertain neurons!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we learned:**\n",
    "1. Backpropagation is repeated application of the chain rule\n",
    "2. Gradients flow backward, computing ∂L/∂θ for all parameters\n",
    "3. Cross-entropy loss connects directly to information theory\n",
    "4. **The entropy connection**: Neurons with high uncertainty (entropy) have high gradient flow\n",
    "\n",
    "**The deep insight**: Training neural networks is fundamentally about reducing the \"surprise\" of predictions — minimizing cross-entropy IS minimizing information-theoretic divergence!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
