# Tutorial 4: VAE, ELBO & Variational Inference

## ðŸŽ¯ The Big Picture

**VAEs solve an intractable problem by turning it into an optimization problem.**

We want to learn a generative model $p(x)$, but exact inference is intractable. VAEs introduce an approximate posterior $q(z|x)$ and optimize a lower bound on the log-likelihoodâ€”the **Evidence Lower Bound (ELBO)**.

---

## 1. The Generative Model

### The Setup

We assume data is generated by:
1. Sample latent variable: $z \sim p(z)$ (prior, usually $\mathcal{N}(0, I)$)
2. Sample observation: $x \sim p(x|z)$ (decoder/likelihood)

The marginal likelihood is:
$$p(x) = \int p(x|z) p(z) \, dz$$

### The Problem: Intractable Posterior

To do inference, we need the posterior:
$$p(z|x) = \frac{p(x|z) p(z)}{p(x)} = \frac{p(x|z) p(z)}{\int p(x|z) p(z) \, dz}$$

**Problem:** The integral $\int p(x|z) p(z) \, dz$ is intractable for complex decoders!

We can't:
- Compute $p(x)$ directly
- Sample from $p(z|x)$ exactly
- Train with exact maximum likelihood

---

## 2. Variational Inference: The Key Idea

### Approximate the Posterior

Instead of computing $p(z|x)$ exactly, we:
1. Define a **variational family** $\mathcal{Q}$ of tractable distributions
2. Find $q^*(z|x) \in \mathcal{Q}$ that best approximates $p(z|x)$
3. Use $q^*$ instead of the true posterior

### The Variational Family

In VAEs, we typically use:
$$q_\phi(z|x) = \mathcal{N}(z | \mu_\phi(x), \text{diag}(\sigma^2_\phi(x)))$$

Where $\mu_\phi$ and $\sigma_\phi$ are neural networks (the **encoder**).

### Measuring "Best": KL Divergence

We want to minimize:
$$D_{KL}(q_\phi(z|x) \| p(z|x))$$

But we can't compute this directly (it involves $p(z|x)$)!

---

## 3. Deriving the ELBO

### The Log-Likelihood Decomposition

Start with the log-marginal-likelihood and add/subtract $q$:

$$\log p(x) = \log \int p(x, z) \, dz = \log \int \frac{p(x, z)}{q(z|x)} q(z|x) \, dz$$

By Jensen's inequality:
$$\log p(x) \geq \int q(z|x) \log \frac{p(x, z)}{q(z|x)} \, dz = \mathcal{L}(\phi, \theta; x)$$

This lower bound is the **Evidence Lower Bound (ELBO)**.

### ELBO Decomposition

$$\mathcal{L}(\phi, \theta; x) = \mathbb{E}_{q_\phi(z|x)}\left[\log \frac{p_\theta(x, z)}{q_\phi(z|x)}\right]$$

Expanding:
$$= \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] + \mathbb{E}_{q_\phi(z|x)}\left[\log \frac{p(z)}{q_\phi(z|x)}\right]$$

$$= \underbrace{\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]}_{\text{Reconstruction term}} - \underbrace{D_{KL}(q_\phi(z|x) \| p(z))}_{\text{Regularization term}}$$

### The ELBO Formula

$$\boxed{\mathcal{L}(\phi, \theta; x) = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x) \| p(z))}$$

---

## 4. Understanding the ELBO Terms

### Term 1: Reconstruction Loss

$$\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]$$

**What it does:**
- Measures how well we can reconstruct $x$ from $z$
- Encourages encoder to preserve information about $x$

**In practice (Gaussian decoder):**
$$\log p(x|z) = -\frac{\|x - \mu_\theta(z)\|^2}{2\sigma^2} + \text{const}$$

This becomes **MSE loss** (up to constants)!

### Term 2: KL Regularization

$$-D_{KL}(q_\phi(z|x) \| p(z))$$

**What it does:**
- Pulls approximate posterior toward prior
- Encourages smooth, regular latent space
- Prevents encoder from using infinite information

**Closed form (for Gaussian with diagonal covariance):**
$$D_{KL}(q_\phi(z|x) \| p(z)) = \frac{1}{2}\sum_{j=1}^d \left(\mu_j^2 + \sigma_j^2 - 1 - \log \sigma_j^2\right)$$

### The Trade-off

| Term | Wants | Effect |
|------|-------|--------|
| Reconstruction | Preserve all information | Large, complex latent codes |
| KL | Match prior | Small, regular latent codes |

This is a **rate-distortion trade-off**:
- **Rate** = how much information flows through $z$
- **Distortion** = reconstruction error

---

## 5. Why KL to Prior? The Information Bottleneck

### The Information View

The KL term limits **mutual information** between $x$ and $z$:

$$I(X; Z) \leq D_{KL}(q(z|x) \| p(z))$$

By minimizing KL, we:
1. **Compress** information about $x$ into $z$
2. **Force** the model to learn only essential features
3. **Enable** generation by sampling from prior

### Why This is Crucial

**Without KL regularization:**
- Encoder could memorize: assign unique $z$ to each training $x$
- $q(z|x)$ would be very narrow (low variance)
- Latent space would have "holes"
- Can't sample from prior for generation

**With KL regularization:**
- Encoder must use the prior's "budget"
- Overlapping posteriors for similar $x$
- Smooth, continuous latent space
- Can generate by sampling $z \sim p(z)$

### The Posterior Collapse Problem

If KL weight is too high:
- Model ignores encoder entirely
- All $q(z|x) \approx p(z)$
- No useful information in $z$
- Decoder relies only on its own capacity

---

## 6. The ELBO-KL Relationship

### ELBO and Log-Likelihood Gap

$$\log p(x) = \mathcal{L}(\phi, \theta; x) + D_{KL}(q_\phi(z|x) \| p_\theta(z|x))$$

Rearranging:
$$D_{KL}(q_\phi(z|x) \| p_\theta(z|x)) = \log p(x) - \mathcal{L}(\phi, \theta; x)$$

**Key insights:**
1. ELBO is always â‰¤ log p(x)
2. Gap = KL from approximate to true posterior
3. Maximizing ELBO both:
   - Increases log p(x) (better model)
   - Decreases gap (better approximation)

### Two Ways to View VAE Training

**View 1: Maximize likelihood bound**
- We can't compute $\log p(x)$ directly
- Maximize ELBO as surrogate

**View 2: Minimize variational approximation error**
- Minimize $D_{KL}(q \| p_{posterior})$
- Subject to the constraint that we only use $q \in \mathcal{Q}$

---

## 7. The VAE Architecture

### Components

```
Input x
    â†“
[Encoder Ï†] â†’ Î¼(x), log_var(x)
    â†“
[Reparameterization] z = Î¼ + Ïƒ âŠ™ Îµ,  Îµ ~ N(0,I)
    â†“
[Decoder Î¸] â†’ p(x|z)
    â†“
Output xÌ‚
```

### The Reparameterization Trick

**Problem:** We need $\nabla_\phi \mathbb{E}_{q_\phi(z|x)}[\cdot]$

Sampling $z \sim q_\phi$ blocks gradients!

**Solution:** Reparameterize:
$$z = \mu_\phi(x) + \sigma_\phi(x) \odot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)$$

Now $z$ is a deterministic function of $\phi$ given $\epsilon$:
$$\nabla_\phi \mathbb{E}_{q_\phi}[f(z)] = \nabla_\phi \mathbb{E}_{\epsilon}[f(g_\phi(x, \epsilon))] = \mathbb{E}_{\epsilon}[\nabla_\phi f(g_\phi(x, \epsilon))]$$

### Loss Function

$$\mathcal{L}_{VAE} = \underbrace{\|x - \hat{x}\|^2}_{\text{Reconstruction (MSE)}} + \underbrace{\beta \cdot D_{KL}(q_\phi(z|x) \| p(z))}_{\text{KL regularization}}$$

(Î²-VAE uses $\beta > 1$ for more disentanglement)

---

## 8. Entropy Perspective on VAE

### Encoder: Finding Efficient Codes

The encoder learns:
$$q(z|x) = \text{argmin}_{q \in \mathcal{Q}} \, D_{KL}(q \| p(z|x))$$

From entropy view:
- $q$ should be as "simple" as possible (close to prior)
- While still allowing good reconstruction
- This is **lossy compression** with rate constraint

### Decoder: Using Codes Efficiently

The decoder learns:
$$p(x|z) = \text{argmax}_p \, \mathbb{E}_{q(z|x)}[\log p(x|z)]$$

This maximizes **information throughput** from $z$ to $x$.

### The Full Picture

$$\text{VAE Training} = \text{Joint Rate-Distortion Optimization}$$

- **Rate:** $I(X; Z) \approx D_{KL}(q(z|x) \| p(z))$
- **Distortion:** Reconstruction error
- Trade-off controlled by $\beta$

---

## 9. From Variational Inference to VAE

### Classical VI vs VAE

| Aspect | Classical VI | VAE |
|--------|-------------|-----|
| $q$ family | Fixed form | Neural network |
| Optimization | Coordinate ascent | SGD |
| Scalability | Small data | Large data |
| Amortization | None | Encoder amortizes |

### Amortized Inference

Classical VI: Optimize $q(z)$ for each $x$ separately.

VAE: Train encoder $q_\phi(z|x)$ to **amortize** inference.
- One forward pass gives $q$ for any $x$
- Much faster at test time
- Shares statistical strength across examples

---

## 10. Summary: The VAE Recipe

### Training

1. **Encode:** $\mu, \log\sigma^2 = \text{Encoder}_\phi(x)$
2. **Sample:** $z = \mu + \sigma \odot \epsilon$, $\epsilon \sim \mathcal{N}(0, I)$
3. **Decode:** $\hat{x} = \text{Decoder}_\theta(z)$
4. **Loss:** $\mathcal{L} = \text{Recon}(x, \hat{x}) + \beta \cdot D_{KL}(q \| p)$
5. **Backprop** through everything (thanks to reparameterization)

### Generation

1. **Sample:** $z \sim \mathcal{N}(0, I)$ (from prior)
2. **Decode:** $x = \text{Decoder}_\theta(z)$

### Key Formulas

| Component | Formula |
|-----------|---------|
| ELBO | $\mathbb{E}_q[\log p(x|z)] - D_{KL}(q(z|x) \| p(z))$ |
| KL (Gaussian) | $\frac{1}{2}\sum_j(\mu_j^2 + \sigma_j^2 - 1 - \log\sigma_j^2)$ |
| Reparameterization | $z = \mu + \sigma \odot \epsilon$ |
| Gap | $\log p(x) - \text{ELBO} = D_{KL}(q \| p_{true})$ |

### Entropy Intuition

- **KL to prior** = Rate (information bottleneck)
- **Reconstruction** = Distortion (information preserved)
- **VAE** = Optimal rate-distortion trade-off
- **Latent space** = Learned compression

---

## Further Reading

1. **Original VAE paper:** Kingma & Welling, "Auto-Encoding Variational Bayes"
2. **Î²-VAE:** Higgins et al., "Î²-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework"
3. **Information bottleneck:** Tishby et al., "The Information Bottleneck Method"
4. **Rate-distortion:** Cover & Thomas, "Elements of Information Theory"
