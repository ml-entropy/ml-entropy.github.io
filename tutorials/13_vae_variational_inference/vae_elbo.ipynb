{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ VAE, ELBO & Variational Inference\n",
    "\n",
    "This notebook implements and visualizes:\n",
    "1. ELBO derivation and intuition\n",
    "2. Full VAE implementation in PyTorch\n",
    "3. Training dynamics and loss curves\n",
    "4. Latent space visualization\n",
    "5. The effect of KL weight (Î²)\n",
    "6. Information bottleneck perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from typing import Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ELBO: The Evidence Lower Bound\n",
    "\n",
    "$$\\mathcal{L}(\\phi, \\theta; x) = \\underbrace{\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]}_{\\text{Reconstruction}} - \\underbrace{D_{KL}(q_\\phi(z|x) \\| p(z))}_{\\text{KL Regularization}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the ELBO decomposition\n",
    "\n",
    "def visualize_elbo_decomposition():\n",
    "    \"\"\"Show how ELBO relates to log p(x) and KL gap.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Left: ELBO components\n",
    "    ax = axes[0]\n",
    "    \n",
    "    # Simulate training progress\n",
    "    epochs = np.arange(100)\n",
    "    \n",
    "    # Reconstruction improves\n",
    "    recon = -100 * np.exp(-epochs/30) - 5\n",
    "    \n",
    "    # KL starts low (posterior = prior), increases, then stabilizes\n",
    "    kl = 20 * (1 - np.exp(-epochs/20)) * (1 - 0.3*np.exp(-epochs/50))\n",
    "    \n",
    "    elbo = recon - kl\n",
    "    \n",
    "    ax.plot(epochs, -recon, 'b-', linewidth=2, label='Reconstruction Loss')\n",
    "    ax.plot(epochs, kl, 'r-', linewidth=2, label='KL Divergence')\n",
    "    ax.plot(epochs, -elbo, 'g--', linewidth=2, label='Total Loss (-ELBO)')\n",
    "    \n",
    "    ax.set_xlabel('Training Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('VAE Loss Components During Training')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Right: ELBO vs true log p(x)\n",
    "    ax = axes[1]\n",
    "    \n",
    "    # True log p(x) is what we want but can't compute\n",
    "    log_px = -50 + 45 * (1 - np.exp(-epochs/40))\n",
    "    \n",
    "    # ELBO is always below\n",
    "    elbo_normalized = log_px - 5 * (1 + np.exp(-epochs/30))\n",
    "    \n",
    "    # Gap = KL(q || p_true_posterior)\n",
    "    gap = log_px - elbo_normalized\n",
    "    \n",
    "    ax.fill_between(epochs, elbo_normalized, log_px, alpha=0.3, color='red', \n",
    "                    label='Gap = KL(q || p_posterior)')\n",
    "    ax.plot(epochs, log_px, 'b-', linewidth=2, label='log p(x) (unknown)')\n",
    "    ax.plot(epochs, elbo_normalized, 'g--', linewidth=2, label='ELBO (what we maximize)')\n",
    "    \n",
    "    ax.set_xlabel('Training Epoch')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.set_title('ELBO is a Lower Bound on log p(x)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_elbo_decomposition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. VAE Implementation in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder\n",
    "    \n",
    "    Architecture:\n",
    "        Encoder: x -> h -> (mu, log_var)\n",
    "        Reparameterization: z = mu + sigma * epsilon\n",
    "        Decoder: z -> h -> x_reconstructed\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dim: int, latent_dim: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Latent space parameters\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_log_var = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()  # Output in [0, 1]\n",
    "        )\n",
    "    \n",
    "    def encode(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Encode input to latent distribution parameters.\"\"\"\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        log_var = self.fc_log_var(h)\n",
    "        return mu, log_var\n",
    "    \n",
    "    def reparameterize(self, mu: torch.Tensor, log_var: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterization trick: z = mu + sigma * epsilon\n",
    "        Allows gradients to flow through stochastic sampling.\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        epsilon = torch.randn_like(std)\n",
    "        z = mu + std * epsilon\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Decode latent code to reconstruction.\"\"\"\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Full forward pass.\"\"\"\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, log_var\n",
    "\n",
    "\n",
    "def vae_loss(x: torch.Tensor, x_recon: torch.Tensor, \n",
    "             mu: torch.Tensor, log_var: torch.Tensor,\n",
    "             beta: float = 1.0) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    VAE Loss = Reconstruction + beta * KL\n",
    "    \n",
    "    Reconstruction: MSE or BCE depending on data\n",
    "    KL: Closed-form for Gaussian to standard normal\n",
    "    \n",
    "    KL = 0.5 * sum(mu^2 + sigma^2 - 1 - log(sigma^2))\n",
    "    \"\"\"\n",
    "    # Reconstruction loss (per sample)\n",
    "    recon_loss = F.mse_loss(x_recon, x, reduction='sum') / x.size(0)\n",
    "    \n",
    "    # KL divergence (per sample)\n",
    "    # D_KL(N(mu, sigma^2) || N(0, 1)) = 0.5 * sum(mu^2 + sigma^2 - 1 - log(sigma^2))\n",
    "    kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp()) / x.size(0)\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = recon_loss + beta * kl_loss\n",
    "    \n",
    "    return total_loss, recon_loss, kl_loss\n",
    "\n",
    "\n",
    "print(\"VAE model defined!\")\n",
    "print(\"\\nKey components:\")\n",
    "print(\"1. Encoder: x -> (Î¼, log ÏƒÂ²)\")\n",
    "print(\"2. Reparameterization: z = Î¼ + Ïƒ âŠ™ Îµ, Îµ ~ N(0,I)\")\n",
    "print(\"3. Decoder: z -> xÌ‚\")\n",
    "print(\"4. Loss: MSE(x, xÌ‚) + Î² Â· KL(q(z|x) || p(z))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the KL loss components\n",
    "\n",
    "def visualize_kl_components():\n",
    "    \"\"\"Show how each part of KL loss works.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # KL loss for different mu (sigma=1)\n",
    "    ax = axes[0]\n",
    "    mu_range = np.linspace(-3, 3, 100)\n",
    "    kl_mu = 0.5 * mu_range**2  # When log_var=0 (sigma=1)\n",
    "    \n",
    "    ax.plot(mu_range, kl_mu, 'b-', linewidth=2)\n",
    "    ax.axvline(x=0, color='r', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlabel('Î¼')\n",
    "    ax.set_ylabel('KL contribution')\n",
    "    ax.set_title('Mean Penalty: $\\\\frac{1}{2}\\\\mu^2$\\n\"Pull mean toward 0\"')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # KL loss for different sigma (mu=0)\n",
    "    ax = axes[1]\n",
    "    log_var_range = np.linspace(-3, 2, 100)\n",
    "    sigma_sq = np.exp(log_var_range)\n",
    "    kl_sigma = 0.5 * (sigma_sq - 1 - log_var_range)  # When mu=0\n",
    "    \n",
    "    ax.plot(sigma_sq, kl_sigma, 'b-', linewidth=2)\n",
    "    ax.axvline(x=1, color='r', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlabel('ÏƒÂ²')\n",
    "    ax.set_ylabel('KL contribution')\n",
    "    ax.set_title('Variance Penalty: $\\\\frac{1}{2}(\\\\sigma^2 - 1 - \\\\log\\\\sigma^2)$\\n\"Pull variance toward 1\"')\n",
    "    ax.set_xlim(0, 5)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Combined KL as heatmap\n",
    "    ax = axes[2]\n",
    "    mu_grid = np.linspace(-3, 3, 100)\n",
    "    logvar_grid = np.linspace(-2, 2, 100)\n",
    "    MU, LOGVAR = np.meshgrid(mu_grid, logvar_grid)\n",
    "    \n",
    "    # KL = 0.5 * (mu^2 + sigma^2 - 1 - log(sigma^2))\n",
    "    KL = 0.5 * (MU**2 + np.exp(LOGVAR) - 1 - LOGVAR)\n",
    "    \n",
    "    contour = ax.contourf(MU, np.exp(LOGVAR/2), KL, levels=20, cmap='viridis')\n",
    "    plt.colorbar(contour, ax=ax, label='KL [nats]')\n",
    "    ax.plot(0, 1, 'r*', markersize=15, label='Minimum (Î¼=0, Ïƒ=1)')\n",
    "    ax.set_xlabel('Î¼')\n",
    "    ax.set_ylabel('Ïƒ')\n",
    "    ax.set_title('Total KL Loss\\nMinimum at standard normal')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_kl_components()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Synthetic Data: 2D Points from Mixture of Gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_2d_clusters(n_samples=2000, n_clusters=5):\n",
    "    \"\"\"Generate 2D data from mixture of Gaussians.\"\"\"\n",
    "    \n",
    "    # Cluster centers arranged in a circle\n",
    "    angles = np.linspace(0, 2*np.pi, n_clusters, endpoint=False)\n",
    "    centers = 3 * np.stack([np.cos(angles), np.sin(angles)], axis=1)\n",
    "    \n",
    "    # Generate points\n",
    "    samples_per_cluster = n_samples // n_clusters\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for i, center in enumerate(centers):\n",
    "        cluster_data = np.random.randn(samples_per_cluster, 2) * 0.5 + center\n",
    "        data.append(cluster_data)\n",
    "        labels.extend([i] * samples_per_cluster)\n",
    "    \n",
    "    data = np.concatenate(data, axis=0)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    data_min = data.min()\n",
    "    data_max = data.max()\n",
    "    data_normalized = (data - data_min) / (data_max - data_min)\n",
    "    \n",
    "    return data_normalized, labels, (data_min, data_max)\n",
    "\n",
    "# Generate data\n",
    "data, labels, (data_min, data_max) = generate_2d_clusters()\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "scatter = ax.scatter(data[:, 0], data[:, 1], c=labels, cmap='tab10', alpha=0.5, s=10)\n",
    "ax.set_xlabel('xâ‚')\n",
    "ax.set_ylabel('xâ‚‚')\n",
    "ax.set_title('Synthetic 2D Data: 5 Gaussian Clusters')\n",
    "ax.set_aspect('equal')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Data range: [{data.min():.3f}, {data.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(data: np.ndarray, latent_dim: int = 2, hidden_dim: int = 64,\n",
    "              n_epochs: int = 100, batch_size: int = 64, lr: float = 1e-3,\n",
    "              beta: float = 1.0) -> Tuple[VAE, dict]:\n",
    "    \"\"\"\n",
    "    Train VAE on data.\n",
    "    \n",
    "    Args:\n",
    "        data: Training data (N, D)\n",
    "        latent_dim: Dimension of latent space\n",
    "        hidden_dim: Hidden layer dimension\n",
    "        n_epochs: Number of training epochs\n",
    "        batch_size: Batch size\n",
    "        lr: Learning rate\n",
    "        beta: KL weight (beta-VAE)\n",
    "    \n",
    "    Returns:\n",
    "        Trained VAE model and training history\n",
    "    \"\"\"\n",
    "    \n",
    "    input_dim = data.shape[1]\n",
    "    \n",
    "    # Create model\n",
    "    model = VAE(input_dim, hidden_dim, latent_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Create dataloader\n",
    "    tensor_data = torch.FloatTensor(data).to(device)\n",
    "    dataset = TensorDataset(tensor_data)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Training history\n",
    "    history = {'total_loss': [], 'recon_loss': [], 'kl_loss': []}\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_total = 0\n",
    "        epoch_recon = 0\n",
    "        epoch_kl = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            x = batch[0]\n",
    "            \n",
    "            # Forward pass\n",
    "            x_recon, mu, log_var = model(x)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss, recon, kl = vae_loss(x, x_recon, mu, log_var, beta)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_total += loss.item()\n",
    "            epoch_recon += recon.item()\n",
    "            epoch_kl += kl.item()\n",
    "            n_batches += 1\n",
    "        \n",
    "        history['total_loss'].append(epoch_total / n_batches)\n",
    "        history['recon_loss'].append(epoch_recon / n_batches)\n",
    "        history['kl_loss'].append(epoch_kl / n_batches)\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs} | \"\n",
    "                  f\"Total: {history['total_loss'][-1]:.4f} | \"\n",
    "                  f\"Recon: {history['recon_loss'][-1]:.4f} | \"\n",
    "                  f\"KL: {history['kl_loss'][-1]:.4f}\")\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train VAE\n",
    "print(\"Training VAE with Î²=1.0...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "model, history = train_vae(\n",
    "    data, \n",
    "    latent_dim=2, \n",
    "    hidden_dim=64,\n",
    "    n_epochs=100,\n",
    "    batch_size=64,\n",
    "    beta=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(history['total_loss'], 'b-', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Total Loss (-ELBO)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(history['recon_loss'], 'g-', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Reconstruction Loss')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[2]\n",
    "ax.plot(history['kl_loss'], 'r-', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('KL Divergence')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing the Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_latent_space(model: VAE, data: np.ndarray, labels: np.ndarray):\n",
    "    \"\"\"Visualize encoder outputs in latent space.\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tensor_data = torch.FloatTensor(data).to(device)\n",
    "        mu, log_var = model.encode(tensor_data)\n",
    "        z = model.reparameterize(mu, log_var)\n",
    "        \n",
    "        mu = mu.cpu().numpy()\n",
    "        z = z.cpu().numpy()\n",
    "        sigma = np.exp(0.5 * log_var.cpu().numpy())\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Latent means\n",
    "    ax = axes[0]\n",
    "    scatter = ax.scatter(mu[:, 0], mu[:, 1], c=labels, cmap='tab10', alpha=0.5, s=10)\n",
    "    ax.set_xlabel('zâ‚')\n",
    "    ax.set_ylabel('zâ‚‚')\n",
    "    ax.set_title('Latent Space (Î¼ only)\\nClusters preserved!')\n",
    "    ax.set_aspect('equal')\n",
    "    plt.colorbar(scatter, ax=ax, label='Cluster')\n",
    "    \n",
    "    # Sampled z\n",
    "    ax = axes[1]\n",
    "    scatter = ax.scatter(z[:, 0], z[:, 1], c=labels, cmap='tab10', alpha=0.5, s=10)\n",
    "    ax.set_xlabel('zâ‚')\n",
    "    ax.set_ylabel('zâ‚‚')\n",
    "    ax.set_title('Latent Space (sampled z)\\nWith added noise')\n",
    "    ax.set_aspect('equal')\n",
    "    plt.colorbar(scatter, ax=ax, label='Cluster')\n",
    "    \n",
    "    # Variance distribution\n",
    "    ax = axes[2]\n",
    "    ax.hist(sigma.flatten(), bins=50, edgecolor='black', alpha=0.7)\n",
    "    ax.axvline(x=1.0, color='r', linestyle='--', linewidth=2, label='Target Ïƒ=1')\n",
    "    ax.set_xlabel('Ïƒ')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Encoder Ïƒ Distribution\\nKL pulls toward 1')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return mu, z, sigma\n",
    "\n",
    "mu, z, sigma = visualize_latent_space(model, data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reconstruction\n",
    "def visualize_reconstruction(model: VAE, data: np.ndarray, labels: np.ndarray, n_samples: int = 500):\n",
    "    \"\"\"Compare original data with reconstructions.\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    idx = np.random.choice(len(data), n_samples, replace=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x = torch.FloatTensor(data[idx]).to(device)\n",
    "        x_recon, _, _ = model(x)\n",
    "        x_recon = x_recon.cpu().numpy()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    ax = axes[0]\n",
    "    ax.scatter(data[idx, 0], data[idx, 1], c=labels[idx], cmap='tab10', alpha=0.5, s=20)\n",
    "    ax.set_xlabel('xâ‚')\n",
    "    ax.set_ylabel('xâ‚‚')\n",
    "    ax.set_title('Original Data')\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    ax = axes[1]\n",
    "    ax.scatter(x_recon[:, 0], x_recon[:, 1], c=labels[idx], cmap='tab10', alpha=0.5, s=20)\n",
    "    ax.set_xlabel('xâ‚')\n",
    "    ax.set_ylabel('xâ‚‚')\n",
    "    ax.set_title('Reconstructed Data')\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Reconstruction error\n",
    "    mse = np.mean((data[idx] - x_recon)**2)\n",
    "    print(f\"Mean Squared Error: {mse:.6f}\")\n",
    "\n",
    "visualize_reconstruction(model, data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generation: Sampling from the Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(model: VAE, n_samples: int = 500):\n",
    "    \"\"\"Generate new samples by sampling from prior.\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Sample from prior p(z) = N(0, I)\n",
    "        z = torch.randn(n_samples, model.latent_dim).to(device)\n",
    "        \n",
    "        # Decode\n",
    "        x_generated = model.decode(z)\n",
    "        \n",
    "        z = z.cpu().numpy()\n",
    "        x_generated = x_generated.cpu().numpy()\n",
    "    \n",
    "    return z, x_generated\n",
    "\n",
    "# Generate samples\n",
    "z_prior, x_generated = generate_samples(model)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Prior samples\n",
    "ax = axes[0]\n",
    "ax.scatter(z_prior[:, 0], z_prior[:, 1], alpha=0.5, s=10, c='purple')\n",
    "ax.set_xlabel('zâ‚')\n",
    "ax.set_ylabel('zâ‚‚')\n",
    "ax.set_title('Samples from Prior\\nz ~ N(0, I)')\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlim(-4, 4)\n",
    "ax.set_ylim(-4, 4)\n",
    "\n",
    "# Generated data\n",
    "ax = axes[1]\n",
    "ax.scatter(x_generated[:, 0], x_generated[:, 1], alpha=0.5, s=10, c='green')\n",
    "ax.set_xlabel('xâ‚')\n",
    "ax.set_ylabel('xâ‚‚')\n",
    "ax.set_title('Generated Data\\nDecoder(z)')\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# Compare with real data\n",
    "ax = axes[2]\n",
    "ax.scatter(data[:500, 0], data[:500, 1], alpha=0.3, s=10, c='blue', label='Real')\n",
    "ax.scatter(x_generated[:, 0], x_generated[:, 1], alpha=0.3, s=10, c='red', label='Generated')\n",
    "ax.set_xlabel('xâ‚')\n",
    "ax.set_ylabel('xâ‚‚')\n",
    "ax.set_title('Real vs Generated')\n",
    "ax.set_aspect('equal')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. The Effect of Î²: Information Bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train VAEs with different beta values\n",
    "betas = [0.0, 0.5, 1.0, 2.0, 5.0]\n",
    "models = {}\n",
    "histories = {}\n",
    "\n",
    "for beta in betas:\n",
    "    print(f\"\\nTraining with Î² = {beta}...\")\n",
    "    model, history = train_vae(\n",
    "        data, \n",
    "        latent_dim=2, \n",
    "        hidden_dim=64,\n",
    "        n_epochs=50,\n",
    "        batch_size=64,\n",
    "        beta=beta\n",
    "    )\n",
    "    models[beta] = model\n",
    "    histories[beta] = history\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare latent spaces\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "\n",
    "for i, beta in enumerate(betas):\n",
    "    model = models[beta]\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        tensor_data = torch.FloatTensor(data).to(device)\n",
    "        mu, log_var = model.encode(tensor_data)\n",
    "        z = model.reparameterize(mu, log_var)\n",
    "        x_recon, _, _ = model(tensor_data)\n",
    "        \n",
    "        mu = mu.cpu().numpy()\n",
    "        x_recon = x_recon.cpu().numpy()\n",
    "    \n",
    "    # Latent space\n",
    "    ax = axes[0, i]\n",
    "    scatter = ax.scatter(mu[:, 0], mu[:, 1], c=labels, cmap='tab10', alpha=0.5, s=5)\n",
    "    ax.set_xlabel('zâ‚')\n",
    "    ax.set_ylabel('zâ‚‚')\n",
    "    ax.set_title(f'Î² = {beta}\\nLatent Space')\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    # Reconstruction\n",
    "    ax = axes[1, i]\n",
    "    ax.scatter(x_recon[:500, 0], x_recon[:500, 1], c=labels[:500], cmap='tab10', alpha=0.5, s=5)\n",
    "    ax.set_xlabel('xâ‚')\n",
    "    ax.set_ylabel('xâ‚‚')\n",
    "    \n",
    "    # Compute final losses\n",
    "    recon_final = histories[beta]['recon_loss'][-1]\n",
    "    kl_final = histories[beta]['kl_loss'][-1]\n",
    "    ax.set_title(f'Recon: {recon_final:.4f}\\nKL: {kl_final:.4f}')\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "plt.suptitle('Effect of Î² on VAE: Information Bottleneck Trade-off', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ”‘ Observations:\")\n",
    "print(\"â€¢ Î² = 0: No KL â†’ encoder memorizes, poor generation\")\n",
    "print(\"â€¢ Î² = 1: Standard VAE â†’ balanced trade-off\")\n",
    "print(\"â€¢ Î² > 1: Strong regularization â†’ smoother latent, worse reconstruction\")\n",
    "print(\"â€¢ High Î²: Risk of posterior collapse (KL â†’ 0, ignores encoder)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rate-distortion trade-off\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "recons = [histories[beta]['recon_loss'][-1] for beta in betas]\n",
    "kls = [histories[beta]['kl_loss'][-1] for beta in betas]\n",
    "\n",
    "ax.scatter(kls, recons, s=200, c=betas, cmap='viridis', edgecolor='black', linewidth=2)\n",
    "\n",
    "for beta, kl, recon in zip(betas, kls, recons):\n",
    "    ax.annotate(f'Î²={beta}', (kl, recon), textcoords='offset points', \n",
    "                xytext=(10, 5), fontsize=12)\n",
    "\n",
    "ax.set_xlabel('Rate (KL Divergence) [nats]')\n",
    "ax.set_ylabel('Distortion (Reconstruction Loss)')\n",
    "ax.set_title('Rate-Distortion Trade-off\\nVAE as Information Bottleneck')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add arrow showing trade-off direction\n",
    "ax.annotate('', xy=(kls[-2], recons[-2]), xytext=(kls[1], recons[1]),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "ax.text(np.mean(kls), np.mean(recons) + 0.002, 'Increasing Î²', color='red', fontsize=12, ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Rate-Distortion Analysis:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Î²':<8} {'Rate (KL)':<15} {'Distortion (MSE)':<15}\")\n",
    "print(\"-\"*50)\n",
    "for beta, kl, recon in zip(betas, kls, recons):\n",
    "    print(f\"{beta:<8} {kl:<15.4f} {recon:<15.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Latent Space Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_latent(model: VAE, x1: np.ndarray, x2: np.ndarray, n_steps: int = 10):\n",
    "    \"\"\"Interpolate between two points in latent space.\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Encode both points\n",
    "        x1_tensor = torch.FloatTensor(x1).unsqueeze(0).to(device)\n",
    "        x2_tensor = torch.FloatTensor(x2).unsqueeze(0).to(device)\n",
    "        \n",
    "        mu1, _ = model.encode(x1_tensor)\n",
    "        mu2, _ = model.encode(x2_tensor)\n",
    "        \n",
    "        # Interpolate in latent space\n",
    "        alphas = np.linspace(0, 1, n_steps)\n",
    "        interpolations = []\n",
    "        latents = []\n",
    "        \n",
    "        for alpha in alphas:\n",
    "            z = (1 - alpha) * mu1 + alpha * mu2\n",
    "            x_interp = model.decode(z)\n",
    "            interpolations.append(x_interp.cpu().numpy())\n",
    "            latents.append(z.cpu().numpy())\n",
    "        \n",
    "        interpolations = np.array(interpolations).squeeze()\n",
    "        latents = np.array(latents).squeeze()\n",
    "    \n",
    "    return interpolations, latents\n",
    "\n",
    "# Select two points from different clusters\n",
    "idx1 = np.where(labels == 0)[0][0]\n",
    "idx2 = np.where(labels == 2)[0][0]\n",
    "\n",
    "x1 = data[idx1]\n",
    "x2 = data[idx2]\n",
    "\n",
    "# Use the Î²=1 model\n",
    "model = models[1.0]\n",
    "interpolations, latents = interpolate_latent(model, x1, x2, n_steps=20)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Latent space interpolation\n",
    "ax = axes[0]\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    mu, _ = model.encode(torch.FloatTensor(data).to(device))\n",
    "    mu = mu.cpu().numpy()\n",
    "\n",
    "ax.scatter(mu[:, 0], mu[:, 1], c=labels, cmap='tab10', alpha=0.2, s=5)\n",
    "ax.plot(latents[:, 0], latents[:, 1], 'r-', linewidth=2, marker='o', markersize=8)\n",
    "ax.plot(latents[0, 0], latents[0, 1], 'gs', markersize=15, label='Start')\n",
    "ax.plot(latents[-1, 0], latents[-1, 1], 'g^', markersize=15, label='End')\n",
    "ax.set_xlabel('zâ‚')\n",
    "ax.set_ylabel('zâ‚‚')\n",
    "ax.set_title('Interpolation in Latent Space')\n",
    "ax.legend()\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# Data space interpolation\n",
    "ax = axes[1]\n",
    "ax.scatter(data[:, 0], data[:, 1], c=labels, cmap='tab10', alpha=0.2, s=5)\n",
    "ax.plot(interpolations[:, 0], interpolations[:, 1], 'r-', linewidth=2, marker='o', markersize=8)\n",
    "ax.plot(x1[0], x1[1], 'gs', markersize=15, label='Start')\n",
    "ax.plot(x2[0], x2[1], 'g^', markersize=15, label='End')\n",
    "ax.set_xlabel('xâ‚')\n",
    "ax.set_ylabel('xâ‚‚')\n",
    "ax.set_title('Corresponding Points in Data Space')\n",
    "ax.legend()\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ”‘ Observation:\")\n",
    "print(\"Interpolation in latent space produces smooth transitions in data space!\")\n",
    "print(\"This is because KL regularization ensures a continuous latent manifold.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### VAE Key Equations\n",
    "\n",
    "| Component | Formula |\n",
    "|-----------|--------|\n",
    "| ELBO | $\\mathcal{L} = \\mathbb{E}_q[\\log p(x|z)] - D_{KL}(q(z|x) \\| p(z))$ |\n",
    "| KL Loss | $\\frac{1}{2}\\sum_j(\\mu_j^2 + \\sigma_j^2 - 1 - \\log\\sigma_j^2)$ |\n",
    "| Reparameterization | $z = \\mu + \\sigma \\odot \\epsilon$, $\\epsilon \\sim N(0,I)$ |\n",
    "| Î²-VAE Loss | $\\text{Recon} + \\beta \\cdot \\text{KL}$ |\n",
    "\n",
    "### Entropy/Information Perspective\n",
    "\n",
    "1. **KL to prior** = Rate (information bottleneck)\n",
    "2. **Reconstruction loss** = Distortion\n",
    "3. **Î²** controls the rate-distortion trade-off\n",
    "4. Higher Î² â†’ more compression â†’ smoother latent space\n",
    "5. Lower Î² â†’ less compression â†’ better reconstruction\n",
    "\n",
    "### Practical Insights\n",
    "\n",
    "1. **Reparameterization** is essential for gradient flow\n",
    "2. **KL regularization** enables generation from prior\n",
    "3. **Î² tuning** is crucial for balancing quality vs smoothness\n",
    "4. **Latent interpolation** reveals learned structure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
