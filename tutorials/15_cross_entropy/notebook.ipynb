{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Tutorial 15: Cross-Entropy — Code\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook provides interactive code examples for the Cross-Entropy tutorial.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"\\n\",\n",
    "    \"sns.set_theme(style=\\\"whitegrid\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Cross-Entropy Implementation\\n\",\n",
    "    \"\\n\",\n",
    "    \"This corresponds to Exercise B1. We'll create a numerically stable function to calculate cross-entropy.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def cross_entropy(P, Q):\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Calculates the cross-entropy between two probability distributions.\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    Args:\\n\",\n",
    "    \"        P (np.array): The true distribution (one-hot).\\n\",\n",
    "    \"        Q (np.array): The predicted distribution.\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    Returns:\\n\",\n",
    "    \"        float: The cross-entropy loss.\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    # Add a small epsilon for numerical stability to prevent log(0)\\n\",\n",
    "    \"    epsilon = 1e-9\\n\",\n",
    "    \"    Q = np.clip(Q, epsilon, 1. - epsilon)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return -np.sum(P * np.log(Q))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Verification\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's test the function with the example from Exercise A3.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"P_banana = np.array([0, 1, 0])\\n\",\n",
    "    \"Q_banana = np.array([0.2, 0.5, 0.3])\\n\",\n",
    "    \"\\n\",\n",
    "    \"loss = cross_entropy(P_banana, Q_banana)\\n\",\n",
    "    \"print(f\\\"Calculated Loss: {loss:.3f}\\\")\\n\",\n",
    "    \"print(f\\\"Expected Loss (-ln(0.5)): {-np.log(0.5):.3f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Visualizing BCE vs. MSE Loss\\n\",\n",
    "    \"\\n\",\n",
    "    \"This plot, from Exercise B2, is crucial for understanding why Cross-Entropy is preferred over Mean Squared Error for classification. It shows how each loss function penalizes incorrect predictions.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Predicted probabilities for the correct class (where true label y=1)\\n\",\n",
    "    \"p_hat = np.linspace(0.01, 0.99, 200)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Binary Cross-Entropy Loss for y=1 is -log(p_hat)\\n\",\n",
    "    \"bce_loss = -np.log(p_hat)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Mean Squared Error Loss for y=1 is (1-p_hat)^2\\n\",\n",
    "    \"mse_loss = (1 - p_hat)**2\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.figure(figsize=(12, 7))\\n\",\n",
    "    \"plt.plot(p_hat, bce_loss, label='Binary Cross-Entropy Loss', color='darkred', lw=2.5)\\n\",\n",
    "    \"plt.plot(p_hat, mse_loss, label='Mean Squared Error Loss', color='darkblue', lw=2.5, linestyle='--')\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.xlabel(\\\"Predicted Probability for Correct Class (p̂)\\\", fontsize=12)\\n\",\n",
    "    \"plt.ylabel(\\\"Loss Value\\\", fontsize=12)\\n\",\n",
    "    \"plt.title(\\\"BCE vs. MSE Loss (When True Label is 1)\\\", fontsize=14)\\n\",\n",
    "    \"plt.legend()\\n\",\n",
    "    \"plt.grid(True, which='both', linestyle='--', linewidth=0.5)\\n\",\n",
    "    \"plt.ylim(0, 5) # Limit y-axis to better see the shapes\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Analysis of the Plot\\n\",\n",
    "    \"\\n\",\n",
    "    \"1.  **When the prediction is good (p̂ → 1)**: Both losses approach 0.\\n\",\n",
    "    \"2.  **When the prediction is bad (p̂ → 0)**:\\n\",\n",
    "    \"    -   **Cross-Entropy** shoots up towards infinity. This provides a large gradient for the model to learn from its mistake.\\n\",\n",
    "    \"    -   **MSE** flattens out. The gradient becomes very small, meaning the model learns very slowly from its most confident mistakes. This is the primary reason MSE is a poor choice for classification.\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.9.7\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}\n"
   ],
   "id": "be7226aa979c25ea"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
