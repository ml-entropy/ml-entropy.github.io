{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìà Normal & Multivariate Normal Distributions\n",
    "\n",
    "This notebook explores:\n",
    "1. Why Gaussians maximize entropy\n",
    "2. Univariate normal distribution\n",
    "3. Multivariate normal distribution\n",
    "4. Covariance matrices and their geometry\n",
    "5. Mahalanobis distance\n",
    "6. The reparameterization trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.linalg import cholesky\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.transforms as transforms\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Gaussians? Maximum Entropy!\n",
    "\n",
    "Among all distributions with fixed mean and variance, Gaussian has **maximum entropy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def differential_entropy_gaussian(sigma):\n",
    "    \"\"\"Differential entropy of Gaussian N(0, sigma^2).\"\"\"\n",
    "    return 0.5 * np.log(2 * np.pi * np.e * sigma**2)\n",
    "\n",
    "def differential_entropy_uniform(a, b):\n",
    "    \"\"\"Differential entropy of Uniform(a, b).\"\"\"\n",
    "    return np.log(b - a)\n",
    "\n",
    "def differential_entropy_laplace(b):\n",
    "    \"\"\"Differential entropy of Laplace(0, b).\"\"\"\n",
    "    return 1 + np.log(2 * b)\n",
    "\n",
    "# Compare distributions with same variance\n",
    "target_variance = 1.0\n",
    "\n",
    "# Gaussian: variance = sigma^2\n",
    "sigma_gaussian = np.sqrt(target_variance)\n",
    "h_gaussian = differential_entropy_gaussian(sigma_gaussian)\n",
    "\n",
    "# Uniform: variance = (b-a)^2 / 12, so b-a = sqrt(12 * var)\n",
    "width = np.sqrt(12 * target_variance)\n",
    "h_uniform = differential_entropy_uniform(-width/2, width/2)\n",
    "\n",
    "# Laplace: variance = 2*b^2, so b = sqrt(var/2)\n",
    "b_laplace = np.sqrt(target_variance / 2)\n",
    "h_laplace = differential_entropy_laplace(b_laplace)\n",
    "\n",
    "print(\"Comparing Distributions with Variance = 1\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Gaussian N(0, 1):    H = {h_gaussian:.4f} nats\")\n",
    "print(f\"Laplace(0, {b_laplace:.3f}):  H = {h_laplace:.4f} nats\")\n",
    "print(f\"Uniform(-{width/2:.2f}, {width/2:.2f}): H = {h_uniform:.4f} nats\")\n",
    "print(f\"\\nüîë Gaussian has maximum entropy!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "x = np.linspace(-4, 4, 1000)\n",
    "\n",
    "# PDFs\n",
    "ax = axes[0]\n",
    "ax.plot(x, stats.norm.pdf(x, 0, sigma_gaussian), 'b-', linewidth=2, \n",
    "        label=f'Gaussian (H={h_gaussian:.3f})')\n",
    "ax.plot(x, stats.laplace.pdf(x, 0, b_laplace), 'r--', linewidth=2, \n",
    "        label=f'Laplace (H={h_laplace:.3f})')\n",
    "ax.plot(x, stats.uniform.pdf(x, -width/2, width), 'g:', linewidth=2, \n",
    "        label=f'Uniform (H={h_uniform:.3f})')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Distributions with Same Variance\\n(Gaussian has Maximum Entropy)')\n",
    "ax.legend()\n",
    "ax.set_xlim(-4, 4)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Entropy vs variance\n",
    "ax = axes[1]\n",
    "variances = np.linspace(0.1, 5, 100)\n",
    "h_gauss = [differential_entropy_gaussian(np.sqrt(v)) for v in variances]\n",
    "\n",
    "ax.plot(variances, h_gauss, 'b-', linewidth=2, label='Gaussian entropy')\n",
    "ax.fill_between(variances, h_gauss, -2, alpha=0.2, color='blue', \n",
    "                label='Region of lower entropy\\n(more assumptions)')\n",
    "ax.set_xlabel('Variance œÉ¬≤')\n",
    "ax.set_ylabel('Differential Entropy H [nats]')\n",
    "ax.set_title('Gaussian Entropy: $H = \\\\frac{1}{2}\\\\log(2\\\\pi e \\\\sigma^2)$')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Univariate Normal Distribution\n",
    "\n",
    "$$\\mathcal{N}(x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_pdf(x, mu, sigma):\n",
    "    \"\"\"Univariate Gaussian PDF.\"\"\"\n",
    "    return (1 / (np.sqrt(2 * np.pi) * sigma)) * np.exp(-0.5 * ((x - mu) / sigma)**2)\n",
    "\n",
    "def gaussian_log_pdf(x, mu, sigma):\n",
    "    \"\"\"Log of Gaussian PDF (more numerically stable).\"\"\"\n",
    "    return -0.5 * np.log(2 * np.pi * sigma**2) - 0.5 * ((x - mu) / sigma)**2\n",
    "\n",
    "# Visualize different Gaussians\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "x = np.linspace(-8, 8, 1000)\n",
    "\n",
    "# Varying mean\n",
    "ax = axes[0]\n",
    "for mu in [-2, 0, 2]:\n",
    "    ax.plot(x, gaussian_pdf(x, mu, 1), linewidth=2, label=f'Œº={mu}')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('p(x)')\n",
    "ax.set_title('Varying Mean (œÉ=1)\\n\"Location parameter\"')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Varying variance\n",
    "ax = axes[1]\n",
    "for sigma in [0.5, 1, 2]:\n",
    "    ax.plot(x, gaussian_pdf(x, 0, sigma), linewidth=2, label=f'œÉ={sigma}')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('p(x)')\n",
    "ax.set_title('Varying Std Dev (Œº=0)\\n\"Scale parameter\"')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 68-95-99.7 rule\n",
    "ax = axes[2]\n",
    "mu, sigma = 0, 1\n",
    "ax.plot(x, gaussian_pdf(x, mu, sigma), 'b-', linewidth=2)\n",
    "\n",
    "# Fill regions\n",
    "x1 = np.linspace(-1, 1, 100)\n",
    "x2 = np.linspace(-2, 2, 100)\n",
    "x3 = np.linspace(-3, 3, 100)\n",
    "\n",
    "ax.fill_between(x3, gaussian_pdf(x3, mu, sigma), alpha=0.2, color='green', label='99.7% (¬±3œÉ)')\n",
    "ax.fill_between(x2, gaussian_pdf(x2, mu, sigma), alpha=0.3, color='yellow', label='95% (¬±2œÉ)')\n",
    "ax.fill_between(x1, gaussian_pdf(x1, mu, sigma), alpha=0.4, color='red', label='68% (¬±1œÉ)')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('p(x)')\n",
    "ax.set_title('The 68-95-99.7 Rule')\n",
    "ax.legend()\n",
    "ax.set_xlim(-4, 4)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE Loss = Gaussian Likelihood\n",
    "print(\"üîë Key Insight: MSE Loss assumes Gaussian noise!\")\n",
    "print(\"\")\n",
    "print(\"If y = f(x) + Œµ, where Œµ ~ N(0, œÉ¬≤)\")\n",
    "print(\"Then: p(y|x) = N(y | f(x), œÉ¬≤)\")\n",
    "print(\"\")\n",
    "print(\"Negative log-likelihood:\")\n",
    "print(\"-log p(y|x) = (1/2)log(2œÄœÉ¬≤) + (y - f(x))¬≤ / (2œÉ¬≤)\")\n",
    "print(\"            ‚àù (y - f(x))¬≤  [MSE!]\")\n",
    "\n",
    "# Demonstrate\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Generate noisy data\n",
    "np.random.seed(42)\n",
    "x_data = np.linspace(0, 10, 50)\n",
    "y_true = 2 * x_data + 1  # True line\n",
    "y_noisy = y_true + np.random.normal(0, 2, len(x_data))  # Add Gaussian noise\n",
    "\n",
    "ax = axes[0]\n",
    "ax.scatter(x_data, y_noisy, alpha=0.6, label='Noisy data')\n",
    "ax.plot(x_data, y_true, 'r-', linewidth=2, label='True function')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('Data with Gaussian Noise')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Show residuals are Gaussian\n",
    "ax = axes[1]\n",
    "residuals = y_noisy - y_true\n",
    "ax.hist(residuals, bins=15, density=True, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "\n",
    "x_fit = np.linspace(-6, 6, 100)\n",
    "ax.plot(x_fit, gaussian_pdf(x_fit, 0, 2), 'r-', linewidth=2, label='N(0, 2¬≤)')\n",
    "\n",
    "ax.set_xlabel('Residual (y - f(x))')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Residuals ‚âà Gaussian\\n‚Üí MSE is appropriate loss')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multivariate Normal Distribution\n",
    "\n",
    "$$\\mathcal{N}(\\mathbf{x} | \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{(2\\pi)^{d/2}|\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_gaussian_pdf(x, mu, sigma):\n",
    "    \"\"\"Multivariate Gaussian PDF.\"\"\"\n",
    "    d = len(mu)\n",
    "    diff = x - mu\n",
    "    sigma_inv = np.linalg.inv(sigma)\n",
    "    det = np.linalg.det(sigma)\n",
    "    \n",
    "    norm_const = 1 / ((2 * np.pi)**(d/2) * np.sqrt(det))\n",
    "    exponent = -0.5 * diff.T @ sigma_inv @ diff\n",
    "    \n",
    "    return norm_const * np.exp(exponent)\n",
    "\n",
    "\n",
    "def plot_2d_gaussian(ax, mu, sigma, color='blue', label='', n_std=3):\n",
    "    \"\"\"Plot 2D Gaussian contours.\"\"\"\n",
    "    # Create grid\n",
    "    x = np.linspace(mu[0] - n_std*3, mu[0] + n_std*3, 100)\n",
    "    y = np.linspace(mu[1] - n_std*3, mu[1] + n_std*3, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    # Compute PDF\n",
    "    pos = np.dstack((X, Y))\n",
    "    rv = stats.multivariate_normal(mu, sigma)\n",
    "    Z = rv.pdf(pos)\n",
    "    \n",
    "    # Plot contours\n",
    "    ax.contour(X, Y, Z, levels=5, colors=color, alpha=0.7)\n",
    "    ax.plot(*mu, 'o', color=color, markersize=10)\n",
    "    \n",
    "    # Add confidence ellipses\n",
    "    for n in [1, 2]:\n",
    "        add_confidence_ellipse(ax, mu, sigma, n, color=color)\n",
    "\n",
    "\n",
    "def add_confidence_ellipse(ax, mu, sigma, n_std, color='blue'):\n",
    "    \"\"\"Add confidence ellipse to plot.\"\"\"\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(sigma)\n",
    "    \n",
    "    # Sort by eigenvalue\n",
    "    order = eigenvalues.argsort()[::-1]\n",
    "    eigenvalues = eigenvalues[order]\n",
    "    eigenvectors = eigenvectors[:, order]\n",
    "    \n",
    "    # Compute angle\n",
    "    angle = np.degrees(np.arctan2(eigenvectors[1, 0], eigenvectors[0, 0]))\n",
    "    \n",
    "    # Width and height are 2*n_std*sqrt(eigenvalue)\n",
    "    width, height = 2 * n_std * np.sqrt(eigenvalues)\n",
    "    \n",
    "    ellipse = Ellipse(mu, width, height, angle=angle, \n",
    "                      fill=False, color=color, linestyle='--', linewidth=1.5)\n",
    "    ax.add_patch(ellipse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize different covariance structures\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "mu = np.array([0, 0])\n",
    "\n",
    "# Spherical covariance\n",
    "ax = axes[0]\n",
    "sigma_spherical = np.array([[1, 0], [0, 1]])\n",
    "samples = np.random.multivariate_normal(mu, sigma_spherical, 500)\n",
    "ax.scatter(samples[:, 0], samples[:, 1], alpha=0.3, s=10)\n",
    "plot_2d_gaussian(ax, mu, sigma_spherical, 'red')\n",
    "ax.set_title('Spherical: Œ£ = I\\n(Uncorrelated, equal variance)')\n",
    "ax.set_xlabel('x‚ÇÅ')\n",
    "ax.set_ylabel('x‚ÇÇ')\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlim(-4, 4)\n",
    "ax.set_ylim(-4, 4)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Diagonal covariance\n",
    "ax = axes[1]\n",
    "sigma_diagonal = np.array([[2, 0], [0, 0.5]])\n",
    "samples = np.random.multivariate_normal(mu, sigma_diagonal, 500)\n",
    "ax.scatter(samples[:, 0], samples[:, 1], alpha=0.3, s=10)\n",
    "plot_2d_gaussian(ax, mu, sigma_diagonal, 'red')\n",
    "ax.set_title('Diagonal: Œ£ = diag(œÉ‚ÇÅ¬≤, œÉ‚ÇÇ¬≤)\\n(Uncorrelated, different variance)')\n",
    "ax.set_xlabel('x‚ÇÅ')\n",
    "ax.set_ylabel('x‚ÇÇ')\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlim(-4, 4)\n",
    "ax.set_ylim(-4, 4)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Full covariance\n",
    "ax = axes[2]\n",
    "sigma_full = np.array([[2, 1.2], [1.2, 1]])\n",
    "samples = np.random.multivariate_normal(mu, sigma_full, 500)\n",
    "ax.scatter(samples[:, 0], samples[:, 1], alpha=0.3, s=10)\n",
    "plot_2d_gaussian(ax, mu, sigma_full, 'red')\n",
    "ax.set_title('Full: Œ£ with off-diagonal terms\\n(Correlated)')\n",
    "ax.set_xlabel('x‚ÇÅ')\n",
    "ax.set_ylabel('x‚ÇÇ')\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlim(-4, 4)\n",
    "ax.set_ylim(-4, 4)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Covariance matrices:\")\n",
    "print(f\"\\nSpherical:\\n{sigma_spherical}\")\n",
    "print(f\"\\nDiagonal:\\n{sigma_diagonal}\")\n",
    "print(f\"\\nFull:\\n{sigma_full}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D visualization of bivariate Gaussian\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Create grid\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = np.linspace(-3, 3, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "pos = np.dstack((X, Y))\n",
    "\n",
    "# Correlated Gaussian\n",
    "mu = np.array([0, 0])\n",
    "sigma = np.array([[1, 0.8], [0.8, 1]])\n",
    "rv = stats.multivariate_normal(mu, sigma)\n",
    "Z = rv.pdf(pos)\n",
    "\n",
    "# 3D surface\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\n",
    "ax1.set_xlabel('x‚ÇÅ')\n",
    "ax1.set_ylabel('x‚ÇÇ')\n",
    "ax1.set_zlabel('p(x)')\n",
    "ax1.set_title('3D View of Bivariate Gaussian\\n(œÅ = 0.8)')\n",
    "\n",
    "# Contour plot\n",
    "ax2 = fig.add_subplot(122)\n",
    "contour = ax2.contourf(X, Y, Z, levels=20, cmap='viridis')\n",
    "plt.colorbar(contour, ax=ax2, label='p(x)')\n",
    "ax2.set_xlabel('x‚ÇÅ')\n",
    "ax2.set_ylabel('x‚ÇÇ')\n",
    "ax2.set_title('Contour View\\n(Elliptical level sets)')\n",
    "ax2.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mahalanobis Distance\n",
    "\n",
    "$$d_M(\\mathbf{x}) = \\sqrt{(\\mathbf{x}-\\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})}$$\n",
    "\n",
    "Points with equal Mahalanobis distance have equal probability density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mahalanobis_distance(x, mu, sigma):\n",
    "    \"\"\"Compute Mahalanobis distance.\"\"\"\n",
    "    diff = x - mu\n",
    "    sigma_inv = np.linalg.inv(sigma)\n",
    "    return np.sqrt(diff.T @ sigma_inv @ diff)\n",
    "\n",
    "\n",
    "def euclidean_distance(x, mu):\n",
    "    \"\"\"Compute Euclidean distance.\"\"\"\n",
    "    return np.linalg.norm(x - mu)\n",
    "\n",
    "\n",
    "# Compare Euclidean vs Mahalanobis\n",
    "mu = np.array([0, 0])\n",
    "sigma = np.array([[4, 2], [2, 2]])\n",
    "\n",
    "# Two points at same Euclidean distance but different Mahalanobis\n",
    "point_a = np.array([2, 0])  # Along major axis\n",
    "point_b = np.array([0, 2])  # Along minor axis\n",
    "\n",
    "print(\"Comparing Euclidean vs Mahalanobis Distance\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nCovariance matrix:\")\n",
    "print(sigma)\n",
    "print(f\"\\nPoint A = {point_a}:\")\n",
    "print(f\"  Euclidean:    {euclidean_distance(point_a, mu):.3f}\")\n",
    "print(f\"  Mahalanobis:  {mahalanobis_distance(point_a, mu, sigma):.3f}\")\n",
    "print(f\"\\nPoint B = {point_b}:\")\n",
    "print(f\"  Euclidean:    {euclidean_distance(point_b, mu):.3f}\")\n",
    "print(f\"  Mahalanobis:  {mahalanobis_distance(point_b, mu, sigma):.3f}\")\n",
    "print(f\"\\nüîë Same Euclidean distance, but different Mahalanobis!\")\n",
    "print(f\"Point B is more 'surprising' given the covariance structure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the difference\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Create grid\n",
    "x = np.linspace(-4, 4, 100)\n",
    "y = np.linspace(-4, 4, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Euclidean distance contours\n",
    "ax = axes[0]\n",
    "Z_euclidean = np.sqrt(X**2 + Y**2)\n",
    "contour = ax.contourf(X, Y, Z_euclidean, levels=20, cmap='viridis')\n",
    "plt.colorbar(contour, ax=ax, label='Euclidean Distance')\n",
    "\n",
    "# Add points\n",
    "ax.plot(*point_a, 'r^', markersize=15, label=f'A: d_E={euclidean_distance(point_a, mu):.1f}')\n",
    "ax.plot(*point_b, 'rs', markersize=15, label=f'B: d_E={euclidean_distance(point_b, mu):.1f}')\n",
    "ax.plot(*mu, 'ko', markersize=10)\n",
    "ax.set_xlabel('x‚ÇÅ')\n",
    "ax.set_ylabel('x‚ÇÇ')\n",
    "ax.set_title('Euclidean Distance\\n(Ignores covariance structure)')\n",
    "ax.legend()\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# Mahalanobis distance contours\n",
    "ax = axes[1]\n",
    "sigma_inv = np.linalg.inv(sigma)\n",
    "Z_mahal = np.zeros_like(X)\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        pt = np.array([X[i,j], Y[i,j]])\n",
    "        Z_mahal[i,j] = mahalanobis_distance(pt, mu, sigma)\n",
    "\n",
    "contour = ax.contourf(X, Y, Z_mahal, levels=20, cmap='viridis')\n",
    "plt.colorbar(contour, ax=ax, label='Mahalanobis Distance')\n",
    "\n",
    "# Add points\n",
    "ax.plot(*point_a, 'r^', markersize=15, label=f'A: d_M={mahalanobis_distance(point_a, mu, sigma):.2f}')\n",
    "ax.plot(*point_b, 'rs', markersize=15, label=f'B: d_M={mahalanobis_distance(point_b, mu, sigma):.2f}')\n",
    "ax.plot(*mu, 'ko', markersize=10)\n",
    "ax.set_xlabel('x‚ÇÅ')\n",
    "ax.set_ylabel('x‚ÇÇ')\n",
    "ax.set_title('Mahalanobis Distance\\n(Accounts for covariance structure)')\n",
    "ax.legend()\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The Reparameterization Trick\n",
    "\n",
    "To sample $\\mathbf{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$:\n",
    "\n",
    "1. Sample $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$\n",
    "2. Compute $\\mathbf{x} = \\boldsymbol{\\mu} + \\mathbf{L}\\boldsymbol{\\epsilon}$ where $\\boldsymbol{\\Sigma} = \\mathbf{L}\\mathbf{L}^T$\n",
    "\n",
    "For diagonal covariance (VAE): $\\mathbf{x} = \\boldsymbol{\\mu} + \\boldsymbol{\\sigma} \\odot \\boldsymbol{\\epsilon}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_reparameterized(mu, sigma, n_samples):\n",
    "    \"\"\"\n",
    "    Sample from N(mu, sigma) using reparameterization trick.\n",
    "    \n",
    "    For full covariance: x = mu + L @ epsilon\n",
    "    where L is Cholesky decomposition of sigma.\n",
    "    \"\"\"\n",
    "    d = len(mu)\n",
    "    L = cholesky(sigma, lower=True)\n",
    "    \n",
    "    # Sample from standard normal\n",
    "    epsilon = np.random.randn(n_samples, d)\n",
    "    \n",
    "    # Transform\n",
    "    samples = mu + (L @ epsilon.T).T\n",
    "    \n",
    "    return samples, epsilon\n",
    "\n",
    "\n",
    "def sample_diagonal_reparameterized(mu, sigma_diag, n_samples):\n",
    "    \"\"\"\n",
    "    Sample from N(mu, diag(sigma^2)) - the VAE case.\n",
    "    x = mu + sigma * epsilon\n",
    "    \"\"\"\n",
    "    d = len(mu)\n",
    "    epsilon = np.random.randn(n_samples, d)\n",
    "    samples = mu + sigma_diag * epsilon\n",
    "    return samples, epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the reparameterization trick\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "n_samples = 1000\n",
    "\n",
    "# Step 1: Sample from standard normal\n",
    "ax = axes[0]\n",
    "mu_standard = np.array([0, 0])\n",
    "sigma_standard = np.eye(2)\n",
    "epsilon = np.random.randn(n_samples, 2)\n",
    "\n",
    "ax.scatter(epsilon[:, 0], epsilon[:, 1], alpha=0.3, s=10, c='blue')\n",
    "ax.set_xlim(-4, 4)\n",
    "ax.set_ylim(-4, 4)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlabel('Œµ‚ÇÅ')\n",
    "ax.set_ylabel('Œµ‚ÇÇ')\n",
    "ax.set_title('Step 1: Œµ ~ N(0, I)\\n(Standard normal noise)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Step 2: Transform\n",
    "ax = axes[1]\n",
    "mu_target = np.array([1, 2])\n",
    "sigma_target = np.array([[2, 1], [1, 1.5]])\n",
    "L = cholesky(sigma_target, lower=True)\n",
    "\n",
    "# Show transformation\n",
    "ax.annotate('', xy=(1.5, 0), xytext=(0, 0),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "ax.annotate('', xy=(0.7, 1.2), xytext=(0, 0),\n",
    "            arrowprops=dict(arrowstyle='->', color='green', lw=2))\n",
    "\n",
    "ax.text(0.8, -0.3, 'L[:, 0]', color='red', fontsize=12)\n",
    "ax.text(-0.5, 0.6, 'L[:, 1]', color='green', fontsize=12)\n",
    "\n",
    "ax.set_xlim(-2, 3)\n",
    "ax.set_ylim(-2, 3)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlabel('x‚ÇÅ')\n",
    "ax.set_ylabel('x‚ÇÇ')\n",
    "ax.set_title('Step 2: Transform\\nx = Œº + L @ Œµ')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Show Cholesky matrix\n",
    "ax.text(0.5, 2.5, f'L = \\n{L.round(2)}', fontsize=10, family='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Step 3: Result\n",
    "ax = axes[2]\n",
    "samples = mu_target + (L @ epsilon.T).T\n",
    "\n",
    "ax.scatter(samples[:, 0], samples[:, 1], alpha=0.3, s=10, c='purple')\n",
    "plot_2d_gaussian(ax, mu_target, sigma_target, 'red')\n",
    "ax.set_xlim(-4, 6)\n",
    "ax.set_ylim(-2, 6)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlabel('x‚ÇÅ')\n",
    "ax.set_ylabel('x‚ÇÇ')\n",
    "ax.set_title(f'Step 3: x ~ N(Œº, Œ£)\\nŒº={mu_target}, det(Œ£)={np.linalg.det(sigma_target):.2f}')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE-style reparameterization (diagonal covariance)\n",
    "print(\"VAE Reparameterization Trick\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nEncoder outputs: Œº(x) and log_var(x)\")\n",
    "print(\"We want to sample: z ~ N(Œº, diag(exp(log_var)))\")\n",
    "print(\"\\nReparameterization:\")\n",
    "print(\"  1. Œµ ~ N(0, I)\")\n",
    "print(\"  2. œÉ = exp(0.5 * log_var)\")\n",
    "print(\"  3. z = Œº + œÉ ‚äô Œµ\")\n",
    "print(\"\\nüîë Key benefit: Gradients flow through Œº and log_var!\")\n",
    "\n",
    "# Demonstrate\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Without reparameterization (can't backprop)\n",
    "ax = axes[0]\n",
    "ax.text(0.5, 0.8, 'x', fontsize=20, ha='center', transform=ax.transAxes)\n",
    "ax.annotate('', xy=(0.5, 0.65), xytext=(0.5, 0.75),\n",
    "            arrowprops=dict(arrowstyle='->', color='black'), \n",
    "            xycoords='axes fraction', textcoords='axes fraction')\n",
    "ax.text(0.5, 0.55, 'Encoder\\nŒº(x), œÉ(x)', fontsize=12, ha='center', transform=ax.transAxes,\n",
    "        bbox=dict(boxstyle='round', facecolor='lightblue'))\n",
    "ax.annotate('', xy=(0.5, 0.4), xytext=(0.5, 0.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='black'),\n",
    "            xycoords='axes fraction', textcoords='axes fraction')\n",
    "ax.text(0.5, 0.3, 'z ~ N(Œº, œÉ¬≤)', fontsize=14, ha='center', transform=ax.transAxes,\n",
    "        bbox=dict(boxstyle='round', facecolor='lightyellow'))\n",
    "ax.text(0.5, 0.15, '‚ùå Can\\'t backprop\\nthrough sampling!', fontsize=12, ha='center', \n",
    "        transform=ax.transAxes, color='red')\n",
    "ax.axis('off')\n",
    "ax.set_title('Without Reparameterization', fontsize=14)\n",
    "\n",
    "# With reparameterization (can backprop)\n",
    "ax = axes[1]\n",
    "ax.text(0.5, 0.9, 'x', fontsize=20, ha='center', transform=ax.transAxes)\n",
    "ax.annotate('', xy=(0.5, 0.8), xytext=(0.5, 0.87),\n",
    "            arrowprops=dict(arrowstyle='->', color='black'),\n",
    "            xycoords='axes fraction', textcoords='axes fraction')\n",
    "ax.text(0.5, 0.7, 'Encoder\\nŒº(x), œÉ(x)', fontsize=12, ha='center', transform=ax.transAxes,\n",
    "        bbox=dict(boxstyle='round', facecolor='lightblue'))\n",
    "\n",
    "# Œµ branch\n",
    "ax.text(0.15, 0.55, 'Œµ ~ N(0,I)', fontsize=11, ha='center', transform=ax.transAxes,\n",
    "        bbox=dict(boxstyle='round', facecolor='lightgray'))\n",
    "ax.annotate('', xy=(0.35, 0.45), xytext=(0.2, 0.52),\n",
    "            arrowprops=dict(arrowstyle='->', color='gray'),\n",
    "            xycoords='axes fraction', textcoords='axes fraction')\n",
    "\n",
    "# Œº, œÉ branch\n",
    "ax.annotate('', xy=(0.4, 0.55), xytext=(0.5, 0.65),\n",
    "            arrowprops=dict(arrowstyle='->', color='blue'),\n",
    "            xycoords='axes fraction', textcoords='axes fraction')\n",
    "ax.annotate('', xy=(0.6, 0.55), xytext=(0.5, 0.65),\n",
    "            arrowprops=dict(arrowstyle='->', color='blue'),\n",
    "            xycoords='axes fraction', textcoords='axes fraction')\n",
    "ax.text(0.35, 0.52, 'Œº', fontsize=14, ha='center', transform=ax.transAxes, color='blue')\n",
    "ax.text(0.65, 0.52, 'œÉ', fontsize=14, ha='center', transform=ax.transAxes, color='blue')\n",
    "\n",
    "# Combine\n",
    "ax.annotate('', xy=(0.5, 0.35), xytext=(0.35, 0.48),\n",
    "            arrowprops=dict(arrowstyle='->', color='blue'),\n",
    "            xycoords='axes fraction', textcoords='axes fraction')\n",
    "ax.annotate('', xy=(0.5, 0.35), xytext=(0.65, 0.48),\n",
    "            arrowprops=dict(arrowstyle='->', color='blue'),\n",
    "            xycoords='axes fraction', textcoords='axes fraction')\n",
    "\n",
    "ax.text(0.5, 0.38, 'z = Œº + œÉ ‚äô Œµ', fontsize=14, ha='center', transform=ax.transAxes,\n",
    "        bbox=dict(boxstyle='round', facecolor='lightgreen'))\n",
    "ax.text(0.5, 0.2, '‚úÖ Gradients flow\\nthrough Œº and œÉ!', fontsize=12, ha='center',\n",
    "        transform=ax.transAxes, color='green')\n",
    "ax.axis('off')\n",
    "ax.set_title('With Reparameterization', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. KL Divergence Between Gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_gaussian_multivariate(mu1, sigma1, mu2, sigma2):\n",
    "    \"\"\"\n",
    "    KL divergence between two multivariate Gaussians.\n",
    "    D_KL(N(mu1, sigma1) || N(mu2, sigma2))\n",
    "    \"\"\"\n",
    "    d = len(mu1)\n",
    "    sigma2_inv = np.linalg.inv(sigma2)\n",
    "    \n",
    "    term1 = np.log(np.linalg.det(sigma2) / np.linalg.det(sigma1))\n",
    "    term2 = -d\n",
    "    term3 = np.trace(sigma2_inv @ sigma1)\n",
    "    term4 = (mu2 - mu1).T @ sigma2_inv @ (mu2 - mu1)\n",
    "    \n",
    "    return 0.5 * (term1 + term2 + term3 + term4)\n",
    "\n",
    "\n",
    "def kl_to_standard_normal_multivariate(mu, log_var):\n",
    "    \"\"\"\n",
    "    VAE KL loss: D_KL(N(mu, diag(exp(log_var))) || N(0, I))\n",
    "    \"\"\"\n",
    "    return 0.5 * np.sum(mu**2 + np.exp(log_var) - 1 - log_var)\n",
    "\n",
    "\n",
    "# Verify the closed-form formula\n",
    "mu1 = np.array([1.0, 2.0])\n",
    "sigma1 = np.array([[1.5, 0.5], [0.5, 1.0]])\n",
    "\n",
    "mu2 = np.array([0.0, 0.0])\n",
    "sigma2 = np.eye(2)\n",
    "\n",
    "kl_analytical = kl_gaussian_multivariate(mu1, sigma1, mu2, sigma2)\n",
    "\n",
    "# Monte Carlo estimate\n",
    "n_samples = 100000\n",
    "samples = np.random.multivariate_normal(mu1, sigma1, n_samples)\n",
    "log_p1 = stats.multivariate_normal.logpdf(samples, mu1, sigma1)\n",
    "log_p2 = stats.multivariate_normal.logpdf(samples, mu2, sigma2)\n",
    "kl_monte_carlo = np.mean(log_p1 - log_p2)\n",
    "\n",
    "print(\"KL Divergence Between Multivariate Gaussians\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"P = N({mu1}, Œ£‚ÇÅ)\")\n",
    "print(f\"Q = N({mu2}, I)\")\n",
    "print(f\"\\nAnalytical: {kl_analytical:.6f} nats\")\n",
    "print(f\"Monte Carlo: {kl_monte_carlo:.6f} nats\")\n",
    "print(f\"Difference: {abs(kl_analytical - kl_monte_carlo):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE KL loss visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# KL loss surface for 1D case\n",
    "ax = axes[0]\n",
    "mu_range = np.linspace(-3, 3, 100)\n",
    "logvar_range = np.linspace(-3, 2, 100)\n",
    "MU, LOGVAR = np.meshgrid(mu_range, logvar_range)\n",
    "\n",
    "# KL for each point\n",
    "KL = 0.5 * (MU**2 + np.exp(LOGVAR) - 1 - LOGVAR)\n",
    "\n",
    "contour = ax.contourf(MU, np.exp(LOGVAR/2), KL, levels=20, cmap='viridis')\n",
    "plt.colorbar(contour, ax=ax, label='KL [nats]')\n",
    "ax.plot(0, 1, 'r*', markersize=15, label='Minimum (Œº=0, œÉ=1)')\n",
    "ax.set_xlabel('Œº')\n",
    "ax.set_ylabel('œÉ')\n",
    "ax.set_title('VAE KL Loss: $D_{KL}(q(z|x) \\\\| p(z))$\\nPulls toward N(0,1)')\n",
    "ax.legend()\n",
    "\n",
    "# Decomposition of KL loss\n",
    "ax = axes[1]\n",
    "sigma_range = np.linspace(0.1, 3, 100)\n",
    "\n",
    "# Components for mu=0\n",
    "kl_total = 0.5 * (sigma_range**2 - 1 - np.log(sigma_range**2))\n",
    "variance_term = 0.5 * (sigma_range**2 - 1)\n",
    "log_term = -0.5 * np.log(sigma_range**2)\n",
    "\n",
    "ax.plot(sigma_range, kl_total, 'b-', linewidth=2, label='Total KL')\n",
    "ax.plot(sigma_range, variance_term, 'r--', linewidth=2, label='$\\\\frac{1}{2}(\\\\sigma^2 - 1)$')\n",
    "ax.plot(sigma_range, log_term, 'g--', linewidth=2, label='$-\\\\frac{1}{2}\\\\log(\\\\sigma^2)$')\n",
    "\n",
    "ax.axvline(x=1, color='gray', linestyle=':', alpha=0.7, label='œÉ=1 (minimum)')\n",
    "ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "ax.set_xlabel('œÉ')\n",
    "ax.set_ylabel('Value [nats]')\n",
    "ax.set_title('KL Loss Components (Œº=0)\\nVariance + Log terms balance at œÉ=1')\n",
    "ax.legend()\n",
    "ax.set_xlim(0.1, 3)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Formulas\n",
    "\n",
    "| Concept | Formula |\n",
    "|---------|--------|\n",
    "| Univariate PDF | $\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$ |\n",
    "| Multivariate PDF | $\\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\right)$ |\n",
    "| Mahalanobis | $d_M = \\sqrt{(x-\\mu)^T\\Sigma^{-1}(x-\\mu)}$ |\n",
    "| Entropy | $\\frac{d}{2}(1 + \\log 2\\pi) + \\frac{1}{2}\\log|\\Sigma|$ |\n",
    "| Reparameterization | $x = \\mu + \\sigma \\odot \\epsilon$, $\\epsilon \\sim N(0,I)$ |\n",
    "| VAE KL | $\\frac{1}{2}\\sum_j(\\mu_j^2 + \\sigma_j^2 - 1 - \\log\\sigma_j^2)$ |\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **Gaussians maximize entropy** given mean and variance constraints\n",
    "2. **Covariance structure** determines the shape of equal-probability contours\n",
    "3. **Mahalanobis distance** is the natural metric that accounts for covariance\n",
    "4. **Reparameterization trick** enables gradient-based learning with stochastic nodes\n",
    "5. **Closed-form KL** makes VAE training efficient"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
