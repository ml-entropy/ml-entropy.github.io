{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 13: Autoencoders — Learning Compressed Representations\n",
    "\n",
    "In this notebook, we implement autoencoders from scratch and explore their variants:\n",
    "- Basic (undercomplete) autoencoder\n",
    "- Sparse autoencoder with KL penalty\n",
    "- Denoising autoencoder\n",
    "- Comparison with PCA\n",
    "- Anomaly detection application\n",
    "\n",
    "We use MNIST digits to visualize everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Style\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST (subset for speed)\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "X = digits.data / 16.0  # Normalize to [0, 1]\n",
    "y = digits.target\n",
    "input_dim = X.shape[1]  # 64 (8x8 images)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'Training samples: {len(X_train)}')\n",
    "print(f'Test samples: {len(X_test)}')\n",
    "print(f'Input dimension: {input_dim}')\n",
    "\n",
    "# Show some samples\n",
    "fig, axes = plt.subplots(2, 10, figsize=(15, 3))\n",
    "for i in range(10):\n",
    "    idx = np.where(y_train == i)[0][0]\n",
    "    axes[0, i].imshow(X_train[idx].reshape(8, 8), cmap='gray')\n",
    "    axes[0, i].set_title(str(i))\n",
    "    axes[0, i].axis('off')\n",
    "    axes[1, i].bar(range(input_dim), X_train[idx], color='steelblue', width=1)\n",
    "    axes[1, i].set_ylim(0, 1)\n",
    "    axes[1, i].set_xticks([])\n",
    "plt.suptitle('MNIST Digits (8x8) and Their Pixel Distributions', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Activation Functions and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_deriv(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "# Xavier initialization\n",
    "def xavier_init(fan_in, fan_out):\n",
    "    limit = np.sqrt(6.0 / (fan_in + fan_out))\n",
    "    return np.random.uniform(-limit, limit, (fan_out, fan_in))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Autoencoder\n",
    "\n",
    "Architecture: `input (64) → hidden (32) → latent (d) → hidden (32) → output (64)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder:\n",
    "    \"\"\"Simple autoencoder with one hidden layer in encoder and decoder.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        # Encoder weights\n",
    "        self.W1 = xavier_init(input_dim, hidden_dim)\n",
    "        self.b1 = np.zeros(hidden_dim)\n",
    "        self.W2 = xavier_init(hidden_dim, latent_dim)\n",
    "        self.b2 = np.zeros(latent_dim)\n",
    "        \n",
    "        # Decoder weights\n",
    "        self.W3 = xavier_init(latent_dim, hidden_dim)\n",
    "        self.b3 = np.zeros(hidden_dim)\n",
    "        self.W4 = xavier_init(hidden_dim, input_dim)\n",
    "        self.b4 = np.zeros(input_dim)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"x -> z\"\"\"\n",
    "        self.a1 = self.W1 @ x + self.b1\n",
    "        self.h1 = relu(self.a1)\n",
    "        self.a2 = self.W2 @ self.h1 + self.b2\n",
    "        self.z = self.a2  # Linear latent layer\n",
    "        return self.z\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"z -> x_hat\"\"\"\n",
    "        self.a3 = self.W3 @ z + self.b3\n",
    "        self.h2 = relu(self.a3)\n",
    "        self.a4 = self.W4 @ self.h2 + self.b4\n",
    "        self.x_hat = sigmoid(self.a4)  # Output in [0, 1]\n",
    "        return self.x_hat\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, z\n",
    "    \n",
    "    def backward(self, x, x_hat, lr=0.001):\n",
    "        \"\"\"Compute gradients and update weights.\"\"\"\n",
    "        # Output gradient (MSE loss)\n",
    "        d_out = 2 * (x_hat - x) / len(x)\n",
    "        \n",
    "        # Decoder output layer\n",
    "        d_a4 = d_out * sigmoid_deriv(self.a4)\n",
    "        dW4 = np.outer(d_a4, self.h2)\n",
    "        db4 = d_a4\n",
    "        d_h2 = self.W4.T @ d_a4\n",
    "        \n",
    "        # Decoder hidden\n",
    "        d_a3 = d_h2 * relu_deriv(self.a3)\n",
    "        dW3 = np.outer(d_a3, self.z)\n",
    "        db3 = d_a3\n",
    "        d_z = self.W3.T @ d_a3\n",
    "        \n",
    "        # Encoder latent (linear)\n",
    "        d_a2 = d_z\n",
    "        dW2 = np.outer(d_a2, self.h1)\n",
    "        db2 = d_a2\n",
    "        d_h1 = self.W2.T @ d_a2\n",
    "        \n",
    "        # Encoder hidden\n",
    "        d_a1 = d_h1 * relu_deriv(self.a1)\n",
    "        dW1 = np.outer(d_a1, x)\n",
    "        db1 = d_a1\n",
    "        \n",
    "        # Update weights (gradient descent)\n",
    "        self.W4 -= lr * dW4\n",
    "        self.b4 -= lr * db4\n",
    "        self.W3 -= lr * dW3\n",
    "        self.b3 -= lr * db3\n",
    "        self.W2 -= lr * dW2\n",
    "        self.b2 -= lr * db2\n",
    "        self.W1 -= lr * dW1\n",
    "        self.b1 -= lr * db1\n",
    "\n",
    "def train_autoencoder(ae, X_train, epochs=50, lr=0.01, verbose=True):\n",
    "    \"\"\"Train autoencoder with SGD.\"\"\"\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle\n",
    "        idx = np.random.permutation(len(X_train))\n",
    "        epoch_loss = 0\n",
    "        for i in idx:\n",
    "            x = X_train[i]\n",
    "            x_hat, z = ae.forward(x)\n",
    "            loss = np.mean((x - x_hat)**2)\n",
    "            ae.backward(x, x_hat, lr=lr)\n",
    "            epoch_loss += loss\n",
    "        epoch_loss /= len(X_train)\n",
    "        losses.append(epoch_loss)\n",
    "        if verbose and (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.6f}')\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with different latent dimensions\n",
    "latent_dims = [2, 8, 16, 32]\n",
    "autoencoders = {}\n",
    "all_losses = {}\n",
    "\n",
    "for d in latent_dims:\n",
    "    print(f'\\n--- Training Autoencoder with latent_dim={d} ---')\n",
    "    ae = Autoencoder(input_dim=64, hidden_dim=32, latent_dim=d)\n",
    "    losses = train_autoencoder(ae, X_train, epochs=30, lr=0.01)\n",
    "    autoencoders[d] = ae\n",
    "    all_losses[d] = losses\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "for d, losses in all_losses.items():\n",
    "    plt.plot(losses, label=f'd={d}')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training Loss vs Latent Dimension')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_reconstructions(ae, X_test, n=10, title=''):\n",
    "    \"\"\"Show original vs reconstructed images.\"\"\"\n",
    "    fig, axes = plt.subplots(2, n, figsize=(15, 3))\n",
    "    indices = np.random.choice(len(X_test), n, replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        x = X_test[idx]\n",
    "        x_hat, _ = ae.forward(x)\n",
    "        \n",
    "        axes[0, i].imshow(x.reshape(8, 8), cmap='gray')\n",
    "        axes[0, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[0, i].set_ylabel('Original', fontsize=12)\n",
    "        \n",
    "        axes[1, i].imshow(x_hat.reshape(8, 8), cmap='gray')\n",
    "        axes[1, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[1, i].set_ylabel('Reconstructed', fontsize=12)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for d in latent_dims:\n",
    "    visualize_reconstructions(autoencoders[d], X_test, \n",
    "                             title=f'Autoencoder Reconstructions (d={d})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Latent Space Visualization (d=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode all test data with the d=2 autoencoder\n",
    "ae_2d = autoencoders[2]\n",
    "Z_test = np.array([ae_2d.encode(x) for x in X_test])\n",
    "\n",
    "# Compare with PCA\n",
    "pca_2d = PCA(n_components=2)\n",
    "Z_pca = pca_2d.fit_transform(X_test)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Autoencoder latent space\n",
    "scatter1 = ax1.scatter(Z_test[:, 0], Z_test[:, 1], c=y_test, \n",
    "                       cmap='tab10', alpha=0.7, s=20)\n",
    "ax1.set_title('Autoencoder Latent Space (d=2)', fontsize=14)\n",
    "ax1.set_xlabel('z₁')\n",
    "ax1.set_ylabel('z₂')\n",
    "plt.colorbar(scatter1, ax=ax1, label='Digit')\n",
    "\n",
    "# PCA projection\n",
    "scatter2 = ax2.scatter(Z_pca[:, 0], Z_pca[:, 1], c=y_test, \n",
    "                       cmap='tab10', alpha=0.7, s=20)\n",
    "ax2.set_title('PCA Projection (d=2)', fontsize=14)\n",
    "ax2.set_xlabel('PC₁')\n",
    "ax2.set_ylabel('PC₂')\n",
    "plt.colorbar(scatter2, ax=ax2, label='Digit')\n",
    "\n",
    "plt.suptitle('Nonlinear AE vs Linear PCA: 2D Representations', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'PCA explained variance ratio: {pca_2d.explained_variance_ratio_.sum():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reconstruction Quality: Autoencoder vs PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare reconstruction MSE\n",
    "dims = [2, 4, 8, 16, 32]\n",
    "mse_ae = []\n",
    "mse_pca = []\n",
    "\n",
    "for d in dims:\n",
    "    # PCA\n",
    "    pca = PCA(n_components=d)\n",
    "    Z_pca = pca.fit_transform(X_train)\n",
    "    X_recon_pca = pca.inverse_transform(pca.transform(X_test))\n",
    "    mse_pca.append(np.mean((X_test - X_recon_pca)**2))\n",
    "    \n",
    "    # Autoencoder (train if not already)\n",
    "    if d not in autoencoders:\n",
    "        ae = Autoencoder(64, 32, d)\n",
    "        train_autoencoder(ae, X_train, epochs=30, lr=0.01, verbose=False)\n",
    "        autoencoders[d] = ae\n",
    "    \n",
    "    ae = autoencoders[d]\n",
    "    errors = [np.mean((x - ae.forward(x)[0])**2) for x in X_test]\n",
    "    mse_ae.append(np.mean(errors))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(dims, mse_pca, 'bo-', label='PCA', linewidth=2, markersize=8)\n",
    "plt.plot(dims, mse_ae, 'rs-', label='Autoencoder', linewidth=2, markersize=8)\n",
    "plt.xlabel('Latent Dimension', fontsize=13)\n",
    "plt.ylabel('Test MSE', fontsize=13)\n",
    "plt.title('Reconstruction Quality: Autoencoder vs PCA', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(dims)\n",
    "plt.show()\n",
    "\n",
    "for d, pca_err, ae_err in zip(dims, mse_pca, mse_ae):\n",
    "    improvement = (pca_err - ae_err) / pca_err * 100\n",
    "    print(f'd={d:2d}: PCA MSE={pca_err:.5f}, AE MSE={ae_err:.5f}, AE improvement={improvement:+.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Denoising Autoencoder\n",
    "\n",
    "Train the autoencoder to reconstruct clean images from corrupted ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_masking_noise(x, noise_level=0.3):\n",
    "    \"\"\"Zero out random pixels.\"\"\"\n",
    "    mask = np.random.binomial(1, 1 - noise_level, size=x.shape)\n",
    "    return x * mask\n",
    "\n",
    "def add_gaussian_noise(x, sigma=0.2):\n",
    "    \"\"\"Add Gaussian noise.\"\"\"\n",
    "    return np.clip(x + np.random.normal(0, sigma, size=x.shape), 0, 1)\n",
    "\n",
    "# Show noise types\n",
    "fig, axes = plt.subplots(3, 8, figsize=(14, 5))\n",
    "for i in range(8):\n",
    "    x = X_test[i]\n",
    "    axes[0, i].imshow(x.reshape(8, 8), cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    x_mask = add_masking_noise(x, 0.4)\n",
    "    axes[1, i].imshow(x_mask.reshape(8, 8), cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "    \n",
    "    x_gauss = add_gaussian_noise(x, 0.3)\n",
    "    axes[2, i].imshow(x_gauss.reshape(8, 8), cmap='gray')\n",
    "    axes[2, i].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('Clean', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Masking', fontsize=12)\n",
    "axes[2, 0].set_ylabel('Gaussian', fontsize=12)\n",
    "plt.suptitle('Noise Types for Denoising Autoencoder', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train denoising autoencoder\n",
    "dae = Autoencoder(input_dim=64, hidden_dim=32, latent_dim=16)\n",
    "\n",
    "dae_losses = []\n",
    "for epoch in range(50):\n",
    "    idx = np.random.permutation(len(X_train))\n",
    "    epoch_loss = 0\n",
    "    for i in idx:\n",
    "        x_clean = X_train[i]\n",
    "        x_noisy = add_masking_noise(x_clean, noise_level=0.3)\n",
    "        \n",
    "        # Forward with noisy input\n",
    "        x_hat, z = dae.forward(x_noisy)\n",
    "        \n",
    "        # Loss against CLEAN input\n",
    "        loss = np.mean((x_clean - x_hat)**2)\n",
    "        \n",
    "        # Backward (target is clean input)\n",
    "        dae.backward(x_clean, x_hat, lr=0.01)\n",
    "        epoch_loss += loss\n",
    "    \n",
    "    epoch_loss /= len(X_train)\n",
    "    dae_losses.append(epoch_loss)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/50, Loss: {epoch_loss:.6f}')\n",
    "\n",
    "# Visualize denoising\n",
    "fig, axes = plt.subplots(3, 8, figsize=(14, 5))\n",
    "for i in range(8):\n",
    "    x = X_test[i]\n",
    "    x_noisy = add_masking_noise(x, 0.3)\n",
    "    x_hat, _ = dae.forward(x_noisy)\n",
    "    \n",
    "    axes[0, i].imshow(x.reshape(8, 8), cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "    axes[1, i].imshow(x_noisy.reshape(8, 8), cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "    axes[2, i].imshow(x_hat.reshape(8, 8), cmap='gray')\n",
    "    axes[2, i].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('Clean', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Corrupted', fontsize=12)\n",
    "axes[2, 0].set_ylabel('Denoised', fontsize=12)\n",
    "plt.suptitle('Denoising Autoencoder Results', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Anomaly Detection with Autoencoders\n",
    "\n",
    "Train on one class of digits, detect other classes as anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train autoencoder on digit '1' only\n",
    "normal_digit = 1\n",
    "X_normal = X_train[y_train == normal_digit]\n",
    "X_test_normal = X_test[y_test == normal_digit]\n",
    "X_test_anomaly = X_test[y_test != normal_digit]\n",
    "\n",
    "print(f'Training on digit {normal_digit}: {len(X_normal)} samples')\n",
    "print(f'Test normal: {len(X_test_normal)}, Test anomaly: {len(X_test_anomaly)}')\n",
    "\n",
    "# Train\n",
    "ae_anomaly = Autoencoder(64, 32, 8)\n",
    "losses = train_autoencoder(ae_anomaly, X_normal, epochs=50, lr=0.01)\n",
    "\n",
    "# Compute reconstruction errors\n",
    "errors_normal = [np.mean((x - ae_anomaly.forward(x)[0])**2) for x in X_test_normal]\n",
    "errors_anomaly = [np.mean((x - ae_anomaly.forward(x)[0])**2) for x in X_test_anomaly]\n",
    "\n",
    "# Plot distributions\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(errors_normal, bins=30, alpha=0.6, label=f'Normal (digit {normal_digit})', color='green', density=True)\n",
    "plt.hist(errors_anomaly, bins=30, alpha=0.6, label='Anomaly (other digits)', color='red', density=True)\n",
    "threshold = np.percentile(errors_normal, 95)\n",
    "plt.axvline(threshold, color='black', linestyle='--', linewidth=2, label=f'Threshold (95th percentile)')\n",
    "plt.xlabel('Reconstruction Error (MSE)', fontsize=13)\n",
    "plt.ylabel('Density', fontsize=13)\n",
    "plt.title('Anomaly Detection: Reconstruction Error Distribution', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Detection metrics\n",
    "tp = np.sum(np.array(errors_anomaly) > threshold)\n",
    "fn = np.sum(np.array(errors_anomaly) <= threshold)\n",
    "fp = np.sum(np.array(errors_normal) > threshold)\n",
    "tn = np.sum(np.array(errors_normal) <= threshold)\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "print(f'\\nPrecision: {precision:.3f}')\n",
    "print(f'Recall: {recall:.3f}')\n",
    "print(f'F1: {2*precision*recall/(precision+recall):.3f}' if (precision+recall) > 0 else 'F1: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Information Bottleneck Visualization\n",
    "\n",
    "How does the reconstruction error change with latent dimension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rate-distortion curve approximation\n",
    "dims_to_test = [1, 2, 4, 8, 12, 16, 24, 32, 48, 64]\n",
    "distortions = []\n",
    "\n",
    "for d in dims_to_test:\n",
    "    ae = Autoencoder(64, max(d+4, 32), d)\n",
    "    train_autoencoder(ae, X_train, epochs=30, lr=0.01, verbose=False)\n",
    "    errors = [np.mean((x - ae.forward(x)[0])**2) for x in X_test[:200]]\n",
    "    distortions.append(np.mean(errors))\n",
    "    print(f'd={d:2d}: MSE={distortions[-1]:.5f}')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(dims_to_test, distortions, 'bo-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Latent Dimension (Rate proxy)', fontsize=13)\n",
    "plt.ylabel('Reconstruction MSE (Distortion)', fontsize=13)\n",
    "plt.title('Rate-Distortion Trade-off in Autoencoders', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Mark input dimension\n",
    "plt.axvline(x=64, color='red', linestyle='--', alpha=0.5, label='Input dim (64)')\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Latent Space Interpolation\n",
    "\n",
    "Interpolate between two digits in latent space to see smooth transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = autoencoders[16]  # Use d=16 autoencoder\n",
    "\n",
    "# Pick two different digits\n",
    "idx_3 = np.where(y_test == 3)[0][0]\n",
    "idx_8 = np.where(y_test == 8)[0][0]\n",
    "\n",
    "x_start = X_test[idx_3]\n",
    "x_end = X_test[idx_8]\n",
    "\n",
    "z_start = ae.encode(x_start)\n",
    "z_end = ae.encode(x_end)\n",
    "\n",
    "# Interpolate\n",
    "n_steps = 10\n",
    "fig, axes = plt.subplots(1, n_steps, figsize=(15, 2))\n",
    "\n",
    "for i, alpha in enumerate(np.linspace(0, 1, n_steps)):\n",
    "    z_interp = (1 - alpha) * z_start + alpha * z_end\n",
    "    x_interp = ae.decode(z_interp)\n",
    "    axes[i].imshow(x_interp.reshape(8, 8), cmap='gray')\n",
    "    axes[i].set_title(f'{alpha:.1f}', fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Latent Space Interpolation: 3 → 8', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Note: Standard autoencoders may have discontinuous latent spaces.')\n",
    "print('VAEs address this by regularizing the latent space with KL divergence.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "1. **Bottleneck matters**: Smaller latent dimension = more compression = higher reconstruction error\n",
    "2. **Nonlinear > Linear**: Autoencoders outperform PCA especially at low dimensions\n",
    "3. **Denoising helps**: Training with corruption forces robust feature learning\n",
    "4. **Anomaly detection**: High reconstruction error indicates out-of-distribution inputs\n",
    "5. **Latent space structure**: Standard AEs have irregular latent spaces; VAEs fix this\n",
    "\n",
    "**Next**: Variational Autoencoders add a probabilistic framework, enabling generation and smooth latent spaces."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
