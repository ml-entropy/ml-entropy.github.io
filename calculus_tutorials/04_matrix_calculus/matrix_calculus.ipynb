{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 04: Matrix Calculus\n",
    "\n",
    "Essential identities for backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Scalar-by-Vector Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x, eps=1e-5):\n",
    "    \"\"\"Compute gradient numerically using central differences.\"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    for i in range(len(x)):\n",
    "        x_plus = x.copy()\n",
    "        x_minus = x.copy()\n",
    "        x_plus[i] += eps\n",
    "        x_minus[i] -= eps\n",
    "        grad[i] = (f(x_plus) - f(x_minus)) / (2 * eps)\n",
    "    return grad\n",
    "\n",
    "# Test Identity 1: d/dx(a^T x) = a\n",
    "a = np.array([1.0, 2.0, 3.0])\n",
    "x = np.array([0.5, 1.5, 2.5])\n",
    "\n",
    "f1 = lambda x: np.dot(a, x)\n",
    "analytical = a\n",
    "numerical = numerical_gradient(f1, x)\n",
    "\n",
    "print(\"Identity 1: ∂/∂x(a^T x) = a\")\n",
    "print(f\"  Analytical: {analytical}\")\n",
    "print(f\"  Numerical:  {numerical}\")\n",
    "print(f\"  Match: {np.allclose(analytical, numerical)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Identity 2: d/dx(x^T x) = 2x\n",
    "f2 = lambda x: np.dot(x, x)\n",
    "analytical = 2 * x\n",
    "numerical = numerical_gradient(f2, x)\n",
    "\n",
    "print(\"Identity 2: ∂/∂x(x^T x) = 2x\")\n",
    "print(f\"  x = {x}\")\n",
    "print(f\"  Analytical: {analytical}\")\n",
    "print(f\"  Numerical:  {numerical}\")\n",
    "print(f\"  Match: {np.allclose(analytical, numerical)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Identity 3: d/dx(x^T A x) = (A + A^T)x\n",
    "A = np.random.randn(3, 3)\n",
    "f3 = lambda x: x @ A @ x\n",
    "analytical = (A + A.T) @ x\n",
    "numerical = numerical_gradient(f3, x)\n",
    "\n",
    "print(\"Identity 3: ∂/∂x(x^T A x) = (A + A^T)x\")\n",
    "print(f\"  Analytical: {analytical}\")\n",
    "print(f\"  Numerical:  {numerical}\")\n",
    "print(f\"  Match: {np.allclose(analytical, numerical)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Identity 4: d/dx||x - a||^2 = 2(x - a)  [L2 loss gradient!]\n",
    "f4 = lambda x: np.sum((x - a)**2)\n",
    "analytical = 2 * (x - a)\n",
    "numerical = numerical_gradient(f4, x)\n",
    "\n",
    "print(\"Identity 4: ∂/∂x ||x - a||² = 2(x - a)  [L2 loss!]\")\n",
    "print(f\"  Analytical: {analytical}\")\n",
    "print(f\"  Numerical:  {numerical}\")\n",
    "print(f\"  Match: {np.allclose(analytical, numerical)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Jacobian of Linear Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_jacobian(f, x, eps=1e-5):\n",
    "    \"\"\"Compute Jacobian numerically.\"\"\"\n",
    "    y = f(x)\n",
    "    m, n = len(y), len(x)\n",
    "    J = np.zeros((m, n))\n",
    "    \n",
    "    for j in range(n):\n",
    "        x_plus = x.copy()\n",
    "        x_minus = x.copy()\n",
    "        x_plus[j] += eps\n",
    "        x_minus[j] -= eps\n",
    "        J[:, j] = (f(x_plus) - f(x_minus)) / (2 * eps)\n",
    "    \n",
    "    return J\n",
    "\n",
    "# Test: Jacobian of Ax = A\n",
    "A = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "x = np.array([1.0, 2.0, 3.0])\n",
    "\n",
    "f_linear = lambda x: A @ x\n",
    "analytical_jacobian = A\n",
    "numerical_J = numerical_jacobian(f_linear, x)\n",
    "\n",
    "print(\"Jacobian of y = Ax is J = A\")\n",
    "print(f\"\\nA = \\n{A}\")\n",
    "print(f\"\\nNumerical Jacobian = \\n{numerical_J}\")\n",
    "print(f\"\\nMatch: {np.allclose(analytical_jacobian, numerical_J)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Backprop Through Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    \"\"\"y = Wx + b\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        self.W = np.random.randn(out_features, in_features) * 0.1\n",
    "        self.b = np.zeros(out_features)\n",
    "        self.x = None  # Cache for backward\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return self.W @ x + self.b\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        grad_output: dL/dy (gradient from upstream)\n",
    "        Returns: dL/dx, dL/dW, dL/db\n",
    "        \"\"\"\n",
    "        # dL/dx = W^T @ dL/dy\n",
    "        grad_x = self.W.T @ grad_output\n",
    "        \n",
    "        # dL/dW = dL/dy @ x^T (outer product)\n",
    "        grad_W = np.outer(grad_output, self.x)\n",
    "        \n",
    "        # dL/db = dL/dy\n",
    "        grad_b = grad_output\n",
    "        \n",
    "        return grad_x, grad_W, grad_b\n",
    "\n",
    "# Test\n",
    "layer = LinearLayer(3, 2)\n",
    "x = np.array([1.0, 2.0, 3.0])\n",
    "y = layer.forward(x)\n",
    "\n",
    "# Suppose loss gradient w.r.t. y is:\n",
    "grad_y = np.array([0.5, -0.3])\n",
    "\n",
    "grad_x, grad_W, grad_b = layer.backward(grad_y)\n",
    "\n",
    "print(\"Linear Layer Backprop:\")\n",
    "print(f\"  x = {x}\")\n",
    "print(f\"  y = Wx + b = {y}\")\n",
    "print(f\"  dL/dy = {grad_y}\")\n",
    "print(f\"\\n  dL/dx = W^T @ dL/dy = {grad_x}\")\n",
    "print(f\"  dL/dW = dL/dy ⊗ x = \\n{grad_W}\")\n",
    "print(f\"  dL/db = dL/dy = {grad_b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify with numerical gradients\n",
    "eps = 1e-5\n",
    "\n",
    "# Simple loss: L = sum(y)\n",
    "def compute_loss(W, b, x):\n",
    "    return np.sum(W @ x + b)\n",
    "\n",
    "# Numerical gradient for W\n",
    "numerical_grad_W = np.zeros_like(layer.W)\n",
    "for i in range(layer.W.shape[0]):\n",
    "    for j in range(layer.W.shape[1]):\n",
    "        layer.W[i, j] += eps\n",
    "        loss_plus = compute_loss(layer.W, layer.b, x)\n",
    "        layer.W[i, j] -= 2 * eps\n",
    "        loss_minus = compute_loss(layer.W, layer.b, x)\n",
    "        layer.W[i, j] += eps  # Restore\n",
    "        numerical_grad_W[i, j] = (loss_plus - loss_minus) / (2 * eps)\n",
    "\n",
    "# For L = sum(y), dL/dy = [1, 1]\n",
    "grad_y_check = np.ones(2)\n",
    "_, analytical_grad_W, _ = layer.backward(grad_y_check)\n",
    "\n",
    "print(\"\\nVerification (L = sum(y), so dL/dy = [1,1]):\")\n",
    "print(f\"  Analytical dL/dW = \\n{analytical_grad_W}\")\n",
    "print(f\"  Numerical dL/dW = \\n{numerical_grad_W}\")\n",
    "print(f\"  Match: {np.allclose(analytical_grad_W, numerical_grad_W)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Chain Rule with Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two-layer network: y = W2 @ relu(W1 @ x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, d_in, d_hidden, d_out):\n",
    "        self.W1 = np.random.randn(d_hidden, d_in) * 0.1\n",
    "        self.W2 = np.random.randn(d_out, d_hidden) * 0.1\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.z1 = self.W1 @ x\n",
    "        self.a1 = relu(self.z1)\n",
    "        self.y = self.W2 @ self.a1\n",
    "        return self.y\n",
    "    \n",
    "    def backward(self, grad_y):\n",
    "        # dL/dW2 = dL/dy @ a1^T\n",
    "        grad_W2 = np.outer(grad_y, self.a1)\n",
    "        \n",
    "        # dL/da1 = W2^T @ dL/dy\n",
    "        grad_a1 = self.W2.T @ grad_y\n",
    "        \n",
    "        # dL/dz1 = dL/da1 * relu'(z1)\n",
    "        grad_z1 = grad_a1 * relu_derivative(self.z1)\n",
    "        \n",
    "        # dL/dW1 = dL/dz1 @ x^T\n",
    "        grad_W1 = np.outer(grad_z1, self.x)\n",
    "        \n",
    "        return grad_W1, grad_W2\n",
    "\n",
    "# Test\n",
    "net = TwoLayerNet(4, 3, 2)\n",
    "x = np.random.randn(4)\n",
    "y = net.forward(x)\n",
    "\n",
    "print(\"Two-layer network: y = W2 @ relu(W1 @ x)\")\n",
    "print(f\"\\nInput x: {x}\")\n",
    "print(f\"Hidden z1 = W1 @ x: {net.z1}\")\n",
    "print(f\"Hidden a1 = relu(z1): {net.a1}\")\n",
    "print(f\"Output y = W2 @ a1: {y}\")\n",
    "\n",
    "# Backward\n",
    "grad_y = np.ones(2)  # dL/dy = [1, 1]\n",
    "grad_W1, grad_W2 = net.backward(grad_y)\n",
    "\n",
    "print(f\"\\ndL/dW2 shape: {grad_W2.shape}\")\n",
    "print(f\"dL/dW1 shape: {grad_W1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify with numerical gradients\n",
    "eps = 1e-5\n",
    "\n",
    "def compute_loss_net(W1, W2, x):\n",
    "    z1 = W1 @ x\n",
    "    a1 = relu(z1)\n",
    "    y = W2 @ a1\n",
    "    return np.sum(y)\n",
    "\n",
    "# Numerical gradient for W1\n",
    "numerical_grad_W1 = np.zeros_like(net.W1)\n",
    "for i in range(net.W1.shape[0]):\n",
    "    for j in range(net.W1.shape[1]):\n",
    "        net.W1[i, j] += eps\n",
    "        loss_plus = compute_loss_net(net.W1, net.W2, x)\n",
    "        net.W1[i, j] -= 2 * eps\n",
    "        loss_minus = compute_loss_net(net.W1, net.W2, x)\n",
    "        net.W1[i, j] += eps\n",
    "        numerical_grad_W1[i, j] = (loss_plus - loss_minus) / (2 * eps)\n",
    "\n",
    "print(\"Verification of dL/dW1:\")\n",
    "print(f\"  Analytical:\\n{grad_W1}\")\n",
    "print(f\"  Numerical:\\n{numerical_grad_W1}\")\n",
    "print(f\"  Max difference: {np.max(np.abs(grad_W1 - numerical_grad_W1)):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Key matrix calculus identities:**\n",
    "| Expression | Derivative |\n",
    "|------------|------------|\n",
    "| $a^T x$ | $a$ |\n",
    "| $x^T x$ | $2x$ |\n",
    "| $x^T A x$ | $(A + A^T)x$ |\n",
    "| $Ax$ | $A$ (Jacobian) |\n",
    "\n",
    "**Backprop through linear layer:**\n",
    "- $\\frac{\\partial L}{\\partial x} = W^T \\frac{\\partial L}{\\partial y}$\n",
    "- $\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial y} x^T$\n",
    "- $\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial y}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
