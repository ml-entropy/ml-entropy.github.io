{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 01: Single Variable Derivatives\n",
    "\n",
    "Visualizing derivatives and understanding them intuitively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Derivative as Slope of Tangent Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "def f_derivative(x):\n",
    "    return 2*x\n",
    "\n",
    "# Point of tangency\n",
    "x0 = 1.5\n",
    "y0 = f(x0)\n",
    "slope = f_derivative(x0)\n",
    "\n",
    "# Plot\n",
    "x = np.linspace(-1, 3, 100)\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Function\n",
    "plt.plot(x, f(x), 'b-', linewidth=2, label='$f(x) = x^2$')\n",
    "\n",
    "# Tangent line: y - y0 = slope * (x - x0)\n",
    "tangent = y0 + slope * (x - x0)\n",
    "plt.plot(x, tangent, 'r--', linewidth=2, label=f'Tangent at x={x0}: slope={slope}')\n",
    "\n",
    "# Point of tangency\n",
    "plt.scatter([x0], [y0], color='red', s=100, zorder=5)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Derivative = Slope of Tangent Line')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(-1, 6)\n",
    "plt.show()\n",
    "\n",
    "print(f\"At x = {x0}: f(x) = {y0}, f'(x) = {slope}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Secant Lines Converging to Tangent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 1.0\n",
    "y0 = f(x0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "x = np.linspace(-0.5, 2.5, 100)\n",
    "\n",
    "h_values = [1.0, 0.5, 0.1]\n",
    "\n",
    "for ax, h in zip(axes, h_values):\n",
    "    # Function\n",
    "    ax.plot(x, f(x), 'b-', linewidth=2, label='$f(x) = x^2$')\n",
    "    \n",
    "    # Secant line\n",
    "    x1 = x0 + h\n",
    "    y1 = f(x1)\n",
    "    secant_slope = (y1 - y0) / h\n",
    "    secant = y0 + secant_slope * (x - x0)\n",
    "    ax.plot(x, secant, 'g--', linewidth=2, alpha=0.7, label=f'Secant (h={h})')\n",
    "    \n",
    "    # Tangent line (true derivative)\n",
    "    tangent = y0 + f_derivative(x0) * (x - x0)\n",
    "    ax.plot(x, tangent, 'r-', linewidth=1.5, label='Tangent')\n",
    "    \n",
    "    # Points\n",
    "    ax.scatter([x0, x1], [y0, y1], color='green', s=80, zorder=5)\n",
    "    \n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(f'h = {h}, secant slope = {secant_slope:.2f}')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(-0.5, 4)\n",
    "\n",
    "plt.suptitle('Secant Lines → Tangent as h → 0', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"True derivative at x=1: f'(1) = {f_derivative(1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Numerical vs Analytical Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_derivative(f, x, h=1e-5):\n",
    "    \"\"\"Central difference approximation.\"\"\"\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)\n",
    "\n",
    "# Test on various functions\n",
    "functions = [\n",
    "    ('x²', lambda x: x**2, lambda x: 2*x),\n",
    "    ('x³', lambda x: x**3, lambda x: 3*x**2),\n",
    "    ('exp(x)', np.exp, np.exp),\n",
    "    ('ln(x)', np.log, lambda x: 1/x),\n",
    "    ('sin(x)', np.sin, np.cos),\n",
    "]\n",
    "\n",
    "x_test = 2.0\n",
    "\n",
    "print(\"Comparing Numerical vs Analytical Derivatives at x = 2.0\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Function':<15} {'Numerical':<15} {'Analytical':<15} {'Error':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, f, df in functions:\n",
    "    num = numerical_derivative(f, x_test)\n",
    "    ana = df(x_test)\n",
    "    error = abs(num - ana)\n",
    "    print(f\"{name:<15} {num:<15.6f} {ana:<15.6f} {error:<15.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Chain Rule Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f(g(x)) where f(u) = u^2 and g(x) = sin(x)\n",
    "# Chain rule: d/dx[sin²(x)] = 2*sin(x)*cos(x)\n",
    "\n",
    "x = np.linspace(0, 2*np.pi, 100)\n",
    "\n",
    "# Composite function\n",
    "y = np.sin(x)**2\n",
    "\n",
    "# Derivative: 2*sin(x)*cos(x) = sin(2x)\n",
    "dy = 2 * np.sin(x) * np.cos(x)\n",
    "dy_simplified = np.sin(2*x)  # Same thing!\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot function\n",
    "axes[0].plot(x, y, 'b-', linewidth=2, label='$f(x) = \\\\sin^2(x)$')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('Function: $\\\\sin^2(x)$')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot derivative\n",
    "axes[1].plot(x, dy, 'r-', linewidth=2, label=\"$f'(x) = 2\\\\sin(x)\\\\cos(x)$\")\n",
    "axes[1].plot(x, dy_simplified, 'g--', linewidth=2, alpha=0.7, label=\"$= \\\\sin(2x)$\")\n",
    "axes[1].axhline(0, color='black', linewidth=0.5)\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel(\"y'\")\n",
    "axes[1].set_title('Derivative via Chain Rule')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Chain rule: d/dx[f(g(x))] = f'(g(x)) · g'(x)\")\n",
    "print(\"Here: d/dx[sin²(x)] = 2·sin(x) · cos(x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: ML Activation Functions and Their Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 200)\n",
    "\n",
    "# Activations and derivatives\n",
    "activations = {\n",
    "    'Sigmoid': {\n",
    "        'f': lambda x: 1 / (1 + np.exp(-x)),\n",
    "        'df': lambda x: (1 / (1 + np.exp(-x))) * (1 - 1 / (1 + np.exp(-x)))\n",
    "    },\n",
    "    'Tanh': {\n",
    "        'f': np.tanh,\n",
    "        'df': lambda x: 1 - np.tanh(x)**2\n",
    "    },\n",
    "    'ReLU': {\n",
    "        'f': lambda x: np.maximum(0, x),\n",
    "        'df': lambda x: (x > 0).astype(float)\n",
    "    },\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "for i, (name, funcs) in enumerate(activations.items()):\n",
    "    y = funcs['f'](x)\n",
    "    dy = funcs['df'](x)\n",
    "    \n",
    "    # Function\n",
    "    axes[0, i].plot(x, y, 'b-', linewidth=2)\n",
    "    axes[0, i].axhline(0, color='black', linewidth=0.5)\n",
    "    axes[0, i].axvline(0, color='black', linewidth=0.5)\n",
    "    axes[0, i].set_title(f'{name}: $f(x)$')\n",
    "    axes[0, i].grid(True, alpha=0.3)\n",
    "    axes[0, i].set_ylim(-1.5, 1.5) if name != 'ReLU' else axes[0, i].set_ylim(-1, 5)\n",
    "    \n",
    "    # Derivative\n",
    "    axes[1, i].plot(x, dy, 'r-', linewidth=2)\n",
    "    axes[1, i].axhline(0, color='black', linewidth=0.5)\n",
    "    axes[1, i].axvline(0, color='black', linewidth=0.5)\n",
    "    axes[1, i].set_title(f\"{name}: $f'(x)$\")\n",
    "    axes[1, i].grid(True, alpha=0.3)\n",
    "    axes[1, i].set_ylim(-0.5, 1.5)\n",
    "\n",
    "plt.suptitle('Activation Functions and Their Derivatives', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice:\")\n",
    "print(\"- Sigmoid/Tanh derivatives → 0 for large |x| (vanishing gradients!)\")\n",
    "print(\"- ReLU derivative = 1 for x > 0, 0 for x < 0 (constant gradient)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Key insights:**\n",
    "1. Derivative = slope of tangent line = instantaneous rate of change\n",
    "2. Chain rule is the foundation of backpropagation\n",
    "3. Numerical differentiation works well for verification\n",
    "4. Activation function derivatives determine gradient flow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
