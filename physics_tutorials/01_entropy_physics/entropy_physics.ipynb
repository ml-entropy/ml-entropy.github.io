{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics Tutorial 01: Entropy Visualized\n",
    "\n",
    "Interactive exploration of entropy, microstates, and the Second Law."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import comb\n",
    "from math import factorial, log\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Boltzmann constant (we'll use k_B = 1 for simplicity in most examples)\n",
    "k_B = 1.38e-23  # J/K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Microstates and Macrostates\n",
    "\n",
    "Let's start with the simplest example: coins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_microstates_coins(n_coins, n_heads):\n",
    "    \"\"\"Number of ways to get n_heads from n_coins.\"\"\"\n",
    "    return comb(n_coins, n_heads, exact=True)\n",
    "\n",
    "def entropy_boltzmann(W):\n",
    "    \"\"\"S = k_B ln(W), using k_B = 1 for simplicity.\"\"\"\n",
    "    if W <= 0:\n",
    "        return 0\n",
    "    return np.log(W)\n",
    "\n",
    "# Analyze for different numbers of coins\n",
    "for n_coins in [4, 10, 20]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"N = {n_coins} coins\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"{'Heads':>6} | {'Microstates (W)':>15} | {'S = ln(W)':>10} | {'Probability':>12}\")\n",
    "    print(f\"{'-'*60}\")\n",
    "    \n",
    "    total_microstates = 2**n_coins\n",
    "    \n",
    "    for n_heads in range(n_coins + 1):\n",
    "        W = count_microstates_coins(n_coins, n_heads)\n",
    "        S = entropy_boltzmann(W)\n",
    "        prob = W / total_microstates\n",
    "        print(f\"{n_heads:>6} | {W:>15,} | {S:>10.4f} | {prob:>12.4%}\")\n",
    "    \n",
    "    print(f\"{'-'*60}\")\n",
    "    print(f\"{'Total':>6} | {total_microstates:>15,} |            | {'100%':>12}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize entropy vs macrostate\n",
    "n_coins = 100\n",
    "\n",
    "heads = np.arange(0, n_coins + 1)\n",
    "microstates = [count_microstates_coins(n_coins, h) for h in heads]\n",
    "entropies = [entropy_boltzmann(W) for W in microstates]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Microstates\n",
    "axes[0].plot(heads, microstates, 'b-', linewidth=2)\n",
    "axes[0].fill_between(heads, microstates, alpha=0.3)\n",
    "axes[0].set_xlabel('Number of Heads', fontsize=12)\n",
    "axes[0].set_ylabel('Number of Microstates W', fontsize=12)\n",
    "axes[0].set_title(f'Microstates vs Macrostate (N={n_coins} coins)', fontsize=14)\n",
    "axes[0].axvline(x=n_coins/2, color='red', linestyle='--', label=f'Max at {n_coins//2} heads')\n",
    "axes[0].legend(fontsize=11)\n",
    "\n",
    "# Entropy\n",
    "axes[1].plot(heads, entropies, 'g-', linewidth=2)\n",
    "axes[1].fill_between(heads, entropies, alpha=0.3, color='green')\n",
    "axes[1].set_xlabel('Number of Heads', fontsize=12)\n",
    "axes[1].set_ylabel('Entropy S = ln(W)', fontsize=12)\n",
    "axes[1].set_title(f'Entropy vs Macrostate (N={n_coins} coins)', fontsize=14)\n",
    "axes[1].axvline(x=n_coins/2, color='red', linestyle='--', label='Maximum entropy')\n",
    "axes[1].legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show maximum\n",
    "max_idx = np.argmax(entropies)\n",
    "print(f\"\\nMaximum entropy at {heads[max_idx]} heads\")\n",
    "print(f\"S_max = ln({microstates[max_idx]:,}) = {entropies[max_idx]:.4f}\")\n",
    "print(f\"Maximum possible S = ln(2^{n_coins}) = {n_coins * np.log(2):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Gas Molecules in a Box\n",
    "\n",
    "The classic entropy example: molecules spreading out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_gas_expansion(n_molecules, n_steps):\n",
    "    \"\"\"\n",
    "    Simulate gas molecules in a 2D box.\n",
    "    Start: all on left half.\n",
    "    Each step: random molecule moves to random new position.\n",
    "    \"\"\"\n",
    "    # Positions: True = left, False = right\n",
    "    positions = np.ones(n_molecules, dtype=bool)  # All start on left\n",
    "    \n",
    "    history = {'step': [0], 'n_left': [n_molecules], 'entropy': [0]}  # S=0 for 1 microstate\n",
    "    \n",
    "    for step in range(1, n_steps + 1):\n",
    "        # Pick a random molecule, move to random half\n",
    "        mol_idx = np.random.randint(n_molecules)\n",
    "        positions[mol_idx] = np.random.random() < 0.5\n",
    "        \n",
    "        n_left = positions.sum()\n",
    "        W = comb(n_molecules, n_left, exact=True)\n",
    "        S = np.log(W) if W > 0 else 0\n",
    "        \n",
    "        history['step'].append(step)\n",
    "        history['n_left'].append(n_left)\n",
    "        history['entropy'].append(S)\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Run simulation\n",
    "np.random.seed(42)\n",
    "n_molecules = 100\n",
    "n_steps = 500\n",
    "\n",
    "history = simulate_gas_expansion(n_molecules, n_steps)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "# Number on left side\n",
    "axes[0].plot(history['step'], history['n_left'], 'b-', linewidth=1, alpha=0.7)\n",
    "axes[0].axhline(y=n_molecules/2, color='red', linestyle='--', linewidth=2, label='Equilibrium')\n",
    "axes[0].set_ylabel('Molecules on Left Side', fontsize=12)\n",
    "axes[0].set_title(f'Gas Expansion: {n_molecules} Molecules', fontsize=14)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].set_ylim(0, n_molecules)\n",
    "\n",
    "# Entropy\n",
    "max_entropy = n_molecules * np.log(2)\n",
    "axes[1].plot(history['step'], history['entropy'], 'g-', linewidth=1, alpha=0.7)\n",
    "axes[1].axhline(y=max_entropy, color='red', linestyle='--', linewidth=2, label=f'Max S = N ln(2) = {max_entropy:.1f}')\n",
    "axes[1].set_xlabel('Time Steps', fontsize=12)\n",
    "axes[1].set_ylabel('Entropy S = ln(W)', fontsize=12)\n",
    "axes[1].legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nStarting entropy: S = ln(1) = 0 (all on one side)\")\n",
    "print(f\"Equilibrium entropy: S = ln(C({n_molecules},{n_molecules//2})) ≈ {np.log(comb(n_molecules, n_molecules//2, exact=True)):.2f}\")\n",
    "print(f\"Maximum possible: S = {n_molecules} × ln(2) = {max_entropy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Why Entropy Increases — Probability\n",
    "\n",
    "Let's see why high entropy states are more probable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability of different macrostates\n",
    "n_molecules = 20\n",
    "\n",
    "n_left_vals = np.arange(0, n_molecules + 1)\n",
    "microstates = [comb(n_molecules, n, exact=True) for n in n_left_vals]\n",
    "total = sum(microstates)\n",
    "probabilities = [W / total for W in microstates]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "bars = ax.bar(n_left_vals, probabilities, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "\n",
    "# Color extreme states\n",
    "bars[0].set_color('red')\n",
    "bars[-1].set_color('red')\n",
    "\n",
    "ax.set_xlabel('Number of Molecules on Left', fontsize=12)\n",
    "ax.set_ylabel('Probability', fontsize=12)\n",
    "ax.set_title(f'Probability of Each Macrostate (N={n_molecules} molecules)', fontsize=14)\n",
    "\n",
    "# Annotate extremes\n",
    "ax.annotate(f'All left\\nP = {probabilities[0]:.2e}', \n",
    "            xy=(0, probabilities[0]), xytext=(2, max(probabilities)/2),\n",
    "            arrowprops=dict(arrowstyle='->', color='red'),\n",
    "            fontsize=11, color='red')\n",
    "\n",
    "ax.annotate(f'Half-half\\nP = {probabilities[n_molecules//2]:.2%}', \n",
    "            xy=(n_molecules//2, probabilities[n_molecules//2]), \n",
    "            xytext=(n_molecules//2 + 3, probabilities[n_molecules//2] + 0.05),\n",
    "            arrowprops=dict(arrowstyle='->', color='green'),\n",
    "            fontsize=11, color='green')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nProbability comparisons:\")\n",
    "print(f\"  All on left: {probabilities[0]:.2e}\")\n",
    "print(f\"  Half and half: {probabilities[n_molecules//2]:.4f}\")\n",
    "print(f\"  Ratio: {probabilities[n_molecules//2] / probabilities[0]:,.0f}x more likely!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how this scales with N\n",
    "n_values = [10, 20, 50, 100, 200]\n",
    "\n",
    "print(\"Probability that ALL molecules stay on one side:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'N':>5} | {'P(all left)':>20} | {'1 in ...':<20}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for n in n_values:\n",
    "    p = 1 / (2**n)\n",
    "    print(f\"{n:>5} | {p:>20.2e} | {2**n:>20,}\")\n",
    "\n",
    "print(\"\\n→ For N = Avogadro's number (6×10²³), this is 1 in 10^(10²³)!\")\n",
    "print(\"   That's a number with 100,000,000,000,000,000,000,000 digits!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Gibbs Entropy for Non-Equal Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs_entropy(probs):\n",
    "    \"\"\"S = -Σ p_i ln(p_i)\"\"\"\n",
    "    probs = np.array(probs)\n",
    "    # Avoid log(0)\n",
    "    mask = probs > 0\n",
    "    return -np.sum(probs[mask] * np.log(probs[mask]))\n",
    "\n",
    "# Compare different probability distributions (3 states)\n",
    "distributions = [\n",
    "    ([1.0, 0.0, 0.0], \"Certain (one state)\"),\n",
    "    ([0.5, 0.5, 0.0], \"Two states, equal\"),\n",
    "    ([0.7, 0.2, 0.1], \"Three states, unequal\"),\n",
    "    ([1/3, 1/3, 1/3], \"Three states, equal (max)\")\n",
    "]\n",
    "\n",
    "print(\"Gibbs Entropy for Different Distributions:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for probs, name in distributions:\n",
    "    S = gibbs_entropy(probs)\n",
    "    print(f\"  {name}\")\n",
    "    print(f\"    p = {probs}\")\n",
    "    print(f\"    S = {S:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: entropy is maximized for uniform distribution\n",
    "# For 2 states: vary p from 0 to 1\n",
    "\n",
    "p_vals = np.linspace(0.001, 0.999, 100)\n",
    "entropies = [-p*np.log(p) - (1-p)*np.log(1-p) for p in p_vals]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(p_vals, entropies, 'b-', linewidth=2)\n",
    "ax.fill_between(p_vals, entropies, alpha=0.3)\n",
    "ax.axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Maximum at p=0.5')\n",
    "ax.plot(0.5, np.log(2), 'ro', markersize=10)\n",
    "\n",
    "ax.set_xlabel('Probability p of state 1', fontsize=12)\n",
    "ax.set_ylabel('Entropy S = -p ln(p) - (1-p) ln(1-p)', fontsize=12)\n",
    "ax.set_title('Gibbs Entropy for 2-State System', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "\n",
    "ax.annotate(f'S_max = ln(2) = {np.log(2):.3f}', xy=(0.5, np.log(2)), \n",
    "            xytext=(0.65, np.log(2)-0.1), fontsize=12,\n",
    "            arrowprops=dict(arrowstyle='->', color='red'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight: Entropy is maximized when probabilities are EQUAL.\")\n",
    "print(\"This is why systems tend toward 'maximum ignorance' - all states equally likely.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Heat Flow and Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_change_heat_transfer(Q, T_hot, T_cold):\n",
    "    \"\"\"\n",
    "    Calculate entropy change when heat Q flows from hot to cold.\n",
    "    \n",
    "    dS_hot = -Q/T_hot (loses heat)\n",
    "    dS_cold = +Q/T_cold (gains heat)\n",
    "    dS_total = Q(1/T_cold - 1/T_hot) > 0 when T_hot > T_cold\n",
    "    \"\"\"\n",
    "    dS_hot = -Q / T_hot\n",
    "    dS_cold = Q / T_cold\n",
    "    dS_total = dS_hot + dS_cold\n",
    "    return dS_hot, dS_cold, dS_total\n",
    "\n",
    "# Example: heat flow between two objects\n",
    "T_hot = 400  # K\n",
    "T_cold = 300  # K\n",
    "Q = 100  # J\n",
    "\n",
    "dS_hot, dS_cold, dS_total = entropy_change_heat_transfer(Q, T_hot, T_cold)\n",
    "\n",
    "print(\"Heat Transfer Entropy Example:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Hot object: T = {T_hot} K\")\n",
    "print(f\"Cold object: T = {T_cold} K\")\n",
    "print(f\"Heat transferred: Q = {Q} J\")\n",
    "print(f\"\\nEntropy changes:\")\n",
    "print(f\"  Hot object: ΔS = {dS_hot:.4f} J/K (decreases)\")\n",
    "print(f\"  Cold object: ΔS = {dS_cold:.4f} J/K (increases)\")\n",
    "print(f\"  Total: ΔS = {dS_total:.4f} J/K (net INCREASE)\")\n",
    "print(f\"\\n>>> Second Law satisfied: ΔS_total > 0 <<<\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize entropy change vs temperature difference\n",
    "T_hot = 400\n",
    "T_cold_vals = np.linspace(100, 399, 100)\n",
    "Q = 100\n",
    "\n",
    "dS_totals = [Q * (1/T_cold - 1/T_hot) for T_cold in T_cold_vals]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(T_cold_vals, dS_totals, 'g-', linewidth=2)\n",
    "ax.fill_between(T_cold_vals, dS_totals, alpha=0.3, color='green')\n",
    "ax.axhline(y=0, color='black', linewidth=0.5)\n",
    "\n",
    "ax.set_xlabel('Cold Temperature (K)', fontsize=12)\n",
    "ax.set_ylabel('Total Entropy Change ΔS (J/K)', fontsize=12)\n",
    "ax.set_title(f'Entropy Change vs Temperature Difference (T_hot = {T_hot}K, Q = {Q}J)', fontsize=14)\n",
    "\n",
    "ax.annotate('Larger T difference\\n= Larger ΔS', xy=(200, Q*(1/200-1/T_hot)), \n",
    "            xytext=(220, 0.3), fontsize=11,\n",
    "            arrowprops=dict(arrowstyle='->', color='green'))\n",
    "\n",
    "ax.annotate('As T_cold → T_hot\\nΔS → 0', xy=(380, Q*(1/380-1/T_hot)), \n",
    "            xytext=(320, 0.1), fontsize=11,\n",
    "            arrowprops=dict(arrowstyle='->', color='red'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight: Heat flow from hot to cold ALWAYS increases total entropy.\")\n",
    "print(\"The larger the temperature difference, the larger the entropy increase.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Concept | Formula | Meaning |\n",
    "|---------|---------|--------|\n",
    "| Boltzmann Entropy | $S = k_B \\ln W$ | More microstates = higher entropy |\n",
    "| Gibbs Entropy | $S = -k_B \\sum p_i \\ln p_i$ | General case with probabilities |\n",
    "| Second Law | $\\Delta S_{total} \\geq 0$ | Entropy never decreases (isolated system) |\n",
    "| Heat Transfer | $\\Delta S = Q/T_{cold} - Q/T_{hot}$ | Heat flow increases entropy |\n",
    "\n",
    "**The deep insight**: Entropy increase is NOT a mysterious force—it's just probability. Systems evolve toward states with more microstates because those states are more probable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
