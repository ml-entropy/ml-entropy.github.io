<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Thermodynamic Entropy | Physics</title>
    <meta name="description" content="Understanding thermodynamic entropy - the physical quantity that inspired information theory.">
    
    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&family=Playfair+Display:wght@400;500;600;700&display=swap" rel="stylesheet">
    
    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}, {left: '\\\\[', right: '\\\\]', display: true}, {left: '\\\\(', right: '\\\\)', display: false}], throwOnError: false});"></script>
    
    <!-- Styles -->
    <link rel="stylesheet" href="../../../css/main.css">
    <link rel="stylesheet" href="../../../css/components.css">
    <link rel="stylesheet" href="../../../css/sidebar.css">
    
    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>‚àû</text></svg>">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar" id="navbar">
        <div class="nav-container">
            <a href="../../../index.html" class="nav-logo">
                <span class="logo-symbol">‚àá</span>
                <span class="logo-text">ML Fundamentals</span>
            </a>
            
            <button class="nav-toggle" id="navToggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            
            <div class="nav-menu" id="navMenu">
                <div class="nav-links">
                    <a href="../../../tutorials/ml/index.html" class="nav-link">Machine Learning</a>
                    <a href="../../../tutorials/linear-algebra/index.html" class="nav-link">Linear Algebra</a>
                    <a href="../../../tutorials/calculus/index.html" class="nav-link">Calculus</a>
                    <a href="../../../tutorials/physics/index.html" class="nav-link active">Physics</a>
                    <a href="../../../index.html#philosophy" class="nav-link">Philosophy</a>
                    <a href="../../../index.html#roadmap" class="nav-link">Roadmap</a>
                    <a href="https://github.com/ml-entropy/ml-entropy.github.io" class="nav-link" target="_blank">GitHub</a>
                </div>
                
                <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
                    <svg class="sun-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="5"/>
                        <path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"/>
                    </svg>
                    <svg class="moon-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                    </svg>
                </button>
            </div>
        </div>
    </nav>

    <!-- Tutorial Header -->
    <header class="tutorial-content-header">
        <div class="container">
            <nav class="breadcrumb">
                <a href="../../../index.html">Home</a>
                <span class="breadcrumb-separator">‚Üí</span>
                <a href="../index.html">Physics</a>
                <span class="breadcrumb-separator">‚Üí</span>
                <span>Thermodynamic Entropy</span>
            </nav>
            
            
            
            
            <div class="tutorial-tabs">
                <a href="#theory" class="tutorial-tab active">Theory</a>
                <a href="#code" class="tutorial-tab">Code</a>
                <a href="#exercises" class="tutorial-tab">Exercises</a>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <div class="tutorial-wrapper">
        
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <aside class="tutorial-sidebar">
            <div class="sidebar-section">
                <h3 class="sidebar-section-title">Physics</h3>
                <nav class="sidebar-nav">
                        <a href="../01-entropy/index.html" class="sidebar-link active">01. Entropy</a>
                    <a href="../02-carnot/index.html" class="sidebar-link">02. Carnot Cycle</a>
                </nav>
            </div>
            
            <div class="sidebar-section" style="margin-top: 2rem;">
                <h3 class="sidebar-section-title">Related Subjects</h3>
                <nav class="sidebar-nav">
                        <a href="../../ml/index.html" class="sidebar-link">Machine Learning</a>
                    <a href="../../linear-algebra/index.html" class="sidebar-link">Linear Algebra</a>
                    <a href="../../calculus/index.html" class="sidebar-link">Calculus</a>
                </nav>
            </div>
        </aside>

        <!-- Main Article: Theory -->
        <main class="tutorial-main">
            
            <article class="article-content" id="theory">
                
                <!-- Section 1 -->
                <h2 id="clausius">Clausius and Thermodynamics</h2>
                
                <p>
                    The concept of entropy began not with information, but with steam engines. In 1865, Rudolf Clausius was trying to understand why heat always flows from hot to cold bodies and never the reverse, a phenomenon that energy conservation alone cannot explain. He defined a new quantity, <strong>Entropy ($S$)</strong>, to quantify this "transformation content" (Verwandlungsinhalt).
                </p>
                
                <div class="definition-box">
                    <div class="box-title">Thermodynamic Definition of Entropy</div>
                    <p>
                        For a reversible process occurring at a constant temperature $T$, the infinitesimal change in entropy $dS$ is:
                    </p>
                    <div class="math-block">
                        $$dS = \frac{\delta Q_{\text{rev}}}{T}$$
                    </div>
                    <p><strong>Understanding the Variables:</strong></p>
                    <ul>
                        <li><strong>$dS$ (Entropy Change)</strong>: The tiny (infinitesimal) change in the system's entropy. Measured in Joules per Kelvin ($J/K$).</li>
                        <li><strong>$\delta Q_{\text{rev}}$ (Reversible Heat)</strong>: The tiny amount of heat energy added to the system. 
                            <ul>
                                <li>The $\delta$ indicates it is a path-dependent quantity (not an exact differential).</li>
                                <li>The subscript $_{\text{rev}}$ means the heat must be added <em>reversibly</em>‚Äîso slowly that the system stays in equilibrium (no turbulence, no friction).</li>
                            </ul>
                        </li>
                        <li><strong>$T$ (Temperature)</strong>: The absolute temperature of the system at the moment heat is added, measured in Kelvin ($K$).
                            <ul>
                                <li>Dividing by $T$ means that adding heat to a cold system creates <em>more</em> entropy than adding the same heat to a hot system. This is crucial for understanding efficiency.</li>
                            </ul>
                        </li>
                    </ul>
                </div>

                <div class="example-box">
                    <div class="box-title">Example: Melting Ice</div>
                    <p>
                        Calculate the entropy change when $1 \text{ kg}$ of ice melts at $0^\circ\text{C}$ ($273.15 \text{ K}$).
                        <br><br>
                        <strong>Given:</strong> Latent heat of fusion for water $L = 334 \text{ kJ/kg}$.
                        <br>
                        <strong>Process:</strong>
                        1. Heat added $Q = m \cdot L = 1 \text{ kg} \cdot 334,000 \text{ J/kg} = 334,000 \text{ J}$.
                        <br>
                        2. Temperature $T = 273.15 \text{ K}$ (constant during phase change).
                        <br>
                        3. Entropy change:
                        $$\Delta S = \frac{Q}{T} = \frac{334,000 \text{ J}}{273.15 \text{ K}} \approx 1,222.7 \text{ J/K}$$
                        <br>
                        The water is more "disordered" than the ice, so its entropy is higher.
                    </p>
                </div>
                
                <!-- Section 2 -->
                <h2 id="second-law">The Second Law and Time</h2>
                
                <p>
                    The Second Law of Thermodynamics is arguably the most profound law in physics. It states that for any <strong>isolated system</strong> (one that exchanges no energy or matter with its surroundings), the total entropy can never decrease.
                </p>
                
                <div class="math-block">
                    $$\Delta S_{\text{universe}} \geq 0$$
                </div>
                
                <p>
                    This inequality ($\geq$) implies a direction. While Newton's laws of motion work equally well forward or backward in time, the Second Law does not.
                </p>
                
                <div class="warning-box">
                    <div class="box-title">The Arrow of Time</div>
                    <p style="margin-bottom: 0;">
                        Entropy provides the "Arrow of Time." We perceive time flowing from past to future because the future is the direction of higher entropy.
                        <ul>
                            <li><strong>Past:</strong> Lower entropy (Ordered state, e.g., an egg).</li>
                            <li><strong>Future:</strong> Higher entropy (Disordered state, e.g., a broken egg).</li>
                        </ul>
                    </p>
                </div>
                
                <!-- Section 3 -->
                <h2 id="boltzmann">Boltzmann's Statistical View</h2>
                
                <p>
                    While Clausius described <em>what</em> entropy does, Ludwig Boltzmann explained <em>what it is</em>. He realized that macro-scale properties (like pressure and temperature) are averages of billions of atoms moving randomly.
                </p>
                
                <div class="definition-box">
                    <div class="box-title">Boltzmann Entropy Formula</div>
                    <p>
                        Boltzmann connected entropy to the number of possible microscopic configurations:
                    </p>
                    <div class="math-block">
                        $$S = k_B \ln \Omega$$
                    </div>
                    <p><strong>Understanding the Variables:</strong></p>
                    <ul>
                        <li><strong>$S$ (Entropy)</strong>: The macroscopic entropy of the system.</li>
                        <li><strong>$k_B$ (Boltzmann Constant)</strong>: A fundamental constant of nature, $k_B \approx 1.3806 \times 10^{-23} \text{ J/K}$.
                            <ul>
                                <li>It acts as a bridge, converting units of "counting states" (dimensionless) into physical units of energy per degree temperature ($J/K$).</li>
                            </ul>
                        </li>
                        <li><strong>$\ln$ (Natural Logarithm)</strong>: The logarithm base $e$. 
                            <ul>
                                <li>Why log? Because probabilities multiply ($P(A \text{ and } B) = P(A) \times P(B)$), but extensive quantities like entropy add ($S_{A+B} = S_A + S_B$). The logarithm turns multiplication into addition: $\ln(AB) = \ln A + \ln B$.</li>
                            </ul>
                        </li>
                        <li><strong>$\Omega$ (Multiplicity)</strong>: The number of distinct <strong>microstates</strong> consistent with the current <strong>macrostate</strong>.
                            <ul>
                                <li><strong>Microstate:</strong> The precise position and velocity of every single particle.</li>
                                <li><strong>Macrostate:</strong> The overall observable state (e.g., "Pressure is 1 atm").</li>
                            </ul>
                        </li>
                    </ul>
                </div>

                <h3>Example: The Coin Toss Visualized</h3>
                <p>
                    Imagine flipping 4 coins. The "Macrostate" is the total number of Heads. Notice how the macrostate "2 Heads" has the most microstates.
                </p>
                
                <div style="overflow-x: auto; margin: 2rem 0;">
                    <svg viewBox="0 0 800 350" xmlns="http://www.w3.org/2000/svg" style="font-family: 'Inter', sans-serif; background: var(--color-bg-alt); border-radius: 12px; border: 1px solid var(--color-border);">
                        <!-- Grid and Labels -->
                        <g transform="translate(50, 40)">
                            
                            <!-- 0 Heads -->
                            <text x="0" y="20" font-weight="bold" fill="var(--color-text)">0 Heads</text>
                            <text x="0" y="40" font-size="12" fill="var(--color-text-muted)">Œ© = 1</text>
                            <g transform="translate(100, 10)">
                                <circle cx="15" cy="15" r="10" fill="#E5E7EB" stroke="#9CA3AF"/><text x="15" y="20" text-anchor="middle" font-size="10" fill="#6B7280">T</text>
                                <circle cx="40" cy="15" r="10" fill="#E5E7EB" stroke="#9CA3AF"/><text x="40" y="20" text-anchor="middle" font-size="10" fill="#6B7280">T</text>
                                <circle cx="65" cy="15" r="10" fill="#E5E7EB" stroke="#9CA3AF"/><text x="65" y="20" text-anchor="middle" font-size="10" fill="#6B7280">T</text>
                                <circle cx="90" cy="15" r="10" fill="#E5E7EB" stroke="#9CA3AF"/><text x="90" y="20" text-anchor="middle" font-size="10" fill="#6B7280">T</text>
                            </g>

                            <!-- 1 Head -->
                            <text x="0" y="90" font-weight="bold" fill="var(--color-text)">1 Head</text>
                            <text x="0" y="110" font-size="12" fill="var(--color-text-muted)">Œ© = 4</text>
                            <g transform="translate(100, 80)">
                                <!-- HTTT -->
                                <circle cx="15" cy="15" r="10" fill="#FCA5A5" stroke="#EF4444"/><text x="15" y="20" text-anchor="middle" font-size="10" fill="#991B1B">H</text>
                                <circle cx="40" cy="15" r="10" fill="#E5E7EB" stroke="#9CA3AF"/><text x="40" y="20" text-anchor="middle" font-size="10" fill="#6B7280">T</text>
                                <circle cx="65" cy="15" r="10" fill="#E5E7EB" stroke="#9CA3AF"/><text x="65" y="20" text-anchor="middle" font-size="10" fill="#6B7280">T</text>
                                <circle cx="90" cy="15" r="10" fill="#E5E7EB" stroke="#9CA3AF"/><text x="90" y="20" text-anchor="middle" font-size="10" fill="#6B7280">T</text>
                                <text x="120" y="20" font-size="12" fill="var(--color-text-muted)">+ 3 more variations...</text>
                            </g>

                            <!-- 2 Heads (Max Entropy) -->
                            <rect x="-10" y="140" width="720" height="70" rx="8" fill="rgba(34, 197, 94, 0.1)" stroke="rgba(34, 197, 94, 0.3)" />
                            <text x="0" y="170" font-weight="bold" fill="#15803D">2 Heads</text>
                            <text x="0" y="190" font-size="12" fill="#15803D">Œ© = 6 (Max)</text>
                            <g transform="translate(100, 160)">
                                <!-- HHTT -->
                                <g transform="translate(0,0)">
                                    <circle cx="15" cy="15" r="10" fill="#FCA5A5" stroke="#EF4444"/><text x="15" y="20" text-anchor="middle" font-size="10" fill="#991B1B">H</text>
                                    <circle cx="40" cy="15" r="10" fill="#FCA5A5" stroke="#EF4444"/><text x="40" y="20" text-anchor="middle" font-size="10" fill="#991B1B">H</text>
                                    <circle cx="65" cy="15" r="10" fill="#E5E7EB" stroke="#9CA3AF"/><text x="65" y="20" text-anchor="middle" font-size="10" fill="#6B7280">T</text>
                                    <circle cx="90" cy="15" r="10" fill="#E5E7EB" stroke="#9CA3AF"/><text x="90" y="20" text-anchor="middle" font-size="10" fill="#6B7280">T</text>
                                </g>
                                <!-- HTHT -->
                                <g transform="translate(130,0)">
                                    <circle cx="15" cy="15" r="10" fill="#FCA5A5" stroke="#EF4444"/><text x="15" y="20" text-anchor="middle" font-size="10" fill="#991B1B">H</text>
                                    <circle cx="40" cy="15" r="10" fill="#E5E7EB" stroke="#9CA3AF"/><text x="40" y="20" text-anchor="middle" font-size="10" fill="#6B7280">T</text>
                                    <circle cx="65" cy="15" r="10" fill="#FCA5A5" stroke="#EF4444"/><text x="65" y="20" text-anchor="middle" font-size="10" fill="#991B1B">H</text>
                                    <circle cx="90" cy="15" r="10" fill="#E5E7EB" stroke="#9CA3AF"/><text x="90" y="20" text-anchor="middle" font-size="10" fill="#6B7280">T</text>
                                </g>
                                <text x="260" y="20" font-size="12" fill="#15803D">+ 4 more (HTTH, THHT, THTH, TTHH)</text>
                            </g>

                            <!-- 3 Heads -->
                            <text x="0" y="250" font-weight="bold" fill="var(--color-text)">3 Heads</text>
                            <text x="0" y="270" font-size="12" fill="var(--color-text-muted)">Œ© = 4</text>
                            <g transform="translate(100, 240)">
                                <circle cx="15" cy="15" r="10" fill="#FCA5A5" stroke="#EF4444"/><text x="15" y="20" text-anchor="middle" font-size="10" fill="#991B1B">H</text>
                                <circle cx="40" cy="15" r="10" fill="#FCA5A5" stroke="#EF4444"/><text x="40" y="20" text-anchor="middle" font-size="10" fill="#991B1B">H</text>
                                <circle cx="65" cy="15" r="10" fill="#FCA5A5" stroke="#EF4444"/><text x="65" y="20" text-anchor="middle" font-size="10" fill="#991B1B">H</text>
                                <circle cx="90" cy="15" r="10" fill="#E5E7EB" stroke="#9CA3AF"/><text x="90" y="20" text-anchor="middle" font-size="10" fill="#6B7280">T</text>
                                <text x="120" y="20" font-size="12" fill="var(--color-text-muted)">+ 3 more variations...</text>
                            </g>
                        </g>
                    </svg>
                </div>
                
                <p>
                    The state "2 Heads" has the highest entropy because it has the most ways to happen (6 ways). The state "0 Heads" (TTTT) has the lowest entropy because there is only 1 way to achieve it.
                </p>
                
                <!-- Section 4 -->
                <h2 id="gibbs">Gibbs Entropy</h2>
                
                <p>
                    Boltzmann's formula assumes every microstate is equally likely. Josiah Willard Gibbs generalized this for cases where microstates have different probabilities $p_i$.
                </p>
                
                <div class="math-block">
                    $$S = -k_B \sum_{i} p_i \ln p_i$$
                </div>
                
                <p><strong>Understanding the Variables:</strong></p>
                <ul>
                    <li><strong>$\sum_{i}$</strong>: Summation over all possible microstates $i$.</li>
                    <li><strong>$p_i$</strong>: The probability of the system being in microstate $i$.</li>
                    <li><strong>$\ln p_i$</strong>: The log-probability. Since $p_i < 1$, this is negative, so the leading minus sign makes the entropy positive.</li>
                </ul>

                <div class="note-box">
                    <div class="box-title">Relation to Information Theory</div>
                    <p style="margin-bottom: 0;">
                        This formula is mathematically identical to <strong>Shannon Entropy</strong> in information theory:
                        $$H = -\sum p_i \log_2 p_i$$
                        The only differences are:
                        1. <strong>Units:</strong> Gibbs uses $k_B$ (Joules/Kelvin), Shannon uses "bits".
                        2. <strong>Base:</strong> Gibbs uses natural log ($\ln$), Shannon uses base 2 ($\log_2$).
                    </p>
                </div>
                
                <!-- Section 5 -->
                <h2 id="examples">Detailed Examples</h2>
                
                <h3>1. Heat Transfer Between Two Objects</h3>
                <p>
                    Suppose a hot block ($T_H = 400 \text{ K}$) touches a cold block ($T_C = 200 \text{ K}$). A small amount of heat $Q = 100 \text{ J}$ flows from Hot to Cold.
                </p>
                <p><strong>Step-by-Step Calculation:</strong></p>
                <ol>
                    <li>
                        <strong>Entropy Change of Hot Block:</strong>
                        It loses heat, so its entropy decreases.
                        $$\Delta S_H = \frac{-Q}{T_H} = \frac{-100}{400} = -0.25 \text{ J/K}$$
                    </li>
                    <li>
                        <strong>Entropy Change of Cold Block:</strong>
                        It gains heat, so its entropy increases.
                        $$\Delta S_C = \frac{+Q}{T_C} = \frac{+100}{200} = +0.50 \text{ J/K}$$
                    </li>
                    <li>
                        <strong>Total Entropy Change:</strong>
                        $$\Delta S_{\text{total}} = \Delta S_H + \Delta S_C = -0.25 + 0.50 = \mathbf{+0.25 \text{ J/K}}$$
                    </li>
                </ol>
                <p>
                    Because $\Delta S_{\text{total}} > 0$, this process is spontaneous and irreversible. If heat flowed the other way (Cold to Hot), the total entropy change would be negative, which is forbidden by the Second Law.
                </p>

                <h3>2. Free Expansion of a Gas</h3>
                <p>
                    A gas with $N$ molecules expands from volume $V$ to $2V$ into a vacuum (no work done, constant temperature).
                </p>
                <p>
                    <strong>Using Boltzmann's Formula:</strong>
                    <br>
                    The number of available positions for each molecule doubles. So the number of microstates $\Omega$ increases by a factor of $2^N$.
                </p>
                <div class="math-block">
                    $$\begin{aligned}
                    \Delta S &= S_{\text{final}} - S_{\text{initial}} \\
                             &= k_B \ln(\Omega_{\text{final}}) - k_B \ln(\Omega_{\text{initial}}) \\
                             &= k_B \ln\left(\frac{\Omega_{\text{final}}}{\Omega_{\text{initial}}}\right) \\
                             &= k_B \ln(2^N) \\
                             &= N k_B \ln 2
                    \end{aligned}$$
                </div>
                <p>
                    For 1 mole of gas ($N \approx 6 \times 10^{23}$), this is a huge increase in entropy!
                </p>
                
                <!-- Navigation -->
                
            <div class="tutorial-footer-summary" style="margin: 3rem 0; padding: 2rem; background: #f8f9fa; border-radius: 8px; border-left: 4px solid #3b82f6;">
                <h1>01. Thermodynamic Entropy</h1>
                <p class="lead">
                Entropy is often described as "disorder," but it is more accurately a measure of hidden information and statistical probability. This tutorial bridges the gap between the steam engine and information theory.
            </p>
            </div>
                <div class="tutorial-nav">
                    <a href="../index.html" class="tutorial-nav-link prev">
                        <span class="nav-label">Previous</span>
                        <span class="nav-title">‚Üê Physics Home</span>
                    </a>
                    <a href="../02-carnot/index.html" class="tutorial-nav-link next">
                        <span class="nav-label">Next</span>
                        <span class="nav-title">The Carnot Cycle ‚Üí</span>
                    </a>
                </div>
                
            </article>

            <!-- Code Article -->
            <article class="article-content" id="code" style="display: none;">
                <h2>Simulating Entropy</h2>
                <p>
                    The following Python code uses `matplotlib` to visualize how entropy behaves in simple systems like coin tosses and gas expansion. You can run this code in a Jupyter Notebook.
                </p>
                
                <h3>1. Entropy of a Coin Toss</h3>
                <p>
                    This script counts the microstates for $N$ coins and calculates the Boltzmann entropy. It uses `scipy.special.comb` for efficient factorial calculations.
                </p>

                <div class="code-block">
                    <div class="code-block-header">
                        <span class="code-block-lang">python</span>
                        <button class="code-block-copy">Copy</button>
                    </div>
                    <pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from scipy.special import comb

def entropy_boltzmann(W):
    """
    Calculate entropy S = k_B * ln(W).
    We use units where k_B = 1 for simplicity.
    """
    if W <= 0:
        return 0
    return np.log(W)

# Parameters
n_coins = 100
heads = np.arange(0, n_coins + 1)

# Calculate Microstates (W) for each macrostate (number of heads)
# W = n! / (k! * (n-k)!)
microstates = [comb(n_coins, h, exact=True) for h in heads]

# Calculate Entropy
entropies = [entropy_boltzmann(W) for W in microstates]

# Plot results
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Plot 1: Microstates
axes[0].plot(heads, microstates, 'b-', linewidth=2)
axes[0].fill_between(heads, microstates, alpha=0.3)
axes[0].set_xlabel('Number of Heads')
axes[0].set_ylabel('Number of Microstates W')
axes[0].set_title(f'Microstates vs Macrostate (N={n_coins})')
axes[0].axvline(x=n_coins/2, color='red', linestyle='--', label='Max W')
axes[0].legend()

# Plot 2: Entropy
axes[1].plot(heads, entropies, 'g-', linewidth=2)
axes[1].fill_between(heads, entropies, alpha=0.3, color='green')
axes[1].set_xlabel('Number of Heads')
axes[1].set_ylabel('Entropy S = ln(W)')
axes[1].set_title(f'Entropy vs Macrostate')
axes[1].axvline(x=n_coins/2, color='red', linestyle='--', label='Max S')
axes[1].legend()

plt.tight_layout()
plt.show()</code></pre>
                </div>

                <h3>2. Gas Expansion Simulation</h3>
                <p>
                    This simulation starts with all molecules on the left side of a box and lets them randomly move to the right. Watch how entropy increases until it fluctuates around a maximum!
                </p>

                <div class="code-block">
                    <div class="code-block-header">
                        <span class="code-block-lang">python</span>
                        <button class="code-block-copy">Copy</button>
                    </div>
                    <pre><code class="language-python">def simulate_gas_expansion(n_molecules, n_steps):
    """
    Simulate gas molecules in a 2D box.
    Start: all on left half.
    Each step: random molecule moves to random new position.
    """
    # Positions: True = left, False = right
    positions = np.ones(n_molecules, dtype=bool)  # All start on left
    
    history = {'step': [], 'entropy': []}
    
    for step in range(n_steps):
        # Pick a random molecule, move to random half
        mol_idx = np.random.randint(n_molecules)
        positions[mol_idx] = np.random.random() < 0.5
        
        # Calculate entropy based on multiplicity
        n_left = positions.sum()
        W = comb(n_molecules, n_left, exact=True)
        S = np.log(W) if W > 0 else 0
        
        history['step'].append(step)
        history['entropy'].append(S)
    
    return history

# Run simulation
n_molecules = 100
history = simulate_gas_expansion(n_molecules, n_steps=500)

# Plot
plt.figure(figsize=(10, 5))
plt.plot(history['step'], history['entropy'], 'g-', alpha=0.8)
max_entropy = n_molecules * np.log(2)
plt.axhline(y=max_entropy, color='red', linestyle='--', label=f'Max S = {max_entropy:.1f}')
plt.xlabel('Time Steps')
plt.ylabel('Entropy')
plt.title('Entropy Increasing with Time')
plt.legend()
plt.show()</code></pre>
                </div>

            </article>

            <!-- Exercises Article -->
            <article class="article-content" id="exercises" style="display: none;">
                <h2>Exercises</h2>
                <p>Test your understanding of entropy, from basic counting to advanced derivations.</p>

                <div class="exercise-list">
                    
                    <!-- Easy -->
                    <h3 style="margin-top: 1rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">üü¢ Easy</h3>
                    
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-difficulty">üü¢</span>
                            <span class="exercise-title">1. Microstate Counting</span>
                            <span class="exercise-toggle">‚ñº</span>
                        </div>
                        <div class="exercise-body">
                            <p>A box has 4 distinct particles (A, B, C, D) that can be in the left (L) or right (R) half.</p>
                            <ol type="a">
                                <li>List all possible microstates (e.g., LLLL, LLLR...).</li>
                                <li>How many microstates have exactly 2 particles on each side?</li>
                                <li>Which macrostate (number of particles on left) has the highest multiplicity?</li>
                            </ol>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>a) Total Microstates ($2^4 = 16$):</strong><br>
                                LLLL (1)<br>
                                LLLR, LLRL, LRLL, RLLL (4)<br>
                                LLRR, LRLR, LRRL, RLLR, RLRL, RRLL (6)<br>
                                LRRR, RLRR, RRLR, RRRL (4)<br>
                                RRRR (1)</p>
                                <p><strong>b) 2 on each side:</strong> There are 6 microstates (see the middle row above). mathematically, $\binom{4}{2} = 6$.</p>
                                <p><strong>c) Highest Multiplicity:</strong> The state with 2 on left / 2 on right has the highest count (6). Extremes (4-0 or 0-4) have the lowest (1).</p>
                            </div>
                        </div>
                    </div>

                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-difficulty">üü¢</span>
                            <span class="exercise-title">2. Calculating Boltzmann Entropy</span>
                            <span class="exercise-toggle">‚ñº</span>
                        </div>
                        <div class="exercise-body">
                            <p>A system has exactly 100 possible microstates consistent with its macrostate. Calculate its entropy in units of Boltzmann's constant $k_B$.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p>Using $S = k_B \ln \Omega$:</p>
                                <p>$$S = k_B \ln(100) \approx 4.605 k_B$$</p>
                            </div>
                        </div>
                    </div>

                    <!-- Medium -->
                    <h3 style="margin-top: 2rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">üü° Medium</h3>

                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-difficulty">üü°</span>
                            <span class="exercise-title">3. Deriving the Boltzmann Formula</span>
                            <span class="exercise-toggle">‚ñº</span>
                        </div>
                        <div class="exercise-body">
                            <p>Start with the following assumptions:</p>
                            <ol>
                                <li>Entropy $S$ depends only on the number of microstates $\Omega$: $S = f(\Omega)$.</li>
                                <li>Entropy is additive: For two independent systems A and B, $S_{total} = S_A + S_B$.</li>
                                <li>Multiplicity is multiplicative: $\Omega_{total} = \Omega_A \times \Omega_B$.</li>
                            </ol>
                            <p>Show that the function $f(\Omega)$ must be a logarithm.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p>We need a function $f$ such that:</p>
                                $$f(\Omega_A \cdot \Omega_B) = f(\Omega_A) + f(\Omega_B)$$
                                <p>Differentiate with respect to $\Omega_A$ (treating $\Omega_B$ as constant):</p>
                                $$\Omega_B f'(\Omega_A \Omega_B) = f'(\Omega_A)$$
                                <p>Multiply by $\Omega_A$:</p>
                                $$\Omega_A \Omega_B f'(\Omega_A \Omega_B) = \Omega_A f'(\Omega_A)$$
                                <p>Let $x = \Omega_A \Omega_B$. Then $x f'(x) = C$ (a constant). Thus $f'(x) = C/x$. Integrating gives $f(x) = C \ln x + K$.</p>
                                <p>Standard convention sets $K=0$ (1 state = 0 entropy), so $S = C \ln \Omega$.</p>
                            </div>
                        </div>
                    </div>
                    
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-difficulty">üü°</span>
                            <span class="exercise-title">4. Mixing Entropy</span>
                            <span class="exercise-toggle">‚ñº</span>
                        </div>
                        <div class="exercise-body">
                            <p>Two distinct ideal gases (A and B), each with $N$ molecules, are in a box separated by a partition. Calculate the entropy change when the partition is removed.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p>Before mixing, gas A occupies volume $V$. After mixing, it occupies $2V$.</p>
                                <p>$\Delta S_A = N k_B \ln(2V/V) = N k_B \ln 2$.</p>
                                <p>Similarly for gas B: $\Delta S_B = N k_B \ln 2$.</p>
                                <p>Total entropy change: $\Delta S_{mix} = 2 N k_B \ln 2$.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Hard -->
                    <h3 style="margin-top: 2rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">üî¥ Hard</h3>

                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-difficulty">üî¥</span>
                            <span class="exercise-title">5. Stirling's Approximation</span>
                            <span class="exercise-toggle">‚ñº</span>
                        </div>
                        <div class="exercise-body">
                            <p>For very large $N$, the factorial function $N!$ is hard to calculate. Stirling's approximation states that $\ln N! \approx N \ln N - N$.</p>
                            <p>Use this to show that for a system with $N$ particles distributed into $k$ states with counts $n_1, n_2, ..., n_k$, the entropy per particle approximates the Shannon entropy:</p>
                            <p>$$\frac{S}{N k_B} \approx -\sum p_i \ln p_i$$ where $p_i = n_i/N$.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p>The number of microstates is given by the multinomial coefficient: $\Omega = \frac{N!}{n_1! n_2! \dots n_k!}$</p>
                                <p>Taking the log:</p>
                                $$\ln \Omega = \ln N! - \sum \ln n_i!$$
                                <p>Using Stirling's $\ln x! \approx x \ln x - x$:</p>
                                $$\ln \Omega \approx (N \ln N - N) - \sum (n_i \ln n_i - n_i)$$
                                <p>Since $\sum n_i = N$, the linear terms cancel:</p>
                                $$\ln \Omega \approx N \ln N - \sum n_i \ln n_i = \sum n_i \ln N - \sum n_i \ln n_i = -\sum n_i (\ln n_i - \ln N) = -\sum n_i \ln(n_i/N)$$
                                <p>Substitute $n_i = N p_i$:</p>
                                $$S = k_B \ln \Omega \approx -k_B \sum N p_i \ln p_i = -N k_B \sum p_i \ln p_i$$
                            </div>
                        </div>
                    </div>

                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-difficulty">üî¥</span>
                            <span class="exercise-title">6. Maxwell's Demon</span>
                            <span class="exercise-toggle">‚ñº</span>
                        </div>
                        <div class="exercise-body">
                            <p>Maxwell proposed a thought experiment: A demon controls a door between two gas chambers. It lets fast molecules pass to the right and slow ones to the left, creating a temperature difference without doing work. This seems to decrease entropy ($\Delta S < 0$). Why doesn't this violate the Second Law?</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p>The solution (resolved by Szilard and Landauer) is that the <strong>Demon itself must record information</strong> about the molecules' speeds.</p>
                                <p>To repeat the cycle, the Demon must eventually erase this memory to make room for new measurements.</p>
                                <p><strong>Landauer's Principle:</strong> Erasing 1 bit of information at temperature $T$ releases at least $k_B T \ln 2$ of heat.</p>
                                <p>This heat generation increases the entropy of the environment by at least as much as the Demon reduced the entropy of the gas. The total entropy of the Universe still increases.</p>
                            </div>
                        </div>
                    </div>

                </div>
            </article>
        
        </main>

        <!-- TOC (Right Side) -->
        <aside class="toc-container">
            <h4 class="toc-title">Contents</h4>
            <nav class="toc-list">
                <a href="#clausius" class="toc-link">Clausius & Thermodynamics</a>
                <a href="#second-law" class="toc-link">The Second Law</a>
                <a href="#boltzmann" class="toc-link">Boltzmann's View</a>
                <a href="#gibbs" class="toc-link">Gibbs Entropy</a>
                <a href="#examples" class="toc-link">Detailed Examples</a>
            </nav>
        </aside>
    </div>
    

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-brand">
                    <span class="logo-symbol">‚àá</span>
                    <span>ML Fundamentals</span>
                </div>
                <p class="footer-tagline">Deep understanding through first principles.</p>
            </div>
            <div class="footer-links">
                <a href="../../../index.html">Home</a>
                <a href="https://github.com/ml-entropy/ml-entropy.github.io" target="_blank">GitHub</a>
            </div>
        </div>
    </footer>

    <!-- Scripts -->
    <script src="../../../js/main.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // KaTeX Rendering
            if (typeof renderMathInElement === 'function') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }

            // Tab Switching Logic
            const tabs = document.querySelectorAll('.tutorial-tab');
            const articles = document.querySelectorAll('.article-content');

            function switchTab(targetId) {
                if (!targetId || targetId === '#') targetId = '#theory';
                
                // Update tab classes
                tabs.forEach(tab => {
                    if (tab.getAttribute('href') === targetId) {
                        tab.classList.add('active');
                    } else {
                        tab.classList.remove('active');
                    }
                });

                // Show/Hide articles
                articles.forEach(article => {
                    const articleId = '#' + article.id;
                    if (articleId === targetId) {
                        article.style.display = 'block';
                    } else {
                        article.style.display = 'none';
                    }
                });
                
                // Update TOC visibility based on tab (optional, but good for UX)
                const toc = document.querySelector('.toc-container');
                if (toc) {
                    if (targetId === '#theory') {
                        toc.style.display = 'block';
                        setTimeout(() => toc.classList.add('visible'), 100);
                    } else {
                        toc.classList.remove('visible');
                        setTimeout(() => toc.style.display = 'none', 300);
                    }
                }
            }

            // Event Listeners
            tabs.forEach(tab => {
                tab.addEventListener('click', (e) => {
                    e.preventDefault();
                    const targetId = tab.getAttribute('href');
                    history.pushState(null, null, targetId);
                    switchTab(targetId);
                });
            });
            
            // Handle browser back/forward buttons
            window.addEventListener('popstate', () => {
                switchTab(window.location.hash);
            });

            // Initial Load
            switchTab(window.location.hash);
        });
    </script>
</body>
</html>