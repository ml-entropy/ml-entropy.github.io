<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Thermodynamic Entropy | Physics</title>
    <meta name="description" content="Understanding thermodynamic entropy - the physical quantity that inspired information theory.">

    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&family=Playfair+Display:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}, {left: '\\\\[', right: '\\\\]', display: true}, {left: '\\\\(', right: '\\\\)', display: false}], throwOnError: false});"></script>

    <!-- Styles -->
    <link rel="stylesheet" href="../../../css/main.css">
    <link rel="stylesheet" href="../../../css/components.css">
    <link rel="stylesheet" href="../../../css/sidebar.css">

    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>∞</text></svg>">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar" id="navbar">
        <div class="nav-container">
            <a href="../../../index.html" class="nav-logo">
                <span class="logo-symbol">∇</span>
                <span class="logo-text">ML Fundamentals</span>
            </a>

            <button class="nav-toggle" id="navToggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>

            <div class="nav-menu" id="navMenu">
                <div class="nav-links">
                    <a href="../../../tutorials/ml/index.html" class="nav-link">Machine Learning</a>
                    <a href="../../../tutorials/linear-algebra/index.html" class="nav-link">Linear Algebra</a>
                    <a href="../../../tutorials/calculus/index.html" class="nav-link">Calculus</a>
                    <a href="../../../tutorials/physics/index.html" class="nav-link active">Physics</a>
                    <a href="../../../index.html#philosophy" class="nav-link">Philosophy</a>
                    <a href="../../../index.html#roadmap" class="nav-link">Roadmap</a>
                    <a href="https://github.com/ml-entropy/ml-entropy.github.io" class="nav-link" target="_blank">GitHub</a>
                </div>

                <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
                    <svg class="sun-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="5"/>
                        <path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"/>
                    </svg>
                    <svg class="moon-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                    </svg>
                </button>
            </div>
        </div>
    </nav>

    <!-- Tutorial Header -->
    <header class="tutorial-content-header">
        <div class="container">
            <nav class="breadcrumb">
                <a href="../../../index.html">Home</a>
                <span class="breadcrumb-separator">&rarr;</span>
                <a href="../index.html">Physics</a>
                <span class="breadcrumb-separator">&rarr;</span>
                <span>Thermodynamic Entropy</span>
            </nav>

            <div class="tutorial-tabs">
                <a href="#theory" class="tutorial-tab active">Theory</a>
                <a href="#code" class="tutorial-tab">Code</a>
                <a href="#exercises" class="tutorial-tab">Exercises</a>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <div class="tutorial-wrapper">

        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <aside class="tutorial-sidebar">
            <div class="sidebar-section">
                <h3 class="sidebar-section-title">Physics</h3>
                <nav class="sidebar-nav">
                        <a href="../01-entropy/index.html" class="sidebar-link active">01. Entropy</a>
                    <a href="../02-carnot/index.html" class="sidebar-link">02. Carnot Cycle</a>
                    <a href="../03-differentials/index.html" class="sidebar-link">03. Differentials</a>
                </nav>
            </div>
            
            <div class="sidebar-section" style="margin-top: 2rem;">
                <h3 class="sidebar-section-title">Related Subjects</h3>
                <nav class="sidebar-nav">
                        <a href="../../ml/index.html" class="sidebar-link">Machine Learning</a>
                    <a href="../../linear-algebra/index.html" class="sidebar-link">Linear Algebra</a>
                    <a href="../../calculus/index.html" class="sidebar-link">Calculus</a>
                </nav>
            </div>
        </aside>

        <!-- Main Article: Theory -->
        <main class="tutorial-main">

            <article class="article-content" id="theory">

                <!-- Section 1: Intuitive Introduction -->
                <h2 id="intuition">What is Entropy, Really?</h2>

                <p>
                    Before any equations, let's build intuition. Entropy answers a deceptively simple question: <strong>how many ways can something be arranged while still looking the same from the outside?</strong>
                </p>

                <p>
                    Think about your bedroom. A "messy" room isn't really about chaos &mdash; it's about counting. There is exactly <em>one</em> way for every object to be in its designated place (books on the shelf, clothes in the drawer). But there are <em>millions</em> of ways for things to be scattered around. If you toss your clothes randomly, you'll almost certainly end up with a "messy" room &mdash; not because the universe prefers mess, but because there are overwhelmingly more messy arrangements than tidy ones.
                </p>

                <p>
                    <strong>Entropy measures this overwhelm.</strong> A system naturally drifts toward states with more possible arrangements, not because of a mysterious force, but because of sheer probability. This is the deepest insight of thermodynamics: <em>what we call "the laws of physics" at the macroscopic scale are really just the laws of overwhelming probability.</em>
                </p>

                <div class="note-box">
                    <div class="box-title">The Key Insight</div>
                    <p style="margin-bottom: 0;">
                        Entropy is not a force that pushes systems toward disorder. It is a <strong>counting argument</strong>: systems evolve toward macrostates that correspond to the largest number of microstates, simply because those macrostates are overwhelmingly more probable.
                    </p>
                </div>

                <!-- Section 2: Clausius -->
                <h2 id="clausius">Clausius and Thermodynamics</h2>

                <div class="note-box">
                    <div class="box-title">Rudolf Clausius (1822&ndash;1888)</div>
                    <p>
                        Born in K&ouml;slin, Prussia (now Koszalin, Poland), the son of a Protestant pastor, <strong>Rudolf Julius Emanuel Clausius</strong> became one of the first true theoretical physicists. He held professorships at the Z&uuml;rich Polytechnikum, the University of W&uuml;rzburg, and the University of Bonn. Beyond entropy, he developed the kinetic theory of gases, introduced the concept of <strong>mean free path</strong>, and formulated the <strong>virial theorem</strong>. He is considered, alongside William Thomson (Lord Kelvin), the founder of thermodynamics as a rigorous science.
                    </p>
                </div>

                <p>
                    The concept of entropy began not with information or probability, but with <strong>steam engines</strong>. In the 1850s&ndash;60s, Clausius was trying to understand a stubborn asymmetry in nature: heat always flows from hot bodies to cold bodies, never the reverse. Energy conservation (the First Law) cannot explain this &mdash; transferring heat from cold to hot would conserve energy just as well. Something deeper was at work.
                </p>

                <p>
                    In his landmark 1850 paper <em>On the Moving Force of Heat</em>, Clausius reconciled Carnot's principle with the conservation of energy. But he needed a new quantity to capture the one-way character of heat flow. In 1865, he coined the term <strong>entropy</strong> and explained his choice of name:
                </p>

                <blockquote class="bio-quote">
                    "I prefer going to the ancient languages for the names of important scientific quantities, so that they may mean the same thing in all living tongues. I propose, therefore, to call $S$ the <em>entropy</em> of a body, after the Greek word &lsquo;transformation.&rsquo; I have designedly coined the word entropy to be similar to <em>energy</em>, for these two quantities are so analogous in their physical significance, that an analogy of denomination seemed to me helpful."
                    <footer class="quote-attribution">&mdash; Rudolf Clausius, 1865</footer>
                </blockquote>

                <div class="definition-box">
                    <div class="box-title">Thermodynamic Definition of Entropy</div>
                    <p>
                        For a reversible process occurring at temperature $T$, the infinitesimal change in entropy $dS$ is:
                    </p>
                    <div class="math-block">
                        $$dS = \frac{\delta Q_{\text{rev}}}{T}$$
                    </div>
                    <p><strong>Understanding the Variables:</strong></p>
                    <ul>
                        <li><strong>$dS$ (Entropy Change)</strong>: The tiny (infinitesimal) change in the system's entropy. Measured in Joules per Kelvin ($\text{J/K}$).</li>
                        <li><strong>$\delta Q_{\text{rev}}$ (Reversible Heat)</strong>: The tiny amount of heat energy added to the system.
                            <ul>
                                <li>The $\delta$ indicates it is a path-dependent quantity (not an exact differential) &mdash; see the detailed explanation below.</li>
                                <li>The subscript $_{\text{rev}}$ means the heat must be added <em>reversibly</em> &mdash; so slowly that the system stays in equilibrium at every instant.</li>
                            </ul>
                        </li>
                        <li><strong>$T$ (Temperature)</strong>: The absolute temperature of the system, measured in Kelvin ($\text{K}$).</li>
                    </ul>
                </div>

                <h3 id="delta-vs-d">Why $\delta$ and not $d$? &mdash; Exact vs. Inexact Differentials</h3>

                <p>
                    In everyday calculus, $d$ means a tiny change in a well-defined quantity. If you know the starting point and the ending point, you know the total change &mdash; the path you took doesn't matter. For example, if a gas goes from temperature $T_1 = 300\text{ K}$ to $T_2 = 400\text{ K}$, then the temperature change is always $dT = 100\text{ K}$, regardless of how you heated it. We call $dT$ an <strong>exact differential</strong>.
                </p>

                <p>
                    Heat ($Q$) doesn't work that way. The amount of heat transferred between two states <strong>depends on the path</strong> &mdash; the specific sequence of steps you take. That's why physicists write $\delta Q$ instead of $dQ$: the $\delta$ is a warning sign that says <em>"this quantity has no single value between two states &mdash; it depends on how you get there."</em>
                </p>

                <div class="example-box">
                    <div class="box-title">Path Dependence: A Concrete Example</div>
                    <p>
                        Imagine compressing a gas from volume $V_1$ to $V_2$. You can do this two different ways:
                    </p>
                    <ul>
                        <li><strong>Path A &mdash; Slow isothermal compression:</strong> Compress the gas very slowly while it stays in thermal contact with a heat reservoir. The gas stays at constant temperature, and a certain amount of heat $Q_A$ flows out to the reservoir.</li>
                        <li><strong>Path B &mdash; Fast adiabatic compression, then cooling:</strong> First, compress quickly with no heat exchange (the gas heats up). Then let the gas cool back to the original temperature. A different amount of heat $Q_B$ flows out during the cooling step.</li>
                    </ul>
                    <p>
                        In both paths, the gas starts and ends at the same state (same $T$, $V_2$, $P$). But $Q_A \neq Q_B$ &mdash; the heat transferred is different. That's path dependence: $Q$ is not a property of the state, it's a property of the <em>process</em>. You can't say "the gas has 500 J of heat" the way you can say "the gas has temperature 350 K."
                    </p>
                </div>

                <p>
                    Here is the summary:
                </p>
                <ul>
                    <li>$d$ (exact differential): the quantity depends only on the current state. Examples: $dT$ (temperature), $dV$ (volume), $dP$ (pressure), $dS$ (entropy).</li>
                    <li>$\delta$ (inexact differential): the quantity depends on the path taken. Examples: $\delta Q$ (heat), $\delta W$ (work).</li>
                </ul>

                <div class="note-box">
                    <div class="box-title">The Magic of $1/T$</div>
                    <p>
                        Here is the beautiful part of Clausius's definition. Heat $\delta Q$ is path-dependent and messy &mdash; it's <em>not</em> a state function. But when you divide it by $T$ during a reversible process, something remarkable happens: the result <em>becomes</em> an exact differential. That is, $\delta Q_{\text{rev}} / T = dS$ is path-independent. Entropy $S$ is a genuine property of the system's state, even though the heat used to define it is not. This is why Clausius's formula works &mdash; dividing by temperature "tames" the path dependence of heat into a well-defined state variable.
                    </p>
                    <p style="margin-bottom: 0;">
                        For the full mathematical treatment &mdash; including the exactness test, integrating factors, and a proof that $1/T$ turns $\delta Q$ into $dS$ &mdash; see <a href="../03-differentials/index.html">Tutorial 03: Exact vs. Inexact Differentials</a>.
                    </p>
                </div>

                <h3 id="reversible-heat">What Does "Reversible Heat" Actually Mean?</h3>

                <p>
                    The subscript "rev" in $\delta Q_{\text{rev}}$ requires the process to be <strong>reversible</strong> &mdash; an idealization where every step happens infinitely slowly, so the system is always infinitesimally close to perfect equilibrium. In a reversible process:
                </p>
                <ul>
                    <li>The system and its surroundings can be restored to their original states with no net change anywhere.</li>
                    <li>There is no friction, turbulence, or sudden expansion.</li>
                    <li>Heat flows between bodies whose temperatures differ by an infinitesimal amount $dT$.</li>
                </ul>
                <p>
                    No real process is truly reversible &mdash; it's an idealization, like a frictionless surface in mechanics. But it serves as the <strong>theoretical baseline</strong>. For any real (irreversible) process between the same two states, more total entropy is produced:
                </p>
                <div class="math-block">
                    $$dS \geq \frac{\delta Q}{T}$$
                </div>
                <p>
                    The equality holds only for reversible processes. For irreversible ones (every real process), the entropy increase is strictly greater than $\delta Q / T$. This is another way of stating the Second Law.
                </p>

                <h3>Why Divide by Temperature?</h3>
                <p>
                    The factor of $1/T$ is not arbitrary &mdash; it captures a profound physical truth. Consider two scenarios:
                </p>
                <ul>
                    <li>Adding $100 \text{ J}$ of heat to an ice cube at $273 \text{ K}$: $\Delta S = 100/273 \approx 0.37 \text{ J/K}$</li>
                    <li>Adding $100 \text{ J}$ of heat to a furnace at $1000 \text{ K}$: $\Delta S = 100/1000 = 0.10 \text{ J/K}$</li>
                </ul>
                <p>
                    The same energy creates <em>more</em> entropy in a cold system than in a hot one. Why? Because a cold system has its molecules in a relatively ordered, slow-moving state. Adding energy to it opens up many new possible arrangements. A hot system already has molecules zooming around in countless configurations &mdash; a bit more energy barely changes the count. This is exactly what $1/T$ encodes: <strong>the colder the system, the more "surprised" it is by new energy.</strong>
                </p>

                <div class="example-box">
                    <div class="box-title">Example: Melting Ice</div>
                    <p>
                        Calculate the entropy change when $1 \text{ kg}$ of ice melts at $0^\circ\text{C}$ ($273.15 \text{ K}$).
                    </p>
                    <p>
                        <strong>Given:</strong> Latent heat of fusion for water $L = 334 \text{ kJ/kg}$.
                    </p>
                    <p><strong>Process:</strong></p>
                    <ol>
                        <li>Heat added: $Q = m \cdot L = 1 \text{ kg} \times 334{,}000 \text{ J/kg} = 334{,}000 \text{ J}$</li>
                        <li>Temperature: $T = 273.15 \text{ K}$ (constant during phase change)</li>
                        <li>Entropy change:
                        $$\Delta S = \frac{Q}{T} = \frac{334{,}000 \text{ J}}{273.15 \text{ K}} \approx 1{,}223 \text{ J/K}$$</li>
                    </ol>
                    <p>
                        <strong>Physical meaning:</strong> In ice, water molecules are locked into a rigid crystal lattice &mdash; each molecule vibrates in place. When the ice melts, molecules can slide past each other, rotate freely, and explore vastly more spatial arrangements. The large entropy increase ($1{,}223 \text{ J/K}$) quantifies this enormous expansion in the number of accessible microstates.
                    </p>
                </div>

                <p>
                    With this definition in hand, Clausius made a breathtaking leap &mdash; from steam engines to the fate of the entire cosmos. He summarized all of thermodynamics in two sentences that Josiah Willard Gibbs would later quote, in the original German, as the epigraph to his own masterwork:
                </p>

                <blockquote class="bio-quote">
                    <p style="font-style: italic; margin-bottom: 0.5rem;"><em>"Die Energie der Welt ist constant. Die Entropie der Welt strebt einem Maximum zu."</em></p>
                    <p style="margin-bottom: 0;"><strong>"The energy of the universe is constant. The entropy of the universe tends to a maximum."</strong></p>
                    <footer class="quote-attribution">&mdash; Rudolf Clausius, <em>The Mechanical Theory of Heat</em>, 1865</footer>
                </blockquote>

                <p>
                    He also gave the most concise statement of the Second Law:
                </p>

                <blockquote class="bio-quote">
                    "Heat can never pass from a colder to a warmer body without some other change, connected therewith, occurring at the same time."
                    <footer class="quote-attribution">&mdash; Rudolf Clausius, 1856</footer>
                </blockquote>

                <!-- Section 3: Second Law -->
                <h2 id="second-law">The Second Law and the Arrow of Time</h2>

                <p>
                    The Second Law of Thermodynamics is arguably the most profound law in all of physics. It states that for any <strong>isolated system</strong> (one that exchanges no energy or matter with its surroundings), the total entropy can never decrease:
                </p>

                <div class="math-block">
                    $$\Delta S_{\text{universe}} \geq 0$$
                </div>

                <p>
                    The equality holds only for perfectly reversible processes (an idealization). Every real process &mdash; friction, mixing, heat flow, chemical reactions &mdash; increases the total entropy of the universe.
                </p>

                <h3>Why Does This Law Exist?</h3>
                <p>
                    The Second Law is not like other laws of physics. Newton's laws, Maxwell's equations, and quantum mechanics are all <strong>time-symmetric</strong> &mdash; they work equally well whether you play the movie forward or backward. So where does the one-way nature of entropy come from?
                </p>
                <p>
                    The answer is <strong>statistics</strong>. Consider a box of gas with $10^{23}$ molecules. The number of microstates where molecules are evenly spread throughout the box is incomprehensibly larger than the number where they're all crammed into one corner. How much larger? If doubling the volume multiplies microstates by $2^N$, then for $N = 10^{23}$ molecules:
                </p>
                <div class="math-block">
                    $$\frac{\Omega_{\text{spread out}}}{\Omega_{\text{one corner}}} = 2^{10^{23}} \approx 10^{3 \times 10^{22}}$$
                </div>
                <p>
                    This number is so large that it dwarfs the number of atoms in the observable universe ($\sim 10^{80}$). The probability of all molecules spontaneously gathering in one corner is not just small &mdash; it is effectively zero. The Second Law is not a fundamental law imposed on the universe; it is the inevitable consequence of <strong>large numbers</strong>.
                </p>

                <div class="warning-box">
                    <div class="box-title">The Arrow of Time</div>
                    <p style="margin-bottom: 0;">
                        Entropy provides the <strong>Arrow of Time</strong> &mdash; the reason we perceive a clear distinction between past and future.
                        <ul>
                            <li><strong>Past:</strong> Lower entropy (ordered state &mdash; an egg on a table).</li>
                            <li><strong>Future:</strong> Higher entropy (disordered state &mdash; a broken egg on the floor).</li>
                        </ul>
                        You never see a broken egg reassemble itself, not because it violates energy conservation, but because the number of microstates corresponding to "broken egg" is astronomically larger than "intact egg." Time flows in the direction of increasing entropy.
                    </p>
                </div>

                <!-- Section 4: Boltzmann -->
                <h2 id="boltzmann">Boltzmann's Statistical View</h2>

                <div class="note-box">
                    <div class="box-title">Ludwig Boltzmann (1844&ndash;1906)</div>
                    <p>
                        Born in Vienna, <strong>Ludwig Eduard Boltzmann</strong> became a full professor of mathematical physics at the University of Graz at the remarkable age of 25. He also held positions at Vienna, Munich, and Leipzig. Boltzmann was a passionate, mercurial personality who jestingly attributed his rapid mood swings to the fact that he was born during the night between Mardi Gras and Ash Wednesday.
                    </p>
                    <p>
                        Boltzmann was the great champion of the <strong>atomic hypothesis</strong> &mdash; the idea that matter is composed of discrete atoms &mdash; at a time when this was bitterly contested. His principal opponents were <strong>Ernst Mach</strong>, who rejected any entity that could not be directly observed, and <strong>Wilhelm Ostwald</strong>, an "energeticist" who insisted that energy, not atoms, was the fundamental reality. When Mach was appointed to a prestigious chair at Vienna, Boltzmann moved to Leipzig in 1900 to escape him &mdash; only to find himself as a colleague of Ostwald.
                    </p>
                    <p style="margin-bottom: 0;">
                        The irony of Boltzmann's story is devastating. On 5 September 1906, while on holiday with his family at the Bay of Duino near Trieste, Boltzmann took his own life. Within just a few years, Jean Perrin's experiments (1908&ndash;09), building on Einstein's 1905 theoretical work, definitively confirmed the existence of atoms and validated everything Boltzmann had fought for.
                    </p>
                </div>

                <blockquote class="bio-quote">
                    "If you are out to describe the truth, leave elegance to the tailor."
                    <footer class="quote-attribution">&mdash; Ludwig Boltzmann (cited by Einstein in the preface to <em>Relativity</em>, 1916)</footer>
                </blockquote>

                <p>
                    While Clausius described <em>what</em> entropy does, Boltzmann explained <em>what it is</em>. He realized that macroscopic properties like temperature and pressure are averages over the chaotic motion of billions of atoms. His key idea: <strong>entropy is a measure of how many microscopic arrangements are compatible with what we observe macroscopically.</strong>
                </p>

                <div class="definition-box">
                    <div class="box-title">Boltzmann Entropy Formula</div>
                    <p>
                        Boltzmann connected entropy to the number of possible microscopic configurations:
                    </p>
                    <div class="math-block">
                        $$S = k_B \ln \Omega$$
                    </div>
                    <p><strong>Understanding the Variables:</strong></p>
                    <ul>
                        <li><strong>$S$ (Entropy)</strong>: The macroscopic entropy of the system.</li>
                        <li><strong>$k_B$ (Boltzmann Constant)</strong>: A fundamental constant, $k_B \approx 1.381 \times 10^{-23} \text{ J/K}$.
                            <ul>
                                <li>It acts as a bridge, converting the dimensionless count of states into physical units of energy per temperature ($\text{J/K}$).</li>
                            </ul>
                        </li>
                        <li><strong>$\ln$ (Natural Logarithm)</strong>: The logarithm base $e$.
                            <ul>
                                <li><strong>Why logarithm?</strong> Because when you combine two independent systems, their microstate counts <em>multiply</em> ($\Omega_{A+B} = \Omega_A \times \Omega_B$), but we want entropy to <em>add</em> ($S_{A+B} = S_A + S_B$). The logarithm is the unique function that converts multiplication into addition: $\ln(AB) = \ln A + \ln B$.</li>
                            </ul>
                        </li>
                        <li><strong>$\Omega$ (Multiplicity)</strong>: The number of distinct <strong>microstates</strong> consistent with the observed <strong>macrostate</strong>.
                            <ul>
                                <li><strong>Microstate:</strong> The exact position and velocity of every single particle.</li>
                                <li><strong>Macrostate:</strong> What we actually measure (e.g., "temperature is 300 K, pressure is 1 atm").</li>
                            </ul>
                        </li>
                    </ul>
                </div>

                <div class="note-box">
                    <div class="box-title">Boltzmann's Tombstone</div>
                    <p>
                        The formula $S = k \log W$ is engraved on Boltzmann's tombstone at the Zentralfriedhof (Central Cemetery) in Vienna. A remarkable historical detail: <strong>Boltzmann himself never wrote down this exact formula</strong> &mdash; it was later formalized by Max Planck, who named the constant $k$ (now $k_B$) in Boltzmann's honour.
                    </p>
                    <p style="margin-bottom: 0;">
                        David Goodstein's textbook <em>States of Matter</em> (1975) opens with this darkly famous passage: "Ludwig Boltzmann, who spent much of his life studying statistical mechanics, died in 1906, by his own hand. Paul Ehrenfest, carrying on the work, died similarly in 1933. Now it is our turn to study statistical mechanics. Perhaps it will be wise to approach the subject cautiously."
                    </p>
                </div>

                <p>
                    Boltzmann also recognized the deep connection between entropy and evolution &mdash; an insight far ahead of his time:
                </p>

                <blockquote class="bio-quote">
                    "The general struggle for existence of animate beings is not a struggle for raw materials &mdash; these, for organisms, are air, water and soil, all abundantly available &mdash; nor for energy which exists in plenty in any body in the form of heat, but a struggle for [negative] entropy, which becomes available through the transition of energy from the hot sun to the cold earth."
                    <footer class="quote-attribution">&mdash; Ludwig Boltzmann, 1886</footer>
                </blockquote>

                <h3>Microstates and Macrostates: A Concrete Example</h3>
                <p>
                    Imagine flipping 4 coins. The <strong>macrostate</strong> is the total number of Heads. The <strong>microstates</strong> are the specific sequences (HHTT, HTHT, etc.). Notice how the macrostate "2 Heads" has the most microstates &mdash; it is the most probable outcome:
                </p>

                <div class="svg-container">
                    <svg viewBox="0 0 800 350" xmlns="http://www.w3.org/2000/svg" style="font-family: 'Inter', sans-serif; background: var(--color-bg-alt); border-radius: 12px; border: 1px solid var(--color-border);">
                        <!-- Grid and Labels -->
                        <g transform="translate(50, 40)">

                            <!-- 0 Heads -->
                            <text x="0" y="20" font-weight="bold" fill="var(--color-text)">0 Heads</text>
                            <text x="0" y="40" font-size="12" fill="var(--color-text-muted)">&Omega; = 1</text>
                            <g transform="translate(100, 10)">
                                <circle cx="15" cy="15" r="10" fill="#E5E7EB" stroke="#9CA3AF"/><text x="15" y="20" text-anchor="middle" font-size="10" fill="#6B7280">T</text>
                                <circle cx="40" cy="15" r="10" fill="#E5E7EB" stroke="#9CA3AF"/><text x="40" y="20" text-anchor="middle" font-size="10" fill="#6B7280">T</text>
                                <circle cx="65" cy="15" r="10" fill="#E5E7EB" stroke="#9CA3AF"/><text x="65" y="20" text-anchor="middle" font-size="10" fill="#6B7280">T</text>
                                <circle cx="90" cy="15" r="10" fill="#E5E7EB" stroke="#9CA3AF"/><text x="90" y="20" text-anchor="middle" font-size="10" fill="#6B7280">T</text>
                            </g>

                            <!-- 1 Head -->
                            <text x="0" y="90" font-weight="bold" fill="var(--color-text)">1 Head</text>
                            <text x="0" y="110" font-size="12" fill="var(--color-text-muted)">&Omega; = 4</text>
                            <g transform="translate(100, 80)">
                                <!-- HTTT -->
                                <circle cx="15" cy="15" r="10" fill="#FCA5A5" stroke="#EF4444"/><text x="15" y="20" text-anchor="middle" font-size="10" fill="#991B1B">H</text>
                                <circle cx="40" cy="15" r="10" fill="#E5E7EB" stroke="#9CA3AF"/><text x="40" y="20" text-anchor="middle" font-size="10" fill="#6B7280">T</text>
                                <circle cx="65" cy="15" r="10" fill="#E5E7EB" stroke="#9CA3AF"/><text x="65" y="20" text-anchor="middle" font-size="10" fill="#6B7280">T</text>
                                <circle cx="90" cy="15" r="10" fill="#E5E7EB" stroke="#9CA3AF"/><text x="90" y="20" text-anchor="middle" font-size="10" fill="#6B7280">T</text>
                                <text x="120" y="20" font-size="12" fill="var(--color-text-muted)">+ 3 more variations...</text>
                            </g>

                            <!-- 2 Heads (Max Entropy) -->
                            <rect x="-10" y="140" width="720" height="70" rx="8" fill="rgba(34, 197, 94, 0.1)" stroke="rgba(34, 197, 94, 0.3)" />
                            <text x="0" y="170" font-weight="bold" fill="#15803D">2 Heads</text>
                            <text x="0" y="190" font-size="12" fill="#15803D">&Omega; = 6 (Max)</text>
                            <g transform="translate(100, 160)">
                                <!-- HHTT -->
                                <g transform="translate(0,0)">
                                    <circle cx="15" cy="15" r="10" fill="#FCA5A5" stroke="#EF4444"/><text x="15" y="20" text-anchor="middle" font-size="10" fill="#991B1B">H</text>
                                    <circle cx="40" cy="15" r="10" fill="#FCA5A5" stroke="#EF4444"/><text x="40" y="20" text-anchor="middle" font-size="10" fill="#991B1B">H</text>
                                    <circle cx="65" cy="15" r="10" fill="#E5E7EB" stroke="#9CA3AF"/><text x="65" y="20" text-anchor="middle" font-size="10" fill="#6B7280">T</text>
                                    <circle cx="90" cy="15" r="10" fill="#E5E7EB" stroke="#9CA3AF"/><text x="90" y="20" text-anchor="middle" font-size="10" fill="#6B7280">T</text>
                                </g>
                                <!-- HTHT -->
                                <g transform="translate(130,0)">
                                    <circle cx="15" cy="15" r="10" fill="#FCA5A5" stroke="#EF4444"/><text x="15" y="20" text-anchor="middle" font-size="10" fill="#991B1B">H</text>
                                    <circle cx="40" cy="15" r="10" fill="#E5E7EB" stroke="#9CA3AF"/><text x="40" y="20" text-anchor="middle" font-size="10" fill="#6B7280">T</text>
                                    <circle cx="65" cy="15" r="10" fill="#FCA5A5" stroke="#EF4444"/><text x="65" y="20" text-anchor="middle" font-size="10" fill="#991B1B">H</text>
                                    <circle cx="90" cy="15" r="10" fill="#E5E7EB" stroke="#9CA3AF"/><text x="90" y="20" text-anchor="middle" font-size="10" fill="#6B7280">T</text>
                                </g>
                                <text x="260" y="20" font-size="12" fill="#15803D">+ 4 more (HTTH, THHT, THTH, TTHH)</text>
                            </g>

                            <!-- 3 Heads -->
                            <text x="0" y="250" font-weight="bold" fill="var(--color-text)">3 Heads</text>
                            <text x="0" y="270" font-size="12" fill="var(--color-text-muted)">&Omega; = 4</text>
                            <g transform="translate(100, 240)">
                                <circle cx="15" cy="15" r="10" fill="#FCA5A5" stroke="#EF4444"/><text x="15" y="20" text-anchor="middle" font-size="10" fill="#991B1B">H</text>
                                <circle cx="40" cy="15" r="10" fill="#FCA5A5" stroke="#EF4444"/><text x="40" y="20" text-anchor="middle" font-size="10" fill="#991B1B">H</text>
                                <circle cx="65" cy="15" r="10" fill="#FCA5A5" stroke="#EF4444"/><text x="65" y="20" text-anchor="middle" font-size="10" fill="#991B1B">H</text>
                                <circle cx="90" cy="15" r="10" fill="#E5E7EB" stroke="#9CA3AF"/><text x="90" y="20" text-anchor="middle" font-size="10" fill="#6B7280">T</text>
                                <text x="120" y="20" font-size="12" fill="var(--color-text-muted)">+ 3 more variations...</text>
                            </g>
                        </g>
                    </svg>
                </div>

                <p>
                    The macrostate "2 Heads" has the highest entropy because it can happen in 6 different ways ($\Omega = 6$, $S = k_B \ln 6$). The macrostate "0 Heads" has the lowest entropy ($\Omega = 1$, $S = k_B \ln 1 = 0$). With 4 coins, the difference is modest. But with $10^{23}$ molecules, the peak is so astronomically sharp that deviations from the most probable macrostate are never observed in practice.
                </p>

                <!-- Section 5: Gibbs -->
                <h2 id="gibbs">Gibbs Entropy</h2>

                <div class="note-box">
                    <div class="box-title">Josiah Willard Gibbs (1839&ndash;1903)</div>
                    <p>
                        Born in New Haven, Connecticut, <strong>Josiah Willard Gibbs</strong> lived virtually his entire life in the same city &mdash; dying in the same house in which he grew up. His father was a professor of sacred literature at Yale; Gibbs himself received what is believed to be the <strong>first doctorate of engineering</strong> conferred in the United States (1863). After studying in Paris, Berlin, and Heidelberg, he was appointed Professor of Mathematical Physics at Yale in 1871 &mdash; <em>before he had published any of his major work</em>.
                    </p>
                    <p>
                        Gibbs served <strong>without salary</strong> for his first nine years, living off his inheritance. It was only in 1880 when Johns Hopkins offered $3,000 to lure him away that Yale countered with $2,000 &mdash; which the modest Gibbs accepted, staying put. A bachelor who lived with his sister's family, Gibbs was described as friendly but withdrawn, producing almost no personal correspondence. Einstein called him <strong>"the greatest mind in American history."</strong>
                    </p>
                    <p style="margin-bottom: 0;">
                        His masterwork, <em>On the Equilibrium of Heterogeneous Substances</em> (1876&ndash;78), is considered one of the greatest scientific papers ever written. He also co-founded statistical mechanics (a term he coined) alongside Maxwell and Boltzmann, transforming physical chemistry from an empirical discipline into a rigorous deductive science. He chose Clausius's famous two-sentence summary &mdash; in the original German &mdash; as the epigraph.
                    </p>
                </div>

                <p>
                    Gibbs was famously taciturn. The most celebrated anecdote about him comes from a Yale faculty meeting debating whether to replace mathematics requirements with foreign language courses. The normally silent Gibbs rose and declared:
                </p>

                <blockquote class="bio-quote">
                    "Gentlemen, mathematics is a language."
                    <footer class="quote-attribution">&mdash; J. Willard Gibbs, Yale faculty meeting (as quoted in <em>The Yale Scientific Magazine</em>, 1927)</footer>
                </blockquote>

                <p>
                    He also articulated the guiding principle behind his approach to thermodynamics:
                </p>

                <blockquote class="bio-quote">
                    "One of the principal objects of theoretical research in my department of knowledge is to find the point of view from which the subject appears in its greatest simplicity."
                    <footer class="quote-attribution">&mdash; J. Willard Gibbs, <em>Proceedings of the American Academy of Arts and Sciences</em>, 1881</footer>
                </blockquote>

                <p>
                    That pursuit of simplicity led Gibbs to his greatest generalization. Boltzmann's formula assumes every microstate is equally likely (the "equal a priori probability" postulate). But what if some microstates are more probable than others? Gibbs generalized the formula for arbitrary probability distributions:
                </p>

                <div class="definition-box">
                    <div class="box-title">Gibbs Entropy Formula</div>
                    <div class="math-block">
                        $$S = -k_B \sum_{i} p_i \ln p_i$$
                    </div>
                    <p><strong>Understanding the Variables:</strong></p>
                    <ul>
                        <li><strong>$\sum_{i}$</strong>: Summation over all possible microstates $i$.</li>
                        <li><strong>$p_i$</strong>: The probability of the system being in microstate $i$. Must satisfy $\sum_i p_i = 1$.</li>
                        <li><strong>$-\ln p_i$</strong>: The "surprise" or "information content" of state $i$. Rare events ($p_i \ll 1$) contribute more surprise ($-\ln p_i \gg 0$); certain events ($p_i = 1$) contribute zero.</li>
                    </ul>
                    <p><strong>Special Case:</strong> If all $\Omega$ microstates are equally likely ($p_i = 1/\Omega$ for all $i$), then:</p>
                    <div class="math-block">
                        $$S = -k_B \sum_{i=1}^{\Omega} \frac{1}{\Omega} \ln \frac{1}{\Omega} = -k_B \cdot \Omega \cdot \frac{1}{\Omega} \ln \frac{1}{\Omega} = k_B \ln \Omega$$
                    </div>
                    <p>Boltzmann's formula is recovered as a special case.</p>
                </div>

                <div class="note-box">
                    <div class="box-title">Connection to Information Theory</div>
                    <p>
                        The Gibbs formula is mathematically identical to <strong>Shannon Entropy</strong> in information theory:
                        $$H = -\sum p_i \log_2 p_i$$
                        The differences are purely conventional:
                    </p>
                    <ul>
                        <li><strong>Units:</strong> Gibbs uses $k_B$ (Joules/Kelvin); Shannon uses "bits."</li>
                        <li><strong>Base:</strong> Gibbs uses $\ln$ (natural log); Shannon uses $\log_2$ (base 2).</li>
                    </ul>
                    <p>
                        This is not a coincidence. When Claude Shannon developed information theory in 1948, he explicitly modeled his formula after Gibbs' entropy. John von Neumann reportedly told Shannon: "Call it entropy &mdash; nobody really knows what entropy is, so in a debate you'll always have the advantage."
                    </p>
                    <p style="margin-bottom: 0;">
                        Gibbs himself anticipated this connection in his final book, <em>Elementary Principles in Statistical Mechanics</em> (1902): "The laws of thermodynamics, as empirically determined, express the approximate and probable behavior of systems of a great number of particles, or, more precisely, they express the laws of mechanics for such systems as they appear to beings who have not the fineness of perception to enable them to appreciate quantities of the order of magnitude of those which relate to single particles."
                    </p>
                </div>

                <!-- Section 6: Common Misconceptions -->
                <h2 id="misconceptions">Common Misconceptions</h2>

                <div class="warning-box">
                    <div class="box-title">Misconception 1: "Entropy = Disorder"</div>
                    <p style="margin-bottom: 0;">
                        This popular shorthand is misleading. A crystal forming from a supersaturated solution <em>looks</em> more ordered, yet the process increases the total entropy of the universe (the heat released to the surroundings more than compensates). Entropy is about the <strong>number of microstates</strong>, not about human notions of tidiness. A better mental model: entropy measures <em>hidden information</em> &mdash; how much you <em>don't</em> know about the microscopic details, given the macroscopic observations.
                    </p>
                </div>

                <div class="warning-box">
                    <div class="box-title">Misconception 2: "Entropy Always Increases"</div>
                    <p style="margin-bottom: 0;">
                        The Second Law says the entropy of an <strong>isolated system</strong> never decreases. But local subsystems can and do decrease in entropy all the time &mdash; your refrigerator cools food (decreasing its entropy), life builds complex organisms from simple molecules, and stars form from diffuse gas clouds. In every case, the entropy decrease in the subsystem is paid for by an even larger entropy increase elsewhere (the refrigerator heats the kitchen; metabolism produces waste heat; gravitational collapse releases radiation). The <em>total</em> entropy of the universe still increases.
                    </p>
                </div>

                <div class="warning-box">
                    <div class="box-title">Misconception 3: "Entropy Violations are Impossible"</div>
                    <p style="margin-bottom: 0;">
                        The Second Law is statistical, not absolute. Tiny fluctuations where entropy momentarily decreases do happen and have been observed in nanoscale systems (the <strong>Fluctuation Theorem</strong> quantifies these). However, for macroscopic systems ($N \sim 10^{23}$), the probability of a noticeable entropy decrease is so vanishingly small ($\sim 10^{-10^{20}}$) that it will never be observed in the lifetime of the universe.
                    </p>
                </div>

                <!-- Section 7: Detailed Examples -->
                <h2 id="examples">Worked Examples</h2>

                <h3>1. Heat Transfer Between Two Objects</h3>
                <p>
                    A hot block ($T_H = 400 \text{ K}$) touches a cold block ($T_C = 200 \text{ K}$). A small amount of heat $Q = 100 \text{ J}$ flows from hot to cold.
                </p>
                <p><strong>Step-by-Step Calculation:</strong></p>
                <ol>
                    <li>
                        <strong>Entropy change of hot block</strong> (it loses heat):
                        $$\Delta S_H = \frac{-Q}{T_H} = \frac{-100}{400} = -0.25 \text{ J/K}$$
                    </li>
                    <li>
                        <strong>Entropy change of cold block</strong> (it gains heat):
                        $$\Delta S_C = \frac{+Q}{T_C} = \frac{+100}{200} = +0.50 \text{ J/K}$$
                    </li>
                    <li>
                        <strong>Total entropy change:</strong>
                        $$\Delta S_{\text{total}} = \Delta S_H + \Delta S_C = -0.25 + 0.50 = \mathbf{+0.25 \text{ J/K}}$$
                    </li>
                </ol>
                <p>
                    Because $\Delta S_{\text{total}} > 0$, this process is spontaneous and irreversible.
                </p>
                <p>
                    <strong>Key insight:</strong> Why can't heat flow the other way? If $100 \text{ J}$ flowed from cold to hot, we'd get $\Delta S = +0.25 - 0.50 = -0.25 \text{ J/K} < 0$, violating the Second Law. The asymmetry in $1/T$ is what makes heat flow irreversible: the cold body gains more entropy per joule than the hot body loses.
                </p>

                <h3>2. Free Expansion of a Gas</h3>
                <p>
                    A gas with $N$ molecules is confined to volume $V$ by a partition. The partition is removed and the gas expands freely into volume $2V$ (no work done, no heat exchanged, constant temperature).
                </p>
                <p>
                    <strong>Using Boltzmann's Formula:</strong> Each molecule now has twice as many spatial positions available. The number of microstates $\Omega$ increases by a factor of $2^N$:
                </p>
                <div class="math-block">
                    $$\begin{aligned}
                    \Delta S &= k_B \ln(\Omega_{\text{final}}) - k_B \ln(\Omega_{\text{initial}}) \\
                             &= k_B \ln\left(\frac{\Omega_{\text{final}}}{\Omega_{\text{initial}}}\right) \\
                             &= k_B \ln(2^N) \\
                             &= N k_B \ln 2
                    \end{aligned}$$
                </div>
                <p>
                    For 1 mole of gas ($N = N_A \approx 6.022 \times 10^{23}$):
                    $$\Delta S = N_A k_B \ln 2 = R \ln 2 \approx 5.76 \text{ J/(mol·K)}$$
                    where $R = N_A k_B \approx 8.314 \text{ J/(mol·K)}$ is the ideal gas constant.
                </p>
                <p>
                    <strong>Why is this irreversible?</strong> The gas will never spontaneously compress itself back into half the volume. The ratio of microstates is $2^{N_A} \approx 10^{1.8 \times 10^{23}}$ &mdash; the expanded state is overwhelmingly more probable.
                </p>

                <h3>3. Entropy of Heating Water</h3>
                <p>
                    Calculate the entropy change when $1 \text{ kg}$ of water is heated from $20^\circ\text{C}$ ($293 \text{ K}$) to $80^\circ\text{C}$ ($353 \text{ K}$) at constant pressure.
                </p>
                <p>
                    <strong>Given:</strong> Specific heat capacity of water $c = 4{,}186 \text{ J/(kg·K)}$.
                </p>
                <p>
                    Since temperature changes continuously, we must integrate:
                </p>
                <div class="math-block">
                    $$\Delta S = \int_{T_1}^{T_2} \frac{mc \, dT}{T} = mc \ln\frac{T_2}{T_1} = 1 \times 4{,}186 \times \ln\frac{353}{293} \approx 786 \text{ J/K}$$
                </div>
                <p>
                    Note: we cannot simply use $\Delta S = Q/T$ here because $T$ is not constant. The integral accounts for the fact that each successive increment of heat is added at a higher temperature and thus produces less entropy per joule.
                </p>

                <!-- Navigation -->
                <div class="tutorial-nav">
                    <a href="../index.html" class="tutorial-nav-link prev">
                        <span class="nav-label">Previous</span>
                        <span class="nav-title">&larr; Physics Home</span>
                    </a>
                    <a href="../02-carnot/index.html" class="tutorial-nav-link next">
                        <span class="nav-label">Next</span>
                        <span class="nav-title">The Carnot Cycle &rarr;</span>
                    </a>
                </div>

            </article>

            <!-- Code Article -->
            <article class="article-content" id="code" style="display: none;">
                <h2>Simulating Entropy</h2>
                <p>
                    The following Python code uses <code>matplotlib</code> to visualize how entropy behaves in simple systems like coin tosses and gas expansion. You can run this code in a Jupyter Notebook.
                </p>

                <h3>1. Entropy of a Coin Toss</h3>
                <p>
                    This script counts the microstates for $N$ coins and calculates the Boltzmann entropy. It uses <code>scipy.special.comb</code> for efficient factorial calculations.
                </p>

                <div class="code-block">
                    <div class="code-block-header">
                        <span class="code-block-lang">python</span>
                        <button class="code-block-copy">Copy</button>
                    </div>
                    <pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from scipy.special import comb

def entropy_boltzmann(W):
    """
    Calculate entropy S = k_B * ln(W).
    We use units where k_B = 1 for simplicity.
    """
    if W <= 0:
        return 0
    return np.log(W)

# Parameters
n_coins = 100
heads = np.arange(0, n_coins + 1)

# Calculate Microstates (W) for each macrostate (number of heads)
# W = n! / (k! * (n-k)!)
microstates = [comb(n_coins, h, exact=True) for h in heads]

# Calculate Entropy
entropies = [entropy_boltzmann(W) for W in microstates]

# Plot results
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Plot 1: Microstates
axes[0].plot(heads, microstates, 'b-', linewidth=2)
axes[0].fill_between(heads, microstates, alpha=0.3)
axes[0].set_xlabel('Number of Heads')
axes[0].set_ylabel('Number of Microstates W')
axes[0].set_title(f'Microstates vs Macrostate (N={n_coins})')
axes[0].axvline(x=n_coins/2, color='red', linestyle='--', label='Max W')
axes[0].legend()

# Plot 2: Entropy
axes[1].plot(heads, entropies, 'g-', linewidth=2)
axes[1].fill_between(heads, entropies, alpha=0.3, color='green')
axes[1].set_xlabel('Number of Heads')
axes[1].set_ylabel('Entropy S = ln(W)')
axes[1].set_title(f'Entropy vs Macrostate')
axes[1].axvline(x=n_coins/2, color='red', linestyle='--', label='Max S')
axes[1].legend()

plt.tight_layout()
plt.show()</code></pre>
                </div>

                <h3>2. Gas Expansion Simulation</h3>
                <p>
                    This simulation starts with all molecules on the left side of a box and lets them randomly move to the right. Watch how entropy increases until it fluctuates around a maximum!
                </p>

                <div class="code-block">
                    <div class="code-block-header">
                        <span class="code-block-lang">python</span>
                        <button class="code-block-copy">Copy</button>
                    </div>
                    <pre><code class="language-python">def simulate_gas_expansion(n_molecules, n_steps):
    """
    Simulate gas molecules in a 2D box.
    Start: all on left half.
    Each step: random molecule moves to random new position.
    """
    # Positions: True = left, False = right
    positions = np.ones(n_molecules, dtype=bool)  # All start on left

    history = {'step': [], 'entropy': []}

    for step in range(n_steps):
        # Pick a random molecule, move to random half
        mol_idx = np.random.randint(n_molecules)
        positions[mol_idx] = np.random.random() < 0.5

        # Calculate entropy based on multiplicity
        n_left = positions.sum()
        W = comb(n_molecules, n_left, exact=True)
        S = np.log(W) if W > 0 else 0

        history['step'].append(step)
        history['entropy'].append(S)

    return history

# Run simulation
n_molecules = 100
history = simulate_gas_expansion(n_molecules, n_steps=500)

# Plot
plt.figure(figsize=(10, 5))
plt.plot(history['step'], history['entropy'], 'g-', alpha=0.8)
max_entropy = n_molecules * np.log(2)
plt.axhline(y=max_entropy, color='red', linestyle='--', label=f'Max S = {max_entropy:.1f}')
plt.xlabel('Time Steps')
plt.ylabel('Entropy')
plt.title('Entropy Increasing with Time')
plt.legend()
plt.show()</code></pre>
                </div>

            </article>

            <!-- Exercises Article -->
            <article class="article-content" id="exercises" style="display: none;">
                <h2>Exercises</h2>
                <p>Test your understanding of entropy, from basic counting to deep derivations. Each exercise has a full solution.</p>

                <div class="exercise-list">

                    <!-- Easy -->
                    <h3 style="margin-top: 1rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">Easy</h3>

                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">1. Microstate Counting</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>A box has 4 distinct particles (A, B, C, D) that can be in the left (L) or right (R) half.</p>
                            <ol type="a">
                                <li>List all possible microstates (e.g., LLLL, LLLR...).</li>
                                <li>How many microstates have exactly 2 particles on each side?</li>
                                <li>Which macrostate (number of particles on left) has the highest multiplicity?</li>
                                <li>Calculate the Boltzmann entropy (in units of $k_B$) for each macrostate.</li>
                            </ol>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>a) Total microstates ($2^4 = 16$):</strong></p>
                                <p>
                                0 on left: RRRR (1)<br>
                                1 on left: LRRR, RLRR, RRLR, RRRL (4)<br>
                                2 on left: LLRR, LRLR, LRRL, RLLR, RLRL, RRLL (6)<br>
                                3 on left: LLLR, LLRL, LRLL, RLLL (4)<br>
                                4 on left: LLLL (1)
                                </p>
                                <p><strong>b)</strong> There are 6 microstates with 2 on each side: $\binom{4}{2} = 6$.</p>
                                <p><strong>c)</strong> The macrostate with 2 on left / 2 on right has the highest multiplicity ($\Omega = 6$). The extremes (4-0 or 0-4) have the lowest ($\Omega = 1$).</p>
                                <p><strong>d)</strong> Entropy for each macrostate:
                                <ul>
                                    <li>$S(0L) = k_B \ln 1 = 0$</li>
                                    <li>$S(1L) = k_B \ln 4 \approx 1.386\, k_B$</li>
                                    <li>$S(2L) = k_B \ln 6 \approx 1.792\, k_B$ (maximum)</li>
                                    <li>$S(3L) = k_B \ln 4 \approx 1.386\, k_B$</li>
                                    <li>$S(4L) = k_B \ln 1 = 0$</li>
                                </ul>
                                </p>
                            </div>
                        </div>
                    </div>

                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">2. Entropy of Boiling Water</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Calculate the entropy change when $0.5 \text{ kg}$ of water at $100^\circ\text{C}$ ($373.15 \text{ K}$) is completely converted to steam. The latent heat of vaporization is $L_v = 2{,}260 \text{ kJ/kg}$.</p>
                            <p>Compare this to the entropy change of melting the same mass of ice ($L_f = 334 \text{ kJ/kg}$ at $273.15 \text{ K}$). Which process creates more entropy, and why does that make physical sense?</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Boiling:</strong></p>
                                <p>$Q = mL_v = 0.5 \times 2{,}260{,}000 = 1{,}130{,}000 \text{ J}$</p>
                                <p>$\Delta S_{\text{boil}} = Q/T = 1{,}130{,}000 / 373.15 \approx 3{,}028 \text{ J/K}$</p>
                                <p><strong>Melting:</strong></p>
                                <p>$Q = mL_f = 0.5 \times 334{,}000 = 167{,}000 \text{ J}$</p>
                                <p>$\Delta S_{\text{melt}} = 167{,}000 / 273.15 \approx 611 \text{ J/K}$</p>
                                <p><strong>Comparison:</strong> Boiling produces about 5 times more entropy than melting. This makes physical sense: when water boils, molecules escape from the liquid surface and fly freely through space as gas. The volume accessible to each molecule increases by roughly a factor of 1,000. This is a far more dramatic expansion of available microstates than melting, where molecules merely transition from a rigid crystal lattice to a still-dense liquid where they remain in close contact.</p>
                            </div>
                        </div>
                    </div>

                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">3. Spontaneity from Entropy</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>A copper block at $500 \text{ K}$ is placed in contact with an iron block at $300 \text{ K}$. They exchange $200 \text{ J}$ of heat.</p>
                            <ol type="a">
                                <li>Calculate the total entropy change of the universe.</li>
                                <li>Is this process spontaneous?</li>
                                <li>What would the total entropy change be if heat flowed from cold to hot instead? Why doesn't this happen?</li>
                            </ol>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>a)</strong> Heat flows from hot (copper) to cold (iron):</p>
                                <p>$\Delta S_{\text{Cu}} = -200/500 = -0.40 \text{ J/K}$ (loses heat)</p>
                                <p>$\Delta S_{\text{Fe}} = +200/300 = +0.667 \text{ J/K}$ (gains heat)</p>
                                <p>$\Delta S_{\text{total}} = -0.40 + 0.667 = +0.267 \text{ J/K}$</p>
                                <p><strong>b)</strong> Yes, $\Delta S_{\text{total}} > 0$, so the process is spontaneous and irreversible.</p>
                                <p><strong>c)</strong> If heat flowed from cold to hot: $\Delta S = +200/500 - 200/300 = +0.40 - 0.667 = -0.267 \text{ J/K} < 0$. This would violate the Second Law. The cold body would lose more entropy per joule than the hot body gains, causing a net decrease in the universe's entropy. Nature forbids this because the reverse process corresponds to an overwhelmingly improbable microscopic fluctuation.</p>
                            </div>
                        </div>
                    </div>

                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">4. Clausius Inequality (Irreversible Processes)</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Clausius showed that for any cyclic process, the following inequality holds:</p>
                            <p>$$\oint \frac{\delta Q}{T} \leq 0$$</p>
                            <p>where equality holds only for reversible processes. This is the <strong>Clausius Inequality</strong>.</p>
                            <ol type="a">
                                <li>A heat engine absorbs $Q_H = 500 \text{ J}$ from a hot reservoir at $T_H = 600 \text{ K}$ and rejects $Q_C = 400 \text{ J}$ to a cold reservoir at $T_C = 300 \text{ K}$. Evaluate $\oint \delta Q / T$ for this cycle. Is the engine reversible?</li>
                                <li>What value of $Q_C$ would make this engine reversible (a Carnot engine)?</li>
                                <li>Using Clausius's own words &mdash; "Heat can never pass from a colder to a warmer body without some other change" &mdash; explain in your own words why the inequality must be $\leq 0$ and not $\geq 0$.</li>
                            </ol>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>a)</strong> For the cycle, heat is absorbed at $T_H$ and rejected at $T_C$:</p>
                                <p>$$\oint \frac{\delta Q}{T} = \frac{Q_H}{T_H} - \frac{Q_C}{T_C} = \frac{500}{600} - \frac{400}{300} = 0.833 - 1.333 = -0.500 \text{ J/K}$$</p>
                                <p>Since $-0.500 < 0$, the inequality is satisfied with strict inequality, confirming this is an <strong>irreversible</strong> engine.</p>
                                <p><strong>b)</strong> For a reversible (Carnot) engine, $\oint \delta Q/T = 0$:</p>
                                <p>$$\frac{Q_H}{T_H} = \frac{Q_C}{T_C} \implies Q_C = Q_H \frac{T_C}{T_H} = 500 \times \frac{300}{600} = 250 \text{ J}$$</p>
                                <p>A Carnot engine would reject only $250 \text{ J}$ (doing $250 \text{ J}$ of work), compared to the $400 \text{ J}$ rejected by this irreversible engine (which does only $100 \text{ J}$ of work).</p>
                                <p><strong>c)</strong> If $\oint \delta Q/T$ were positive, it would mean the engine creates more entropy when absorbing heat than it destroys when rejecting heat &mdash; implying net entropy could be destroyed in the surroundings, which would allow heat to spontaneously flow from cold to hot. Clausius's principle forbids exactly this: any real process must produce at least as much entropy in the cold reservoir as it removes from the hot reservoir, so the integral must be non-positive.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Medium -->
                    <h3 style="margin-top: 2rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">Medium</h3>

                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">5. Boltzmann Entropy: Real Gas System</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Boltzmann wrote: "The general struggle for existence of animate beings is... a struggle for [negative] entropy." To understand this quantitatively, consider the following.</p>
                            <p>A container is divided into 10 equal cells. 6 indistinguishable gas molecules are distributed among the cells (at most one per cell, for simplicity).</p>
                            <ol type="a">
                                <li>Calculate the total number of microstates $\Omega$ using the binomial coefficient $\binom{10}{6}$.</li>
                                <li>Calculate the Boltzmann entropy $S = k_B \ln \Omega$.</li>
                                <li>Now suppose the gas is compressed into 7 cells (same 6 molecules). Calculate the new $\Omega$ and $S$.</li>
                                <li>What is $\Delta S$? Does compression increase or decrease entropy? Relate this to Boltzmann's insight about the "struggle for entropy."</li>
                            </ol>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>a)</strong> $\Omega_{10} = \binom{10}{6} = \frac{10!}{6! \cdot 4!} = 210$</p>
                                <p><strong>b)</strong> $S_{10} = k_B \ln 210 \approx 5.347\, k_B$</p>
                                <p><strong>c)</strong> $\Omega_7 = \binom{7}{6} = 7$, so $S_7 = k_B \ln 7 \approx 1.946\, k_B$</p>
                                <p><strong>d)</strong> $\Delta S = S_7 - S_{10} = (1.946 - 5.347)\, k_B = -3.401\, k_B$</p>
                                <p>Compression <em>decreases</em> entropy because fewer spatial arrangements are available. This is why compression requires work &mdash; you must pay an entropy cost. Boltzmann's insight was that living organisms maintain their low-entropy (highly organized) state by continuously extracting "negative entropy" from their environment &mdash; consuming low-entropy food and sunlight and expelling high-entropy waste heat. The struggle for existence is fundamentally a struggle to keep $\Omega$ small locally while dumping entropy into the surroundings.</p>
                            </div>
                        </div>
                    </div>

                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">6. Entropy of Heating at Variable Temperature</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>$2 \text{ kg}$ of aluminum ($c = 900 \text{ J/(kg·K)}$) is heated from $300 \text{ K}$ to $600 \text{ K}$.</p>
                            <ol type="a">
                                <li>Calculate the exact entropy change using integration.</li>
                                <li>A student approximates this by using the average temperature $T_{\text{avg}} = 450 \text{ K}$ and computing $\Delta S \approx Q/T_{\text{avg}}$. Calculate this approximation and the percent error.</li>
                                <li>Why does the approximation overestimate or underestimate? (Hint: think about the concavity of $\ln T$.)</li>
                            </ol>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>a) Exact:</strong></p>
                                <p>$$\Delta S = mc \ln\frac{T_2}{T_1} = 2 \times 900 \times \ln\frac{600}{300} = 1800 \ln 2 \approx 1247.7 \text{ J/K}$$</p>
                                <p><strong>b) Approximation:</strong></p>
                                <p>$Q = mc\Delta T = 2 \times 900 \times 300 = 540{,}000 \text{ J}$</p>
                                <p>$\Delta S_{\text{approx}} = Q/T_{\text{avg}} = 540{,}000/450 = 1200.0 \text{ J/K}$</p>
                                <p>Percent error: $(1247.7 - 1200)/1247.7 \approx 3.8\%$ underestimate.</p>
                                <p><strong>c)</strong> The approximation <em>underestimates</em> the entropy change. The function $1/T$ is convex (curves upward), so averaging $T$ in the denominator gives a value smaller than the integral $\int dT/T$. Physically, the heat added at the lower temperatures (near $300 \text{ K}$) produces disproportionately more entropy per joule than the heat added at higher temperatures (near $600 \text{ K}$), and the simple average misses this asymmetry.</p>
                            </div>
                        </div>
                    </div>

                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">7. Deriving the Boltzmann Formula</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Start with the following assumptions:</p>
                            <ol>
                                <li>Entropy $S$ depends only on the number of microstates $\Omega$: $S = f(\Omega)$.</li>
                                <li>Entropy is additive: For two independent systems A and B, $S_{\text{total}} = S_A + S_B$.</li>
                                <li>Multiplicity is multiplicative: $\Omega_{\text{total}} = \Omega_A \times \Omega_B$.</li>
                            </ol>
                            <p>Show that the function $f(\Omega)$ must be a logarithm.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p>We need a function $f$ such that:</p>
                                $$f(\Omega_A \cdot \Omega_B) = f(\Omega_A) + f(\Omega_B)$$
                                <p>This is Cauchy's functional equation for the logarithm. To prove it rigorously, differentiate both sides with respect to $\Omega_A$ (treating $\Omega_B$ as constant):</p>
                                $$\Omega_B \, f'(\Omega_A \Omega_B) = f'(\Omega_A)$$
                                <p>Multiply both sides by $\Omega_A$:</p>
                                $$\Omega_A \Omega_B \, f'(\Omega_A \Omega_B) = \Omega_A \, f'(\Omega_A)$$
                                <p>Let $x = \Omega_A \Omega_B$ on the left and $x = \Omega_A$ on the right. This shows that the function $g(x) = x f'(x)$ is constant: $g(x) = C$. Therefore $f'(x) = C/x$.</p>
                                <p>Integrating: $f(x) = C \ln x + K$.</p>
                                <p>Setting $K = 0$ (so that a single microstate has zero entropy: $f(1) = 0$) and identifying $C = k_B$ gives Boltzmann's formula: $S = k_B \ln \Omega$.</p>
                            </div>
                        </div>
                    </div>

                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">8. Mixing Entropy and the Gibbs Paradox</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Two distinct ideal gases (A and B), each with $N$ molecules at the same temperature and pressure, occupy separate chambers of volume $V$ separated by a partition.</p>
                            <ol type="a">
                                <li>Calculate the entropy change when the partition is removed.</li>
                                <li>What if the gases are <em>identical</em> (both gas A)? What should the entropy change be, and why? This is known as the <strong>Gibbs Paradox</strong>.</li>
                            </ol>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>a) Distinct gases:</strong></p>
                                <p>After removal, gas A can access volume $2V$ instead of $V$. By the free expansion result:</p>
                                <p>$\Delta S_A = N k_B \ln(2V/V) = N k_B \ln 2$</p>
                                <p>Similarly: $\Delta S_B = N k_B \ln 2$</p>
                                <p>Total: $\Delta S_{\text{mix}} = 2 N k_B \ln 2$</p>
                                <p><strong>b) Identical gases:</strong></p>
                                <p>If both chambers contain the same gas at the same $T$ and $P$, removing the partition changes nothing observable &mdash; the system is in the same macroscopic state before and after. Therefore $\Delta S = 0$.</p>
                                <p>The <strong>Gibbs Paradox</strong> asks: the formula from part (a) gives $\Delta S = 2Nk_B \ln 2$ regardless of whether the gases are identical, which is clearly wrong. The resolution is that identical particles are <em>indistinguishable</em> &mdash; swapping two identical molecules does not create a new microstate. When we correct for this over-counting (dividing $\Omega$ by $N!$ for each species), the entropy of mixing identical gases correctly becomes zero. This was one of the first hints that quantum mechanics would require fundamentally indistinguishable particles.</p>
                            </div>
                        </div>
                    </div>

                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">9. Gibbs Entropy: From Simplicity to Depth</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Gibbs wrote that the laws of thermodynamics "express the laws of mechanics... as they appear to beings who have not the fineness of perception to enable them to appreciate quantities... which relate to single particles." His entropy formula $S = -k_B \sum p_i \ln p_i$ quantifies exactly this coarse-grained ignorance.</p>
                            <p>A quantum system has 3 energy levels with probabilities $p_1 = 0.7$, $p_2 = 0.2$, $p_3 = 0.1$.</p>
                            <ol type="a">
                                <li>Calculate the Gibbs entropy $S = -k_B \sum p_i \ln p_i$.</li>
                                <li>What would the entropy be if all three states were equally likely?</li>
                                <li>Which case has higher entropy, and why?</li>
                                <li>What is the <em>minimum</em> possible entropy for a 3-state system?</li>
                            </ol>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>a)</strong></p>
                                <p>$S = -k_B [0.7 \ln 0.7 + 0.2 \ln 0.2 + 0.1 \ln 0.1]$</p>
                                <p>$= -k_B [0.7 \times (-0.357) + 0.2 \times (-1.609) + 0.1 \times (-2.303)]$</p>
                                <p>$= -k_B [-0.250 - 0.322 - 0.230]$</p>
                                <p>$= -k_B \times (-0.802) = 0.802\, k_B$</p>
                                <p><strong>b)</strong> Equal probabilities: $p_i = 1/3$ for all $i$.</p>
                                <p>$S = -k_B \times 3 \times \frac{1}{3} \ln\frac{1}{3} = k_B \ln 3 \approx 1.099\, k_B$</p>
                                <p><strong>c)</strong> The uniform distribution has <em>higher</em> entropy ($1.099\, k_B$ vs $0.802\, k_B$). This is a general result: <strong>for a fixed number of states, entropy is maximized when all states are equally probable.</strong> The uniform distribution represents maximum ignorance about which state the system is in.</p>
                                <p><strong>d)</strong> Minimum entropy occurs when one state has probability 1 and the others have probability 0: $S = -k_B [1 \cdot \ln 1] = 0$. This represents complete certainty &mdash; you know exactly which state the system is in, so there is zero hidden information.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Hard -->
                    <h3 style="margin-top: 2rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">Hard</h3>

                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">10. Stirling's Approximation and Shannon Entropy</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>For very large $N$, the factorial function $N!$ is intractable. Stirling's approximation states that $\ln N! \approx N \ln N - N$.</p>
                            <p>Use this to show that for a system with $N$ particles distributed into $k$ states with counts $n_1, n_2, \ldots, n_k$, the entropy per particle approaches the Shannon entropy:</p>
                            <p>$$\frac{S}{N k_B} \approx -\sum_{i} p_i \ln p_i \quad \text{where } p_i = n_i/N$$</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p>The number of microstates is given by the multinomial coefficient:</p>
                                <p>$\Omega = \frac{N!}{n_1! \, n_2! \cdots n_k!}$</p>
                                <p>Taking the log:</p>
                                $$\ln \Omega = \ln N! - \sum_{i} \ln n_i!$$
                                <p>Applying Stirling's approximation ($\ln x! \approx x \ln x - x$) to each term:</p>
                                $$\ln \Omega \approx (N \ln N - N) - \sum_{i} (n_i \ln n_i - n_i)$$
                                <p>Since $\sum_i n_i = N$, the linear terms cancel:</p>
                                $$\ln \Omega \approx N \ln N - \sum_{i} n_i \ln n_i$$
                                <p>Rewrite by adding and subtracting $\sum n_i \ln N = N \ln N$:</p>
                                $$\ln \Omega = \sum_i n_i \ln N - \sum_i n_i \ln n_i = -\sum_i n_i \ln\frac{n_i}{N}$$
                                <p>Substituting $n_i = N p_i$:</p>
                                $$\ln \Omega = -\sum_i N p_i \ln p_i = -N \sum_i p_i \ln p_i$$
                                <p>Therefore:</p>
                                $$S = k_B \ln \Omega \approx -N k_B \sum_i p_i \ln p_i$$
                                <p>Dividing both sides by $N k_B$:</p>
                                $$\frac{S}{N k_B} = -\sum_i p_i \ln p_i$$
                                <p>This is exactly the Shannon entropy formula (with natural log). This derivation reveals the deep connection: Shannon entropy emerges naturally from counting microstates in the large-$N$ limit.</p>
                            </div>
                        </div>
                    </div>

                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">11. Maxwell's Demon</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Maxwell proposed a thought experiment: A "demon" controls a tiny door between two gas chambers. It observes each molecule approaching the door and lets fast molecules pass to the right and slow ones to the left. Over time, the right chamber heats up and the left cools down &mdash; creating a temperature difference without doing work.</p>
                            <ol type="a">
                                <li>This seems to decrease entropy ($\Delta S < 0$). Why doesn't this violate the Second Law?</li>
                                <li>Quantify: how much entropy does the demon generate per bit of information it erases? (Use Landauer's principle.)</li>
                            </ol>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>a)</strong> The resolution (due to Szilard, Brillouin, and Landauer) is that the demon must <strong>acquire, store, and eventually erase information</strong> about each molecule's speed.</p>
                                <p>The demon's memory is a physical system. To operate repeatedly, it must erase old measurements to make room for new ones. <strong>Landauer's Principle</strong> (1961) proves that erasing information is a fundamentally irreversible process: it must increase the entropy of the environment.</p>
                                <p><strong>b)</strong> Landauer's Principle states that erasing 1 bit of information at temperature $T$ dissipates at least:</p>
                                <p>$$Q_{\min} = k_B T \ln 2$$</p>
                                <p>This generates entropy:</p>
                                <p>$$\Delta S_{\text{erase}} = \frac{Q_{\min}}{T} = k_B \ln 2 \approx 9.57 \times 10^{-24} \text{ J/K per bit}$$</p>
                                <p>The entropy generated by erasing the demon's memory is always at least as large as the entropy decrease it achieves in the gas. The Second Law is saved: the total entropy of (gas + demon + environment) never decreases.</p>
                                <p>This resolution beautifully connects thermodynamics and information theory: <strong>information is physical</strong>, and processing it has unavoidable thermodynamic costs.</p>
                            </div>
                        </div>
                    </div>

                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">12. Gibbs Free Energy and Spontaneity</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Gibbs's most powerful practical contribution was the <strong>Gibbs free energy</strong> $G = H - TS$, where $H$ is enthalpy, $T$ is temperature, and $S$ is entropy. For a process at constant temperature and pressure, the change in Gibbs free energy determines spontaneity:</p>
                            <p>$$\Delta G = \Delta H - T \Delta S$$</p>
                            <p>A process is spontaneous if $\Delta G < 0$.</p>
                            <ol type="a">
                                <li>A chemical reaction has $\Delta H = -30 \text{ kJ/mol}$ (exothermic) and $\Delta S = -100 \text{ J/(mol·K)}$. At what temperature does this reaction become non-spontaneous?</li>
                                <li>Another reaction has $\Delta H = +50 \text{ kJ/mol}$ (endothermic) and $\Delta S = +200 \text{ J/(mol·K)}$. At what temperature does it become spontaneous?</li>
                                <li>Gibbs sought "the point of view from which the subject appears in its greatest simplicity." Explain why $\Delta G$ is a simpler criterion for spontaneity than separately tracking the entropy changes of the system and its surroundings.</li>
                            </ol>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>a)</strong> The reaction is spontaneous when $\Delta G < 0$:</p>
                                <p>$\Delta G = \Delta H - T\Delta S = -30{,}000 - T(-100) = -30{,}000 + 100T$</p>
                                <p>Setting $\Delta G = 0$: $T = 30{,}000/100 = 300 \text{ K}$ ($27^\circ\text{C}$)</p>
                                <p>Below $300 \text{ K}$, $\Delta G < 0$ (spontaneous). Above $300 \text{ K}$, $\Delta G > 0$ (non-spontaneous). The favorable enthalpy ($\Delta H < 0$) is eventually overcome by the unfavorable entropy ($\Delta S < 0$) as temperature rises.</p>
                                <p><strong>b)</strong> $\Delta G = 50{,}000 - 200T$. Setting $\Delta G = 0$: $T = 50{,}000/200 = 250 \text{ K}$ ($-23^\circ\text{C}$)</p>
                                <p>Above $250 \text{ K}$, the reaction is spontaneous. The unfavorable enthalpy is overcome by the favorable entropy term $T\Delta S$ at high enough temperature.</p>
                                <p><strong>c)</strong> Without Gibbs free energy, determining spontaneity requires calculating the entropy change of both the system <em>and</em> the surroundings: $\Delta S_{\text{univ}} = \Delta S_{\text{sys}} + \Delta S_{\text{surr}}$, where $\Delta S_{\text{surr}} = -\Delta H / T$. The genius of $\Delta G$ is that it packages both contributions into a single quantity involving only properties of the system: $\Delta G = \Delta H - T\Delta S = -T\Delta S_{\text{univ}}$. One number tells you everything &mdash; exactly the kind of simplification Gibbs valued.</p>
                            </div>
                        </div>
                    </div>

                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">13. Entropy Maximization and the Boltzmann Distribution</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>A system has energy levels $E_1, E_2, \ldots, E_k$ with probabilities $p_1, p_2, \ldots, p_k$. The average energy is fixed at $\langle E \rangle = \sum_i p_i E_i = U$.</p>
                            <p>Using the method of Lagrange multipliers, maximize the Gibbs entropy $S = -k_B \sum p_i \ln p_i$ subject to the constraints:</p>
                            <ol>
                                <li>$\sum_i p_i = 1$ (normalization)</li>
                                <li>$\sum_i p_i E_i = U$ (fixed average energy)</li>
                            </ol>
                            <p>Show that the entropy-maximizing distribution is the <strong>Boltzmann distribution</strong>: $p_i = \frac{e^{-\beta E_i}}{Z}$, where $Z = \sum_i e^{-\beta E_i}$ is the partition function and $\beta = 1/(k_B T)$.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p>We maximize the Lagrangian:</p>
                                $$\mathcal{L} = -k_B \sum_i p_i \ln p_i - \alpha\left(\sum_i p_i - 1\right) - \lambda\left(\sum_i p_i E_i - U\right)$$
                                <p>Taking the partial derivative with respect to $p_j$ and setting it to zero:</p>
                                $$\frac{\partial \mathcal{L}}{\partial p_j} = -k_B(\ln p_j + 1) - \alpha - \lambda E_j = 0$$
                                <p>Solving for $\ln p_j$:</p>
                                $$\ln p_j = -\frac{\alpha + k_B}{k_B} - \frac{\lambda}{k_B} E_j$$
                                <p>Let $\beta = \lambda / k_B$. Then:</p>
                                $$p_j = e^{-(\alpha + k_B)/k_B} \cdot e^{-\beta E_j} = C \cdot e^{-\beta E_j}$$
                                <p>where $C$ is a constant. Applying the normalization constraint $\sum_j p_j = 1$:</p>
                                $$C = \frac{1}{\sum_j e^{-\beta E_j}} = \frac{1}{Z}$$
                                <p>Therefore:</p>
                                $$\boxed{p_j = \frac{e^{-\beta E_j}}{Z}}$$
                                <p>The Lagrange multiplier $\beta$ is identified as $1/(k_B T)$ by comparing with the thermodynamic relation $\partial S / \partial U = 1/T$.</p>
                                <p><strong>Physical meaning:</strong> The Boltzmann distribution is not just one possible distribution &mdash; it is the <em>unique</em> distribution that maximizes entropy for a given average energy. Nature "chooses" it because it corresponds to the macrostate with the most microstates. This is a profound result: the equilibrium state of a system at fixed temperature is determined entirely by the principle of maximum entropy.</p>
                            </div>
                        </div>
                    </div>

                </div>
            </article>

        </main>

        <!-- TOC (Right Side) -->
        <aside class="toc-container">
            <h4 class="toc-title">Contents</h4>
            <nav class="toc-list">
                <a href="#intuition" class="toc-link">What is Entropy?</a>
                <a href="#clausius" class="toc-link">Clausius & Thermodynamics</a>
                <a href="#delta-vs-d" class="toc-link">$\delta$ vs $d$ Notation</a>
                <a href="#reversible-heat" class="toc-link">Reversible Heat</a>
                <a href="#second-law" class="toc-link">The Second Law</a>
                <a href="#boltzmann" class="toc-link">Boltzmann's View</a>
                <a href="#gibbs" class="toc-link">Gibbs Entropy</a>
                <a href="#misconceptions" class="toc-link">Common Misconceptions</a>
                <a href="#examples" class="toc-link">Worked Examples</a>
            </nav>
        </aside>
    </div>


    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-brand">
                    <span class="logo-symbol">&nabla;</span>
                    <span>ML Fundamentals</span>
                </div>
                <p class="footer-tagline">Deep understanding through first principles.</p>
            </div>
            <div class="footer-links">
                <a href="../../../index.html">Home</a>
                <a href="https://github.com/ml-entropy/ml-entropy.github.io" target="_blank">GitHub</a>
            </div>
        </div>
    </footer>

    <!-- Scripts -->
    <script src="../../../js/main.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // KaTeX Rendering
            if (typeof renderMathInElement === 'function') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }

            // Tab Switching Logic
            const tabs = document.querySelectorAll('.tutorial-tab');
            const articles = document.querySelectorAll('.article-content');

            function switchTab(targetId) {
                if (!targetId || targetId === '#') targetId = '#theory';

                // Update tab classes
                tabs.forEach(tab => {
                    if (tab.getAttribute('href') === targetId) {
                        tab.classList.add('active');
                    } else {
                        tab.classList.remove('active');
                    }
                });

                // Show/Hide articles
                articles.forEach(article => {
                    const articleId = '#' + article.id;
                    if (articleId === targetId) {
                        article.style.display = 'block';
                    } else {
                        article.style.display = 'none';
                    }
                });

                // Update TOC visibility based on tab
                const toc = document.querySelector('.toc-container');
                if (toc) {
                    if (targetId === '#theory') {
                        toc.style.display = 'block';
                        setTimeout(() => toc.classList.add('visible'), 100);
                    } else {
                        toc.classList.remove('visible');
                        setTimeout(() => toc.style.display = 'none', 300);
                    }
                }
            }

            // Event Listeners
            tabs.forEach(tab => {
                tab.addEventListener('click', (e) => {
                    e.preventDefault();
                    const targetId = tab.getAttribute('href');
                    history.pushState(null, null, targetId);
                    switchTab(targetId);
                });
            });

            // Handle browser back/forward buttons
            window.addEventListener('popstate', () => {
                switchTab(window.location.hash);
            });

            // Initial Load
            switchTab(window.location.hash);
        });
    </script>
</body>
</html>