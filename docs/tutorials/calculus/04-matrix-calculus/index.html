<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Matrix Calculus for ML | Calculus</title>
    <meta name="description" content="Understanding derivatives with respect to vectors and matrices - essential for understanding backpropagation.">
    
    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&family=Playfair+Display:wght@400;500;600;700&display=swap" rel="stylesheet">
    
    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}, {left: '\\\\[', right: '\\\\]', display: true}, {left: '\\\\(', right: '\\\\)', display: false}], throwOnError: false});"></script>
    
    <!-- Styles -->
    <link rel="stylesheet" href="../../../css/main.css">
    <link rel="stylesheet" href="../../../css/components.css">
    
    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>∞</text></svg>">
<link rel="stylesheet" href="../../../css/sidebar.css">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar" id="navbar">
        <div class="nav-container">
            <a href="../../../index.html" class="nav-logo">
                <span class="logo-symbol">∇</span>
                <span class="logo-text">ML Fundamentals</span>
            </a>
            
            <button class="nav-toggle" id="navToggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            
            <div class="nav-menu" id="navMenu">
                <div class="nav-links">
                    <a href="../../../tutorials/ml/index.html" class="nav-link">Machine Learning</a>
                    <a href="../../../tutorials/linear-algebra/index.html" class="nav-link">Linear Algebra</a>
                    <a href="../../../tutorials/calculus/index.html" class="nav-link active">Calculus</a>
                    <a href="../../../tutorials/physics/index.html" class="nav-link">Physics</a>
                    <a href="../../../index.html#philosophy" class="nav-link">Philosophy</a>
                    <a href="../../../index.html#roadmap" class="nav-link">Roadmap</a>
                    <a href="https://github.com/ml-entropy/ml-entropy.github.io" class="nav-link" target="_blank">GitHub</a>
                </div>
                
                <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
                    <svg class="sun-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="5"/>
                        <path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"/>
                    </svg>
                    <svg class="moon-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                    </svg>
                </button>
            </div>
        </div>
    </nav>

    <!-- Tutorial Header -->
    <header class="tutorial-content-header">
        <div class="container">
            <nav class="breadcrumb">
                <a href="../../../index.html">Home</a>
                <span class="breadcrumb-separator">→</span>
                <a href="../index.html">Calculus</a>
                <span class="breadcrumb-separator">→</span>
                <span>Matrix Calculus</span>
            </nav>
            
            
            
            
            <div class="tutorial-tabs">
                <a href="#theory" class="tutorial-tab active">Theory</a>
                <a href="#code" class="tutorial-tab">Code</a>
                <a href="#exercises" class="tutorial-tab">Exercises</a>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    
    <!-- Main Content -->
    <div class="tutorial-wrapper">
        
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <aside class="tutorial-sidebar">
            <div class="sidebar-section">
                <h3 class="sidebar-section-title">Calculus</h3>
                <nav class="sidebar-nav">
                        <a href="../01-derivatives/index.html" class="sidebar-link">01. Derivatives</a>
                    <a href="../02-multivariable/index.html" class="sidebar-link">02. Multivariable</a>
                    <a href="../03-directional/index.html" class="sidebar-link">03. Directional Derivatives</a>
                    <a href="../04-matrix-calculus/index.html" class="sidebar-link active">04. Matrix Calculus</a>
                </nav>
            </div>
            
            <div class="sidebar-section" style="margin-top: 2rem;">
                <h3 class="sidebar-section-title">Related Subjects</h3>
                <nav class="sidebar-nav">
                        <a href="../../ml/index.html" class="sidebar-link">Machine Learning</a>
                    <a href="../../linear-algebra/index.html" class="sidebar-link">Linear Algebra</a>
                    <a href="../../physics/index.html" class="sidebar-link">Physics</a>
                </nav>
            </div>
        </aside>

        <!-- Main Article -->
        <main class="tutorial-main">
            
            <article class="article-content" id="theory">
                
                <!-- Section 1 -->
                <h2 id="notation">Layout Conventions</h2>
                
                <p>
                    There are two conventions for matrix calculus. We use <strong>denominator layout</strong> 
                    (also called "Hessian layout"), which is common in ML:
                </p>
                
                <div class="definition-box">
                    <div class="box-title">Denominator Layout</div>
                    <p style="margin-bottom: 0;">
                        The gradient of a scalar $f$ with respect to a column vector $\mathbf{x}$ is a 
                        <strong>column vector</strong>:
                        $$\nabla_{\mathbf{x}} f = \frac{\partial f}{\partial \mathbf{x}} = \begin{pmatrix} \frac{\partial f}{\partial x_1} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{pmatrix}$$
                    </p>
                </div>
                
                <!-- Section 2 -->
                <h2 id="vector-derivatives">Vector-by-Vector Derivatives</h2>
                
                <p>
                    When $\mathbf{y} = f(\mathbf{x})$ is a vector function, the derivative is the 
                    <strong>Jacobian matrix</strong>:
                </p>
                
                <div class="math-block">
                    $$\frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \begin{pmatrix} 
                    \frac{\partial y_1}{\partial x_1} & \cdots & \frac{\partial y_1}{\partial x_n} \\
                    \vdots & \ddots & \vdots \\
                    \frac{\partial y_m}{\partial x_1} & \cdots & \frac{\partial y_m}{\partial x_n}
                    \end{pmatrix}$$
                </div>
                
                <p>The Jacobian is $m \times n$ for $\mathbf{y} \in \mathbb{R}^m$ and $\mathbf{x} \in \mathbb{R}^n$.</p>
                
                <!-- Section 3 -->
                <h2 id="common-results">Common Results</h2>
                
                <div class="math-derivation">
                    <div class="math-derivation-title">Essential Matrix Calculus Identities</div>
                    
                    <div class="math-step">
                        <div class="math-step-number">1</div>
                        <div class="math-step-content">
                            $\frac{\partial}{\partial \mathbf{x}}(\mathbf{a}^T \mathbf{x}) = \mathbf{a}$
                        </div>
                    </div>
                    
                    <div class="math-step">
                        <div class="math-step-number">2</div>
                        <div class="math-step-content">
                            $\frac{\partial}{\partial \mathbf{x}}(\mathbf{x}^T \mathbf{x}) = 2\mathbf{x}$
                        </div>
                    </div>
                    
                    <div class="math-step">
                        <div class="math-step-number">3</div>
                        <div class="math-step-content">
                            $\frac{\partial}{\partial \mathbf{x}}(\mathbf{x}^T A \mathbf{x}) = (A + A^T)\mathbf{x}$ (or $2A\mathbf{x}$ if $A$ symmetric)
                        </div>
                    </div>
                    
                    <div class="math-step">
                        <div class="math-step-number">4</div>
                        <div class="math-step-content">
                            $\frac{\partial}{\partial \mathbf{x}}(A\mathbf{x}) = A$ (Jacobian)
                        </div>
                    </div>
                    
                    <div class="math-step">
                        <div class="math-step-number">5</div>
                        <div class="math-step-content">
                            $\frac{\partial}{\partial X}\text{tr}(AX) = A^T$
                        </div>
                    </div>
                </div>
                
                <!-- Section 4 -->
                <h2 id="chain-rule">The Chain Rule for Vectors</h2>
                
                <p>If $\mathbf{y} = g(\mathbf{x})$ and $z = f(\mathbf{y})$, then:</p>
                
                <div class="math-block">
                    $$\frac{\partial z}{\partial \mathbf{x}} = \left(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right)^T \frac{\partial z}{\partial \mathbf{y}}$$
                </div>
                
                <p>Or equivalently:</p>
                
                <div class="math-block">
                    $$\nabla_{\mathbf{x}} z = J^T \nabla_{\mathbf{y}} z$$
                </div>
                
                <div class="warning-box">
                    <div class="box-title">This is Backpropagation!</div>
                    <p style="margin-bottom: 0;">
                        In a neural network, we compute $\nabla_{\mathbf{y}} L$ (gradient of loss w.r.t. layer output), 
                        then multiply by $J^T$ (transpose of layer's Jacobian) to get $\nabla_{\mathbf{x}} L$ 
                        (gradient w.r.t. layer input). Repeat backwards through layers.
                    </p>
                </div>
                
                <!-- Section 5 -->
                <h2 id="linear-layer">Example: Linear Layer</h2>
                
                <p>
                    A linear layer computes $\mathbf{y} = W\mathbf{x} + \mathbf{b}$. Given $\frac{\partial L}{\partial \mathbf{y}}$ 
                    from the next layer, we need:
                </p>
                
                <div class="math-derivation">
                    <div class="math-derivation-title">Gradients for Linear Layer</div>
                    
                    <div class="math-step">
                        <div class="math-step-number">1</div>
                        <div class="math-step-content">
                            <strong>Gradient w.r.t. input:</strong> $\frac{\partial L}{\partial \mathbf{x}} = W^T \frac{\partial L}{\partial \mathbf{y}}$
                        </div>
                    </div>
                    
                    <div class="math-step">
                        <div class="math-step-number">2</div>
                        <div class="math-step-content">
                            <strong>Gradient w.r.t. weights:</strong> $\frac{\partial L}{\partial W} = \frac{\partial L}{\partial \mathbf{y}} \mathbf{x}^T$
                        </div>
                    </div>
                    
                    <div class="math-step">
                        <div class="math-step-number">3</div>
                        <div class="math-step-content">
                            <strong>Gradient w.r.t. bias:</strong> $\frac{\partial L}{\partial \mathbf{b}} = \frac{\partial L}{\partial \mathbf{y}}$
                        </div>
                    </div>
                </div>
                
                <!-- Section 6 -->
                <h2 id="softmax">Example: Softmax</h2>
                
                <p>The softmax function $\sigma(\mathbf{z})_i = \frac{e^{z_i}}{\sum_j e^{z_j}}$ has Jacobian:</p>
                
                <div class="math-block">
                    $$\frac{\partial \sigma_i}{\partial z_j} = \sigma_i(\delta_{ij} - \sigma_j)$$
                </div>
                
                <p>
                    where $\delta_{ij}$ is 1 if $i = j$, 0 otherwise. In matrix form:
                </p>
                
                <div class="math-block">
                    $$J = \text{diag}(\boldsymbol{\sigma}) - \boldsymbol{\sigma}\boldsymbol{\sigma}^T$$
                </div>
                
                <div class="note-box">
                    <div class="box-title">Practical Tip</div>
                    <p style="margin-bottom: 0;">
                        When combined with cross-entropy loss, the gradient simplifies beautifully to 
                        $\boldsymbol{\sigma} - \mathbf{y}$ (predicted minus true labels). This is why 
                        softmax + cross-entropy is so popular—the gradient is simple and numerically stable.
                    </p>
                </div>
                
                <!-- Navigation -->
                
            <div class="tutorial-footer-summary" style="margin: 3rem 0; padding: 2rem; background: #f8f9fa; border-radius: 8px; border-left: 4px solid #3b82f6;">
                <h1>04. Matrix Calculus for ML</h1>
                <p class="lead">
                Neural networks are compositions of matrix operations. To train them, we need 
                derivatives with respect to vectors and matrices—the language of backpropagation.
            </p>
            </div>
                <div class="tutorial-nav">
                    <a href="../03-directional/index.html" class="tutorial-nav-link prev">
                        <span class="nav-label">Previous</span>
                        <span class="nav-title">← Directional Derivatives</span>
                    </a>
                    <a href="../index.html" class="tutorial-nav-link next">
                        <span class="nav-label">Next</span>
                        <span class="nav-title">Calculus Home →</span>
                    </a>
                </div>
                
            </article>
        
        </main>

        <!-- TOC (Right Side) -->
        <aside class="toc-container">
        <h4 class="toc-title">Contents</h4>
        <nav class="toc-list">
            <a href="#notation" class="toc-link">Layout Conventions</a>
            <a href="#vector-derivatives" class="toc-link">Vector-by-Vector</a>
            <a href="#common-results" class="toc-link">Common Results</a>
            <a href="#chain-rule" class="toc-link">Chain Rule</a>
            <a href="#linear-layer" class="toc-link">Linear Layer</a>
            <a href="#softmax" class="toc-link">Softmax</a>
        </nav>
    </aside>
    </div>
    

    <!-- Table of Contents -->
    

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-brand">
                    <span class="logo-symbol">∇</span>
                    <span>ML Fundamentals</span>
                </div>
                <p class="footer-tagline">Deep understanding through first principles.</p>
            </div>
            <div class="footer-links">
                <a href="../../../index.html">Home</a>
                <a href="https://github.com/ml-entropy/ml-entropy.github.io" target="_blank">GitHub</a>
            </div>
        </div>
    </footer>

    <!-- Scripts -->
    <script src="../../../js/main.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            if (typeof renderMathInElement === 'function') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }
        });
    </script>
</body>
</html>
