<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>FST Applications (Speech, MT, Text Processing) | ML Fundamentals</title>
    <meta name="description" content="Practical applications of Finite State Transducers in speech recognition, machine translation, text normalization, and morphological analysis.">

    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&family=Playfair+Display:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}, {left: '\\\\[', right: '\\\\]', display: true}, {left: '\\\\(', right: '\\\\)', display: false}], throwOnError: false});"></script>

    <!-- Styles -->
    <link rel="stylesheet" href="../../../css/main.css">
    <link rel="stylesheet" href="../../../css/components.css">
    <link rel="stylesheet" href="../../../css/sidebar.css">

    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>&#x221E;</text></svg>">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar" id="navbar">
        <div class="nav-container">
            <a href="../../../index.html" class="nav-logo">
                <span class="logo-symbol">&nabla;</span>
                <span class="logo-text">ML Fundamentals</span>
            </a>

            <button class="nav-toggle" id="navToggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>

            <div class="nav-menu" id="navMenu">
                <div class="nav-links">
                    <a href="../../../tutorials/ml/index.html" class="nav-link active">Machine Learning</a>
                    <a href="../../../tutorials/linear-algebra/index.html" class="nav-link">Linear Algebra</a>
                    <a href="../../../tutorials/calculus/index.html" class="nav-link">Calculus</a>
                    <a href="../../../tutorials/physics/index.html" class="nav-link">Physics</a>
                    <a href="../../../index.html#philosophy" class="nav-link">Philosophy</a>
                    <a href="../../../index.html#roadmap" class="nav-link">Roadmap</a>
                    <a href="https://github.com/ml-entropy/ml-entropy.github.io" class="nav-link" target="_blank">GitHub</a>
                </div>

                <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
                    <svg class="sun-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="5"/>
                        <path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"/>
                    </svg>
                    <svg class="moon-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                    </svg>
                </button>
            </div>
        </div>
    </nav>

    <!-- Tutorial Header -->
    <header class="tutorial-content-header">
        <div class="container">
            <nav class="breadcrumb">
                <a href="../../../index.html">Home</a>
                <span class="breadcrumb-separator">&rarr;</span>
                <a href="../index.html">Machine Learning</a>
                <span class="breadcrumb-separator">&rarr;</span>
                <span>FST Applications</span>
            </nav>

            <div class="tutorial-tabs">
                <a href="#theory" class="tutorial-tab active">Theory</a>
                <a href="#code" class="tutorial-tab">Code</a>
                <a href="#exercises" class="tutorial-tab">Exercises</a>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <div class="tutorial-wrapper">

        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <aside class="tutorial-sidebar">
            <div class="sidebar-section">
                <h3 class="sidebar-section-title">Machine Learning</h3>
                <nav class="sidebar-nav">
                        <a href="../00-probability/index.html" class="sidebar-link">00. Probability Foundations</a>
                    <a href="../04-logarithms/index.html" class="sidebar-link">01. Why Logarithms?</a>
                    <a href="../05-combinatorics/index.html" class="sidebar-link">02. Combinatorics</a>
                    <a href="../03-distributions/index.html" class="sidebar-link">03. Normal Distributions</a>
                    <a href="../01-entropy/index.html" class="sidebar-link">04. Entropy Fundamentals</a>
                    <a href="../02-cross-entropy/index.html" class="sidebar-link">05. Cross-Entropy</a>
                    <a href="../02-kl-divergence/index.html" class="sidebar-link">06. KL Divergence</a>
                    <a href="../14-entropy-connections/index.html" class="sidebar-link">07. Entropy Connections</a>
                    <a href="../06-backpropagation/index.html" class="sidebar-link">08. Backpropagation</a>
                    <a href="../07-regularization/index.html" class="sidebar-link">09. Regularization</a>
                    <a href="../08-batch-normalization/index.html" class="sidebar-link">10. Batch Normalization</a>
                    <a href="../09-learning-rate/index.html" class="sidebar-link">11. Learning Rate</a>
                    <a href="../10-cnn/index.html" class="sidebar-link">12. CNNs</a>
                    <a href="../11-rnn/index.html" class="sidebar-link">13. RNNs</a>
                    <a href="../15-autoencoder/index.html" class="sidebar-link">14. Autoencoders</a>
                    <a href="../13-variational-inference/index.html" class="sidebar-link">15. Variational Inference</a>
                    <a href="../12-vae/index.html" class="sidebar-link">16. VAE</a>
                    <a href="../16-inductive-bias/index.html" class="sidebar-link">17. Inductive Bias</a>
                    <a href="../17-architectural-biases/index.html" class="sidebar-link">18. Architectural Biases</a>
                    <a href="../18-designing-biases/index.html" class="sidebar-link">19. Designing Biases</a>
                    <a href="../19-fst-fundamentals/index.html" class="sidebar-link">20. FST Fundamentals</a>
                    <a href="../20-weighted-fsts/index.html" class="sidebar-link">21. Weighted FSTs</a>
                    <a href="../21-fst-libraries/index.html" class="sidebar-link">22. FST Libraries</a>
                    <a href="../22-fst-applications/index.html" class="sidebar-link active">23. FST Applications</a>
                    <a href="../23-neural-symbolic/index.html" class="sidebar-link">24. Neural-Symbolic Hybrids</a>
                    <a href="../24-sequence-alignment/index.html" class="sidebar-link">25. Sequence Alignment</a>
                    <a href="../25-mas-algorithm/index.html" class="sidebar-link">26. MAS Algorithm</a>
                    <a href="../26-forced-alignment/index.html" class="sidebar-link">27. Forced Alignment & MFA</a>
                    <a href="../27-tts-fundamentals/index.html" class="sidebar-link">28. TTS Fundamentals</a>
                    <a href="../28-neural-vocoders/index.html" class="sidebar-link">29. Neural Vocoders</a>
                    <a href="../29-tacotron/index.html" class="sidebar-link">30. Tacotron & Attention TTS</a>
                    <a href="../30-fastspeech/index.html" class="sidebar-link">31. FastSpeech & Non-AR TTS</a>
                    <a href="../31-glow-tts/index.html" class="sidebar-link">32. Glow-TTS & Flows</a>
                    <a href="../32-vits/index.html" class="sidebar-link">33. VITS: End-to-End TTS</a>
                    <a href="../33-bilingual-tts/index.html" class="sidebar-link">34. Bilingual TTS: RU+KY</a>
                </nav>
            </div>
            
            <div class="sidebar-section" style="margin-top: 2rem;">
                <h3 class="sidebar-section-title">Related Subjects</h3>
                <nav class="sidebar-nav">
                        <a href="../../linear-algebra/index.html" class="sidebar-link">Linear Algebra</a>
                    <a href="../../calculus/index.html" class="sidebar-link">Calculus</a>
                    <a href="../../physics/index.html" class="sidebar-link">Physics</a>
                </nav>
            </div>
        </aside>

        <!-- Main Article -->
        <main class="tutorial-main">

            <!-- ==================== THEORY TAB ==================== -->
            <article class="article-content" id="theory">

                <!-- Section 1: FSTs in Speech Recognition -->
                <h2 id="speech-recognition">FSTs in Speech Recognition</h2>

                <p>
                    Speech recognition is the canonical application of Weighted Finite State Transducers. The insight that made modern ASR systems practical was recognizing that each component of a speech recognizer &mdash; acoustic model, pronunciation dictionary, and language model &mdash; can be represented as a WFST, and the entire decoding problem reduces to <strong>composition and shortest-path search</strong> on a single composed transducer.
                </p>

                <p>
                    The key idea is the <strong>WFST cascade</strong>: the speech signal passes through a chain of transducers, each converting one representation to another. The acoustic model converts feature vectors to phones, the pronunciation lexicon converts phones to words, and the language model scores word sequences. Composing these into a single WFST allows efficient Viterbi decoding in one pass.
                </p>

                <div class="definition-box">
                    <div class="box-title">The WFST Cascade for Speech Recognition</div>
                    <p>The standard decoding pipeline is a composition of transducers:</p>
                    <div class="math-block">
                        $$\text{Search} = H \circ C \circ L \circ G$$
                    </div>
                    <ul style="margin-bottom: 0;">
                        <li>$H$ &mdash; <strong>HMM transducer:</strong> maps HMM states to context-dependent phones (triphones)</li>
                        <li>$C$ &mdash; <strong>Context-dependency transducer:</strong> maps context-dependent phones to context-independent phones</li>
                        <li>$L$ &mdash; <strong>Lexicon transducer:</strong> maps phone sequences to words</li>
                        <li>$G$ &mdash; <strong>Grammar/Language model:</strong> scores word sequences (n-gram WFST)</li>
                    </ul>
                </div>

                <div class="definition-box">
                    <div class="box-title">HCLG Cascade</div>
                    <p>The speech recognition search graph is built by composing four WFSTs:</p>
                    <ul>
                        <li><strong>$H$ (HMM):</strong> Maps HMM states to context-dependent phonemes</li>
                        <li><strong>$C$ (Context):</strong> Maps context-dependent to context-independent phonemes</li>
                        <li><strong>$L$ (Lexicon):</strong> Maps phoneme sequences to words</li>
                        <li><strong>$G$ (Grammar):</strong> N-gram language model over words</li>
                    </ul>
                    <p style="margin-bottom: 0;">The composed transducer $\text{HCLG} = H \circ C \circ L \circ G$ maps directly from HMM states to word sequences, enabling efficient Viterbi search.</p>
                </div>

                <p>
                    The <strong>Kaldi toolkit</strong> (Povey et al., 2011) popularized this approach and remains one of the most widely used ASR frameworks. Kaldi implements the full HCLG cascade using OpenFst, applying determinization, minimization, and weight pushing to create a highly optimized decoding graph. The result is a single WFST where finding the best path through the graph corresponds to finding the most likely word sequence given the acoustic input.
                </p>

                <p><strong>Why FSTs for speech recognition?</strong></p>
                <ul>
                    <li><strong>Modularity:</strong> Each component (acoustic model, lexicon, language model) can be built and tested independently, then composed.</li>
                    <li><strong>Efficient search:</strong> The composed WFST can be determinized and minimized, enabling efficient Viterbi beam search.</li>
                    <li><strong>Formal guarantees:</strong> The WFST framework provides mathematical guarantees about the optimality of the decoded result (within the beam).</li>
                    <li><strong>Flexibility:</strong> Changing the language model or lexicon only requires recomposing &mdash; the acoustic model remains unchanged.</li>
                </ul>

                <!-- Section 2: Pronunciation Lexicon FSTs -->
                <h2 id="pronunciation-lexicon">Pronunciation Lexicon FSTs</h2>

                <p>
                    The pronunciation lexicon (the $L$ transducer) maps sequences of phonemes to words. This is one of the most natural and intuitive uses of FSTs: each dictionary entry becomes a path through the transducer, where the input labels are phones and the output label is the word.
                </p>

                <p>
                    Consider the word "read" which has two pronunciations depending on tense:
                </p>
                <ul>
                    <li><strong>Present tense:</strong> "read" &rarr; /r iy d/ (rhymes with "seed")</li>
                    <li><strong>Past tense:</strong> "read" &rarr; /r eh d/ (rhymes with "red")</li>
                </ul>

                <p>
                    In the lexicon FST, both pronunciations are represented as separate paths that produce the same output word "read". The non-determinism is intentional: given either pronunciation as input, the transducer produces the word "read". During decoding, the acoustic model scores determine which pronunciation path is more likely.
                </p>

                <p><strong>Grapheme-to-Phoneme (G2P) Conversion:</strong></p>
                <p>
                    For words not in the lexicon (out-of-vocabulary or OOV words), a <strong>Grapheme-to-Phoneme</strong> model generates pronunciations from spelling. Traditional G2P systems are themselves FSTs: they convert letter sequences to phone sequences using learned rules. For example:
                </p>
                <ul>
                    <li>"ph" &rarr; /f/ (as in "phone")</li>
                    <li>"tion" &rarr; /sh ax n/ (as in "nation")</li>
                    <li>"ght" &rarr; /t/ (as in "night")</li>
                </ul>

                <p>
                    Handling OOV words is critical for production ASR systems. The G2P FST is composed with the lexicon FST so that any word &mdash; known or unknown &mdash; can be decoded. The Phonetisaurus toolkit is a well-known FST-based G2P system that learns grapheme-to-phoneme mappings from a pronunciation dictionary.
                </p>

                <!-- Section 3: Language Model FSTs -->
                <h2 id="language-models">Language Model FSTs</h2>

                <p>
                    An n-gram language model can be represented exactly as a Weighted Finite State Acceptor (WFSA), which is a special case of a WFST where input and output labels are identical. Each state represents an (n-1)-gram history, and each arc represents extending the history by one word, weighted by the conditional probability.
                </p>

                <div class="math-derivation">
                    <div class="math-derivation-title">N-gram LM as WFST</div>

                    <div class="math-step">
                        <div class="math-step-number">1</div>
                        <div class="math-step-content">
                            <p>For a bigram model $P(w_i \mid w_{i-1})$, each state represents a word $w_{i-1}$:</p>
                            <div class="math-block">
                                $$q_{w_{i-1}} \xrightarrow{w_i : w_i / -\log P(w_i \mid w_{i-1})} q_{w_i}$$
                            </div>
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">2</div>
                        <div class="math-step-content">
                            <p>Weights are negative log-probabilities (tropical semiring), so the shortest path corresponds to the most probable word sequence:</p>
                            <div class="math-block">
                                $$\text{weight}(w_1, w_2, \ldots, w_n) = -\sum_{i=1}^{n} \log P(w_i \mid w_{i-1})$$
                            </div>
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">3</div>
                        <div class="math-step-content">
                            <p>For trigram and higher-order models, states represent longer histories. A trigram state is a pair $(w_{i-2}, w_{i-1})$.</p>
                        </div>
                    </div>
                </div>

                <p>
                    <strong>Backoff and smoothing</strong> are handled elegantly in the WFST framework. When an n-gram has not been observed, the model "backs off" to a shorter history. In the WFST, this is implemented as epsilon transitions (also called failure transitions) from the full-context state to the shorter-context state, weighted by the backoff weight:
                </p>
                <div class="math-block">
                    $$q_{(w_{i-2}, w_{i-1})} \xrightarrow{\varepsilon : \varepsilon / -\log \alpha(w_{i-2}, w_{i-1})} q_{w_{i-1}}$$
                </div>
                <p>
                    where $\alpha$ is the backoff weight that ensures proper normalization.
                </p>

                <div class="math-derivation">
                    <div class="math-derivation-title">Backoff in a Bigram WFST</div>

                    <div class="math-step">
                        <div class="math-step-number">1</div>
                        <div class="math-step-content">
                            <p>For an observed bigram $(w_1, w_2)$, add a direct transition from state $q_{w_1}$ to state $q_{w_2}$ with weight:</p>
                            <div class="math-block">
                                $$q_{w_1} \xrightarrow{w_2 : w_2 / -\log P(w_2 \mid w_1)} q_{w_2}$$
                            </div>
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">2</div>
                        <div class="math-step-content">
                            <p>For unseen bigrams, add an epsilon backoff transition from $q_{w_1}$ to the unigram state $q_{\text{backoff}}$ with the backoff weight:</p>
                            <div class="math-block">
                                $$q_{w_1} \xrightarrow{\varepsilon : \varepsilon / -\log \alpha(w_1)} q_{\text{backoff}}$$
                            </div>
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">3</div>
                        <div class="math-step-content">
                            <p>From the unigram backoff state, all words have arcs weighted by their unigram probabilities:</p>
                            <div class="math-block">
                                $$q_{\text{backoff}} \xrightarrow{w : w / -\log P(w)} q_{w}$$
                            </div>
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">4</div>
                        <div class="math-step-content">
                            <p>The total path weight for a word $w_2$ given history $w_1$ is:</p>
                            <div class="math-block">
                                $$\text{weight} = \begin{cases} -\log P(w_2 \mid w_1) & \text{if bigram observed} \\ -\log \alpha(w_1) + (-\log P(w_2)) & \text{otherwise (backoff)} \end{cases}$$
                            </div>
                        </div>
                    </div>
                </div>

                <div class="note-box">
                    <div class="box-title">Neural Language Models and FSTs</div>
                    <p style="margin-bottom: 0;">
                        Neural language models (RNNs, Transformers) do <strong>not</strong> naturally fit into the FST framework because they have continuous hidden states rather than a finite set of discrete states. However, <strong>hybrid approaches</strong> exist: (1) neural LMs can be approximated by n-gram models and then converted to WFSTs, (2) on-the-fly composition allows neural LM scores to be incorporated during beam search without materializing the full WFST, and (3) lattice rescoring uses neural LMs to rescore n-best lists or lattices produced by the WFST decoder.
                    </p>
                </div>

                <!-- Section 4: FSTs in Machine Translation -->
                <h2 id="machine-translation">FSTs in Machine Translation</h2>

                <p>
                    Before the neural revolution (pre-2016), phrase-based statistical machine translation (SMT) was the dominant paradigm. The key insight of the WFST approach to SMT is that phrase-based translation can be decomposed into a cascade of transducers, analogous to the speech recognition pipeline.
                </p>

                <p>
                    The phrase-based MT WFST cascade typically includes:
                </p>
                <ul>
                    <li><strong>Segmentation transducer:</strong> Segments the source sentence into phrases.</li>
                    <li><strong>Translation transducer:</strong> Maps source phrases to target phrases, weighted by translation probabilities.</li>
                    <li><strong>Reordering transducer:</strong> Handles word order differences between source and target languages.</li>
                    <li><strong>Target language model:</strong> Scores the fluency of the target word sequence.</li>
                </ul>

                <p>
                    <strong>Historical context:</strong> The WFST approach to MT was pioneered by Bangalore and Riccardi (2000) and Kumar and Byrne (2003). It provided a mathematically elegant framework where complex translation decisions were decomposed into simple, composable transductions. The IBM word-alignment models that underlie phrase tables can also be viewed through the FST lens.
                </p>

                <div class="warning-box">
                    <div class="box-title">FST-based MT: Largely Replaced by Neural MT</div>
                    <p style="margin-bottom: 0;">
                        Neural machine translation (NMT) using encoder-decoder architectures with attention has largely replaced phrase-based SMT in production systems since 2016. NMT handles long-range dependencies, captures semantic nuance, and requires less feature engineering. However, FST-based approaches remain valuable for: (1) <strong>low-resource languages</strong> where parallel training data is scarce, (2) <strong>domain-specific translation</strong> with strict terminology requirements (medical, legal), and (3) <strong>hybrid systems</strong> where FSTs handle preprocessing (tokenization, transliteration) while neural models handle the core translation.
                    </p>
                </div>

                <div class="note-box">
                    <div class="box-title">Where FSTs Still Win in MT</div>
                    <ol style="margin-bottom: 0;">
                        <li><strong>Transliteration of named entities between scripts</strong> (Arabic&#x2194;Latin), where the mapping is rule-governed and must be consistent across a document.</li>
                        <li><strong>Number and date format conversion across locales</strong>, e.g., converting "2/16/2026" to "16.02.2026" or "16 f&eacute;vrier 2026" depending on the target locale.</li>
                        <li><strong>Low-resource languages</strong> where parallel data is scarce but linguistic rules are known, allowing rule-based translation of morphological and syntactic patterns.</li>
                        <li><strong>Domain-specific terminology mapping</strong> in specialized fields (medical, legal), where exact term translation is legally or clinically required and cannot be left to a probabilistic model.</li>
                    </ol>
                </div>

                <!-- Section 5: Text Normalization -->
                <h2 id="text-normalization">Text Normalization</h2>

                <p>
                    Text normalization is perhaps the most widespread <em>current</em> production use of FSTs. The task is to convert non-standard text into a standard spoken or written form. This is critical for Text-to-Speech (TTS) systems, search engines, and any NLP pipeline that must handle messy real-world text.
                </p>

                <p><strong>Common text normalization tasks:</strong></p>
                <ul>
                    <li><strong>Number expansion:</strong> "42" &rarr; "forty-two", "$3.50" &rarr; "three dollars and fifty cents"</li>
                    <li><strong>Abbreviation expansion:</strong> "Dr." &rarr; "Doctor", "St." &rarr; "Street" or "Saint" (context-dependent)</li>
                    <li><strong>Date/time formats:</strong> "2/16/2026" &rarr; "February sixteenth, twenty twenty-six"</li>
                    <li><strong>Unit conversion:</strong> "5km" &rarr; "five kilometers", "98.6F" &rarr; "ninety-eight point six degrees Fahrenheit"</li>
                    <li><strong>Currency formatting:</strong> "$1,234.56" &rarr; "one thousand two hundred thirty-four dollars and fifty-six cents"</li>
                </ul>

                <p>
                    <strong>Production use cases:</strong> Google's Kestrel TTS normalizer and the Sparrowhawk open-source text normalizer are FST-based systems that handle text normalization for dozens of languages. The key advantages of FSTs for this task are:
                </p>
                <ul>
                    <li><strong>Deterministic output:</strong> A TTS system reading "$100" must always produce the same spoken form. FSTs guarantee this.</li>
                    <li><strong>Debuggability:</strong> When a normalization error occurs, you can trace exactly which rule fired and fix it.</li>
                    <li><strong>Language portability:</strong> The same FST framework handles number expansion in English, Chinese, Arabic, etc. &mdash; only the rules change.</li>
                    <li><strong>Composability:</strong> Different normalization stages (detect number type, expand, format) compose into a single efficient transducer.</li>
                </ul>

                <p>
                    <strong>Search indexing</strong> also relies heavily on text normalization FSTs. When a user searches for "Dr Smith", the search engine needs to match documents containing "Doctor Smith", "Dr. Smith", and "DR SMITH". FSTs that normalize all variants to a canonical form enable this matching.
                </p>

                <!-- Section 6: Morphological Analysis -->
                <h2 id="morphological-analysis">Morphological Analysis</h2>

                <p>
                    Morphological analysis &mdash; decomposing words into their constituent morphemes (stems, prefixes, suffixes) &mdash; is one of the oldest and most elegant applications of FSTs. The field was pioneered by the <strong>Xerox finite-state morphology</strong> tradition (Beesley and Karttunen, 2003), which showed that the morphological rules of natural languages can be captured by two-level finite-state transducers.
                </p>

                <p><strong>Key morphological operations as FSTs:</strong></p>
                <ul>
                    <li><strong>Stemming:</strong> "running" &rarr; "run", "walked" &rarr; "walk", "cities" &rarr; "city"</li>
                    <li><strong>Lemmatization:</strong> "better" &rarr; "good", "went" &rarr; "go", "mice" &rarr; "mouse"</li>
                    <li><strong>Morpheme segmentation:</strong> "unhappiness" &rarr; "un+happy+ness"</li>
                    <li><strong>Generation:</strong> "walk+PAST" &rarr; "walked", "city+PLURAL" &rarr; "cities"</li>
                </ul>

                <p>
                    <strong>Turkish: A showcase for FST morphology.</strong> Turkish is an <em>agglutinative</em> language where words are formed by concatenating many suffixes. A single Turkish verb form can encode tense, aspect, mood, person, number, negation, and more:
                </p>
                <ul>
                    <li>"gelebilecekmiydik" = gel+ebil+ecek+miy+di+k</li>
                    <li>Meaning: "Were we supposedly going to be able to come?"</li>
                </ul>
                <p>
                    An FST for Turkish morphology has states representing positions in the suffix chain. Each suffix transition consumes the surface form characters and produces the morphological analysis. The number of possible word forms is astronomically large (millions), but the FST represents them compactly through state sharing.
                </p>

                <p>
                    The <strong>Xerox finite-state tools</strong> (xfst, lexc, twolc) and their open-source successors (Foma, HFST) define a two-level model: the <em>lexical</em> level represents the morphological analysis (e.g., "walk+PAST") and the <em>surface</em> level represents the actual spelling (e.g., "walked"). The FST maps between these two levels bidirectionally &mdash; it can both analyze and generate.
                </p>

                <!-- Section 7: When NOT to Use FSTs -->
                <h2 id="when-not-to-use">When NOT to Use FSTs</h2>

                <p>
                    Knowing when <em>not</em> to use FSTs is just as important as knowing when to use them. While FSTs are powerful for rule-based, deterministic transformations, there are many tasks where they are the wrong tool.
                </p>

                <div class="warning-box">
                    <div class="box-title">When NOT to Use FSTs</div>
                    <table style="width: 100%; border-collapse: collapse; margin-top: 0.5rem; margin-bottom: 0;">
                        <thead>
                            <tr>
                                <th style="border: 1px solid #ddd; padding: 8px; text-align: left;">Limitation</th>
                                <th style="border: 1px solid #ddd; padding: 8px; text-align: left;">FST Weakness</th>
                                <th style="border: 1px solid #ddd; padding: 8px; text-align: left;">Neural Alternative</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td style="border: 1px solid #ddd; padding: 8px;"><strong>Learning from data</strong></td>
                                <td style="border: 1px solid #ddd; padding: 8px;">FSTs don't learn &mdash; they execute hand-crafted or algorithmically compiled rules.</td>
                                <td style="border: 1px solid #ddd; padding: 8px;">Neural seq2seq models learn arbitrary mappings from (input, output) pairs without explicit rules.</td>
                            </tr>
                            <tr>
                                <td style="border: 1px solid #ddd; padding: 8px;"><strong>Long-range dependencies</strong></td>
                                <td style="border: 1px solid #ddd; padding: 8px;">Finite memory bounded by the number of states. Cannot capture context far away in the input (e.g., coreference, document-level translation).</td>
                                <td style="border: 1px solid #ddd; padding: 8px;">Transformers with attention over long sequences capture dependencies across thousands of tokens.</td>
                            </tr>
                            <tr>
                                <td style="border: 1px solid #ddd; padding: 8px;"><strong>Ambiguity requiring context</strong></td>
                                <td style="border: 1px solid #ddd; padding: 8px;">"read" is /riyd/ in "I will read" but /rEd/ in "I have read." FSTs lack syntactic/semantic understanding to resolve this.</td>
                                <td style="border: 1px solid #ddd; padding: 8px;">Neural models trained on large corpora learn contextual representations that disambiguate based on surrounding text.</td>
                            </tr>
                            <tr>
                                <td style="border: 1px solid #ddd; padding: 8px;"><strong>Rapidly changing rules</strong></td>
                                <td style="border: 1px solid #ddd; padding: 8px;">If rules evolve with new data (trending slang, evolving terminology), maintaining an FST is a never-ending manual process.</td>
                                <td style="border: 1px solid #ddd; padding: 8px;">Learned models can be retrained on new data automatically, adapting to changing patterns without manual rule engineering.</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <p>
                    The fundamental question is: <strong>Do you know the rules, or do you need to learn them?</strong> If you know the rules, an FST provides guaranteed correctness, interpretability, and efficiency. If you need to learn from data, a neural model is almost always the better choice.
                </p>

                <p>
                    In practice, the best systems combine both: FSTs handle the deterministic preprocessing (text normalization, tokenization, known transformations) while neural models handle the ambiguous, context-dependent core task (translation, recognition, generation). This hybrid approach is the topic of Tutorial 24.
                </p>

                <!-- Section 8: Summary -->
                <h2 id="summary">Summary</h2>

                <p><strong>Application guide &mdash; where FSTs excel:</strong></p>
                <ol>
                    <li><strong>Speech recognition:</strong> The HCLG cascade ($H \circ C \circ L \circ G$) composes HMM, context-dependency, lexicon, and language model into a single decodable WFST. Kaldi is the standard toolkit.</li>
                    <li><strong>Pronunciation lexicons:</strong> Map phone sequences to words with multiple pronunciations. G2P handles OOV words.</li>
                    <li><strong>Language models:</strong> N-gram LMs are naturally WFSAs with backoff implemented as epsilon transitions.</li>
                    <li><strong>Machine translation:</strong> Pre-neural phrase-based MT used WFST cascades. Still useful for low-resource and domain-specific translation.</li>
                    <li><strong>Text normalization:</strong> The most active current production use. Numbers, dates, abbreviations, currencies. Critical for TTS and search.</li>
                    <li><strong>Morphological analysis:</strong> Two-level morphology maps between surface forms and morphological analyses. Essential for agglutinative languages.</li>
                </ol>

                <p><strong>When to choose FSTs vs neural models:</strong></p>
                <ul>
                    <li>Known rules + deterministic output + interpretability needed &rarr; <strong>FST</strong></li>
                    <li>Unknown rules + learning from data + context-dependent &rarr; <strong>Neural</strong></li>
                    <li>Both + production quality &rarr; <strong>Hybrid</strong> (FST preprocessing + neural core)</li>
                </ul>

                <p>
                    In the next tutorial, we explore <strong>Neural-Symbolic Hybrids</strong> &mdash; architectures that combine the guaranteed correctness of FSTs with the learning power of neural networks, giving us the best of both worlds.
                </p>

                <!-- Navigation -->
                <div class="tutorial-footer-summary" style="margin: 3rem 0; padding: 2rem; background: #f8f9fa; border-radius: 8px; border-left: 4px solid #3b82f6;">
                    <h1>23. FST Applications</h1>
                    <p class="lead">
                        Speech recognition, machine translation, text normalization, and morphological analysis: the practical power of Finite State Transducers.
                    </p>
                </div>
                <div class="tutorial-nav">
                    <a href="../21-fst-libraries/index.html" class="tutorial-nav-link prev">
                        <span class="nav-label">Previous</span>
                        <span class="nav-title">&larr; FST Libraries</span>
                    </a>
                    <a href="../23-neural-symbolic/index.html" class="tutorial-nav-link next">
                        <span class="nav-label">Next</span>
                        <span class="nav-title">Neural-Symbolic Hybrids &rarr;</span>
                    </a>
                </div>

            </article>

            <!-- ==================== CODE TAB ==================== -->
            <article class="article-content" id="code" style="display: none;">
                <h2>Python Code Examples</h2>
                <p>Four code examples: grapheme-to-phoneme rules, text normalization pipeline, morphological analysis, and n-gram language model FST.</p>

                <!-- Code Example 1 -->
                <h3>1. G2P with Rules: Simple Grapheme-to-Phoneme</h3>
                <p>
                    A rule-based grapheme-to-phoneme converter for an English subset. This demonstrates how G2P rules are naturally expressed as FST-like state transitions &mdash; each rule matches a letter context and produces phonemes.
                </p>

                <div class="code-block">
                    <div class="code-block-header">
                        <span class="code-block-lang">python</span>
                        <button class="code-block-copy">Copy</button>
                    </div>
                    <pre><code class="language-python">class RuleBasedG2P:
    """
    Simple rule-based Grapheme-to-Phoneme converter.

    Rules are ordered: longer/more-specific matches take priority.
    Each rule: (grapheme_pattern, phoneme_output, context)
    This mimics how an FST would process letter sequences.
    """

    def __init__(self):
        # Rules: (letter_sequence, phoneme_sequence)
        # Ordered by specificity (longest match first)
        self.rules = [
            # Multi-letter rules (higher priority)
            ('tion', 'SH AH N'),
            ('sion', 'ZH AH N'),
            ('ough', 'AO'),       # simplified: "thought"
            ('ight', 'AY T'),     # "night", "light"
            ('ght', 'T'),         # after vowel digraph
            ('ph', 'F'),          # "phone"
            ('th', 'TH'),         # "think" (simplified)
            ('sh', 'SH'),         # "ship"
            ('ch', 'CH'),         # "chip"
            ('ck', 'K'),          # "back"
            ('ng', 'NG'),         # "ring"
            ('wh', 'W'),          # "what"
            ('wr', 'R'),          # "write"
            ('kn', 'N'),          # "know"
            ('ee', 'IY'),         # "see"
            ('ea', 'IY'),         # "read" (present)
            ('oo', 'UW'),         # "food"
            ('ou', 'AW'),         # "out"
            ('ow', 'OW'),         # "low"
            ('ai', 'EY'),         # "rain"
            ('ay', 'EY'),         # "day"
            ('oy', 'OY'),         # "boy"
            ('ar', 'AA R'),       # "car"
            ('er', 'ER'),         # "her"
            ('ir', 'ER'),         # "sir"
            ('or', 'AO R'),       # "for"
            ('ur', 'ER'),         # "fur"
            # Single letter rules (lower priority)
            ('a', 'AE'),          # "cat"
            ('b', 'B'),
            ('c', 'K'),           # default: "cat"
            ('d', 'D'),
            ('e', 'EH'),          # "bed"
            ('f', 'F'),
            ('g', 'G'),           # default: "go"
            ('h', 'HH'),
            ('i', 'IH'),          # "sit"
            ('j', 'JH'),
            ('k', 'K'),
            ('l', 'L'),
            ('m', 'M'),
            ('n', 'N'),
            ('o', 'AA'),          # "hot"
            ('p', 'P'),
            ('q', 'K'),
            ('r', 'R'),
            ('s', 'S'),
            ('t', 'T'),
            ('u', 'AH'),          # "cup"
            ('v', 'V'),
            ('w', 'W'),
            ('x', 'K S'),
            ('y', 'Y'),
            ('z', 'Z'),
        ]

    def convert(self, word):
        """Convert a word to its phoneme sequence."""
        word = word.lower().strip()
        phonemes = []
        pos = 0

        while pos < len(word):
            matched = False
            # Try longest match first
            for grapheme, phoneme in self.rules:
                end = pos + len(grapheme)
                if word[pos:end] == grapheme:
                    phonemes.append(phoneme)
                    pos = end
                    matched = True
                    break

            if not matched:
                # Skip unknown characters
                pos += 1

        return ' '.join(phonemes)


# Test the G2P system
g2p = RuleBasedG2P()

test_words = [
    'cat', 'phone', 'night', 'ship', 'think',
    'read', 'know', 'write', 'church', 'ring',
    'food', 'rain', 'boy', 'car', 'her',
    'back', 'nation', 'vision', 'day', 'out'
]

print("Rule-Based Grapheme-to-Phoneme Converter")
print("=" * 55)
for word in test_words:
    phonemes = g2p.convert(word)
    print(f"  {word:12s} -> {phonemes}")

print(f"\nTotal rules: {len(g2p.rules)}")
print("Note: A production G2P would have thousands of")
print("context-sensitive rules and handle exceptions.")</code></pre>
                </div>

                <!-- Code Example 2 -->
                <h3>2. Text Normalization Pipeline</h3>
                <p>
                    A complete text normalization system handling dates, numbers, abbreviations, and currencies. Each normalizer acts as a transducer stage in a pipeline, mimicking FST composition.
                </p>

                <div class="code-block">
                    <div class="code-block-header">
                        <span class="code-block-lang">python</span>
                        <button class="code-block-copy">Copy</button>
                    </div>
                    <pre><code class="language-python">import re

class TextNormalizer:
    """
    Text normalization pipeline modeled as FST composition.

    Each normalize_* method acts as a transducer:
    input text -> normalized text

    The full pipeline composes them: T1 o T2 o T3 o T4
    """

    # Number words
    ONES = ['', 'one', 'two', 'three', 'four', 'five',
            'six', 'seven', 'eight', 'nine', 'ten',
            'eleven', 'twelve', 'thirteen', 'fourteen', 'fifteen',
            'sixteen', 'seventeen', 'eighteen', 'nineteen']
    TENS = ['', '', 'twenty', 'thirty', 'forty', 'fifty',
            'sixty', 'seventy', 'eighty', 'ninety']
    MONTHS = {
        '01': 'January', '02': 'February', '03': 'March',
        '04': 'April', '05': 'May', '06': 'June',
        '07': 'July', '08': 'August', '09': 'September',
        '10': 'October', '11': 'November', '12': 'December',
        '1': 'January', '2': 'February', '3': 'March',
        '4': 'April', '5': 'May', '6': 'June',
        '7': 'July', '8': 'August', '9': 'September',
    }

    def number_to_words(self, n):
        """Convert integer to English words (0-9999)."""
        if n == 0:
            return 'zero'
        if n < 0:
            return 'negative ' + self.number_to_words(-n)

        parts = []
        if n >= 1000:
            parts.append(self.ONES[n // 1000] + ' thousand')
            n %= 1000
        if n >= 100:
            parts.append(self.ONES[n // 100] + ' hundred')
            n %= 100
        if n >= 20:
            tens_word = self.TENS[n // 10]
            ones_word = self.ONES[n % 10]
            if ones_word:
                parts.append(tens_word + '-' + ones_word)
            else:
                parts.append(tens_word)
        elif n > 0:
            parts.append(self.ONES[n])

        return ' '.join(parts)

    def ordinal(self, n):
        """Convert number to ordinal word."""
        word = self.number_to_words(n)
        # Simple suffix rules (order matters: check compound endings first)
        if word.endswith('one'):
            return word[:-3] + 'first'
        elif word.endswith('two'):
            return word[:-3] + 'second'
        elif word.endswith('three'):
            return word[:-5] + 'third'
        elif word.endswith('five'):
            return word[:-4] + 'fifth'
        elif word.endswith('twelve'):
            return word[:-2] + 'fth'
        elif word.endswith('eight') and not word.endswith('eighteen'):
            # "eight" -> "eighth", but "eighteen" -> "eighteenth"
            return word + 'h'
        elif word.endswith('nine') and not word.endswith('nineteen'):
            # "nine" -> "ninth", but "nineteen" -> "nineteenth"
            return word[:-1] + 'th'
        elif word.endswith('y'):
            return word[:-1] + 'ieth'
        else:
            return word + 'th'

    def normalize_dates(self, text):
        """Transducer T1: Date normalization."""
        # MM/DD/YYYY -> Month DDth, YYYY
        def replace_date(match):
            month = self.MONTHS.get(match.group(1), match.group(1))
            day = int(match.group(2))
            year = int(match.group(3))
            day_word = self.ordinal(day)

            # Year as spoken
            if 2000 <= year <= 2009:
                year_word = 'two thousand'
                if year > 2000:
                    year_word += ' ' + self.ONES[year - 2000]
            elif 2010 <= year <= 2099:
                year_word = 'twenty ' + self.number_to_words(year - 2000)
            else:
                year_word = self.number_to_words(year)

            return f"{month} {day_word}, {year_word}"

        text = re.sub(r'(\d{1,2})/(\d{1,2})/(\d{4})', replace_date, text)
        return text

    def normalize_numbers(self, text):
        """Transducer T2: Number expansion."""
        def replace_number(match):
            num = int(match.group(0))
            if num <= 9999:
                return self.number_to_words(num)
            return match.group(0)  # Leave large numbers as-is

        text = re.sub(r'\b\d+\b', replace_number, text)
        return text

    def normalize_abbreviations(self, text):
        """Transducer T3: Abbreviation expansion."""
        abbrevs = {
            r'\bDr\.': 'Doctor',
            r'\bMr\.': 'Mister',
            r'\bMrs\.': 'Missus',
            r'\bMs\.': 'Ms',
            r'\bSt\.': 'Street',
            r'\bAve\.': 'Avenue',
            r'\bBlvd\.': 'Boulevard',
            r'\bvs\.': 'versus',
            r'\betc\.': 'et cetera',
            r'\be\.g\.': 'for example',
            r'\bi\.e\.': 'that is',
        }
        for pattern, replacement in abbrevs.items():
            text = re.sub(pattern, replacement, text)
        return text

    def normalize_currency(self, text):
        """Transducer T4: Currency normalization."""
        def replace_currency(match):
            dollars = int(match.group(1))
            cents = match.group(2)

            result = self.number_to_words(dollars)
            result += ' dollar' + ('s' if dollars != 1 else '')

            if cents:
                cents_num = int(cents)
                if cents_num > 0:
                    result += ' and ' + self.number_to_words(cents_num)
                    result += ' cent' + ('s' if cents_num != 1 else '')

            return result

        text = re.sub(r'\$(\d+)(?:\.(\d{2}))?', replace_currency, text)
        return text

    def normalize(self, text):
        """Full pipeline: compose all transducers."""
        # Order matters (like FST composition order)
        text = self.normalize_currency(text)    # T4: currency first
        text = self.normalize_dates(text)        # T1: then dates
        text = self.normalize_abbreviations(text)# T3: then abbreviations
        text = self.normalize_numbers(text)      # T2: numbers last
        return text


# Test the normalizer
normalizer = TextNormalizer()

test_inputs = [
    "Dr. Smith lives at 42 Oak St.",
    "The meeting is on 2/16/2026 at 3pm.",
    "Total cost: $125.99 for 3 items.",
    "Mrs. Johnson vs. Mr. Davis, e.g. in court.",
    "Send 500 units to 123 Main Ave. by 12/25/2026.",
]

print("Text Normalization Pipeline")
print("=" * 65)
for text in test_inputs:
    normalized = normalizer.normalize(text)
    print(f"\n  Input:  {text}")
    print(f"  Output: {normalized}")

print("\n" + "=" * 65)
print("Pipeline order: Currency -> Dates -> Abbreviations -> Numbers")
print("Each stage is a 'transducer' composing into the full pipeline.")</code></pre>
                </div>

                <!-- Code Example 3 -->
                <h3>3. Simple Morphological Analyzer</h3>
                <p>
                    An FST-based morphological analyzer for English verb conjugation. Given a surface form like "walked", it produces the analysis "walk+PAST". The FST operates bidirectionally: it can both analyze (surface to analysis) and generate (analysis to surface).
                </p>

                <div class="code-block">
                    <div class="code-block-header">
                        <span class="code-block-lang">python</span>
                        <button class="code-block-copy">Copy</button>
                    </div>
                    <pre><code class="language-python">class MorphologicalFST:
    """
    FST for English verb morphology (simplified).

    Handles regular verbs:
      walk  -> walk+PRESENT
      walks -> walk+3SG
      walked -> walk+PAST
      walking -> walk+GERUND

    Also handles:
      - consonant doubling: run -> running
      - e-deletion: make -> making
      - y->ie: try -> tries
    """

    def __init__(self):
        self.verbs = {}  # stem -> properties

    def add_verb(self, stem, doubles_consonant=False,
                 drops_e=False, y_to_ie=False):
        """Register a verb stem with its morphological properties."""
        self.verbs[stem] = {
            'doubles': doubles_consonant,
            'drops_e': drops_e,
            'y_to_ie': y_to_ie,
        }

    def generate(self, stem, tag):
        """Generate surface form from stem + morphological tag."""
        if stem not in self.verbs:
            return None

        props = self.verbs[stem]

        if tag == 'PRESENT' or tag == 'INF':
            return stem

        elif tag == '3SG':  # third person singular
            if props['y_to_ie']:
                return stem[:-1] + 'ies'
            elif stem[-1] in 'sxz' or stem[-2:] in ('ch', 'sh'):
                return stem + 'es'
            else:
                return stem + 's'

        elif tag == 'PAST':
            if props['drops_e']:
                return stem + 'd'
            elif props['y_to_ie']:
                return stem[:-1] + 'ied'
            elif props['doubles']:
                return stem + stem[-1] + 'ed'
            else:
                return stem + 'ed'

        elif tag == 'GERUND':
            if props['drops_e']:
                return stem[:-1] + 'ing'
            elif props['doubles']:
                return stem + stem[-1] + 'ing'
            else:
                return stem + 'ing'

        return None

    def analyze(self, surface_form):
        """Analyze a surface form into stem + tag (inverse transduction)."""
        results = []

        for stem, props in self.verbs.items():
            # Check each possible tag
            for tag in ['PRESENT', '3SG', 'PAST', 'GERUND']:
                generated = self.generate(stem, tag)
                if generated == surface_form:
                    results.append(f"{stem}+{tag}")

        return results


# Build the morphological FST
morph = MorphologicalFST()

# Add verbs with their properties
morph.add_verb('walk')
morph.add_verb('talk')
morph.add_verb('play')
morph.add_verb('jump')
morph.add_verb('push')
morph.add_verb('watch')
morph.add_verb('run', doubles_consonant=True)
morph.add_verb('stop', doubles_consonant=True)
morph.add_verb('plan', doubles_consonant=True)
morph.add_verb('make', drops_e=True)
morph.add_verb('hope', drops_e=True)
morph.add_verb('live', drops_e=True)
morph.add_verb('try', y_to_ie=True)
morph.add_verb('cry', y_to_ie=True)
morph.add_verb('carry', y_to_ie=True)

# Demo: Generation (analysis -> surface)
print("Morphological FST: Generation")
print("=" * 50)
for stem in ['walk', 'run', 'make', 'try', 'watch']:
    print(f"\n  Stem: {stem}")
    for tag in ['PRESENT', '3SG', 'PAST', 'GERUND']:
        form = morph.generate(stem, tag)
        print(f"    {stem}+{tag:8s} -> {form}")

# Demo: Analysis (surface -> analysis)
print("\n\nMorphological FST: Analysis (Inverse)")
print("=" * 50)
test_forms = [
    'walked', 'walking', 'walks',
    'running', 'stopped', 'making',
    'tries', 'tried', 'crying',
    'pushed', 'watches', 'hoped',
]

for form in test_forms:
    analyses = morph.analyze(form)
    print(f"  {form:12s} -> {analyses}")

print("\nKey insight: The FST is bidirectional.")
print("Generation and analysis are inverse transductions.")</code></pre>
                </div>

                <!-- Code Example 4 -->
                <h3>4. N-gram FST (Simplified): Bigram Language Model</h3>
                <p>
                    Build a bigram language model as a Weighted FST. Each state represents a word history, and arc weights are negative log-probabilities. Finding the shortest path gives the most probable word sequence.
                </p>

                <div class="code-block">
                    <div class="code-block-header">
                        <span class="code-block-lang">python</span>
                        <button class="code-block-copy">Copy</button>
                    </div>
                    <pre><code class="language-python">import math
from collections import defaultdict

class BigramLMFST:
    """
    Bigram language model represented as a Weighted FST.

    States = word histories (previous word)
    Arcs = word transitions with -log P(w | prev) weights

    In the tropical semiring:
      - weight of a path = sum of arc weights
      - best path = minimum weight path = most probable sequence
    """

    def __init__(self, smoothing=0.01):
        self.bigram_counts = defaultdict(lambda: defaultdict(int))
        self.unigram_counts = defaultdict(int)
        self.vocab = set()
        self.smoothing = smoothing
        self.total_count = 0

    def train(self, sentences):
        """Train bigram model from list of sentences (word lists).
        Each sentence is wrapped with &lt;s&gt; (start) and &lt;/s&gt; (end)
        boundary tokens to model sentence-initial and sentence-final
        transitions as explicit FST states."""
        for sentence in sentences:
            words = ['&lt;s&gt;'] + sentence + ['&lt;/s&gt;']
            for i in range(len(words)):
                self.unigram_counts[words[i]] += 1
                self.total_count += 1
                self.vocab.add(words[i])
                if i > 0:
                    self.bigram_counts[words[i-1]][words[i]] += 1

    def log_prob(self, word, prev_word):
        """Compute -log P(word | prev_word) with add-k smoothing."""
        bigram_count = self.bigram_counts[prev_word][word]
        prev_count = self.unigram_counts[prev_word]
        vocab_size = len(self.vocab)

        prob = (bigram_count + self.smoothing) / \
               (prev_count + self.smoothing * vocab_size)
        return -math.log(prob)

    def score_sequence(self, words):
        """Score a word sequence (lower = more probable)."""
        words = ['&lt;s&gt;'] + words + ['&lt;/s&gt;']
        total_weight = 0.0
        transitions = []

        for i in range(1, len(words)):
            weight = self.log_prob(words[i], words[i-1])
            transitions.append((words[i-1], words[i], weight))
            total_weight += weight

        return total_weight, transitions

    def print_fst_structure(self):
        """Display the WFST structure."""
        print("Bigram LM as WFST")
        print("=" * 60)
        print(f"States (word histories): {len(self.vocab)}")

        # Count non-zero arcs
        arc_count = 0
        for prev in self.bigram_counts:
            arc_count += len(self.bigram_counts[prev])
        print(f"Arcs (observed bigrams): {arc_count}")
        print(f"Vocabulary size: {len(self.vocab)}")
        print(f"Special tokens: &lt;s&gt; (start), &lt;/s&gt; (end)")

        # Show all transitions
        print(f"\nAll transitions (state --word/-logP--> state):")
        for prev in sorted(self.bigram_counts.keys()):
            for word in sorted(self.bigram_counts[prev].keys()):
                weight = self.log_prob(word, prev)
                count = self.bigram_counts[prev][word]
                print(f"  q[{prev}] --{word}/{weight:.3f}--> "
                      f"q[{word}]  (count={count})")


# Training data: simple sentences
training_sentences = [
    ['the', 'cat', 'sat', 'on', 'the', 'mat'],
    ['the', 'dog', 'sat', 'on', 'the', 'rug'],
    ['the', 'cat', 'chased', 'the', 'dog'],
    ['the', 'dog', 'chased', 'the', 'cat'],
    ['a', 'cat', 'sat', 'on', 'a', 'mat'],
    ['the', 'cat', 'is', 'on', 'the', 'mat'],
    ['a', 'dog', 'is', 'on', 'the', 'rug'],
    ['the', 'cat', 'and', 'the', 'dog', 'sat'],
]

# Build the bigram WFST
lm = BigramLMFST(smoothing=0.1)
lm.train(training_sentences)
lm.print_fst_structure()

# Score different word sequences
print("\n\nScoring word sequences (lower weight = more probable):")
print("=" * 60)

test_sequences = [
    ['the', 'cat', 'sat', 'on', 'the', 'mat'],
    ['the', 'dog', 'sat', 'on', 'the', 'rug'],
    ['the', 'mat', 'sat', 'on', 'the', 'cat'],  # unusual
    ['a', 'rug', 'chased', 'a', 'mat'],           # nonsense
]

for seq in test_sequences:
    weight, transitions = lm.score_sequence(seq)
    prob = math.exp(-weight)
    print(f"\n  Sequence: {' '.join(seq)}")
    print(f"  Total weight: {weight:.3f} (prob: {prob:.6f})")
    print(f"  Transitions:")
    for prev, word, w in transitions:
        print(f"    q[{prev}] --{word}/{w:.2f}--> q[{word}]")

print("\n\nKey: In the tropical semiring, shortest path = most probable.")
print("This is exactly how Kaldi and other ASR decoders work.")</code></pre>
                </div>

            </article>

            <!-- ==================== EXERCISES TAB ==================== -->
            <article class="article-content" id="exercises" style="display: none;">
                <h2>Exercises</h2>
                <p>Test your understanding of FST applications across speech recognition, machine translation, text normalization, and morphological analysis. Exercises range from designing simple rules to building complete systems.</p>

                <div class="exercise-list">

                    <!-- Easy -->
                    <h3 style="margin-top: 1rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">Easy</h3>

                    <!-- Exercise 1 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">1. Design G2P Rules for 20 Words</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Write grapheme-to-phoneme rules (letter pattern &rarr; phoneme) that correctly handle the following 20 common English words. Identify which rules conflict and how you would resolve the conflicts using context.</p>
                            <p><em>Words:</em> cat, phone, night, through, thought, though, tough, cough, knight, know, write, ship, church, ring, nation, city, gentle, gym, guess, ghost</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Rules needed (in priority order):</strong></p>
                                <ul>
                                    <li><code>ough</code> &rarr; various (this is the hardest part!):
                                        <ul>
                                            <li><code>ough#</code> (word-final after 'thr') &rarr; /uw/ ("through")</li>
                                            <li><code>ought</code> &rarr; /ao t/ ("thought")</li>
                                            <li><code>ough#</code> (word-final after 'th') &rarr; /ow/ ("though")</li>
                                            <li><code>ough#</code> (after 't') &rarr; /ah f/ ("tough", "cough")</li>
                                        </ul>
                                    </li>
                                    <li><code>tion</code> &rarr; /sh ah n/ ("nation")</li>
                                    <li><code>ight</code> &rarr; /ay t/ ("night", "knight")</li>
                                    <li><code>kn</code> (word-initial) &rarr; /n/ ("knight", "know")</li>
                                    <li><code>wr</code> (word-initial) &rarr; /r/ ("write")</li>
                                    <li><code>gh</code> (word-initial) &rarr; /g/ ("ghost")</li>
                                    <li><code>gh</code> (elsewhere) &rarr; silent</li>
                                    <li><code>gu</code> (before vowel) &rarr; /g/ ("guess")</li>
                                    <li><code>ph</code> &rarr; /f/ ("phone")</li>
                                    <li><code>ch</code> &rarr; /ch/ ("church")</li>
                                    <li><code>sh</code> &rarr; /sh/ ("ship")</li>
                                    <li><code>ng</code> &rarr; /ng/ ("ring")</li>
                                    <li><code>c</code> (before e,i,y) &rarr; /s/ ("city")</li>
                                    <li><code>c</code> (elsewhere) &rarr; /k/ ("cat")</li>
                                    <li><code>g</code> (before e,i,y) &rarr; /jh/ ("gentle", "gym")</li>
                                    <li><code>g</code> (elsewhere) &rarr; /g/ ("ghost")</li>
                                </ul>
                                <p><strong>Conflicts:</strong> The "ough" family is the most problematic &mdash; the same spelling has 4+ pronunciations. This is exactly the kind of ambiguity where FSTs struggle and need either (a) lexicon lookup for exceptions or (b) context-dependent rules that examine surrounding letters. In practice, G2P systems handle these as exceptions in the pronunciation dictionary rather than rules.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 2 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">2. List Three Speech Tasks Where FSTs Help</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Name three specific tasks in a speech processing pipeline (beyond basic recognition) where FSTs provide a significant advantage over purely neural approaches. For each, explain <em>why</em> the FST approach is preferred.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>1. Pronunciation lexicon construction and lookup:</strong> The lexicon mapping phones to words is a fixed, discrete mapping that must be 100% accurate. An FST guarantees that "k ae t" always maps to "cat" &mdash; it never hallucinates a different word. Neural models might produce plausible but incorrect words for rare phoneme sequences.</p>
                                <p><strong>2. Inverse text normalization (ITN) for ASR output:</strong> After recognition, spoken numbers, dates, and entities must be formatted correctly: "one hundred twenty three main street" &rarr; "123 Main Street". FSTs provide deterministic, auditable rules for this formatting. Errors in ITN (e.g., misformatting a phone number) have direct business impact, so guaranteed correctness is essential.</p>
                                <p><strong>3. Keyword spotting / wake word detection:</strong> Detecting specific trigger phrases ("Hey Siri", "OK Google") can be modeled as a small FST that accepts only the target phrase's phoneme sequences. The FST is tiny (fits on embedded devices), runs in real-time, has near-zero false positives on the defined vocabulary, and consumes minimal power &mdash; all critical for always-on listening.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 3 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">3. Identify Where Neural Nets Beat FSTs</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>For each of the following tasks, explain why a neural network is a better choice than an FST:</p>
                            <ol type="a">
                                <li>Sentiment analysis of product reviews</li>
                                <li>Machine translation of literary text</li>
                                <li>Predicting the next word in a sentence</li>
                            </ol>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>(a) Sentiment analysis:</strong> Sentiment depends on semantic meaning, which cannot be captured by pattern-matching rules. Consider: "This movie is not bad" (positive despite "not" and "bad"), "I expected more" (negative without any negative words), "The battery lasts forever" (positive) vs "The download takes forever" (negative). The same word ("forever") has opposite sentiment in different contexts. FSTs match surface patterns; neural models learn contextual semantics from labeled data.</p>
                                <p><strong>(b) Literary machine translation:</strong> Literary text involves metaphors, cultural references, style, and tone that require deep semantic understanding. "It was the best of times, it was the worst of times" requires understanding parallelism and literary structure. FSTs can handle word-level or phrase-level translation but cannot capture the long-range dependencies, stylistic choices, and semantic nuances required for literary translation. Neural encoder-decoder models with attention can capture these relationships across entire sentences and paragraphs.</p>
                                <p><strong>(c) Next-word prediction:</strong> Predicting the next word requires understanding context at multiple levels: syntactic (what part of speech is expected), semantic (what makes sense), and pragmatic (what is likely in this domain). "The cat sat on the ___" could be "mat", "chair", "roof", etc. &mdash; the probability distribution over thousands of words depends on context that may span many sentences. Neural language models learn these distributions from massive corpora; an FST would need an explicit probability for every possible context, which is intractable for vocabularies of 50K+ words.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Medium -->
                    <h3 style="margin-top: 2rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">Medium</h3>

                    <!-- Exercise 4 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">4. Implement Text Normalizer for Twitter Text</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Design (and optionally implement) a text normalization FST pipeline for informal Twitter/social media text. Your normalizer should handle:</p>
                            <ul>
                                <li>Repeated characters: "sooooo" &rarr; "so", "happyyy" &rarr; "happy"</li>
                                <li>Common abbreviations: "u" &rarr; "you", "r" &rarr; "are", "2" &rarr; "to/too"</li>
                                <li>Hashtag splitting: "#MachineLearning" &rarr; "Machine Learning"</li>
                                <li>@mention removal or replacement</li>
                            </ul>
                            <p>Describe the states and transitions for at least the repeated-character normalizer.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Repeated character normalizer FST:</strong></p>
                                <p>The key insight is that we need to collapse runs of 3+ identical characters to 2 or 1. The FST tracks "how many of the current character have we seen?"</p>
                                <p><strong>States:</strong></p>
                                <ul>
                                    <li><code>q0</code>: Initial state. No character buffered.</li>
                                    <li><code>q1(c)</code>: Seen one instance of character <code>c</code>. Output <code>c</code>.</li>
                                    <li><code>q2(c)</code>: Seen two instances of <code>c</code>. Output <code>c</code> (allow doubles like "happy").</li>
                                    <li><code>q3(c)</code>: Seen three+ instances of <code>c</code>. Output nothing (suppress extras).</li>
                                </ul>
                                <p><strong>Transitions:</strong></p>
                                <ul>
                                    <li><code>q0 --c:c--> q1(c)</code> for any character c</li>
                                    <li><code>q1(c) --c:c--> q2(c)</code> (second instance, output it)</li>
                                    <li><code>q2(c) --c:eps--> q3(c)</code> (third instance, suppress)</li>
                                    <li><code>q3(c) --c:eps--> q3(c)</code> (more instances, keep suppressing)</li>
                                    <li><code>q1(c) --d:d--> q1(d)</code> for d != c (new character)</li>
                                    <li><code>q2(c) --d:d--> q1(d)</code> (new character after double)</li>
                                    <li><code>q3(c) --d:d--> q1(d)</code> (new character after triple+)</li>
                                </ul>
                                <p><strong>Implementation sketch:</strong></p>
                                <div class="code-block">
                                    <div class="code-block-header">
                                        <span class="code-block-lang">python</span>
                                        <button class="code-block-copy">Copy</button>
                                    </div>
                                    <pre><code class="language-python">import re

def normalize_repeated(text):
    """Collapse 3+ repeated chars to max 2."""
    return re.sub(r'(.)\1{2,}', r'\1\1', text)

def normalize_abbreviations(text):
    """Expand common social media abbreviations."""
    abbrevs = {
        r'\bu\b': 'you', r'\br\b': 'are',
        r'\bur\b': 'your', r'\bpls\b': 'please',
        r'\bthx\b': 'thanks', r'\btbh\b': 'to be honest',
        r'\bimo\b': 'in my opinion',
    }
    for pat, repl in abbrevs.items():
        text = re.sub(pat, repl, text, flags=re.IGNORECASE)
    return text

def split_hashtag(text):
    """Split CamelCase hashtags."""
    def split(match):
        tag = match.group(1)
        words = re.sub(r'([a-z])([A-Z])', r'\1 \2', tag)
        return words
    return re.sub(r'#(\w+)', split, text)

# Test
test = "sooooo happyyy #MachineLearning u r amazing!!!"
result = normalize_repeated(test)
result = normalize_abbreviations(result)
result = split_hashtag(result)
print(f"'{test}' -> '{result}'")
# Output: 'soo happyy Machine Learning you are amazing!!!'</code></pre>
                                </div>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 5 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">5. Design Morphological FST for Spanish Verbs</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Design an FST that handles regular Spanish verb conjugation in the present tense for -ar verbs (e.g., "hablar" = to speak). The six forms are:</p>
                            <ul>
                                <li>yo habl<strong>o</strong> (I speak)</li>
                                <li>tu habl<strong>as</strong> (you speak)</li>
                                <li>el/ella habl<strong>a</strong> (he/she speaks)</li>
                                <li>nosotros habl<strong>amos</strong> (we speak)</li>
                                <li>vosotros habl<strong>ais</strong> (you pl. speak)</li>
                                <li>ellos habl<strong>an</strong> (they speak)</li>
                            </ul>
                            <p>Describe the FST states and transitions. How would you extend it to -er and -ir verbs?</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>FST Design:</strong> The transducer maps from analysis form (stem+features) to surface form.</p>
                                <p><strong>Input format:</strong> <code>hablar+PRES+1SG</code> (verb infinitive + tense + person/number)</p>
                                <p><strong>States and transitions:</strong></p>
                                <ul>
                                    <li><code>q_start</code>: Begin reading the stem. For each letter of the stem, output it unchanged: <code>q_start --c:c--> q_start</code></li>
                                    <li><code>q_start --a:eps--> q_ar</code>: Detected -ar conjugation class (consume 'a', don't output yet)</li>
                                    <li><code>q_ar --r:eps--> q_inf</code>: Consumed full infinitive ending 'ar'</li>
                                    <li><code>q_inf --+:eps--> q_tag</code>: Start reading morphological tags</li>
                                    <li><code>q_tag</code>: Read the tense+person tag and produce the appropriate suffix:</li>
                                </ul>
                                <p><strong>Suffix transitions from q_tag (for -ar present tense):</strong></p>
                                <ul>
                                    <li><code>PRES+1SG: output "o"</code></li>
                                    <li><code>PRES+2SG: output "as"</code></li>
                                    <li><code>PRES+3SG: output "a"</code></li>
                                    <li><code>PRES+1PL: output "amos"</code></li>
                                    <li><code>PRES+2PL: output "ais"</code> (simplified, ignoring accent)</li>
                                    <li><code>PRES+3PL: output "an"</code></li>
                                </ul>
                                <p><strong>Extension to -er and -ir verbs:</strong> Add parallel states <code>q_er</code> and <code>q_ir</code> that branch after detecting the verb class. The suffix tables differ: -er present gives o/es/e/emos/eis/en; -ir present gives o/es/e/imos/is/en. The stem-reading portion is shared across all conjugation classes, keeping the FST compact.</p>
                                <p>The beauty of the FST approach is that the stem is passed through unchanged (identity transduction), and only the suffix is transformed according to the conjugation class and morphological features.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 6 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">6. Estimate FST Size for 10K-Word Pronunciation Lexicon</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>A pronunciation lexicon for English contains 10,000 words. The average word has 7 phonemes and 1.2 pronunciations (some words have alternate pronunciations). Estimate:</p>
                            <ol type="a">
                                <li>The number of arcs in a naive (unoptimized) lexicon FST</li>
                                <li>The number of states after prefix sharing (trie structure)</li>
                                <li>The additional savings from suffix sharing (minimization)</li>
                            </ol>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>(a) Naive FST (no sharing):</strong></p>
                                <p>Each pronunciation is a separate linear path. Total pronunciations: $10{,}000 \times 1.2 = 12{,}000$. Each path has ~7 arcs (one per phoneme) + 1 arc for the word output label. Total arcs: $12{,}000 \times 8 = 96{,}000$. Total states: $12{,}000 \times 7 + 1 \approx 84{,}001$ (each path creates 7 intermediate states plus shared start state).</p>
                                <p><strong>(b) After prefix sharing (trie/determinization):</strong></p>
                                <p>Many words share phoneme prefixes: "cat" /k ae t/ and "cap" /k ae p/ share the prefix /k ae/. In a trie structure, shared prefixes reduce to single paths. Empirically, English pronunciation tries achieve about 60-70% state reduction. Estimated states: $84{,}000 \times 0.35 \approx 29{,}000$. Arcs reduce proportionally to about $96{,}000 \times 0.5 \approx 48{,}000$.</p>
                                <p><strong>(c) After suffix sharing (minimization):</strong></p>
                                <p>Many words also share phoneme <em>suffixes</em>: "cat" and "bat" both end in /ae t/. Minimization merges states with identical future behavior. For pronunciation lexicons, this typically gives an additional 30-50% reduction. Estimated final states: $29{,}000 \times 0.6 \approx 17{,}000$. Estimated final arcs: $48{,}000 \times 0.65 \approx 31{,}000$.</p>
                                <p><strong>Summary:</strong> From ~84K states (naive) to ~17K states (optimized) &mdash; roughly a 5x reduction. This is why FST optimization (determinization + minimization) is critical for practical ASR systems with 100K+ word vocabularies.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 7 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">7. Compare FST vs Regex vs Neural for Text Normalization</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Compare FSTs, regular expressions, and neural seq2seq models for text normalization along these dimensions:</p>
                            <ul>
                                <li>Accuracy on known patterns</li>
                                <li>Handling of unseen patterns</li>
                                <li>Composability</li>
                                <li>Debuggability</li>
                                <li>Development effort</li>
                            </ul>
                            <p>Which approach would you recommend for a production TTS system? Why?</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <table style="width: 100%; border-collapse: collapse; margin: 1rem 0;">
                                    <thead>
                                        <tr>
                                            <th style="border: 1px solid #ddd; padding: 8px; text-align: left;">Dimension</th>
                                            <th style="border: 1px solid #ddd; padding: 8px; text-align: left;">Regex</th>
                                            <th style="border: 1px solid #ddd; padding: 8px; text-align: left;">FST</th>
                                            <th style="border: 1px solid #ddd; padding: 8px; text-align: left;">Neural</th>
                                        </tr>
                                    </thead>
                                    <tbody>
                                        <tr>
                                            <td style="border: 1px solid #ddd; padding: 8px;">Accuracy (known)</td>
                                            <td style="border: 1px solid #ddd; padding: 8px;">100%</td>
                                            <td style="border: 1px solid #ddd; padding: 8px;">100%</td>
                                            <td style="border: 1px solid #ddd; padding: 8px;">~95-99%</td>
                                        </tr>
                                        <tr>
                                            <td style="border: 1px solid #ddd; padding: 8px;">Unseen patterns</td>
                                            <td style="border: 1px solid #ddd; padding: 8px;">Fails silently</td>
                                            <td style="border: 1px solid #ddd; padding: 8px;">Fails explicitly</td>
                                            <td style="border: 1px solid #ddd; padding: 8px;">Generalizes (may hallucinate)</td>
                                        </tr>
                                        <tr>
                                            <td style="border: 1px solid #ddd; padding: 8px;">Composability</td>
                                            <td style="border: 1px solid #ddd; padding: 8px;">Limited (ordered rules)</td>
                                            <td style="border: 1px solid #ddd; padding: 8px;">Excellent (formal composition)</td>
                                            <td style="border: 1px solid #ddd; padding: 8px;">None (monolithic)</td>
                                        </tr>
                                        <tr>
                                            <td style="border: 1px solid #ddd; padding: 8px;">Debuggability</td>
                                            <td style="border: 1px solid #ddd; padding: 8px;">Medium</td>
                                            <td style="border: 1px solid #ddd; padding: 8px;">Excellent (trace paths)</td>
                                            <td style="border: 1px solid #ddd; padding: 8px;">Poor (black box)</td>
                                        </tr>
                                        <tr>
                                            <td style="border: 1px solid #ddd; padding: 8px;">Dev effort</td>
                                            <td style="border: 1px solid #ddd; padding: 8px;">Low (quick prototyping)</td>
                                            <td style="border: 1px solid #ddd; padding: 8px;">Medium (FST tooling)</td>
                                            <td style="border: 1px solid #ddd; padding: 8px;">High (data + training)</td>
                                        </tr>
                                    </tbody>
                                </table>
                                <p><strong>Recommendation for production TTS:</strong> FSTs, with neural fallback.</p>
                                <p>Production TTS requires 100% accuracy on known patterns (a number read wrong is unacceptable), full debuggability (when a bug is reported, you must find and fix it immediately), and modularity (adding support for a new format shouldn't break existing rules). FSTs excel on all three.</p>
                                <p>For the long tail of unusual formats, a neural model can serve as a fallback, but its output should be reviewed and the common cases should be added as FST rules. This is the approach used by Google, Amazon, and Apple in their TTS systems.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Hard -->
                    <h3 style="margin-top: 2rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">Hard</h3>

                    <!-- Exercise 8 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">8. Build Cascaded WFST for Simple Speech Recognizer</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Design a simplified WFST cascade for a speech recognizer with the following components:</p>
                            <ul>
                                <li><strong>Vocabulary:</strong> {cat, bat, cap, map, mat}</li>
                                <li><strong>Phones:</strong> {k, b, m, ae, t, p}</li>
                                <li><strong>Lexicon (L):</strong> cat=/k ae t/, bat=/b ae t/, cap=/k ae p/, map=/m ae p/, mat=/m ae t/</li>
                                <li><strong>Grammar (G):</strong> bigram probabilities (make up reasonable ones)</li>
                            </ul>
                            <p>Write out the states and transitions for $L$ and $G$ separately, then describe what $L \circ G$ would look like. What is the total number of states in the composed transducer?</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Lexicon FST (L):</strong> Input = phones, Output = words</p>
                                <p>States: $q_0$ (start), and intermediate states for shared prefixes.</p>
                                <ul>
                                    <li>$q_0 \xrightarrow{k:\varepsilon} q_k$, $q_0 \xrightarrow{b:\varepsilon} q_b$, $q_0 \xrightarrow{m:\varepsilon} q_m$</li>
                                    <li>$q_k \xrightarrow{ae:\varepsilon} q_{kae}$, $q_b \xrightarrow{ae:\varepsilon} q_{bae}$, $q_m \xrightarrow{ae:\varepsilon} q_{mae}$</li>
                                    <li>$q_{kae} \xrightarrow{t:\text{cat}} q_0$, $q_{kae} \xrightarrow{p:\text{cap}} q_0$</li>
                                    <li>$q_{bae} \xrightarrow{t:\text{bat}} q_0$</li>
                                    <li>$q_{mae} \xrightarrow{p:\text{map}} q_0$, $q_{mae} \xrightarrow{t:\text{mat}} q_0$</li>
                                </ul>
                                <p>States in L: 7 ($q_0$, $q_k$, $q_b$, $q_m$, $q_{kae}$, $q_{bae}$, $q_{mae}$). Arcs: 10.</p>

                                <p><strong>Grammar WFST (G):</strong> Input = Output = words. Bigram probabilities:</p>
                                <ul>
                                    <li>States: $q_{\langle s \rangle}$ (start), $q_{\text{cat}}$, $q_{\text{bat}}$, $q_{\text{cap}}$, $q_{\text{map}}$, $q_{\text{mat}}$</li>
                                    <li>Example arcs (with weights $w = -\log P$):
                                        <ul>
                                            <li>$q_{\langle s \rangle} \xrightarrow{\text{cat}/0.7} q_{\text{cat}}$, $q_{\langle s \rangle} \xrightarrow{\text{bat}/1.6} q_{\text{bat}}$, ...</li>
                                            <li>$q_{\text{cat}} \xrightarrow{\text{mat}/0.5} q_{\text{mat}}$, $q_{\text{cat}} \xrightarrow{\text{bat}/1.2} q_{\text{bat}}$, ...</li>
                                        </ul>
                                    </li>
                                </ul>
                                <p>States in G: 6. Arcs: up to $6 \times 5 = 30$ (fully connected bigram).</p>

                                <p><strong>Composed $L \circ G$:</strong></p>
                                <p>States are pairs $(q_L, q_G)$: $7 \times 6 = 42$ maximum. In practice, many pairs are unreachable. The reachable states are those where $L$'s current state is consistent with $G$'s history. Since $L$ returns to $q_0$ after each word, the reachable composed states are approximately $(q_0, q_G)$ for each grammar state (6 states) plus the intermediate L states paired with each grammar state: $(q_k, q_G)$, $(q_{kae}, q_G)$, etc.</p>
                                <p>Estimated reachable states: $\sim 7 \times 6 = 42$, but after determinization and minimization, likely closer to 25-30. The composed transducer takes phone sequences as input and produces the most likely word sequence, with weights combining lexicon structure and language model probabilities.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 9 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">9. Design Context-Sensitive Rewrite Rules</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>In phonology, many rules are context-sensitive: a sound changes only in specific environments. Design FST rewrite rules for these English phonological processes:</p>
                            <ol type="a">
                                <li><strong>Flapping:</strong> /t/ and /d/ become a flap [D] between vowels (e.g., "butter" = /b ah D er/, "ladder" = /l ae D er/)</li>
                                <li><strong>Aspiration:</strong> Voiceless stops (/p/, /t/, /k/) are aspirated at the beginning of a stressed syllable (e.g., "pin" = [p_h ih n] but "spin" = [s p ih n])</li>
                                <li><strong>Vowel nasalization:</strong> Vowels become nasalized before nasal consonants (/m/, /n/, /ng/)</li>
                            </ol>
                            <p>For each rule, describe the FST states, the context-checking mechanism, and how the rule interacts with the others when composed.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>(a) Flapping rule: /t,d/ &rarr; [D] / V_V</strong></p>
                                <p>This rule says: /t/ or /d/ becomes [D] when between two vowels.</p>
                                <p><strong>States:</strong></p>
                                <ul>
                                    <li><code>q_default</code>: Haven't just seen a vowel. Pass through all symbols unchanged.</li>
                                    <li><code>q_after_vowel</code>: Just output a vowel. If next input is /t/ or /d/, buffer it (don't output yet).</li>
                                    <li><code>q_buffered_t(V)</code>: Seen vowel then /t/. If next input is a vowel, output [D]; otherwise output the buffered /t/.</li>
                                </ul>
                                <p><strong>Key transitions:</strong></p>
                                <ul>
                                    <li><code>q_default --V:V--> q_after_vowel</code> (vowel seen)</li>
                                    <li><code>q_after_vowel --t:eps--> q_buffered_t</code> (buffer /t/)</li>
                                    <li><code>q_buffered_t --V:D V--> q_after_vowel</code> (flap! then new vowel context)</li>
                                    <li><code>q_buffered_t --C:t C--> q_default</code> (no flap, output buffered /t/)</li>
                                </ul>

                                <p><strong>(b) Aspiration: voiceless stop &rarr; aspirated / #_ or onset of stressed syllable</strong></p>
                                <p>Simplified to word-initial position: track whether we're at word beginning.</p>
                                <ul>
                                    <li><code>q_word_start</code>: At word boundary. If /p,t,k/ &rarr; output aspirated version.</li>
                                    <li><code>q_after_s</code>: After /s/. If /p,t,k/ follows, do NOT aspirate.</li>
                                    <li><code>q_mid_word</code>: Mid-word, default.</li>
                                </ul>

                                <p><strong>(c) Vowel nasalization: V &rarr; V~ / _N</strong></p>
                                <p>Buffer vowels; if a nasal follows, output nasalized version.</p>
                                <ul>
                                    <li><code>q_default</code>: Normal processing.</li>
                                    <li><code>q_buffered_V</code>: Saw a vowel, buffered it. If next is nasal (/m,n,ng/), output nasalized vowel + nasal. If next is non-nasal, output original vowel.</li>
                                </ul>

                                <p><strong>Composition order matters:</strong> These rules should be composed as Flapping $\circ$ Aspiration $\circ$ Nasalization. Flapping must apply first because it changes /t/ to [D], which affects whether aspiration applies. Nasalization is independent and can be applied at any stage. The composed FST applies all three rules in a single pass.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 10 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">10. Propose a Hybrid FST-Neural Architecture</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Design a hybrid architecture for a text-to-speech (TTS) system that combines FSTs and neural networks. Your design should:</p>
                            <ol type="a">
                                <li>Identify which components should be FST-based and which should be neural.</li>
                                <li>Specify the interfaces between FST and neural components.</li>
                                <li>Explain how errors in one component propagate to others.</li>
                                <li>Describe a fallback strategy when the FST component encounters an input it cannot handle.</li>
                            </ol>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Proposed Hybrid TTS Architecture:</strong></p>

                                <p><strong>(a) Component assignment:</strong></p>
                                <ul>
                                    <li><strong>FST components:</strong>
                                        <ul>
                                            <li>Text normalization: numbers, dates, abbreviations, URLs, emails &rarr; spoken form</li>
                                            <li>Pronunciation lexicon: known words &rarr; phoneme sequences</li>
                                            <li>Phonological rules: allophonic variation, assimilation, liaison</li>
                                            <li>Prosody markup: punctuation/structure &rarr; break/emphasis tags</li>
                                        </ul>
                                    </li>
                                    <li><strong>Neural components:</strong>
                                        <ul>
                                            <li>G2P for OOV words: learned grapheme-to-phoneme for unseen words</li>
                                            <li>Disambiguation: "read" /riyd/ vs /rEd/ based on sentence context</li>
                                            <li>Prosody prediction: duration, pitch, energy from text features</li>
                                            <li>Acoustic model: phonemes + prosody &rarr; mel spectrograms (e.g., Tacotron)</li>
                                            <li>Vocoder: mel spectrograms &rarr; waveform (e.g., WaveNet, HiFi-GAN)</li>
                                        </ul>
                                    </li>
                                </ul>

                                <p><strong>(b) Interfaces:</strong></p>
                                <ul>
                                    <li>FST text normalizer &rarr; normalized text string &rarr; Neural disambiguator</li>
                                    <li>FST lexicon + Neural G2P &rarr; phoneme sequence with stress marks &rarr; FST phonological rules</li>
                                    <li>FST phonological rules &rarr; surface phonemes + prosody tags &rarr; Neural acoustic model</li>
                                    <li>Each interface passes a well-defined symbolic representation (strings, phoneme sequences, tag sequences)</li>
                                </ul>

                                <p><strong>(c) Error propagation:</strong></p>
                                <ul>
                                    <li>Text normalization errors (e.g., "$100" &rarr; "one zero zero" instead of "one hundred") propagate through the entire pipeline and result in incorrect speech. This is why FSTs are preferred here: they never make this kind of error on known patterns.</li>
                                    <li>G2P errors (wrong pronunciation) produce intelligible but mispronounced words. These are contained to the word level and don't affect surrounding words.</li>
                                    <li>Disambiguation errors (wrong homograph reading) produce the wrong word entirely. The acoustic model faithfully renders the wrong pronunciation.</li>
                                    <li>Acoustic model errors produce artifacts (muffled speech, wrong prosody) but the words are usually still intelligible.</li>
                                </ul>

                                <p><strong>(d) Fallback strategy:</strong></p>
                                <ul>
                                    <li><strong>Unknown text format:</strong> If the FST normalizer encounters an unrecognized pattern (e.g., an unusual date format "XVI.II.MMXXVI"), fall back to a neural seq2seq normalizer trained on diverse formats. Log the input for future FST rule development.</li>
                                    <li><strong>OOV words:</strong> If the pronunciation lexicon doesn't contain the word, use the neural G2P model. If G2P confidence is low, spell out the word letter by letter.</li>
                                    <li><strong>Confidence monitoring:</strong> Each neural component outputs a confidence score. If below threshold, use a conservative fallback (slower speech rate, neutral prosody, spelled-out pronunciation).</li>
                                    <li><strong>Continuous improvement:</strong> All fallback events are logged. High-frequency fallbacks are converted to FST rules in the next release cycle, ensuring the system gets more deterministic over time.</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                </div>
            </article>

        </main>

        <!-- TOC (Right Side) -->
        <aside class="toc-container">
            <h4 class="toc-title">Contents</h4>
            <nav class="toc-list">
                <a href="#speech-recognition" class="toc-link">FSTs in Speech Recognition</a>
                <a href="#pronunciation-lexicon" class="toc-link">Pronunciation Lexicon FSTs</a>
                <a href="#language-models" class="toc-link">Language Model FSTs</a>
                <a href="#machine-translation" class="toc-link">FSTs in Machine Translation</a>
                <a href="#text-normalization" class="toc-link">Text Normalization</a>
                <a href="#morphological-analysis" class="toc-link">Morphological Analysis</a>
                <a href="#when-not-to-use" class="toc-link">When NOT to Use FSTs</a>
                <a href="#summary" class="toc-link">Summary</a>
            </nav>
        </aside>
    </div>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-brand">
                    <span class="logo-symbol">&nabla;</span>
                    <span>ML Fundamentals</span>
                </div>
                <p class="footer-tagline">Deep understanding through first principles.</p>
            </div>
            <div class="footer-links">
                <a href="../../../index.html">Home</a>
                <a href="https://github.com/ml-entropy/ml-entropy.github.io" target="_blank">GitHub</a>
            </div>
        </div>
    </footer>

    <!-- Scripts -->
    <script src="../../../js/main.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // KaTeX Rendering
            if (typeof renderMathInElement === 'function') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }

            // Tab Switching Logic
            const tabs = document.querySelectorAll('.tutorial-tab');
            const articles = document.querySelectorAll('.article-content');

            function switchTab(targetId) {
                if (!targetId || targetId === '#') targetId = '#theory';

                tabs.forEach(tab => {
                    if (tab.getAttribute('href') === targetId) {
                        tab.classList.add('active');
                    } else {
                        tab.classList.remove('active');
                    }
                });

                articles.forEach(article => {
                    const articleId = '#' + article.id;
                    if (articleId === targetId) {
                        article.style.display = 'block';
                    } else {
                        article.style.display = 'none';
                    }
                });

                if (typeof renderMathInElement === 'function') {
                    renderMathInElement(document.body, {
                        delimiters: [
                            {left: '$$', right: '$$', display: true},
                            {left: '$', right: '$', display: false},
                            {left: '\\[', right: '\\]', display: true},
                            {left: '\\(', right: '\\)', display: false}
                        ],
                        throwOnError: false
                    });
                }

                const toc = document.querySelector('.toc-container');
                if (toc) {
                    if (targetId === '#theory') {
                        toc.style.display = 'block';
                        setTimeout(() => toc.classList.add('visible'), 100);
                    } else {
                        toc.classList.remove('visible');
                        setTimeout(() => toc.style.display = 'none', 300);
                    }
                }
            }

            tabs.forEach(tab => {
                tab.addEventListener('click', (e) => {
                    e.preventDefault();
                    const targetId = tab.getAttribute('href');
                    history.pushState(null, null, targetId);
                    switchTab(targetId);
                });
            });

            window.addEventListener('popstate', () => {
                switchTab(window.location.hash);
            });

            switchTab(window.location.hash);
        });
    </script>
</body>
</html>
