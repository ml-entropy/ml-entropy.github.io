<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Inductive Bias | ML Fundamentals</title>
    <meta name="description" content="Understanding why every learning algorithm must make assumptions, and how these assumptions determine what can be learned.">

    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&family=Playfair+Display:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}, {left: '\\\\[', right: '\\\\]', display: true}, {left: '\\\\(', right: '\\\\)', display: false}], throwOnError: false});"></script>

    <!-- Styles -->
    <link rel="stylesheet" href="../../../css/main.css">
    <link rel="stylesheet" href="../../../css/components.css">
    <link rel="stylesheet" href="../../../css/sidebar.css">

    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>&#x221E;</text></svg>">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar" id="navbar">
        <div class="nav-container">
            <a href="../../../index.html" class="nav-logo">
                <span class="logo-symbol">&nabla;</span>
                <span class="logo-text">ML Fundamentals</span>
            </a>

            <button class="nav-toggle" id="navToggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>

            <div class="nav-menu" id="navMenu">
                <div class="nav-links">
                    <a href="../../../tutorials/ml/index.html" class="nav-link active">Machine Learning</a>
                    <a href="../../../tutorials/linear-algebra/index.html" class="nav-link">Linear Algebra</a>
                    <a href="../../../tutorials/calculus/index.html" class="nav-link">Calculus</a>
                    <a href="../../../tutorials/physics/index.html" class="nav-link">Physics</a>
                    <a href="../../../index.html#philosophy" class="nav-link">Philosophy</a>
                    <a href="../../../index.html#roadmap" class="nav-link">Roadmap</a>
                    <a href="https://github.com/ml-entropy/ml-entropy.github.io" class="nav-link" target="_blank">GitHub</a>
                </div>

                <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
                    <svg class="sun-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="5"/>
                        <path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"/>
                    </svg>
                    <svg class="moon-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                    </svg>
                </button>
            </div>
        </div>
    </nav>

    <!-- Tutorial Header -->
    <header class="tutorial-content-header">
        <div class="container">
            <nav class="breadcrumb">
                <a href="../../../index.html">Home</a>
                <span class="breadcrumb-separator">&rarr;</span>
                <a href="../index.html">Machine Learning</a>
                <span class="breadcrumb-separator">&rarr;</span>
                <span>Inductive Bias</span>
            </nav>

            <div class="tutorial-tabs">
                <a href="#theory" class="tutorial-tab active">Theory</a>
                <a href="#code" class="tutorial-tab">Code</a>
                <a href="#exercises" class="tutorial-tab">Exercises</a>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <div class="tutorial-wrapper">

        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <aside class="tutorial-sidebar">
            <div class="sidebar-section">
                <h3 class="sidebar-section-title">Machine Learning</h3>
                <nav class="sidebar-nav">
                        <a href="../00-probability/index.html" class="sidebar-link">00. Probability Foundations</a>
                    <a href="../01-logarithms/index.html" class="sidebar-link">01. Why Logarithms?</a>
                    <a href="../02-combinatorics/index.html" class="sidebar-link">02. Combinatorics</a>
                    <a href="../03-distributions/index.html" class="sidebar-link">03. Normal Distributions</a>
                    <a href="../04-entropy/index.html" class="sidebar-link">04. Entropy Fundamentals</a>
                    <a href="../05-cross-entropy/index.html" class="sidebar-link">05. Cross-Entropy</a>
                    <a href="../06-kl-divergence/index.html" class="sidebar-link">06. KL Divergence</a>
                    <a href="../07-entropy-connections/index.html" class="sidebar-link">07. Entropy Connections</a>
                    <a href="../08-backpropagation/index.html" class="sidebar-link">08. Backpropagation</a>
                    <a href="../09-regularization/index.html" class="sidebar-link">09. Regularization</a>
                    <a href="../10-batch-normalization/index.html" class="sidebar-link">10. Batch Normalization</a>
                    <a href="../11-learning-rate/index.html" class="sidebar-link">11. Learning Rate</a>
                    <a href="../12-cnn/index.html" class="sidebar-link">12. CNNs</a>
                    <a href="../13-rnn/index.html" class="sidebar-link">13. RNNs</a>
                    <a href="../14-rate-distortion/index.html" class="sidebar-link">14. Rate-Distortion Theory</a>
                    <a href="../15-autoencoder/index.html" class="sidebar-link">15. Autoencoders</a>
                    <a href="../16-variational-inference/index.html" class="sidebar-link">16. Variational Inference</a>
                    <a href="../17-vae/index.html" class="sidebar-link">17. VAE</a>
                    <a href="../18-inductive-bias/index.html" class="sidebar-link active">18. Inductive Bias</a>
                    <a href="../19-architectural-biases/index.html" class="sidebar-link">19. Architectural Biases</a>
                    <a href="../20-designing-biases/index.html" class="sidebar-link">20. Designing Biases</a>
                    <a href="../21-fst-fundamentals/index.html" class="sidebar-link">21. FST Fundamentals</a>
                    <a href="../22-weighted-fsts/index.html" class="sidebar-link">22. Weighted FSTs</a>
                    <a href="../23-fst-libraries/index.html" class="sidebar-link">23. FST Libraries</a>
                    <a href="../24-fst-applications/index.html" class="sidebar-link">24. FST Applications</a>
                    <a href="../25-neural-symbolic/index.html" class="sidebar-link">25. Neural-Symbolic Hybrids</a>
                    <a href="../26-sequence-alignment/index.html" class="sidebar-link">26. Sequence Alignment</a>
                    <a href="../27-mas-algorithm/index.html" class="sidebar-link">27. MAS Algorithm</a>
                    <a href="../28-forced-alignment/index.html" class="sidebar-link">28. Forced Alignment & MFA</a>
                    <a href="../29-tts-fundamentals/index.html" class="sidebar-link">29. TTS Fundamentals</a>
                    <a href="../30-neural-vocoders/index.html" class="sidebar-link">30. Neural Vocoders</a>
                    <a href="../31-tacotron/index.html" class="sidebar-link">31. Tacotron & Attention TTS</a>
                    <a href="../32-fastspeech/index.html" class="sidebar-link">32. FastSpeech & Non-AR TTS</a>
                    <a href="../33-glow-tts/index.html" class="sidebar-link">33. Glow-TTS & Flows</a>
                    <a href="../34-vits/index.html" class="sidebar-link">34. VITS: End-to-End TTS</a>
                    <a href="../35-bilingual-tts/index.html" class="sidebar-link">35. Bilingual TTS: RU+KY</a>
                </nav>
            </div>
            
            <div class="sidebar-section" style="margin-top: 2rem;">
                <h3 class="sidebar-section-title">Related Subjects</h3>
                <nav class="sidebar-nav">
                        <a href="../../linear-algebra/index.html" class="sidebar-link">Linear Algebra</a>
                    <a href="../../calculus/index.html" class="sidebar-link">Calculus</a>
                    <a href="../../physics/index.html" class="sidebar-link">Physics</a>
                </nav>
            </div>
        </aside>

        <!-- Main Article -->
        <main class="tutorial-main">

            <!-- ==================== THEORY TAB ==================== -->
            <article class="article-content" id="theory">

                <!-- Section 1: What is Inductive Bias? -->
                <h2 id="what-is-inductive-bias">What is Inductive Bias?</h2>

                <p>
                    Every machine learning model makes assumptions about the data it will encounter. These assumptions &mdash; sometimes explicit, sometimes hidden deep in the architecture &mdash; are what allow a model to generalize from a finite training set to unseen inputs. Without them, learning is impossible.
                </p>

                <div class="definition-box">
                    <div class="box-title">Inductive Bias</div>
                    <p>
                        First, the <strong>hypothesis space</strong> $\mathcal{H}$ is the set of all functions a learning algorithm can consider. For linear regression, $\mathcal{H} = \{x \mapsto w^Tx + b : w \in \mathbb{R}^d, b \in \mathbb{R}\}$ — only linear functions. For a neural network with a fixed architecture, $\mathcal{H}$ is the set of all input-output mappings achievable by varying its weights. The choice of $\mathcal{H}$ is itself a powerful assumption.
                    </p>
                    <p>
                        The <strong>inductive bias</strong> is the set of assumptions that a learning algorithm uses to predict outputs for inputs it has not yet encountered. More formally, given a hypothesis space $\mathcal{H}$ and a learning algorithm $A$, the inductive bias is the set of additional assumptions needed to derive predictions from $A$ beyond what is logically entailed by the training data alone.
                    </p>
                </div>

                <p>
                    <strong>Intuition:</strong> Consider fitting a curve to 5 points on a line. Why do we predict the 6th point lies on the same line? Nothing in the data forces this &mdash; the points could be the beginning of a polynomial of degree 100. Our assumption of linearity IS an inductive bias.
                </p>

                <div class="math-block">
                    $$\text{Given training data } D = \{(x_i, y_i)\}_{i=1}^n, \text{ a learner selects hypothesis } h \in \mathcal{H} \text{ based on both } D \text{ and its inductive bias } \mathcal{B}\text{:}$$
                </div>
                <div class="math-block">
                    $$h^* = \arg\min_{h \in \mathcal{H}} \mathcal{L}(h, D) \text{ subject to preferences from } \mathcal{B}$$
                </div>

                <!-- Section 2: No Free Lunch -->
                <h2 id="no-free-lunch">The No Free Lunch Theorem</h2>

                <p>
                    Is there a single learning algorithm that works best on every problem? The No Free Lunch theorem answers this definitively: <strong>no</strong>. Any algorithm's advantage on one class of problems is exactly offset by its disadvantage on another.
                </p>

                <div class="definition-box">
                    <div class="box-title">No Free Lunch Theorem (Wolpert &amp; Macready, 1997)</div>
                    <p>
                        For any two learning algorithms $A_1$ and $A_2$, when averaged over ALL possible target functions $f: \mathcal{X} \to \mathcal{Y}$:
                    </p>
                    <div class="math-block">
                        $$\sum_f \mathcal{L}(A_1, f) = \sum_f \mathcal{L}(A_2, f)$$
                    </div>
                    <p style="margin-bottom: 0;">
                        No algorithm is universally better than any other.
                    </p>
                </div>

                <div class="math-derivation">
                    <div class="math-derivation-title">Proof Sketch</div>

                    <div class="math-step">
                        <div class="math-step-number">1</div>
                        <div class="math-step-content">
                            <p>Consider all possible binary functions $f: \{0,1\}^d \to \{0,1\}$. There are $2^{2^d}$ such functions. The crucial assumption: we place a <strong>uniform distribution</strong> over these functions — every function is equally likely to be the true target. This is the key to the theorem, and also its main limitation: real-world problems never have this property. In practice, smooth functions are far more likely than random ones, and structured data rules out most of the $2^{2^d}$ possibilities.</p>
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">2</div>
                        <div class="math-step-content">
                            <p>For any fixed training set, each unseen input has equal probability of being 0 or 1 under a uniform prior over functions.</p>
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">3</div>
                        <div class="math-step-content">
                            <p>Therefore, the expected loss on unseen data is the same for ANY prediction strategy:</p>
                            <div class="math-block">
                                $$\mathbb{E}_f[\mathcal{L}] = \frac{1}{2} \text{ for binary classification under uniform prior.}$$
                            </div>
                        </div>
                    </div>
                </div>

                <div class="note-box">
                    <div class="box-title">Implication for Practice</div>
                    <p style="margin-bottom: 0;">
                        NFL does NOT mean all algorithms are equal in practice. It means: if algorithm A beats B on some problems, B must beat A on others. The key insight: we never face uniform distributions over all possible functions. Real-world data has structure, and our inductive biases should match that structure.
                    </p>
                </div>

                <!-- Section 3: Bias-Variance Revisited -->
                <h2 id="bias-variance-revisited">Bias-Variance Tradeoff Revisited</h2>

                <p>
                    Inductive bias is intimately connected to the bias-variance decomposition, one of the most fundamental results in statistical learning theory. The choice of inductive bias directly determines where a model sits on the bias-variance spectrum.
                </p>

                <div class="math-block">
                    $$\text{MSE}(h) = \underbrace{\text{Bias}^2(h)}_{\text{systematic error}} + \underbrace{\text{Var}(h)}_{\text{sensitivity to data}} + \underbrace{\sigma^2}_{\text{irreducible noise}}$$
                </div>

                <div class="math-derivation">
                    <div class="math-derivation-title">How Inductive Bias Affects the Tradeoff</div>

                    <div class="math-step">
                        <div class="math-step-number">1</div>
                        <div class="math-step-content">
                            <p>Strong inductive bias $\to$ smaller hypothesis space $\mathcal{H}$</p>
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">2</div>
                        <div class="math-step-content">
                            <p>Smaller $\mathcal{H}$ $\to$ higher bias (may miss true function) but lower variance (more stable predictions)</p>
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">3</div>
                        <div class="math-step-content">
                            <p>The optimal bias matches the structure of the data: $\text{Bias}^2 \downarrow$ without $\text{Var} \uparrow$</p>
                        </div>
                    </div>
                </div>

                <div class="note-box">
                    <div class="box-title">Connection to Tutorial 09 (Regularization)</div>
                    <p>
                        Regularization is a form of inductive bias that doesn't restrict $\mathcal{H}$ directly but adds a preference for simpler hypotheses. L2 regularization, for example, biases toward small weights &mdash; equivalent to assuming the true function is smooth.
                    </p>
                    <p style="margin-bottom: 0;">
                        <strong>How L2 changes the landscape without shrinking $\mathcal{H}$:</strong> The regularized loss $\mathcal{L}(h) + \lambda\|w\|^2$ doesn't remove any hypothesis from the search space — every function in $\mathcal{H}$ is still representable. Instead, it reshapes the cost landscape so that high-norm solutions sit in "valleys" with steep walls, making them harder for gradient descent to find. The hypothesis space is the same; the optimization terrain is different. This is the essence of preference bias.
                    </p>
                </div>

                <!-- Section 4: Taxonomy -->
                <h2 id="taxonomy">Taxonomy of Inductive Biases</h2>

                <p>
                    Inductive biases come in two fundamental flavors, distinguished by <em>how</em> they constrain learning.
                </p>

                <div class="definition-box">
                    <div class="box-title">Language Bias (Restriction Bias)</div>
                    <p style="margin-bottom: 0;">
                        The hypothesis space $\mathcal{H}$ itself is the bias. By choosing $\mathcal{H}$, we exclude functions outside it. Example: linear regression restricts to $\mathcal{H} = \{w^T x + b\}$.
                    </p>
                </div>

                <p>
                    These two types of bias are not mutually exclusive — most practical systems use both. Language bias is "hard": functions outside $\mathcal{H}$ are impossible to represent, no matter how much data you have. Preference bias is "soft": disfavored solutions are still learnable, just harder to reach. A neural network with L2 regularization, for example, has language bias (the architecture limits $\mathcal{H}$) and preference bias (regularization favors small weights within that $\mathcal{H}$).
                </p>

                <div class="definition-box">
                    <div class="box-title">Preference Bias (Search Bias)</div>
                    <p>
                        Given $\mathcal{H}$, the algorithm prefers certain hypotheses over others. Example: gradient descent with small initialization prefers functions of low complexity.
                    </p>
                    <p style="margin-bottom: 0;">
                        <strong>Implicit regularization</strong> is preference bias that arises from the optimization algorithm itself, not from an explicit penalty term. For example, gradient descent on an underdetermined linear system converges to the minimum-norm solution $w^* = \arg\min_w \|w\| \text{ s.t. } Xw = y$, even though no regularizer was added to the loss. Similarly, SGD with small learning rate and small initialization biases neural networks toward low-rank, low-frequency functions. The optimizer is a source of inductive bias, not just a tool for finding minima.
                    </p>
                </div>

                <table style="width: 100%; border-collapse: collapse; margin: 1.5rem 0;">
                    <thead>
                        <tr style="border-bottom: 2px solid var(--color-border);">
                            <th style="text-align: left; padding: 0.75rem;">Type</th>
                            <th style="text-align: left; padding: 0.75rem;">What it constrains</th>
                            <th style="text-align: left; padding: 0.75rem;">Example Algorithm</th>
                            <th style="text-align: left; padding: 0.75rem;">Example Bias</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.75rem;">Language</td>
                            <td style="padding: 0.75rem;">Hypothesis space</td>
                            <td style="padding: 0.75rem;">Linear SVM</td>
                            <td style="padding: 0.75rem;">Linear decision boundaries</td>
                        </tr>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.75rem;">Language</td>
                            <td style="padding: 0.75rem;">Hypothesis space</td>
                            <td style="padding: 0.75rem;">Decision tree</td>
                            <td style="padding: 0.75rem;">Axis-aligned splits</td>
                        </tr>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.75rem;">Preference</td>
                            <td style="padding: 0.75rem;">Search strategy</td>
                            <td style="padding: 0.75rem;">L2-regularized NN</td>
                            <td style="padding: 0.75rem;">Small weights (smooth functions)</td>
                        </tr>
                        <tr>
                            <td style="padding: 0.75rem;">Preference</td>
                            <td style="padding: 0.75rem;">Search strategy</td>
                            <td style="padding: 0.75rem;">SGD with small init</td>
                            <td style="padding: 0.75rem;">Low-rank solutions</td>
                        </tr>
                    </tbody>
                </table>

                <!-- Section 5: Historical Context -->
                <h2 id="historical-context">Historical Context</h2>

                <h3>Occam's Razor</h3>
                <p>
                    William of Ockham (c. 1287&ndash;1347) &mdash; <em>"Entities should not be multiplied beyond necessity."</em> The oldest inductive bias: prefer simpler explanations. This principle has guided scientific reasoning for seven centuries and remains the philosophical foundation for regularization, model selection, and the Minimum Description Length principle in machine learning.
                </p>

                <h3>Mitchell (1980)</h3>
                <p>
                    Tom Mitchell formally defined inductive bias in <em>"The Need for Biases in Learning Generalizations,"</em> showing that without bias, a learning system cannot generalize beyond the training data at all. His key result: a learner that makes no assumptions can only memorize &mdash; it has no basis for preferring one generalization over another.
                </p>

                <h3>Wolpert &amp; Macready (1997)</h3>
                <p>
                    The No Free Lunch theorems provided the mathematical proof that bias is not optional &mdash; it is essential. Every successful learner must assume something about the structure of the problem. The theorems transformed inductive bias from a philosophical preference into a mathematical necessity.
                </p>

                <!-- Section 6: Classical ML Biases -->
                <h2 id="classical-ml-biases">Inductive Bias in Classical ML</h2>

                <p>
                    Each classical machine learning algorithm encodes specific assumptions about the structure of the data. Understanding these biases is essential for choosing the right tool for a given problem.
                </p>

                <table style="width: 100%; border-collapse: collapse; margin: 1.5rem 0;">
                    <thead>
                        <tr style="border-bottom: 2px solid var(--color-border);">
                            <th style="text-align: left; padding: 0.75rem;">Algorithm</th>
                            <th style="text-align: left; padding: 0.75rem;">Key Inductive Bias</th>
                            <th style="text-align: left; padding: 0.75rem;">Implication</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.75rem;">Linear Regression</td>
                            <td style="padding: 0.75rem;">Linearity</td>
                            <td style="padding: 0.75rem;">Output is linear combination of features</td>
                        </tr>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.75rem;">Decision Trees</td>
                            <td style="padding: 0.75rem;">Axis-aligned splits</td>
                            <td style="padding: 0.75rem;">Features are independently informative</td>
                        </tr>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.75rem;">k-NN</td>
                            <td style="padding: 0.75rem;">Locality (smoothness)</td>
                            <td style="padding: 0.75rem;">Similar inputs have similar outputs</td>
                        </tr>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.75rem;">SVM (linear)</td>
                            <td style="padding: 0.75rem;">Maximum margin</td>
                            <td style="padding: 0.75rem;">Best boundary maximizes gap between classes</td>
                        </tr>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.75rem;">SVM (RBF)</td>
                            <td style="padding: 0.75rem;">Smoothness + locality</td>
                            <td style="padding: 0.75rem;">Decision boundary is smooth, influenced by nearby points</td>
                        </tr>
                        <tr>
                            <td style="padding: 0.75rem;">Naive Bayes</td>
                            <td style="padding: 0.75rem;">Feature independence</td>
                            <td style="padding: 0.75rem;">Features are conditionally independent given class</td>
                        </tr>
                    </tbody>
                </table>

                <!-- Section 7: Summary -->
                <h2 id="summary">Summary</h2>

                <ul>
                    <li><strong>Inductive bias is unavoidable:</strong> Every learning algorithm must make assumptions to generalize beyond training data.</li>
                    <li><strong>No Free Lunch:</strong> No algorithm is universally best &mdash; advantage on one problem class implies disadvantage on another.</li>
                    <li><strong>Bias-variance connection:</strong> Stronger bias reduces variance but increases systematic error. The sweet spot depends on the data.</li>
                    <li><strong>Two types:</strong> Language bias restricts the hypothesis space; preference bias guides the search within it.</li>
                    <li><strong>Historical foundations:</strong> From Occam's Razor to Mitchell's formal definition to the NFL theorems, the necessity of bias is both philosophically and mathematically grounded.</li>
                    <li><strong>Matching bias to data:</strong> The art of machine learning is choosing biases that match the structure of your problem.</li>
                </ul>

                <div class="note-box">
                    <div class="box-title">Looking Ahead: Tutorial 18</div>
                    <p style="margin-bottom: 0;">
                        Now that we understand what inductive bias is and why it matters, the next tutorial examines the most powerful form of bias in deep learning: architectural choice. We'll see how CNNs, RNNs, Transformers, and GNNs each encode different assumptions through their structure.
                    </p>
                </div>

                <!-- Navigation -->
                <div class="tutorial-footer-summary" style="margin: 3rem 0; padding: 2rem; background: #f8f9fa; border-radius: 8px; border-left: 4px solid #3b82f6;">
                    <h1>17. Inductive Bias</h1>
                    <p class="lead">
                        Understanding why every learning algorithm must make assumptions, and how these assumptions determine what can be learned.
                    </p>
                </div>
                <div class="tutorial-nav">
                    <a href="../17-vae/index.html" class="tutorial-nav-link prev">
                        <span class="nav-label">Previous</span>
                        <span class="nav-title">&larr; VAE</span>
                    </a>
                    <a href="../19-architectural-biases/index.html" class="tutorial-nav-link next">
                        <span class="nav-label">Next</span>
                        <span class="nav-title">Architectural Biases &rarr;</span>
                    </a>
                </div>

            </article>

            <!-- ==================== CODE TAB ==================== -->
            <article class="article-content" id="code" style="display: none;">
                <h2>Python Code Examples</h2>
                <p>Three code examples demonstrating inductive bias concepts: bias-variance tradeoff with polynomial fitting, the No Free Lunch theorem verified empirically, and decision boundary visualization across classifiers with different biases.</p>

                <!-- Code Example 1 -->
                <h3>1. Bias-Variance Demo: Polynomial Fitting</h3>
                <p>
                    Fit polynomials of varying degree to noisy sine data and quantify the bias-variance tradeoff through repeated experiments.
                </p>

                <div class="code-block">
                    <div class="code-block-header">
                        <span class="code-block-lang">python</span>
                        <button class="code-block-copy">Copy</button>
                    </div>
                    <pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

# Generate noisy sine data
np.random.seed(42)
n_samples = 20
X = np.sort(np.random.uniform(0, 2 * np.pi, n_samples))
y_true = np.sin(X)
y = y_true + np.random.normal(0, 0.3, n_samples)

# Fit polynomials of different degrees
degrees = [1, 3, 9, 15]
X_plot = np.linspace(0, 2 * np.pi, 200)

fig, axes = plt.subplots(2, 2, figsize=(12, 10))
axes = axes.ravel()

for i, deg in enumerate(degrees):
    coeffs = np.polyfit(X, y, deg)
    y_pred = np.polyval(coeffs, X_plot)

    axes[i].scatter(X, y, c='steelblue', s=40, zorder=5, label='Training data')
    axes[i].plot(X_plot, np.sin(X_plot), 'g--', alpha=0.5, label='True function')
    axes[i].plot(X_plot, y_pred, 'r-', linewidth=2, label=f'Degree {deg} fit')
    axes[i].set_title(f'Polynomial Degree {deg}', fontsize=13)
    axes[i].set_ylim(-2, 2)
    axes[i].legend(fontsize=9)
    axes[i].grid(True, alpha=0.3)

plt.suptitle('Inductive Bias: Polynomial Degree Controls Hypothesis Space', fontsize=14)
plt.tight_layout()
plt.savefig('bias_variance_demo.png', dpi=150)
plt.show()

# Quantify bias-variance with repeated experiments
n_experiments = 200
predictions = {deg: [] for deg in degrees}

for _ in range(n_experiments):
    X_exp = np.sort(np.random.uniform(0, 2 * np.pi, n_samples))
    y_exp = np.sin(X_exp) + np.random.normal(0, 0.3, n_samples)

    for deg in degrees:
        coeffs = np.polyfit(X_exp, y_exp, deg)
        predictions[deg].append(np.polyval(coeffs, X_plot))

print(f"{'Degree':<10} {'Bias\u00b2':<12} {'Variance':<12} {'Total MSE':<12}")
print("-" * 46)
for deg in degrees:
    preds = np.array(predictions[deg])
    mean_pred = preds.mean(axis=0)
    bias_sq = np.mean((mean_pred - np.sin(X_plot))**2)
    variance = np.mean(preds.var(axis=0))
    print(f"{deg:<10} {bias_sq:<12.4f} {variance:<12.4f} {bias_sq + variance:<12.4f}")</code></pre>
                </div>

                <!-- Code Example 2 -->
                <h3>2. No Free Lunch Demo</h3>
                <p>
                    Verify the NFL theorem by evaluating two predictors over ALL possible binary functions on a small input space.
                </p>

                <div class="code-block">
                    <div class="code-block-header">
                        <span class="code-block-lang">python</span>
                        <button class="code-block-copy">Copy</button>
                    </div>
                    <pre><code class="language-python">import numpy as np
from itertools import product

def majority_vote_predictor(train_X, train_y, test_x):
    """Predict based on majority class in training data."""
    return int(np.mean(train_y) >= 0.5)

def nearest_neighbor_predictor(train_X, train_y, test_x):
    """Predict based on nearest training point (Hamming distance)."""
    distances = np.sum(train_X != test_x, axis=1)
    return train_y[np.argmin(distances)]

# Setup: 3-bit input space, binary output
d = 3
all_inputs = np.array(list(product([0, 1], repeat=d)))  # 8 possible inputs

# Use first 4 as training, last 4 as test
train_idx = [0, 1, 2, 3]
test_idx = [4, 5, 6, 7]
train_X = all_inputs[train_idx]
test_X = all_inputs[test_idx]

# Enumerate ALL possible target functions (2^8 = 256)
n_functions = 2 ** len(all_inputs)
losses_mv = []  # majority vote
losses_nn = []  # nearest neighbor

for func_id in range(n_functions):
    # Convert function id to binary truth table
    all_y = np.array([(func_id >> i) & 1 for i in range(len(all_inputs))])
    train_y = all_y[train_idx]
    test_y = all_y[test_idx]

    # Evaluate both predictors on test set
    loss_mv = 0
    loss_nn = 0
    for j, test_x in enumerate(test_X):
        pred_mv = majority_vote_predictor(train_X, train_y, test_x)
        pred_nn = nearest_neighbor_predictor(train_X, train_y, test_x)
        loss_mv += (pred_mv != test_y[j])
        loss_nn += (pred_nn != test_y[j])

    losses_mv.append(loss_mv / len(test_X))
    losses_nn.append(loss_nn / len(test_X))

print("No Free Lunch Verification")
print("=" * 40)
print(f"Average loss (Majority Vote): {np.mean(losses_mv):.4f}")
print(f"Average loss (Nearest Neighbor): {np.mean(losses_nn):.4f}")
print(f"\nBoth equal 0.5 \u2014 confirming NFL theorem!")

# But on STRUCTURED tasks, algorithms differ:
print("\n--- On a STRUCTURED task (XOR-like) ---")
structured_y = np.array([0, 1, 1, 0, 1, 0, 0, 1])  # XOR of all bits
train_y_s = structured_y[train_idx]
test_y_s = structured_y[test_idx]

correct_nn = sum(
    nearest_neighbor_predictor(train_X, train_y_s, test_X[j]) == test_y_s[j]
    for j in range(len(test_X))
)
correct_mv = sum(
    majority_vote_predictor(train_X, train_y_s, test_X[j]) == test_y_s[j]
    for j in range(len(test_X))
)
print(f"NN accuracy: {correct_nn}/{len(test_X)}")
print(f"MV accuracy: {correct_mv}/{len(test_X)}")
print("On structured data, algorithms ARE different!")</code></pre>
                </div>

                <!-- Code Example 3 -->
                <h3>3. Hypothesis Space: Decision Boundaries</h3>
                <p>
                    Visualize how four classifiers with different inductive biases produce entirely different decision boundaries on the same dataset.
                </p>

                <div class="code-block">
                    <div class="code-block-header">
                        <span class="code-block-lang">python</span>
                        <button class="code-block-copy">Copy</button>
                    </div>
                    <pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier

# Generate 2D classification data (two moons)
np.random.seed(42)
n = 200
# Moon 1
theta1 = np.linspace(0, np.pi, n // 2)
X1 = np.column_stack([np.cos(theta1), np.sin(theta1)]) + np.random.normal(0, 0.15, (n // 2, 2))
# Moon 2
theta2 = np.linspace(0, np.pi, n // 2)
X2 = np.column_stack([1 - np.cos(theta2), 0.5 - np.sin(theta2)]) + np.random.normal(0, 0.15, (n // 2, 2))
X = np.vstack([X1, X2])
y = np.array([0] * (n // 2) + [1] * (n // 2))

# Four classifiers with different inductive biases
classifiers = [
    ("Linear SVM\n(linearity bias)", SVC(kernel='linear')),
    ("RBF SVM\n(smoothness bias)", SVC(kernel='rbf', gamma=2)),
    ("Decision Tree\n(axis-aligned bias)", DecisionTreeClassifier(max_depth=6)),
    ("k-NN (k=5)\n(locality bias)", KNeighborsClassifier(n_neighbors=5))
]

fig, axes = plt.subplots(2, 2, figsize=(12, 10))
axes = axes.ravel()

# Create mesh for decision boundary
h = 0.02
x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

for i, (name, clf) in enumerate(classifiers):
    clf.fit(X, y)
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)

    axes[i].contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')
    axes[i].scatter(X[y == 0, 0], X[y == 0, 1], c='blue', s=15, alpha=0.6)
    axes[i].scatter(X[y == 1, 0], X[y == 1, 1], c='red', s=15, alpha=0.6)
    axes[i].set_title(name, fontsize=12)

    # Training accuracy
    acc = clf.score(X, y)
    axes[i].text(0.02, 0.98, f'Acc: {acc:.1%}', transform=axes[i].transAxes,
                 fontsize=10, verticalalignment='top',
                 bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

plt.suptitle('Same Data, Different Inductive Biases \u2192 Different Decision Boundaries', fontsize=13)
plt.tight_layout()
plt.savefig('decision_boundaries.png', dpi=150)
plt.show()</code></pre>
                </div>

            </article>

            <!-- ==================== EXERCISES TAB ==================== -->
            <article class="article-content" id="exercises" style="display: none;">
                <h2>Exercises</h2>
                <p>Test your understanding of inductive bias, the No Free Lunch theorem, and the bias-variance tradeoff. Each exercise has a full worked solution.</p>

                <div class="exercise-list">

                    <!-- Easy -->
                    <h3 style="margin-top: 1rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">Easy</h3>

                    <!-- Exercise 1 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">1. Define Inductive Bias</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>In your own words, define inductive bias and give two examples from everyday reasoning (not ML).</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p>Inductive bias is any assumption a learner makes to generalize beyond observed data. Examples:</p>
                                <ol>
                                    <li><strong>Temporal regularity bias:</strong> Assuming the sun will rise tomorrow because it has every day so far.</li>
                                    <li><strong>Similarity/locality bias:</strong> Assuming a restaurant will be good because similar-looking restaurants were good.</li>
                                </ol>
                                <p>In both cases, nothing in the observed data <em>logically entails</em> the prediction &mdash; the assumption is what bridges the gap between observation and generalization.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 2 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">2. NFL Implication</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>If algorithm A achieves 95% accuracy on image classification, what does the NFL theorem say about its performance on all possible tasks?</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p>NFL says that averaged over ALL possible input-output mappings, A performs no better than random guessing (50% for binary classification). The 95% accuracy on images means A's inductive biases match image structure well, but there must exist tasks where A performs below 50% to compensate.</p>
                                <p>In other words, the algorithm's excellent performance on structured image data is "paid for" by poor performance on adversarially structured or random tasks that violate its assumptions.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 3 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">3. Identify the Bias</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>For each algorithm, identify its primary inductive bias:</p>
                            <ol type="a">
                                <li>Linear regression</li>
                                <li>k-NN with $k=1$</li>
                                <li>Decision tree</li>
                                <li>Naive Bayes</li>
                            </ol>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>(a) Linear regression:</strong> Linearity &mdash; output is a linear function of inputs.</p>
                                <p><strong>(b) k-NN with $k=1$:</strong> Extreme locality &mdash; prediction equals the nearest training point.</p>
                                <p><strong>(c) Decision tree:</strong> Axis-aligned splits &mdash; features are independently informative at each node.</p>
                                <p><strong>(d) Naive Bayes:</strong> Conditional independence &mdash; features are independent given the class label.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Medium -->
                    <h3 style="margin-top: 2rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">Medium</h3>

                    <!-- Exercise 4 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">4. Language vs Preference Bias</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Classify each as language bias or preference bias:</p>
                            <ol type="a">
                                <li>Using a neural network with 2 hidden layers</li>
                                <li>Early stopping</li>
                                <li>Restricting to polynomial features of degree $\leq 3$</li>
                                <li>Using Adam optimizer instead of SGD</li>
                            </ol>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>(a) Neural network with 2 hidden layers:</strong> Language bias &mdash; defines the hypothesis space (the set of functions representable by that architecture).</p>
                                <p><strong>(b) Early stopping:</strong> Preference bias &mdash; selects simpler functions from the space by halting before the model fully fits the training data.</p>
                                <p><strong>(c) Polynomial features of degree $\leq 3$:</strong> Language bias &mdash; restricts the feature space (and thus the hypothesis space) to cubic polynomials and below.</p>
                                <p><strong>(d) Adam optimizer instead of SGD:</strong> Preference bias &mdash; changes the search strategy over the same hypothesis space, potentially converging to different solutions.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 5 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">5. Bias-Variance Calculation</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>A model has $\text{Bias}^2 = 0.04$, $\text{Variance} = 0.01$, and irreducible noise $\sigma^2 = 0.02$.</p>
                            <ol type="a">
                                <li>What is the expected MSE?</li>
                                <li>If we increase model complexity, bias drops to 0.01 but variance increases to 0.08. Is this a good trade?</li>
                                <li>What is the optimal situation?</li>
                            </ol>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>(a)</strong> MSE $= 0.04 + 0.01 + 0.02 = 0.07$</p>
                                <p><strong>(b)</strong> New MSE $= 0.01 + 0.08 + 0.02 = 0.11$. No, MSE increased from 0.07 to 0.11. The variance increase more than offset the bias reduction.</p>
                                <p><strong>(c)</strong> The optimal situation is when the marginal decrease in bias&sup2; equals the marginal increase in variance:</p>
                                <div class="math-block">
                                    $$\frac{d(\text{Bias}^2)}{d(\text{complexity})} = -\frac{d(\text{Var})}{d(\text{complexity})}$$
                                </div>
                                <p>At this point, any further increase in complexity would increase MSE (variance grows faster than bias&sup2; shrinks), and any decrease in complexity would also increase MSE (bias&sup2; grows faster than variance shrinks).</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 6 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">6. Counting Hypotheses</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Consider binary classification on 4 boolean features.</p>
                            <ol type="a">
                                <li>How many possible target functions exist?</li>
                                <li>How many hypotheses does a linear classifier (threshold on weighted sum) consider?</li>
                                <li>What fraction of all functions can a linear classifier represent?</li>
                            </ol>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>(a)</strong> The input space has $2^4 = 16$ points. The number of binary functions is $2^{16} = 65536$.</p>
                                <p><strong>(b)</strong> A linear classifier in 4D considers all linear separations. By a result in combinatorics, the number of linearly separable functions on $2^4$ points in general position is bounded. For 4D boolean inputs, there are exactly 1882 linearly separable functions.</p>
                                <p><strong>(c)</strong> $1882/65536 \approx 2.9\%$. The linear classifier's language bias restricts it to under 3% of all possible functions &mdash; a dramatic restriction that is beneficial when the true function is approximately linear, but harmful otherwise.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 7 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">7. NFL Proof for Restricted Case</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Prove the NFL theorem for the following restricted case: Input space $\mathcal{X} = \{0, 1\}$, output space $\mathcal{Y} = \{0, 1\}$, training set has one point $(x=0, y=?)$, and we must predict $f(1)$. Show that no predictor is better than random when averaged over all 4 possible functions.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p>The 4 functions are:</p>
                                <ul>
                                    <li>$f_1(0)=0, \; f_1(1)=0$</li>
                                    <li>$f_2(0)=0, \; f_2(1)=1$</li>
                                    <li>$f_3(0)=1, \; f_3(1)=0$</li>
                                    <li>$f_4(0)=1, \; f_4(1)=1$</li>
                                </ul>
                                <p><strong>Given training point $x=0$:</strong></p>
                                <p>If $y=0$ (functions $f_1, f_2$): We must predict $f(1)$. For $f_1$, the correct answer is 0; for $f_2$, the correct answer is 1. No matter what we predict, we are correct for exactly one of the two functions. Average accuracy = 50%.</p>
                                <p>If $y=1$ (functions $f_3, f_4$): We must predict $f(1)$. For $f_3$, the correct answer is 0; for $f_4$, the correct answer is 1. Again, any prediction is correct for exactly one function. Average accuracy = 50%.</p>
                                <p>No matter what rule we use (even one that conditions on the training label), the average over all functions gives 50% accuracy. No predictor beats random guessing when averaged over all possible target functions.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Hard -->
                    <h3 style="margin-top: 2rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">Hard</h3>

                    <!-- Exercise 8 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">8. MDL from Bayesian Reasoning</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Show that the Minimum Description Length (MDL) principle emerges from Bayesian reasoning:</p>
                            <div class="math-block">
                                $$\arg\max_h P(h|D) = \arg\min_h [-\log P(D|h) - \log P(h)]$$
                            </div>
                            <p>Interpret each term.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p>By Bayes' theorem:</p>
                                <div class="math-block">
                                    $$P(h|D) = \frac{P(D|h)P(h)}{P(D)}$$
                                </div>
                                <p>Since $P(D)$ is constant with respect to $h$:</p>
                                <div class="math-block">
                                    $$\arg\max_h P(h|D) = \arg\max_h P(D|h)P(h) = \arg\min_h [-\log P(D|h) - \log P(h)]$$
                                </div>
                                <p><strong>Interpretation of each term:</strong></p>
                                <ul>
                                    <li><strong>$-\log P(D|h)$:</strong> Description length of data given hypothesis &mdash; how well $h$ fits the data. High likelihood means short description (few "surprises").</li>
                                    <li><strong>$-\log P(h)$:</strong> Description length of the hypothesis itself &mdash; the complexity of $h$. Simpler hypotheses have higher prior probability and shorter description length.</li>
                                </ul>
                                <p><strong>MDL = minimize total description length = data misfit + model complexity.</strong></p>
                                <p>This is the Bayesian justification for Occam's razor: prefer simpler models that still explain the data. The prior $P(h)$ encodes our inductive bias toward simplicity, and the likelihood $P(D|h)$ ensures the model still fits the observed data.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 9 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">9. Cross-Validation as Bias Selection</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Explain how $k$-fold cross-validation acts as a mechanism for selecting the appropriate level of inductive bias. Consider a hyperparameter that controls bias strength (e.g., regularization $\lambda$ or polynomial degree).</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p>For each candidate bias level (e.g., $\lambda$ value):</p>
                                <ol>
                                    <li>Train on $k-1$ folds, evaluate on the held-out fold.</li>
                                    <li>This estimates the expected test error, which $= \text{Bias}^2 + \text{Variance}$.</li>
                                </ol>
                                <p><strong>Low bias strength</strong> (e.g., small $\lambda$, high polynomial degree): low Bias&sup2;, high Variance &rarr; high test error on some folds due to overfitting.</p>
                                <p><strong>High bias strength</strong> (e.g., large $\lambda$, low polynomial degree): high Bias&sup2;, low Variance &rarr; consistently mediocre performance due to underfitting.</p>
                                <p>Cross-validation selects the $\lambda$ that minimizes the estimated test error, effectively finding the sweet spot in the bias-variance tradeoff.</p>
                                <p><strong>Key insight:</strong> CV doesn't remove the need for inductive bias &mdash; it selects among candidate biases. The set of candidate $\lambda$ values is itself a meta-level inductive bias (you still must choose what range to search over).</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 10 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">10. Design a Bias for Rotationally Symmetric Data</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>You know your data has rotational symmetry: $f(x) = f(Rx)$ for any rotation matrix $R$.</p>
                            <ol type="a">
                                <li>What inductive bias should your model have?</li>
                                <li>Propose a feature transformation that encodes this bias.</li>
                                <li>How does this reduce the hypothesis space?</li>
                            </ol>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>(a)</strong> The model should be invariant to rotations: $h(x) = h(Rx)$ for all rotations $R$. This is a language bias that restricts the hypothesis space to rotation-invariant functions.</p>
                                <p><strong>(b)</strong> Transform input $(x_1, x_2)$ to $r = \sqrt{x_1^2 + x_2^2}$ (the radius). Any function of $r$ alone is automatically rotation-invariant. For higher dimensions, use $r = \|x\|$ and possibly the angles as needed for partial symmetries.</p>
                                <p><strong>(c)</strong> Instead of searching over all functions $f: \mathbb{R}^2 \to \mathbb{R}$, we search over functions $g: \mathbb{R}^+ \to \mathbb{R}$ where $f(x) = g(\|x\|)$. This reduces the input dimensionality from 2 to 1, exponentially shrinking the hypothesis space and the amount of data needed to learn.</p>
                                <p>Concretely: if we discretize each input dimension into $k$ bins, the original space has $k^2$ cells but the reduced space has only $k$ cells. The sample complexity drops accordingly, and we are guaranteed to never learn a non-symmetric function (which would be wrong by assumption).</p>
                            </div>
                        </div>
                    </div>

                </div>
            </article>

        </main>

        <!-- TOC (Right Side) -->
        <aside class="toc-container">
            <h4 class="toc-title">Contents</h4>
            <nav class="toc-list">
                <a href="#what-is-inductive-bias" class="toc-link">What is Inductive Bias?</a>
                <a href="#no-free-lunch" class="toc-link">No Free Lunch Theorem</a>
                <a href="#bias-variance-revisited" class="toc-link">Bias-Variance Revisited</a>
                <a href="#taxonomy" class="toc-link">Taxonomy of Biases</a>
                <a href="#historical-context" class="toc-link">Historical Context</a>
                <a href="#classical-ml-biases" class="toc-link">Classical ML Biases</a>
                <a href="#summary" class="toc-link">Summary</a>
            </nav>
        </aside>
    </div>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-brand">
                    <span class="logo-symbol">&nabla;</span>
                    <span>ML Fundamentals</span>
                </div>
                <p class="footer-tagline">Deep understanding through first principles.</p>
            </div>
            <div class="footer-links">
                <a href="../../../index.html">Home</a>
                <a href="https://github.com/ml-entropy/ml-entropy.github.io" target="_blank">GitHub</a>
            </div>
        </div>
    </footer>

    <!-- Scripts -->
    <script src="../../../js/main.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // KaTeX Rendering
            if (typeof renderMathInElement === 'function') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }

            // Tab Switching Logic
            const tabs = document.querySelectorAll('.tutorial-tab');
            const articles = document.querySelectorAll('.article-content');

            function switchTab(targetId) {
                if (!targetId || targetId === '#') targetId = '#theory';

                // Update tab classes
                tabs.forEach(tab => {
                    if (tab.getAttribute('href') === targetId) {
                        tab.classList.add('active');
                    } else {
                        tab.classList.remove('active');
                    }
                });

                // Show/Hide articles
                articles.forEach(article => {
                    const articleId = '#' + article.id;
                    if (articleId === targetId) {
                        article.style.display = 'block';
                    } else {
                        article.style.display = 'none';
                    }
                });

                // Re-render KaTeX for newly visible content
                if (typeof renderMathInElement === 'function') {
                    renderMathInElement(document.body, {
                        delimiters: [
                            {left: '$$', right: '$$', display: true},
                            {left: '$', right: '$', display: false},
                            {left: '\\[', right: '\\]', display: true},
                            {left: '\\(', right: '\\)', display: false}
                        ],
                        throwOnError: false
                    });
                }

                // Update TOC visibility based on tab
                const toc = document.querySelector('.toc-container');
                if (toc) {
                    if (targetId === '#theory') {
                        toc.style.display = 'block';
                        setTimeout(() => toc.classList.add('visible'), 100);
                    } else {
                        toc.classList.remove('visible');
                        setTimeout(() => toc.style.display = 'none', 300);
                    }
                }
            }

            // Event Listeners
            tabs.forEach(tab => {
                tab.addEventListener('click', (e) => {
                    e.preventDefault();
                    const targetId = tab.getAttribute('href');
                    history.pushState(null, null, targetId);
                    switchTab(targetId);
                });
            });

            // Handle browser back/forward buttons
            window.addEventListener('popstate', () => {
                switchTab(window.location.hash);
            });

            // Initial Load
            switchTab(window.location.hash);
        });
    </script>
</body>
</html>
