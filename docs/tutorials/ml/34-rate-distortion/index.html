<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Shannon's Rate-Distortion Theory | ML Fundamentals</title>
    <meta name="description" content="Understanding the fundamental limits of lossy compression — how many bits you need to represent data at a given quality level.">

    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&family=Playfair+Display:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}, {left: '\\\\[', right: '\\\\]', display: true}, {left: '\\\\(', right: '\\\\)', display: false}], throwOnError: false});"></script>

    <!-- Styles -->
    <link rel="stylesheet" href="../../../css/main.css">
    <link rel="stylesheet" href="../../../css/components.css">

    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>∞</text></svg>">
    <link rel="stylesheet" href="../../../css/sidebar.css">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar" id="navbar">
        <div class="nav-container">
            <a href="../../../index.html" class="nav-logo">
                <span class="logo-symbol">∇</span>
                <span class="logo-text">ML Fundamentals</span>
            </a>

            <button class="nav-toggle" id="navToggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>

            <div class="nav-menu" id="navMenu">
                <div class="nav-links">
                    <a href="../../../tutorials/ml/index.html" class="nav-link active">Machine Learning</a>
                    <a href="../../../tutorials/linear-algebra/index.html" class="nav-link">Linear Algebra</a>
                    <a href="../../../tutorials/calculus/index.html" class="nav-link">Calculus</a>
                    <a href="../../../tutorials/physics/index.html" class="nav-link">Physics</a>
                    <a href="../../../index.html#philosophy" class="nav-link">Philosophy</a>
                    <a href="../../../index.html#roadmap" class="nav-link">Roadmap</a>
                    <a href="https://github.com/ml-entropy/ml-entropy.github.io" class="nav-link" target="_blank">GitHub</a>
                </div>

                <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
                    <svg class="sun-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="5"/>
                        <path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"/>
                    </svg>
                    <svg class="moon-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                    </svg>
                </button>
            </div>
        </div>
    </nav>

    <!-- Tutorial Header -->
    <header class="tutorial-content-header">
        <div class="container">
            <nav class="breadcrumb">
                <a href="../../../index.html">Home</a>
                <span class="breadcrumb-separator">→</span>
                <a href="../index.html">Machine Learning</a>
                <span class="breadcrumb-separator">→</span>
                <span>Rate-Distortion Theory</span>
            </nav>

            <div class="tutorial-tabs">
                <a href="#theory" class="tutorial-tab active" onclick="showTab('theory')">Theory</a>
                <a href="#code" class="tutorial-tab" onclick="showTab('code')">Code</a>
                <a href="#exercises" class="tutorial-tab" onclick="showTab('exercises')">Exercises</a>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <div class="tutorial-wrapper">

        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <aside class="tutorial-sidebar">
            <div class="sidebar-section">
                <h3 class="sidebar-section-title">Machine Learning</h3>
                <nav class="sidebar-nav">
                        <a href="../00-probability/index.html" class="sidebar-link">00. Probability Foundations</a>
                    <a href="../04-logarithms/index.html" class="sidebar-link">01. Why Logarithms?</a>
                    <a href="../05-combinatorics/index.html" class="sidebar-link">02. Combinatorics</a>
                    <a href="../03-distributions/index.html" class="sidebar-link">03. Normal Distributions</a>
                    <a href="../01-entropy/index.html" class="sidebar-link">04. Entropy Fundamentals</a>
                    <a href="../02-cross-entropy/index.html" class="sidebar-link">05. Cross-Entropy</a>
                    <a href="../02-kl-divergence/index.html" class="sidebar-link">06. KL Divergence</a>
                    <a href="../14-entropy-connections/index.html" class="sidebar-link">07. Entropy Connections</a>
                    <a href="../06-backpropagation/index.html" class="sidebar-link">08. Backpropagation</a>
                    <a href="../07-regularization/index.html" class="sidebar-link">09. Regularization</a>
                    <a href="../08-batch-normalization/index.html" class="sidebar-link">10. Batch Normalization</a>
                    <a href="../09-learning-rate/index.html" class="sidebar-link">11. Learning Rate</a>
                    <a href="../10-cnn/index.html" class="sidebar-link">12. CNNs</a>
                    <a href="../11-rnn/index.html" class="sidebar-link">13. RNNs</a>
                    <a href="../34-rate-distortion/index.html" class="sidebar-link active">14. Rate-Distortion Theory</a>
                    <a href="../15-autoencoder/index.html" class="sidebar-link">15. Autoencoders</a>
                    <a href="../13-variational-inference/index.html" class="sidebar-link">16. Variational Inference</a>
                    <a href="../12-vae/index.html" class="sidebar-link">17. VAE</a>
                    <a href="../16-inductive-bias/index.html" class="sidebar-link">18. Inductive Bias</a>
                    <a href="../17-architectural-biases/index.html" class="sidebar-link">19. Architectural Biases</a>
                    <a href="../18-designing-biases/index.html" class="sidebar-link">20. Designing Biases</a>
                    <a href="../19-fst-fundamentals/index.html" class="sidebar-link">21. FST Fundamentals</a>
                    <a href="../20-weighted-fsts/index.html" class="sidebar-link">22. Weighted FSTs</a>
                    <a href="../21-fst-libraries/index.html" class="sidebar-link">23. FST Libraries</a>
                    <a href="../22-fst-applications/index.html" class="sidebar-link">24. FST Applications</a>
                    <a href="../23-neural-symbolic/index.html" class="sidebar-link">25. Neural-Symbolic Hybrids</a>
                    <a href="../24-sequence-alignment/index.html" class="sidebar-link">26. Sequence Alignment</a>
                    <a href="../25-mas-algorithm/index.html" class="sidebar-link">27. MAS Algorithm</a>
                    <a href="../26-forced-alignment/index.html" class="sidebar-link">28. Forced Alignment & MFA</a>
                    <a href="../27-tts-fundamentals/index.html" class="sidebar-link">29. TTS Fundamentals</a>
                    <a href="../28-neural-vocoders/index.html" class="sidebar-link">30. Neural Vocoders</a>
                    <a href="../29-tacotron/index.html" class="sidebar-link">31. Tacotron & Attention TTS</a>
                    <a href="../30-fastspeech/index.html" class="sidebar-link">32. FastSpeech & Non-AR TTS</a>
                    <a href="../31-glow-tts/index.html" class="sidebar-link">33. Glow-TTS & Flows</a>
                    <a href="../32-vits/index.html" class="sidebar-link">34. VITS: End-to-End TTS</a>
                    <a href="../33-bilingual-tts/index.html" class="sidebar-link">35. Bilingual TTS: RU+KY</a>
                </nav>
            </div>
            
            <div class="sidebar-section" style="margin-top: 2rem;">
                <h3 class="sidebar-section-title">Related Subjects</h3>
                <nav class="sidebar-nav">
                        <a href="../../linear-algebra/index.html" class="sidebar-link">Linear Algebra</a>
                    <a href="../../calculus/index.html" class="sidebar-link">Calculus</a>
                    <a href="../../physics/index.html" class="sidebar-link">Physics</a>
                </nav>
            </div>
        </aside>

        <!-- Main Article -->
        <main class="tutorial-main">

            <!-- ============== THEORY TAB ============== -->
            <article class="article-content" id="theory">

                <div class="tutorial-footer-summary" style="margin: 0 0 3rem 0; padding: 2rem; background: #f8f9fa; border-radius: 8px; border-left: 4px solid #3b82f6;">
                    <h1>14. Shannon's Rate-Distortion Theory</h1>
                    <p class="lead">
                        How many bits do you truly need to represent data at a given quality level?
                        Rate-distortion theory gives the mathematical answer — the fundamental limit
                        of lossy compression that no algorithm can beat.
                    </p>
                </div>

                <!-- Section 1: The Lossy Compression Problem -->
                <h2 id="lossy-compression">The Lossy Compression Problem</h2>

                <p>
                    Consider sending a photo over a limited bandwidth channel. The raw image might be
                    10 MB, but you only have capacity for 100 KB. You <em>must</em> lose some information.
                    The question is: what is the <strong>best possible quality</strong> you can achieve
                    at 100 KB? And conversely, what is the <strong>minimum file size</strong> needed to
                    achieve a given quality?
                </p>

                <p>
                    Shannon's <strong>lossless</strong> source coding theorem (1948) tells us the minimum
                    bits for <em>perfect</em> reconstruction: the entropy $H(X)$. But for continuous
                    sources, $H(X) = \infty$ — you'd need infinite bits for perfect reproduction.
                    In practice, we always accept some distortion: JPEG, MP3, video codecs all discard
                    information strategically.
                </p>

                <div class="note-box">
                    <div class="box-title">The Central Question</div>
                    <p style="margin-bottom: 0;">
                        Given a source $X$ and a maximum tolerable distortion $D$, what is the
                        <strong>minimum number of bits per symbol</strong> (the <em>rate</em>) needed
                        to represent $X$ such that the average distortion is at most $D$?
                    </p>
                </div>

                <p>
                    Shannon answered this in 1959 with <strong>rate-distortion theory</strong>. The
                    answer is a function $R(D)$ — the rate-distortion function — that gives the
                    exact minimum rate for each distortion level.
                </p>

                <!-- Section 2: The Framework -->
                <h2 id="framework">The Rate-Distortion Framework</h2>

                <p>
                    Rate-distortion theory has four ingredients. Understanding each one clearly is
                    essential before we can state the main result.
                </p>

                <div class="definition-box">
                    <div class="box-title">The Four Ingredients</div>
                    <p><strong>1. Source $X$:</strong> The data to compress. A random variable drawn from
                    distribution $p(x)$. Examples: pixel values, audio samples, sensor readings.</p>

                    <p><strong>2. Encoder $p(z|x)$:</strong> Maps input $x$ to a compressed code $z$.
                    This is a <em>conditional probability distribution</em> — given input $x$, it specifies
                    the probability of producing each possible code $z$.</p>

                    <p><strong>3. Decoder $p(\hat{x}|z)$:</strong> Reconstructs an estimate $\hat{x}$
                    from the code $z$. For a given code $z$, the optimal decoder outputs the $\hat{x}$
                    that minimizes expected distortion.</p>

                    <p style="margin-bottom: 0;"><strong>4. Distortion measure $d(x, \hat{x})$:</strong>
                    Quantifies how different the reconstruction is from the original. Common choices:
                    <ul style="margin-bottom: 0;">
                        <li><strong>Squared error (MSE):</strong> $d(x, \hat{x}) = (x - \hat{x})^2$</li>
                        <li><strong>Absolute error:</strong> $d(x, \hat{x}) = |x - \hat{x}|$</li>
                        <li><strong>Hamming distance:</strong> $d(x, \hat{x}) = \mathbb{1}[x \neq \hat{x}]$ (for discrete sources)</li>
                    </ul></p>
                </div>

                <p>
                    The full pipeline is: $X \xrightarrow{p(z|x)} Z \xrightarrow{p(\hat{x}|z)} \hat{X}$.
                    The encoder compresses, the channel transmits $Z$, and the decoder reconstructs.
                    The <strong>rate</strong> is the mutual information $I(X; Z)$ — the number of bits
                    the encoder actually communicates about $X$ through $Z$.
                </p>

                <!-- Section 3: Mutual Information as Rate -->
                <h2 id="mutual-information">Mutual Information as Rate</h2>

                <p>
                    Why does mutual information $I(X; Z)$ measure the rate? Because it quantifies
                    exactly how much information $Z$ carries about $X$:
                </p>

                <div class="math-block">
                    $$I(X; Z) = H(X) - H(X|Z) = H(Z) - H(Z|X)$$
                </div>

                <p>
                    Three intuitions for why this is the right measure of "bits transmitted":
                </p>

                <div class="math-derivation">
                    <div class="math-derivation-title">Why $I(X; Z)$ = Rate</div>

                    <div class="math-step">
                        <div class="math-step-number">1</div>
                        <div class="math-step-content">
                            <strong>Uncertainty reduction:</strong> $I(X;Z) = H(X) - H(X|Z)$.
                            Before seeing $Z$, our uncertainty about $X$ is $H(X)$. After seeing $Z$,
                            it drops to $H(X|Z)$. The difference is how many bits of information
                            $Z$ reveals about $X$.
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">2</div>
                        <div class="math-step-content">
                            <strong>Minimum description length:</strong> Shannon's source coding theorem
                            says you need at least $H(Z)$ bits to losslessly represent $Z$. But
                            $H(Z|X)$ bits of $Z$ are "noise" (randomness from the stochastic encoder
                            that doesn't depend on $X$). The useful information is $H(Z) - H(Z|X) = I(X;Z)$.
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">3</div>
                        <div class="math-step-content">
                            <strong>Operational meaning:</strong> For long sequences of $n$ i.i.d. symbols,
                            Shannon proved that $nI(X;Z)$ bits are necessary and sufficient to encode the
                            sequence with the encoder $p(z|x)$. This is the <em>operational</em> rate.
                        </div>
                    </div>
                </div>

                <div class="warning-box">
                    <div class="box-title">Key Point</div>
                    <p style="margin-bottom: 0;">
                        If the encoder is <strong>deterministic</strong> ($z = f(x)$, no randomness),
                        then $H(Z|X) = 0$, so $I(X;Z) = H(Z)$. All the entropy of $Z$ is useful
                        information. If the encoder is <strong>stochastic</strong>, some of $H(Z)$
                        is wasted on encoder noise, so $I(X;Z) < H(Z)$.
                    </p>
                </div>

                <!-- Section 4: The Rate-Distortion Function -->
                <h2 id="rate-distortion-function">The Rate-Distortion Function $R(D)$</h2>

                <p>
                    We now have all the pieces. The rate-distortion function answers: for a given
                    distortion budget $D$, what is the minimum rate?
                </p>

                <div class="definition-box">
                    <div class="box-title">Rate-Distortion Function</div>
                    <p style="margin-bottom: 0;">
                        $$R(D) = \min_{p(z|x): \, \mathbb{E}[d(X, \hat{X})] \leq D} I(X; Z)$$

                        <strong>Read this as:</strong> Among all encoders $p(z|x)$ whose average distortion
                        $\mathbb{E}[d(X, \hat{X})]$ is at most $D$, find the one that minimizes the
                        mutual information $I(X; Z)$. That minimum is $R(D)$.
                    </p>
                </div>

                <p>
                    The optimization searches over <em>all possible conditional distributions</em>
                    $p(z|x)$. This is an infinite-dimensional optimization — not over a finite set
                    of parameters, but over the space of all probability distributions. Despite this
                    complexity, the problem is <strong>convex</strong>, which means it has a unique
                    global minimum that can be found reliably.
                </p>

                <h3>The Lagrangian Formulation</h3>

                <p>
                    Like all constrained optimizations, we can convert to an unconstrained problem
                    using a Lagrange multiplier $s \geq 0$:
                </p>

                <div class="math-block">
                    $$\mathcal{L}(p(z|x), s) = I(X; Z) + s \cdot \mathbb{E}[d(X, \hat{X})]$$
                </div>

                <p>
                    Minimizing $\mathcal{L}$ over $p(z|x)$ for a fixed $s$ gives a point on the
                    $R(D)$ curve. The parameter $s$ controls the trade-off: large $s$ penalizes
                    distortion heavily (low distortion, high rate); small $s$ allows more distortion
                    (high distortion, low rate). As $s$ sweeps from $0$ to $\infty$, we trace the
                    entire $R(D)$ curve.
                </p>

                <div class="note-box">
                    <div class="box-title">Connection to VAE</div>
                    <p style="margin-bottom: 0;">
                        Compare the Lagrangian $I(X;Z) + s \cdot \mathbb{E}[d(X,\hat{X})]$ to the
                        VAE loss: $D_{KL}(q(z|x) \| p(z)) + \beta \cdot \mathbb{E}[-\log p(x|z)]$.
                        They have the same structure: a rate term (mutual information / KL divergence)
                        plus a weighted distortion term (reconstruction error). The VAE is performing
                        approximate rate-distortion optimization! The parameter $\beta$ plays the role
                        of $s$.
                    </p>
                </div>

                <!-- Section 5: Properties of R(D) -->
                <h2 id="properties">Properties of $R(D)$</h2>

                <p>
                    The rate-distortion function has several important properties that hold for
                    <em>any</em> source and <em>any</em> distortion measure:
                </p>

                <div class="math-derivation">
                    <div class="math-derivation-title">Properties of $R(D)$</div>

                    <div class="math-step">
                        <div class="math-step-number">1</div>
                        <div class="math-step-content">
                            <strong>Non-negative:</strong> $R(D) \geq 0$ for all $D$. You can never
                            need fewer than zero bits.
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">2</div>
                        <div class="math-step-content">
                            <strong>Monotonically decreasing:</strong> If $D_1 < D_2$, then
                            $R(D_1) \geq R(D_2)$. Allowing more distortion can only reduce or maintain
                            the required rate. More tolerance = fewer bits needed.
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">3</div>
                        <div class="math-step-content">
                            <strong>Convex:</strong> $R(\lambda D_1 + (1-\lambda)D_2) \leq \lambda R(D_1) + (1-\lambda) R(D_2)$.
                            The curve bows toward the origin. Intuitively: the first few bits you add
                            give the biggest quality improvement; later bits give diminishing returns.
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">4</div>
                        <div class="math-step-content">
                            <strong>At $D = 0$:</strong> Perfect reconstruction. For continuous sources,
                            $R(0) = \infty$. For discrete sources with alphabet $\mathcal{X}$,
                            $R(0) = H(X)$ (lossless compression limit).
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">5</div>
                        <div class="math-step-content">
                            <strong>At $D = D_{\max}$:</strong> $R(D_{\max}) = 0$. When distortion is
                            so large that the best "encoder" is to ignore $X$ entirely and output a
                            constant, no bits are needed. For MSE: $D_{\max} = \text{Var}(X)$, and
                            the decoder always outputs $\hat{x} = \mathbb{E}[X]$.
                        </div>
                    </div>
                </div>

                <!-- Section 6: Deterministic vs Stochastic Encoders -->
                <h2 id="encoders">Deterministic vs Stochastic Encoders</h2>

                <p>
                    A crucial subtlety in rate-distortion theory is that the optimization is over
                    <em>all</em> conditional distributions $p(z|x)$ — not just deterministic functions.
                    Let's understand what this means.
                </p>

                <h3>Deterministic Encoders: $z = f(x)$</h3>

                <p>
                    A <strong>deterministic encoder</strong> is a function $f$ that maps each input
                    $x$ to exactly one code $z = f(x)$. Given the same input, you always get the
                    same output. Examples:
                </p>
                <ul>
                    <li>Rounding: $f(3.7) = 4$, $f(3.2) = 3$</li>
                    <li>Quantization: $f(x) = \lfloor x / \Delta \rfloor \cdot \Delta$ (round to nearest multiple of $\Delta$)</li>
                    <li>Neural network encoder: $f(x) = \text{encoder}_\theta(x)$ with fixed weights</li>
                </ul>

                <p>
                    In probabilistic notation, a deterministic encoder corresponds to:
                </p>

                <div class="math-block">
                    $$p(z|x) = \delta(z - f(x))$$
                </div>

                <p>
                    The <strong>Dirac delta function</strong> $\delta(z - c)$ is a distribution that
                    puts all probability mass at the single point $z = c$:
                </p>

                <div class="definition-box">
                    <div class="box-title">The Dirac Delta Function</div>
                    <p>
                        $\delta(z - c)$ is defined by two properties:
                    </p>
                    <ul>
                        <li>$\delta(z - c) = 0$ for all $z \neq c$ (zero everywhere except at $c$)</li>
                        <li>$\int_{-\infty}^{\infty} \delta(z - c) \, dz = 1$ (integrates to 1)</li>
                    </ul>
                    <p style="margin-bottom: 0;">
                        <strong>The sifting property:</strong> $\int g(z) \, \delta(z - c) \, dz = g(c)$.
                        Integrating any function against $\delta(z-c)$ "picks out" its value at $c$.
                        Think of $\delta$ as an infinitely tall, infinitely narrow spike at $c$.
                    </p>
                </div>

                <h3>Stochastic Encoders: $p(z|x)$ is a Soft Distribution</h3>

                <p>
                    A <strong>stochastic encoder</strong> doesn't map each $x$ to a single $z$. Instead,
                    it defines a probability distribution over possible codes. Given the same input $x$,
                    you might get different codes $z$ on different runs.
                </p>

                <p>
                    A <strong>soft distribution</strong> is one that spreads probability across multiple
                    values, as opposed to concentrating it all at one point (which would be a delta).
                    Examples:
                </p>

                <ul>
                    <li><strong>Gaussian encoder:</strong> $p(z|x) = \mathcal{N}(z; \mu(x), \sigma^2)$.
                    The code is centered at $\mu(x)$ but has random noise with variance $\sigma^2$.</li>
                    <li><strong>Uniform encoder:</strong> $p(z|x) = \text{Uniform}(f(x) - \epsilon, f(x) + \epsilon)$.
                    The code is $f(x)$ plus uniform noise in $[-\epsilon, \epsilon]$.</li>
                </ul>

                <h3>Why Allow Stochastic Encoders?</h3>

                <p>
                    The encoder controls <strong>both rate and distortion</strong>:
                </p>

                <ul>
                    <li><strong>Rate effect:</strong> Adding noise to the encoder <em>reduces</em>
                    $I(X;Z)$ because the noise destroys information about $X$. This is good — fewer
                    bits to transmit.</li>
                    <li><strong>Distortion effect:</strong> Adding noise means the decoder has less
                    information, so reconstruction quality degrades. This is bad — higher distortion.</li>
                </ul>

                <p>
                    The optimal encoder finds the best trade-off. For some sources, the optimal encoder
                    at certain distortion levels is genuinely stochastic — adding a precise amount of
                    noise achieves lower rate than any deterministic encoder at the same distortion.
                </p>

                <div class="warning-box">
                    <div class="box-title">VAE Connection</div>
                    <p style="margin-bottom: 0;">
                        The VAE encoder $q(z|x) = \mathcal{N}(\mu_\theta(x), \sigma^2_\theta(x))$ is a
                        learned stochastic encoder. The KL term in the VAE loss penalizes the rate
                        $I(X;Z)$, and the reconstruction term penalizes distortion. The VAE is literally
                        performing (approximate) rate-distortion optimization with a Gaussian encoder family.
                    </p>
                </div>

                <!-- Section 7: The Gaussian Source -->
                <h2 id="gaussian">The Gaussian Source</h2>

                <p>
                    For a Gaussian source with MSE distortion, Shannon derived the exact $R(D)$ in
                    closed form. This is one of the few exactly solvable cases and builds powerful intuition.
                </p>

                <div class="definition-box">
                    <div class="box-title">Setup</div>
                    <p style="margin-bottom: 0;">
                        Source: $X \sim \mathcal{N}(0, \sigma^2)$<br>
                        Distortion: $d(x, \hat{x}) = (x - \hat{x})^2$ (MSE)<br>
                        Constraint: $\mathbb{E}[(X - \hat{X})^2] \leq D$
                    </p>
                </div>

                <div class="math-derivation">
                    <div class="math-derivation-title">Deriving $R(D)$ for Gaussian</div>

                    <div class="math-step">
                        <div class="math-step-number">1</div>
                        <div class="math-step-content">
                            <strong>The optimal test channel:</strong> The optimal encoder adds
                            Gaussian noise. The joint distribution that achieves $R(D)$ has the form
                            $Z = X + N$ where $N \sim \mathcal{N}(0, \sigma_N^2)$ is independent noise.
                            Actually, it's more natural to write $X = Z + W$ where $W$ is the reconstruction
                            error. The optimal structure is: $\hat{X} = Z$ and $X = Z + W$ with
                            $W \sim \mathcal{N}(0, D)$ independent of $Z$.
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">2</div>
                        <div class="math-step-content">
                            <strong>Variance decomposition:</strong> Since $X = Z + W$ with $Z$ and $W$
                            independent: $\sigma^2 = \text{Var}(Z) + D$, so $\text{Var}(Z) = \sigma^2 - D$.
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">3</div>
                        <div class="math-step-content">
                            <strong>Mutual information:</strong> For jointly Gaussian $(X, Z)$:
                            $$I(X; Z) = h(X) - h(X|Z) = h(X) - h(W)$$
                            where $h(\cdot)$ denotes differential entropy. Using
                            $h(\mathcal{N}(0, v)) = \frac{1}{2}\log_2(2\pi e \, v)$:
                            $$I(X;Z) = \frac{1}{2}\log_2(2\pi e \, \sigma^2) - \frac{1}{2}\log_2(2\pi e \, D) = \frac{1}{2}\log_2\frac{\sigma^2}{D}$$
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">4</div>
                        <div class="math-step-content">
                            <strong>Result:</strong>
                            $$\boxed{R(D) = \frac{1}{2}\log_2\frac{\sigma^2}{D}, \quad 0 \leq D \leq \sigma^2}$$
                            For $D > \sigma^2$: $R(D) = 0$ (just output $\hat{x} = 0$, zero bits needed).
                        </div>
                    </div>
                </div>

                <h3>Example Values</h3>

                <p>For $\sigma^2 = 1$ (standard normal source):</p>

                <table style="width: 100%; border-collapse: collapse; margin: 1rem 0;">
                    <tr style="border-bottom: 2px solid #e5e7eb;">
                        <th style="text-align: left; padding: 8px;">Distortion $D$</th>
                        <th style="text-align: left; padding: 8px;">$R(D)$ (bits)</th>
                        <th style="text-align: left; padding: 8px;">Interpretation</th>
                    </tr>
                    <tr style="border-bottom: 1px solid #e5e7eb;">
                        <td style="padding: 8px;">$1.0$</td>
                        <td style="padding: 8px;">$0$</td>
                        <td style="padding: 8px;">Output constant $\hat{x}=0$. MSE = variance.</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #e5e7eb;">
                        <td style="padding: 8px;">$0.5$</td>
                        <td style="padding: 8px;">$0.5$</td>
                        <td style="padding: 8px;">Half a bit halves the distortion.</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #e5e7eb;">
                        <td style="padding: 8px;">$0.25$</td>
                        <td style="padding: 8px;">$1.0$</td>
                        <td style="padding: 8px;">1 bit reduces distortion to 25% of variance.</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #e5e7eb;">
                        <td style="padding: 8px;">$0.01$</td>
                        <td style="padding: 8px;">$3.32$</td>
                        <td style="padding: 8px;">High fidelity needs many bits.</td>
                    </tr>
                    <tr>
                        <td style="padding: 8px;">$0 \to$</td>
                        <td style="padding: 8px;">$\infty$</td>
                        <td style="padding: 8px;">Perfect reconstruction is impossible with finite bits.</td>
                    </tr>
                </table>

                <p>
                    The inverse relationship $D = \sigma^2 \cdot 2^{-2R}$ shows that each additional
                    bit of rate <strong>halves the distortion squared</strong> (6 dB improvement per bit),
                    which is a classic result in quantization theory.
                </p>

                <!-- Section 8: Multivariate Sources & Water-Filling -->
                <h2 id="water-filling">Multivariate Sources &amp; Water-Filling</h2>

                <p>
                    For a multivariate Gaussian source $\mathbf{X} \sim \mathcal{N}(\mathbf{0}, \Sigma)$
                    with $n$ components, the rate-distortion problem becomes: how should we allocate
                    bits across the $n$ dimensions?
                </p>

                <p>
                    First, diagonalize $\Sigma$ using eigendecomposition: $\Sigma = U \Lambda U^T$
                    where $\Lambda = \text{diag}(\lambda_1, \ldots, \lambda_n)$. In the eigenbasis, the
                    components are independent, each with variance $\lambda_i$.
                </p>

                <div class="definition-box">
                    <div class="box-title">Water-Filling Solution</div>
                    <p>
                        The optimal distortion for each component is:
                        $$D_i = \min(\theta, \lambda_i)$$
                        where $\theta > 0$ is chosen so that $\sum_{i=1}^n D_i = D$ (total distortion budget).
                    </p>
                    <p style="margin-bottom: 0;">
                        The total rate is:
                        $$R(D) = \sum_{i=1}^n \frac{1}{2} \log_2 \frac{\lambda_i}{D_i} = \sum_{i:\lambda_i > \theta} \frac{1}{2} \log_2 \frac{\lambda_i}{\theta}$$
                    </p>
                </div>

                <p>
                    <strong>Why "water-filling"?</strong> Imagine the eigenvalues $\lambda_i$ as
                    columns of different heights. Pour water until the total "distortion" (water level
                    $\theta$ minus column height, clamped at 0) equals $D$. Tall columns (high-variance
                    dimensions) get more bits; short columns (low-variance dimensions) may get zero
                    bits and are ignored entirely.
                </p>

                <div class="warning-box">
                    <div class="box-title">Connection to PCA</div>
                    <p style="margin-bottom: 0;">
                        This is exactly what PCA does! PCA keeps the top-$k$ eigenvectors (highest variance)
                        and discards the rest. The water-filling solution says this is
                        <strong>information-theoretically optimal</strong> for Gaussian sources — the
                        best lossy compressor allocates bits to the principal components.
                    </p>
                </div>

                <!-- Section 9: The Binary Source -->
                <h2 id="binary-source">The Binary Source</h2>

                <p>
                    Another exactly solvable case: a binary source with Hamming distortion.
                </p>

                <div class="definition-box">
                    <div class="box-title">Binary Source Setup</div>
                    <p style="margin-bottom: 0;">
                        Source: $X \sim \text{Bernoulli}(p)$ with $p \leq 1/2$<br>
                        Distortion: $d(x, \hat{x}) = \mathbb{1}[x \neq \hat{x}]$ (Hamming distance)<br>
                        Constraint: $\Pr(X \neq \hat{X}) \leq D$
                    </p>
                </div>

                <div class="math-block">
                    $$R(D) = \begin{cases} H(p) - H(D) & \text{if } 0 \leq D \leq p \\ 0 & \text{if } D > p \end{cases}$$
                </div>

                <p>
                    where $H(p) = -p\log_2 p - (1-p)\log_2(1-p)$ is the binary entropy function.
                </p>

                <div class="math-derivation">
                    <div class="math-derivation-title">Interpreting the Binary $R(D)$</div>

                    <div class="math-step">
                        <div class="math-step-number">1</div>
                        <div class="math-step-content">
                            At $D = 0$: $R(0) = H(p) - H(0) = H(p)$. Perfect reproduction requires
                            the full entropy — this matches Shannon's lossless coding theorem.
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">2</div>
                        <div class="math-step-content">
                            At $D = p$: $R(p) = H(p) - H(p) = 0$. If we tolerate error rate $p$,
                            we need zero bits — just always guess the more likely symbol.
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">3</div>
                        <div class="math-step-content">
                            The optimal "test channel" is a binary symmetric channel (BSC) with crossover
                            probability $D$. The encoder deliberately introduces $D$ fraction of errors
                            to save $H(D)$ bits.
                        </div>
                    </div>
                </div>

                <!-- Section 10: Blahut-Arimoto Algorithm -->
                <h2 id="blahut-arimoto">The Blahut-Arimoto Algorithm</h2>

                <p>
                    For sources without closed-form $R(D)$, the <strong>Blahut-Arimoto algorithm</strong>
                    (1972) computes $R(D)$ numerically by alternating optimization.
                </p>

                <div class="math-derivation">
                    <div class="math-derivation-title">Blahut-Arimoto Iteration</div>

                    <div class="math-step">
                        <div class="math-step-number">1</div>
                        <div class="math-step-content">
                            <strong>Initialize:</strong> Choose an initial marginal $q(z)$ (e.g., uniform
                            over a discrete set of reproduction symbols).
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">2</div>
                        <div class="math-step-content">
                            <strong>Update encoder:</strong> For fixed $q(z)$ and Lagrange parameter $s$:
                            $$p(z|x) = \frac{q(z) \exp(-s \cdot d(x, z))}{\sum_{z'} q(z') \exp(-s \cdot d(x, z'))}$$
                            This is a softmax over distortions — lower distortion codes get higher probability.
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">3</div>
                        <div class="math-step-content">
                            <strong>Update marginal:</strong> $q(z) = \sum_x p(x) p(z|x)$.
                            The marginal distribution of codes induced by the encoder.
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">4</div>
                        <div class="math-step-content">
                            <strong>Repeat</strong> steps 2-3 until convergence. The algorithm is
                            guaranteed to converge to the global optimum because the problem is convex.
                        </div>
                    </div>
                </div>

                <div class="note-box">
                    <div class="box-title">Connection to EM Algorithm</div>
                    <p style="margin-bottom: 0;">
                        The Blahut-Arimoto algorithm has the same alternating-optimization structure as
                        the EM algorithm: fix one variable, optimize the other, repeat. Step 2 is like
                        the E-step (compute posterior given current parameters), and step 3 is like the
                        M-step (update parameters given posterior).
                    </p>
                </div>

                <!-- Section 11: Distortion-Rate Function D(R) -->
                <h2 id="distortion-rate">The Distortion-Rate Function $D(R)$</h2>

                <p>
                    The <strong>distortion-rate function</strong> $D(R)$ is simply the inverse of $R(D)$.
                    Instead of asking "how many bits for distortion $D$?", it asks "what distortion
                    can I achieve with rate $R$?"
                </p>

                <div class="math-block">
                    $$D(R) = \min_{p(z|x): \, I(X;Z) \leq R} \mathbb{E}[d(X, \hat{X})]$$
                </div>

                <p>For the Gaussian source:</p>

                <div class="math-block">
                    $$D(R) = \sigma^2 \cdot 2^{-2R}$$
                </div>

                <p>
                    This shows <strong>exponential decay</strong>: each additional bit of rate
                    reduces distortion by a factor of 4 (6.02 dB). This "6 dB per bit" rule is
                    a fundamental benchmark in signal processing.
                </p>

                <!-- Section 12: Connection to Autoencoders -->
                <h2 id="connection-autoencoders">Connection to Autoencoders &amp; VAEs</h2>

                <p>
                    Rate-distortion theory provides the theoretical foundation for understanding
                    autoencoders. The parallels are deep:
                </p>

                <table style="width: 100%; border-collapse: collapse; margin: 1rem 0;">
                    <tr style="border-bottom: 2px solid #e5e7eb;">
                        <th style="text-align: left; padding: 8px;">Rate-Distortion</th>
                        <th style="text-align: left; padding: 8px;">Autoencoder / VAE</th>
                    </tr>
                    <tr style="border-bottom: 1px solid #e5e7eb;">
                        <td style="padding: 8px;">Source $X$</td>
                        <td style="padding: 8px;">Training data</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #e5e7eb;">
                        <td style="padding: 8px;">Encoder $p(z|x)$</td>
                        <td style="padding: 8px;">Encoder network $q_\phi(z|x)$</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #e5e7eb;">
                        <td style="padding: 8px;">Code $Z$</td>
                        <td style="padding: 8px;">Latent representation $z$</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #e5e7eb;">
                        <td style="padding: 8px;">Decoder $p(\hat{x}|z)$</td>
                        <td style="padding: 8px;">Decoder network $p_\theta(x|z)$</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #e5e7eb;">
                        <td style="padding: 8px;">Rate $I(X;Z)$</td>
                        <td style="padding: 8px;">KL divergence $D_{KL}(q(z|x) \| p(z))$</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #e5e7eb;">
                        <td style="padding: 8px;">Distortion $\mathbb{E}[d(X,\hat{X})]$</td>
                        <td style="padding: 8px;">Reconstruction loss $\mathbb{E}[-\log p(x|z)]$</td>
                    </tr>
                    <tr>
                        <td style="padding: 8px;">Lagrange multiplier $s$</td>
                        <td style="padding: 8px;">$\beta$ in $\beta$-VAE</td>
                    </tr>
                </table>

                <div class="definition-box">
                    <div class="box-title">The Information Bottleneck View</div>
                    <p>
                        An autoencoder with bottleneck dimension $d$ operates on the $R(D)$ curve.
                        The bottleneck constrains the rate: fewer latent dimensions = fewer bits =
                        more information discarded.
                    </p>
                    <p style="margin-bottom: 0;">
                        <strong>Standard autoencoder:</strong> Deterministic encoder $z = f_\theta(x)$.
                        Minimizes distortion for a fixed bottleneck size. Operates at one point on $R(D)$.<br>
                        <strong>VAE:</strong> Stochastic encoder $q(z|x) = \mathcal{N}(\mu, \sigma^2)$.
                        The KL term explicitly penalizes rate. By varying $\beta$, we sweep the $R(D)$
                        curve, trading off rate and distortion continuously.
                    </p>
                </div>

                <div class="warning-box">
                    <div class="box-title">The Deep Insight</div>
                    <p style="margin-bottom: 0;">
                        Rate-distortion theory tells us that <strong>no autoencoder</strong> — no matter
                        how large the network, how clever the architecture — can achieve distortion
                        below $D(R)$ at rate $R$. The curve $R(D)$ is the <strong>Pareto frontier</strong>:
                        any point below it is information-theoretically impossible. Good autoencoders
                        approach this frontier; they cannot cross it.
                    </p>
                </div>

                <!-- Navigation -->
                <div class="tutorial-nav">
                    <a href="../11-rnn/index.html" class="tutorial-nav-link prev">
                        <span class="nav-label">Previous</span>
                        <span class="nav-title">← RNNs</span>
                    </a>
                    <a href="../15-autoencoder/index.html" class="tutorial-nav-link next">
                        <span class="nav-label">Next</span>
                        <span class="nav-title">Autoencoders →</span>
                    </a>
                </div>

            </article>

            <!-- ============== CODE TAB ============== -->
            <article class="article-content" id="code" style="display: none;">

                <h2>Code Examples</h2>

                <!-- Code Example 1 -->
                <div class="code-example">
                    <h3>1. Computing and Plotting R(D) for a Gaussian Source</h3>
                    <pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

def R_gaussian(D, sigma2=1.0):
    """Rate-distortion function for Gaussian source with MSE."""
    D = np.clip(D, 1e-10, sigma2)
    return 0.5 * np.log2(sigma2 / D)

def D_gaussian(R, sigma2=1.0):
    """Distortion-rate function (inverse of R(D))."""
    return sigma2 * 2**(-2 * R)

# Plot R(D) curve
sigma2 = 1.0
D = np.linspace(0.01, sigma2, 200)
R = R_gaussian(D, sigma2)

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# R(D) curve
axes[0].plot(D, R, 'b-', linewidth=2)
axes[0].fill_between(D, R, alpha=0.1, color='blue', label='Achievable region')
axes[0].set_xlabel('Distortion D')
axes[0].set_ylabel('Rate R (bits)')
axes[0].set_title('Rate-Distortion Function R(D)')
axes[0].set_xlim(0, sigma2)
axes[0].set_ylim(0, 5)
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# D(R) curve
R_vals = np.linspace(0, 4, 200)
D_vals = D_gaussian(R_vals, sigma2)

axes[1].plot(R_vals, D_vals, 'r-', linewidth=2)
axes[1].set_xlabel('Rate R (bits)')
axes[1].set_ylabel('Distortion D')
axes[1].set_title('Distortion-Rate Function D(R)')
axes[1].set_ylim(0, sigma2)
axes[1].grid(True, alpha=0.3)

# Annotate 6dB per bit
for r in [1, 2, 3]:
    d = D_gaussian(r, sigma2)
    axes[1].annotate(f'R={r}: D={d:.3f}', (r, d),
                     textcoords="offset points", xytext=(10, 10))

plt.tight_layout()
plt.savefig('rate_distortion_gaussian.png', dpi=150)
plt.show()

print("Key values for sigma^2 = 1:")
for D_val in [1.0, 0.5, 0.25, 0.1, 0.01]:
    print(f"  D = {D_val:.2f} -> R(D) = {R_gaussian(D_val):.3f} bits")</code></pre>
                </div>

                <!-- Code Example 2 -->
                <div class="code-example">
                    <h3>2. Blahut-Arimoto Algorithm</h3>
                    <pre><code class="language-python">import numpy as np

def blahut_arimoto(px, distortion_matrix, s, max_iter=100, tol=1e-8):
    """
    Blahut-Arimoto algorithm for computing R(D).

    Args:
        px: Source distribution, shape (|X|,)
        distortion_matrix: d(x, z) matrix, shape (|X|, |Z|)
        s: Lagrange parameter (slope of R(D) curve)
        max_iter: Maximum iterations
        tol: Convergence tolerance

    Returns:
        rate: I(X; Z) in bits
        distortion: E[d(X, X_hat)]
        pz_given_x: Optimal encoder, shape (|X|, |Z|)
    """
    n_x, n_z = distortion_matrix.shape

    # Initialize marginal q(z) uniformly
    qz = np.ones(n_z) / n_z

    for iteration in range(max_iter):
        # Step 1: Update encoder p(z|x)
        # p(z|x) = q(z) * exp(-s * d(x,z)) / normalizer
        log_numerator = np.log(qz[None, :] + 1e-30) - s * distortion_matrix
        log_normalizer = np.logaddexp.reduce(log_numerator, axis=1, keepdims=True)
        pz_given_x = np.exp(log_numerator - log_normalizer)

        # Step 2: Update marginal q(z) = sum_x p(x) p(z|x)
        qz_new = px @ pz_given_x

        # Check convergence
        if np.max(np.abs(qz_new - qz)) < tol:
            break
        qz = qz_new

    # Compute rate I(X; Z) and distortion E[d(X, X_hat)]
    pxz = px[:, None] * pz_given_x  # Joint p(x, z)
    pz = pxz.sum(axis=0)

    # I(X; Z) = sum p(x,z) log(p(z|x) / p(z))
    mask = pxz > 1e-30
    rate = np.sum(pxz[mask] * np.log2(pz_given_x[mask] / pz[None, :][mask]))

    distortion = np.sum(pxz * distortion_matrix)

    return rate, distortion, pz_given_x

# Example: Binary source with Hamming distortion
p = 0.3  # Bernoulli parameter
px = np.array([1-p, p])

# Hamming distortion: d(x, x_hat) = 1 if x != x_hat
distortion_matrix = np.array([[0, 1],
                               [1, 0]], dtype=float)

# Sweep s to trace R(D) curve
s_values = np.logspace(-1, 3, 50)
rates, distortions = [], []

for s in s_values:
    r, d, _ = blahut_arimoto(px, distortion_matrix, s)
    rates.append(r)
    distortions.append(d)

# Compare with theoretical R(D) = H(p) - H(D)
def binary_entropy(q):
    if q <= 0 or q >= 1:
        return 0
    return -q * np.log2(q) - (1-q) * np.log2(1-q)

D_theory = np.linspace(0.001, p - 0.001, 100)
R_theory = [binary_entropy(p) - binary_entropy(d) for d in D_theory]

import matplotlib.pyplot as plt
plt.figure(figsize=(8, 5))
plt.plot(distortions, rates, 'ro', markersize=4, label='Blahut-Arimoto')
plt.plot(D_theory, R_theory, 'b-', linewidth=2, label='Theoretical R(D)')
plt.xlabel('Distortion D')
plt.ylabel('Rate R (bits)')
plt.title(f'Binary Source R(D), p={p}')
plt.legend()
plt.grid(True, alpha=0.3)
plt.xlim(0, p + 0.05)
plt.savefig('blahut_arimoto_binary.png', dpi=150)
plt.show()</code></pre>
                </div>

                <!-- Code Example 3 -->
                <div class="code-example">
                    <h3>3. Water-Filling for Multivariate Gaussian</h3>
                    <pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

def water_filling(eigenvalues, D_total):
    """
    Compute water-filling solution for multivariate Gaussian R(D).

    Args:
        eigenvalues: Eigenvalues of covariance matrix (variances per component)
        D_total: Total distortion budget

    Returns:
        Di: Per-component distortion allocation
        Ri: Per-component rate allocation
        theta: Water level
    """
    lambdas = np.sort(eigenvalues)[::-1]  # Sort descending
    n = len(lambdas)

    # Binary search for water level theta
    theta_lo, theta_hi = 0, max(lambdas)

    for _ in range(100):  # Binary search iterations
        theta = (theta_lo + theta_hi) / 2
        Di = np.minimum(theta, lambdas)
        total_D = np.sum(Di)

        if total_D < D_total:
            theta_lo = theta
        else:
            theta_hi = theta

    theta = (theta_lo + theta_hi) / 2
    Di = np.minimum(theta, lambdas)

    # Rate per component
    Ri = np.where(lambdas > theta, 0.5 * np.log2(lambdas / theta), 0)

    return Di, Ri, theta

# Example: 5D Gaussian with varying eigenvalues
eigenvalues = np.array([4.0, 2.0, 1.0, 0.5, 0.1])
D_total = 3.0  # Total distortion budget

Di, Ri, theta = water_filling(eigenvalues, D_total)

print(f"Water level theta = {theta:.4f}")
print(f"\nComponent | Variance | Distortion | Rate (bits)")
print("-" * 52)
for i, (lam, d, r) in enumerate(zip(eigenvalues, Di, Ri)):
    status = "encoded" if r > 0 else "DROPPED"
    print(f"    {i+1}     |  {lam:.2f}    |   {d:.4f}   |  {r:.3f}  {status}")

print(f"\nTotal distortion: {np.sum(Di):.4f} (budget: {D_total})")
print(f"Total rate: {np.sum(Ri):.3f} bits")

# Visualize water-filling
fig, ax = plt.subplots(figsize=(10, 6))
x = np.arange(len(eigenvalues))
width = 0.4

# Draw eigenvalue bars
bars = ax.bar(x, eigenvalues, width, color='steelblue', alpha=0.7, label='Variance λᵢ')

# Draw water level
ax.axhline(y=theta, color='red', linestyle='--', linewidth=2, label=f'Water level θ={theta:.3f}')

# Draw distortion (water)
for i in range(len(eigenvalues)):
    if eigenvalues[i] > theta:
        ax.bar(i, theta, width, color='lightcoral', alpha=0.5)
    else:
        ax.bar(i, eigenvalues[i], width, color='lightcoral', alpha=0.5)

ax.set_xlabel('Component i')
ax.set_ylabel('Variance / Distortion')
ax.set_title('Water-Filling Bit Allocation')
ax.set_xticks(x)
ax.set_xticklabels([f'λ{i+1}={v}' for i, v in enumerate(eigenvalues)])
ax.legend()
ax.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.savefig('water_filling.png', dpi=150)
plt.show()</code></pre>
                </div>

                <!-- Navigation -->
                <div class="tutorial-nav">
                    <a href="../11-rnn/index.html" class="tutorial-nav-link prev">
                        <span class="nav-label">Previous</span>
                        <span class="nav-title">← RNNs</span>
                    </a>
                    <a href="../15-autoencoder/index.html" class="tutorial-nav-link next">
                        <span class="nav-label">Next</span>
                        <span class="nav-title">Autoencoders →</span>
                    </a>
                </div>
            </article>

            <!-- ============== EXERCISES TAB ============== -->
            <article class="article-content" id="exercises" style="display: none;">

                <h2>Exercises</h2>
                <p>30 exercises covering all aspects of rate-distortion theory. Solutions included.</p>

                <h3>Easy (Exercises 1-10)</h3>

                <!-- Exercise 1 -->
                <div class="exercise-item">
                    <h4>Exercise 1: Gaussian R(D) Computation</h4>
                    <p>A Gaussian source has variance $\sigma^2 = 4$. Compute $R(D)$ for $D = 4, 2, 1, 0.25$.</p>
                    <button class="solution-toggle" onclick="this.nextElementSibling.style.display = this.nextElementSibling.style.display === 'none' ? 'block' : 'none'">Show Solution</button>
                    <div class="solution-content" style="display: none;">
                        <p>Using $R(D) = \frac{1}{2}\log_2\frac{\sigma^2}{D}$ with $\sigma^2 = 4$:</p>
                        <ul>
                            <li>$D = 4$: $R(4) = \frac{1}{2}\log_2\frac{4}{4} = \frac{1}{2}\log_2 1 = 0$ bits</li>
                            <li>$D = 2$: $R(2) = \frac{1}{2}\log_2\frac{4}{2} = \frac{1}{2}\log_2 2 = 0.5$ bits</li>
                            <li>$D = 1$: $R(1) = \frac{1}{2}\log_2\frac{4}{1} = \frac{1}{2}\log_2 4 = 1.0$ bit</li>
                            <li>$D = 0.25$: $R(0.25) = \frac{1}{2}\log_2\frac{4}{0.25} = \frac{1}{2}\log_2 16 = 2.0$ bits</li>
                        </ul>
                        <p>Each doubling of quality (halving $D$) costs exactly 1 additional bit.</p>
                    </div>
                </div>

                <!-- Exercise 2 -->
                <div class="exercise-item">
                    <h4>Exercise 2: Maximum Distortion</h4>
                    <p>A source $X$ has mean $\mu = 3$ and variance $\sigma^2 = 5$. Under MSE distortion, what is $D_{\max}$? What constant does the decoder output at $R = 0$?</p>
                    <button class="solution-toggle" onclick="this.nextElementSibling.style.display = this.nextElementSibling.style.display === 'none' ? 'block' : 'none'">Show Solution</button>
                    <div class="solution-content" style="display: none;">
                        <p>At $R = 0$, no information is transmitted. The decoder outputs a constant $c$ that minimizes $\mathbb{E}[(X - c)^2]$.</p>
                        <p>Taking the derivative: $\frac{d}{dc}\mathbb{E}[(X-c)^2] = -2\mathbb{E}[X-c] = 0$, so $c = \mathbb{E}[X] = \mu = 3$.</p>
                        <p>The resulting distortion is:</p>
                        <p>$D_{\max} = \mathbb{E}[(X - 3)^2] = \text{Var}(X) = \sigma^2 = 5$</p>
                        <p>So $D_{\max} = 5$ and the decoder always outputs $\hat{x} = 3$.</p>
                    </div>
                </div>

                <!-- Exercise 3 -->
                <div class="exercise-item">
                    <h4>Exercise 3: 6 dB Per Bit Rule</h4>
                    <p>For a Gaussian source with $\sigma^2 = 1$, verify the "6 dB per bit" rule: show that increasing the rate by 1 bit reduces the distortion by a factor of 4 (which is $10\log_{10}(4) \approx 6.02$ dB).</p>
                    <button class="solution-toggle" onclick="this.nextElementSibling.style.display = this.nextElementSibling.style.display === 'none' ? 'block' : 'none'">Show Solution</button>
                    <div class="solution-content" style="display: none;">
                        <p>The distortion-rate function is $D(R) = \sigma^2 \cdot 2^{-2R} = 2^{-2R}$.</p>
                        <p>At rate $R$: $D(R) = 2^{-2R}$.</p>
                        <p>At rate $R + 1$: $D(R+1) = 2^{-2(R+1)} = 2^{-2R-2} = \frac{2^{-2R}}{4}$.</p>
                        <p>The ratio: $\frac{D(R)}{D(R+1)} = 4$.</p>
                        <p>In decibels: $10\log_{10}(4) = 10 \times 0.6021 = 6.021$ dB.</p>
                        <p>So each additional bit reduces distortion by exactly a factor of 4, or approximately 6 dB.</p>
                    </div>
                </div>

                <!-- Exercise 4 -->
                <div class="exercise-item">
                    <h4>Exercise 4: Delta Function Practice</h4>
                    <p>Evaluate the following integrals involving the Dirac delta function:
                    (a) $\int_{-\infty}^{\infty} x^3 \delta(x - 2) \, dx$,
                    (b) $\int_{-\infty}^{\infty} e^{-x} \delta(x) \, dx$,
                    (c) $\int_{-\infty}^{\infty} \cos(x) \delta(x - \pi) \, dx$.</p>
                    <button class="solution-toggle" onclick="this.nextElementSibling.style.display = this.nextElementSibling.style.display === 'none' ? 'block' : 'none'">Show Solution</button>
                    <div class="solution-content" style="display: none;">
                        <p>Using the sifting property $\int g(z)\delta(z - c)\,dz = g(c)$:</p>
                        <p>(a) $g(x) = x^3$, $c = 2$: $\int x^3 \delta(x-2)\,dx = 2^3 = 8$</p>
                        <p>(b) $g(x) = e^{-x}$, $c = 0$: $\int e^{-x}\delta(x)\,dx = e^0 = 1$</p>
                        <p>(c) $g(x) = \cos(x)$, $c = \pi$: $\int \cos(x)\delta(x-\pi)\,dx = \cos(\pi) = -1$</p>
                    </div>
                </div>

                <!-- Exercise 5 -->
                <div class="exercise-item">
                    <h4>Exercise 5: Deterministic or Stochastic?</h4>
                    <p>Classify each encoder as deterministic or stochastic:
                    (a) $z = \text{round}(x)$,
                    (b) $p(z|x) = \mathcal{N}(x, 0.1)$,
                    (c) $z = \text{sign}(x)$,
                    (d) $p(z = 1|x) = \sigma(x)$, $p(z = 0|x) = 1 - \sigma(x)$ where $\sigma$ is sigmoid.</p>
                    <button class="solution-toggle" onclick="this.nextElementSibling.style.display = this.nextElementSibling.style.display === 'none' ? 'block' : 'none'">Show Solution</button>
                    <div class="solution-content" style="display: none;">
                        <p>(a) <strong>Deterministic.</strong> $z = \text{round}(x)$ is a fixed function — same input always gives same output. $p(z|x) = \delta(z - \text{round}(x))$.</p>
                        <p>(b) <strong>Stochastic.</strong> $z$ is drawn from a Gaussian centered at $x$ with variance 0.1. Different runs produce different $z$ values.</p>
                        <p>(c) <strong>Deterministic.</strong> $z = \text{sign}(x) \in \{-1, +1\}$ is a fixed function.</p>
                        <p>(d) <strong>Stochastic.</strong> Given $x = 0$ (where $\sigma(0) = 0.5$), the encoder outputs $z = 1$ or $z = 0$ each with probability 0.5. The output is random.</p>
                    </div>
                </div>

                <!-- Exercise 6 -->
                <div class="exercise-item">
                    <h4>Exercise 6: Binary Entropy</h4>
                    <p>Compute the binary entropy $H(p)$ for $p = 0, 0.1, 0.25, 0.5, 0.75, 1.0$. What value of $p$ maximizes $H(p)$?</p>
                    <button class="solution-toggle" onclick="this.nextElementSibling.style.display = this.nextElementSibling.style.display === 'none' ? 'block' : 'none'">Show Solution</button>
                    <div class="solution-content" style="display: none;">
                        <p>$H(p) = -p\log_2 p - (1-p)\log_2(1-p)$:</p>
                        <ul>
                            <li>$H(0) = 0$ bits (certain outcome)</li>
                            <li>$H(0.1) = -0.1\log_2(0.1) - 0.9\log_2(0.9) = 0.332 + 0.137 = 0.469$ bits</li>
                            <li>$H(0.25) = -0.25\log_2(0.25) - 0.75\log_2(0.75) = 0.5 + 0.311 = 0.811$ bits</li>
                            <li>$H(0.5) = -0.5\log_2(0.5) - 0.5\log_2(0.5) = 0.5 + 0.5 = 1.0$ bit</li>
                            <li>$H(0.75) = 0.811$ bits (symmetric with $p=0.25$)</li>
                            <li>$H(1.0) = 0$ bits (certain outcome)</li>
                        </ul>
                        <p>$H(p)$ is maximized at $p = 0.5$, where $H(0.5) = 1$ bit. Maximum uncertainty = maximum entropy.</p>
                    </div>
                </div>

                <!-- Exercise 7 -->
                <div class="exercise-item">
                    <h4>Exercise 7: Binary Source R(D)</h4>
                    <p>For a binary source with $p = 0.2$, compute $R(D)$ at $D = 0, 0.05, 0.1, 0.2$.</p>
                    <button class="solution-toggle" onclick="this.nextElementSibling.style.display = this.nextElementSibling.style.display === 'none' ? 'block' : 'none'">Show Solution</button>
                    <div class="solution-content" style="display: none;">
                        <p>$R(D) = H(p) - H(D)$ for $0 \leq D \leq p$. First: $H(0.2) = -0.2\log_2(0.2) - 0.8\log_2(0.8) = 0.4644 + 0.2575 = 0.722$ bits.</p>
                        <ul>
                            <li>$D = 0$: $R(0) = 0.722 - 0 = 0.722$ bits (lossless)</li>
                            <li>$D = 0.05$: $H(0.05) = 0.286$, so $R(0.05) = 0.722 - 0.286 = 0.436$ bits</li>
                            <li>$D = 0.1$: $H(0.1) = 0.469$, so $R(0.1) = 0.722 - 0.469 = 0.253$ bits</li>
                            <li>$D = 0.2$: $H(0.2) = 0.722$, so $R(0.2) = 0.722 - 0.722 = 0$ bits</li>
                        </ul>
                        <p>At $D = p = 0.2$, we need zero bits — just always output the more likely symbol (0).</p>
                    </div>
                </div>

                <!-- Exercise 8 -->
                <div class="exercise-item">
                    <h4>Exercise 8: Mutual Information Calculation</h4>
                    <p>Let $X \in \{0, 1\}$ with $p(0) = 0.5, p(1) = 0.5$. The encoder is: $p(z=0|x=0) = 0.9, p(z=1|x=0) = 0.1, p(z=0|x=1) = 0.3, p(z=1|x=1) = 0.7$. Compute $I(X; Z)$.</p>
                    <button class="solution-toggle" onclick="this.nextElementSibling.style.display = this.nextElementSibling.style.display === 'none' ? 'block' : 'none'">Show Solution</button>
                    <div class="solution-content" style="display: none;">
                        <p>First compute the joint distribution $p(x, z) = p(x) p(z|x)$:</p>
                        <ul>
                            <li>$p(0, 0) = 0.5 \times 0.9 = 0.45$</li>
                            <li>$p(0, 1) = 0.5 \times 0.1 = 0.05$</li>
                            <li>$p(1, 0) = 0.5 \times 0.3 = 0.15$</li>
                            <li>$p(1, 1) = 0.5 \times 0.7 = 0.35$</li>
                        </ul>
                        <p>Marginal $p(z)$: $p(z=0) = 0.45 + 0.15 = 0.6$, $p(z=1) = 0.05 + 0.35 = 0.4$.</p>
                        <p>$I(X;Z) = \sum_{x,z} p(x,z) \log_2 \frac{p(x,z)}{p(x)p(z)}$:</p>
                        <p>$= 0.45\log_2\frac{0.45}{0.5 \times 0.6} + 0.05\log_2\frac{0.05}{0.5 \times 0.4} + 0.15\log_2\frac{0.15}{0.5 \times 0.6} + 0.35\log_2\frac{0.35}{0.5 \times 0.4}$</p>
                        <p>$= 0.45\log_2 1.5 + 0.05\log_2 0.25 + 0.15\log_2 0.5 + 0.35\log_2 1.75$</p>
                        <p>$= 0.45(0.585) + 0.05(-2) + 0.15(-1) + 0.35(0.807)$</p>
                        <p>$= 0.263 - 0.1 - 0.15 + 0.282 = 0.295$ bits</p>
                    </div>
                </div>

                <!-- Exercise 9 -->
                <div class="exercise-item">
                    <h4>Exercise 9: R(D) Boundary Conditions</h4>
                    <p>Prove that for any source with MSE distortion, $R(D_{\max}) = 0$ where $D_{\max} = \text{Var}(X)$. <em>Hint: construct an encoder that achieves zero rate with distortion exactly $\text{Var}(X)$.</em></p>
                    <button class="solution-toggle" onclick="this.nextElementSibling.style.display = this.nextElementSibling.style.display === 'none' ? 'block' : 'none'">Show Solution</button>
                    <div class="solution-content" style="display: none;">
                        <p>Consider the "trivial encoder" that ignores $X$ entirely: $p(z|x) = p(z)$ for all $x$. This makes $Z$ independent of $X$, so $I(X; Z) = 0$ (zero rate).</p>
                        <p>The optimal decoder with zero information outputs the constant $\hat{x} = c$ that minimizes $\mathbb{E}[(X - c)^2]$. Taking the derivative and setting to zero: $c = \mathbb{E}[X]$.</p>
                        <p>The distortion is $\mathbb{E}[(X - \mathbb{E}[X])^2] = \text{Var}(X) = D_{\max}$.</p>
                        <p>Since $R(D)$ is the <em>minimum</em> rate for distortion $\leq D$, and we found a scheme with rate 0 and distortion $D_{\max}$: $R(D_{\max}) \leq 0$. Since $R(D) \geq 0$ always, we get $R(D_{\max}) = 0$. $\square$</p>
                    </div>
                </div>

                <!-- Exercise 10 -->
                <div class="exercise-item">
                    <h4>Exercise 10: Distortion-Rate Inversion</h4>
                    <p>Given $R(D) = \frac{1}{2}\log_2\frac{\sigma^2}{D}$ for a Gaussian source, derive the inverse function $D(R)$. Then compute: how many bits per sample are needed to achieve SNR = 40 dB for $\sigma^2 = 1$? (SNR in dB = $10\log_{10}(\sigma^2/D)$.)</p>
                    <button class="solution-toggle" onclick="this.nextElementSibling.style.display = this.nextElementSibling.style.display === 'none' ? 'block' : 'none'">Show Solution</button>
                    <div class="solution-content" style="display: none;">
                        <p><strong>Inversion:</strong> $R = \frac{1}{2}\log_2\frac{\sigma^2}{D}$ → $2R = \log_2\frac{\sigma^2}{D}$ → $\frac{\sigma^2}{D} = 2^{2R}$ → $D(R) = \sigma^2 \cdot 2^{-2R}$.</p>
                        <p><strong>SNR calculation:</strong> SNR = $10\log_{10}(\sigma^2/D)$ = 40 dB.</p>
                        <p>So $\sigma^2/D = 10^{4} = 10000$. With $\sigma^2 = 1$: $D = 10^{-4} = 0.0001$.</p>
                        <p>$R = \frac{1}{2}\log_2\frac{1}{0.0001} = \frac{1}{2}\log_2 10000 = \frac{1}{2} \times 13.288 = 6.644$ bits.</p>
                        <p>So about 6.6 bits per sample are needed for 40 dB SNR.</p>
                    </div>
                </div>

                <h3>Medium (Exercises 11-20)</h3>

                <!-- Exercise 11 -->
                <div class="exercise-item">
                    <h4>Exercise 11: Derive Gaussian R(D) from Scratch</h4>
                    <p>Starting from the definition $R(D) = \min_{p(z|x): \mathbb{E}[(X-\hat{X})^2] \leq D} I(X;Z)$, derive $R(D) = \frac{1}{2}\log_2\frac{\sigma^2}{D}$ for $X \sim \mathcal{N}(0, \sigma^2)$. <em>Hint: Use the fact that Gaussians maximize entropy for a given variance, so the optimal test channel must be Gaussian.</em></p>
                    <button class="solution-toggle" onclick="this.nextElementSibling.style.display = this.nextElementSibling.style.display === 'none' ? 'block' : 'none'">Show Solution</button>
                    <div class="solution-content" style="display: none;">
                        <p><strong>Step 1: Lower bound.</strong> For any encoder, $I(X;Z) = h(X) - h(X|Z)$. Since $\mathbb{E}[(X - \hat{X})^2] \leq D$ and $\hat{X} = \mathbb{E}[X|Z]$ is optimal, the conditional variance satisfies $\text{Var}(X|Z) \leq D$ on average.</p>
                        <p><strong>Step 2:</strong> Since the Gaussian maximizes differential entropy for a given variance: $h(X|Z) \leq \frac{1}{2}\log_2(2\pi e D)$.</p>
                        <p><strong>Step 3:</strong> Therefore $I(X;Z) = h(X) - h(X|Z) \geq \frac{1}{2}\log_2(2\pi e\sigma^2) - \frac{1}{2}\log_2(2\pi e D) = \frac{1}{2}\log_2\frac{\sigma^2}{D}$.</p>
                        <p><strong>Step 4: Achievability.</strong> The decomposition $X = Z + W$ with $Z \sim \mathcal{N}(0, \sigma^2 - D)$ and $W \sim \mathcal{N}(0, D)$ independent achieves this bound with equality. The encoder is $p(z|x) = \mathcal{N}(z; \frac{\sigma^2 - D}{\sigma^2}x, \frac{(\sigma^2-D)D}{\sigma^2})$.</p>
                        <p>The lower bound is tight, so $R(D) = \frac{1}{2}\log_2\frac{\sigma^2}{D}$. $\square$</p>
                    </div>
                </div>

                <!-- Exercise 12 -->
                <div class="exercise-item">
                    <h4>Exercise 12: Water-Filling by Hand</h4>
                    <p>A 3D Gaussian source has covariance eigenvalues $\lambda_1 = 8, \lambda_2 = 2, \lambda_3 = 0.5$. Total distortion budget $D = 4.5$. Find the water level $\theta$, per-component distortion $D_i$, per-component rate $R_i$, and total rate $R$.</p>
                    <button class="solution-toggle" onclick="this.nextElementSibling.style.display = this.nextElementSibling.style.display === 'none' ? 'block' : 'none'">Show Solution</button>
                    <div class="solution-content" style="display: none;">
                        <p>The water-filling rule: $D_i = \min(\theta, \lambda_i)$ with $\sum D_i = D$.</p>
                        <p><strong>Try $\theta = 2$:</strong> $D_1 = \min(2, 8) = 2$, $D_2 = \min(2, 2) = 2$, $D_3 = \min(2, 0.5) = 0.5$. Total $= 4.5$. This matches our budget!</p>
                        <p>So $\theta = 2$.</p>
                        <p>Per-component rates (only components where $\lambda_i > \theta$):</p>
                        <ul>
                            <li>$R_1 = \frac{1}{2}\log_2\frac{8}{2} = \frac{1}{2}\log_2 4 = 1$ bit</li>
                            <li>$R_2 = \frac{1}{2}\log_2\frac{2}{2} = \frac{1}{2}\log_2 1 = 0$ bits (at threshold)</li>
                            <li>$R_3 = 0$ bits (below threshold, $\lambda_3 < \theta$, this component is dropped)</li>
                        </ul>
                        <p><strong>Total rate: $R = 1 + 0 + 0 = 1$ bit.</strong></p>
                        <p>Only the first component (highest variance) gets any bits. The second is at the threshold (zero rate), and the third is discarded entirely.</p>
                    </div>
                </div>

                <!-- Exercise 13 -->
                <div class="exercise-item">
                    <h4>Exercise 13: Prove R(D) is Convex</h4>
                    <p>Show that $R(D)$ is a convex function of $D$. <em>Hint: Consider time-sharing between two encoder-decoder pairs.</em></p>
                    <button class="solution-toggle" onclick="this.nextElementSibling.style.display = this.nextElementSibling.style.display === 'none' ? 'block' : 'none'">Show Solution</button>
                    <div class="solution-content" style="display: none;">
                        <p>Let $p_1(z|x)$ achieve $(R_1, D_1)$ and $p_2(z|x)$ achieve $(R_2, D_2)$ on the $R(D)$ curve.</p>
                        <p><strong>Time-sharing argument:</strong> Consider a scheme that uses $p_1$ for fraction $\lambda$ of symbols and $p_2$ for fraction $(1-\lambda)$:</p>
                        <ul>
                            <li>Average rate = $\lambda R_1 + (1-\lambda)R_2$</li>
                            <li>Average distortion = $\lambda D_1 + (1-\lambda)D_2$</li>
                        </ul>
                        <p>This time-sharing scheme is a <em>valid</em> encoder achieving distortion $D' = \lambda D_1 + (1-\lambda)D_2$ at rate $R' = \lambda R_1 + (1-\lambda)R_2$.</p>
                        <p>Since $R(D)$ is the <em>minimum</em> rate for each distortion level:</p>
                        <p>$$R(\lambda D_1 + (1-\lambda)D_2) \leq \lambda R_1 + (1-\lambda)R_2 = \lambda R(D_1) + (1-\lambda)R(D_2)$$</p>
                        <p>This is the definition of convexity. $\square$</p>
                    </div>
                </div>

                <!-- Exercise 14 -->
                <div class="exercise-item">
                    <h4>Exercise 14: Lagrangian Slope</h4>
                    <p>The Lagrangian is $\mathcal{L} = I(X;Z) + s \cdot \mathbb{E}[d(X,\hat{X})]$. Show that the Lagrange multiplier $s$ equals $-R'(D) = -\frac{dR}{dD}$ (the negative slope of the R(D) curve). Verify this for the Gaussian case.</p>
                    <button class="solution-toggle" onclick="this.nextElementSibling.style.display = this.nextElementSibling.style.display === 'none' ? 'block' : 'none'">Show Solution</button>
                    <div class="solution-content" style="display: none;">
                        <p><strong>General case:</strong> At the optimum, the constraint $\mathbb{E}[d] = D$ is active. By the KKT conditions, the optimal rate satisfies $R(D) = \min_{p(z|x)} [I(X;Z) + s(D - \mathbb{E}[d])] + \text{const}$. Differentiating with respect to $D$: $R'(D) = -s$, so $s = -R'(D)$.</p>
                        <p><strong>Gaussian verification:</strong> $R(D) = \frac{1}{2}\log_2\frac{\sigma^2}{D} = \frac{1}{2\ln 2}\ln\frac{\sigma^2}{D}$.</p>
                        <p>$R'(D) = \frac{1}{2\ln 2} \cdot \frac{-1}{D} = \frac{-1}{2D\ln 2}$.</p>
                        <p>$s = -R'(D) = \frac{1}{2D\ln 2}$.</p>
                        <p>At $D = \sigma^2$ (zero rate): $s = \frac{1}{2\sigma^2\ln 2}$ (small, flat curve → distortion cheap).</p>
                        <p>At $D \to 0$ (high rate): $s \to \infty$ (steep curve → distortion very expensive).</p>
                    </div>
                </div>

                <!-- Exercise 15 -->
                <div class="exercise-item">
                    <h4>Exercise 15: Compare Two Encoders</h4>
                    <p>Source $X \sim \mathcal{N}(0, 1)$. Compare these two encoders at rate $R = 1$ bit:
                    (a) Deterministic 1-bit quantizer: $z = \text{sign}(x)$. (b) Optimal R(D) encoder.
                    What distortion does each achieve?</p>
                    <button class="solution-toggle" onclick="this.nextElementSibling.style.display = this.nextElementSibling.style.display === 'none' ? 'block' : 'none'">Show Solution</button>
                    <div class="solution-content" style="display: none;">
                        <p><strong>(a) Sign quantizer:</strong> $z = \text{sign}(x) \in \{-1, +1\}$. Rate $= H(Z) = 1$ bit (since $P(Z=1) = P(Z=-1) = 0.5$).</p>
                        <p>Optimal decoder: $\hat{x} = \mathbb{E}[X | Z = z]$. For $Z = +1$: $\hat{x} = \mathbb{E}[X | X > 0] = \sqrt{2/\pi} \approx 0.798$. By symmetry, for $Z = -1$: $\hat{x} = -0.798$.</p>
                        <p>MSE: $\mathbb{E}[(X - \hat{X})^2] = \mathbb{E}[X^2] - \mathbb{E}[\hat{X}^2] = 1 - 2/\pi \approx 1 - 0.637 = 0.363$.</p>
                        <p><strong>(b) Optimal encoder:</strong> $D(R) = \sigma^2 \cdot 2^{-2R} = 1 \cdot 2^{-2} = 0.25$.</p>
                        <p><strong>Comparison:</strong> The sign quantizer achieves $D = 0.363$, while the optimal achieves $D = 0.25$. The quantizer wastes $0.363 / 0.25 = 1.45\times$ more distortion — it's 45% suboptimal.</p>
                    </div>
                </div>

                <!-- Exercise 16 -->
                <div class="exercise-item">
                    <h4>Exercise 16: Blahut-Arimoto First Iteration</h4>
                    <p>Source $X \in \{a, b\}$ with $p(a) = 0.7, p(b) = 0.3$. Reproduction alphabet $\hat{X} \in \{a, b\}$. Hamming distortion. Initialize $q(z)$ uniformly: $q(a) = q(b) = 0.5$. With $s = 2$, compute one iteration of the Blahut-Arimoto algorithm (update $p(z|x)$, then update $q(z)$).</p>
                    <button class="solution-toggle" onclick="this.nextElementSibling.style.display = this.nextElementSibling.style.display === 'none' ? 'block' : 'none'">Show Solution</button>
                    <div class="solution-content" style="display: none;">
                        <p>Distortion matrix: $d(a,a) = 0, d(a,b) = 1, d(b,a) = 1, d(b,b) = 0$.</p>
                        <p><strong>Update encoder:</strong> $p(z|x) \propto q(z)\exp(-s \cdot d(x,z))$.</p>
                        <p>For $x = a$:</p>
                        <ul>
                            <li>$p(a|a) \propto 0.5 \cdot e^{-2 \cdot 0} = 0.5$</li>
                            <li>$p(b|a) \propto 0.5 \cdot e^{-2 \cdot 1} = 0.5 \cdot 0.135 = 0.0677$</li>
                            <li>Normalizing: $p(a|a) = \frac{0.5}{0.5 + 0.0677} = 0.881$, $p(b|a) = 0.119$</li>
                        </ul>
                        <p>For $x = b$ (by symmetry of distortion): $p(a|b) = 0.119$, $p(b|b) = 0.881$.</p>
                        <p><strong>Update marginal:</strong> $q(z) = \sum_x p(x)p(z|x)$.</p>
                        <ul>
                            <li>$q(a) = 0.7 \times 0.881 + 0.3 \times 0.119 = 0.617 + 0.036 = 0.653$</li>
                            <li>$q(b) = 0.7 \times 0.119 + 0.3 \times 0.881 = 0.083 + 0.264 = 0.347$</li>
                        </ul>
                        <p>After one iteration, $q(z)$ has shifted from uniform toward the source distribution — more probability on the more likely symbol $a$.</p>
                    </div>
                </div>

                <!-- Exercise 17 -->
                <div class="exercise-item">
                    <h4>Exercise 17: Rate-Distortion for Uniform Source</h4>
                    <p>Let $X \sim \text{Uniform}(0, 1)$ with MSE distortion. Show that $R(D) = \frac{1}{2}\log_2\frac{1}{12D}$ for $0 \leq D \leq 1/12$. <em>Hint: The uniform distribution has variance $1/12$ and has the maximum entropy among distributions supported on $[0,1]$.</em></p>
                    <button class="solution-toggle" onclick="this.nextElementSibling.style.display = this.nextElementSibling.style.display === 'none' ? 'block' : 'none'">Show Solution</button>
                    <div class="solution-content" style="display: none;">
                        <p>The Shannon lower bound states: $R(D) \geq h(X) - \frac{1}{2}\log_2(2\pi e D)$.</p>
                        <p>For $X \sim \text{Uniform}(0,1)$: $h(X) = \log_2 1 = 0$ (differential entropy).</p>
                        <p>Wait — this gives $R(D) \geq -\frac{1}{2}\log_2(2\pi e D)$, which isn't the claimed result. Let me reconsider.</p>
                        <p>For continuous uniform $X \sim U(0,1)$: $\text{Var}(X) = 1/12$. The variance-based upper bound (Gaussian with same variance) gives: $R(D) \leq \frac{1}{2}\log_2\frac{1/12}{D}$.</p>
                        <p>For uniform quantization with $N$ levels on $[0,1]$: step size $\Delta = 1/N$, distortion $D = \Delta^2/12 = 1/(12N^2)$, rate $R = \log_2 N$. So $N = 2^R$ and $D = 1/(12 \cdot 4^R) = \frac{1}{12} \cdot 2^{-2R}$, giving $R = \frac{1}{2}\log_2\frac{1}{12D}$.</p>
                        <p>For the uniform source, this actually matches the true $R(D)$ because uniform quantization is optimal for a uniform source. The key insight is that for a uniform source, the optimal encoder is indeed uniform quantization, and $R(D) = \frac{1}{2}\log_2\frac{\sigma^2}{D}$ with $\sigma^2 = 1/12$.</p>
                    </div>
                </div>

                <!-- Exercise 18 -->
                <div class="exercise-item">
                    <h4>Exercise 18: Stochastic Encoder Advantage</h4>
                    <p>Consider a ternary source $X \in \{0, 1, 2\}$ with $p(0) = 0.5, p(1) = 0.25, p(2) = 0.25$ and binary code $Z \in \{0, 1\}$ with Hamming-like distortion $d(x, \hat{x}) = \mathbb{1}[x \neq \hat{x}]$. Show that the best deterministic encoder achieves rate $H(Z)$ with some distortion $D$. Then show a stochastic encoder can achieve the same distortion at a lower rate.</p>
                    <button class="solution-toggle" onclick="this.nextElementSibling.style.display = this.nextElementSibling.style.display === 'none' ? 'block' : 'none'">Show Solution</button>
                    <div class="solution-content" style="display: none;">
                        <p><strong>Best deterministic encoder:</strong> Map $\{0\} \to z=0$ and $\{1,2\} \to z=1$. Decoder: $\hat{x}(0) = 0, \hat{x}(1) = 1$ (or 2). Distortion: $D = p(2) = 0.25$ (symbol 2 mapped to code for 1). Rate: $H(Z) = H(0.5) = 1$ bit.</p>
                        <p><strong>Stochastic encoder:</strong> Let $p(z=0|x=1) = 0.5, p(z=1|x=1) = 0.5$ (randomly assign $x=1$ to either code). Keep $x=0 \to z=0$ and $x=2 \to z=1$ deterministic.</p>
                        <p>Marginal: $p(z=0) = 0.5 + 0.25 \times 0.5 = 0.625$, $p(z=1) = 0.25 + 0.25 \times 0.5 = 0.375$.</p>
                        <p>Distortion: The decoder for $z=0$ sees both $x=0$ (with higher posterior) and $x=1$, still misclassifies some. But the key: $I(X;Z)$ is now less than $H(Z) = 1$ because $H(Z|X) > 0$ (the encoder is noisy for $x=1$). Specifically $H(Z|X) = 0.25 \times 1 = 0.25$ bits, so $I(X;Z) = H(Z) - H(Z|X) = H(0.625) - 0.25 \approx 0.954 - 0.25 = 0.704$ bits.</p>
                        <p>The stochastic encoder can achieve similar distortion at rate $0.704 < 1$ bit.</p>
                    </div>
                </div>

                <!-- Exercise 19 -->
                <div class="exercise-item">
                    <h4>Exercise 19: R(D) is Monotonically Decreasing</h4>
                    <p>Prove that if $D_1 < D_2$, then $R(D_1) \geq R(D_2)$.</p>
                    <button class="solution-toggle" onclick="this.nextElementSibling.style.display = this.nextElementSibling.style.display === 'none' ? 'block' : 'none'">Show Solution</button>
                    <div class="solution-content" style="display: none;">
                        <p>Let $\mathcal{P}(D) = \{p(z|x) : \mathbb{E}[d(X,\hat{X})] \leq D\}$ be the set of encoders satisfying the distortion constraint at level $D$.</p>
                        <p>If $D_1 < D_2$, then $\mathcal{P}(D_1) \subseteq \mathcal{P}(D_2)$. Any encoder that achieves distortion $\leq D_1$ certainly achieves distortion $\leq D_2$ (since $D_1 < D_2$).</p>
                        <p>Since $R(D) = \min_{p(z|x) \in \mathcal{P}(D)} I(X;Z)$, minimizing over a larger set can only decrease (or maintain) the minimum:</p>
                        <p>$$R(D_2) = \min_{p(z|x) \in \mathcal{P}(D_2)} I(X;Z) \leq \min_{p(z|x) \in \mathcal{P}(D_1)} I(X;Z) = R(D_1)$$</p>
                        <p>Therefore $R(D_1) \geq R(D_2)$. $\square$</p>
                    </div>
                </div>

                <!-- Exercise 20 -->
                <div class="exercise-item">
                    <h4>Exercise 20: VAE as Rate-Distortion Optimization</h4>
                    <p>The VAE loss is $\mathcal{L}_{\text{VAE}} = \mathbb{E}_{q(z|x)}[-\log p(x|z)] + \beta \cdot D_{KL}(q(z|x) \| p(z))$.
                    (a) Identify the "rate" and "distortion" terms.
                    (b) What is the role of $\beta$?
                    (c) What happens at $\beta = 0$? At $\beta \to \infty$?</p>
                    <button class="solution-toggle" onclick="this.nextElementSibling.style.display = this.nextElementSibling.style.display === 'none' ? 'block' : 'none'">Show Solution</button>
                    <div class="solution-content" style="display: none;">
                        <p><strong>(a) Identification:</strong></p>
                        <ul>
                            <li><strong>Distortion:</strong> $\mathbb{E}_{q(z|x)}[-\log p(x|z)]$ — the reconstruction error. Measures how well the decoder recovers $x$ from $z$.</li>
                            <li><strong>Rate:</strong> $D_{KL}(q(z|x) \| p(z))$ — measures how much information the encoder transmits. This is an upper bound on $I(X;Z)$.</li>
                        </ul>
                        <p><strong>(b) Role of $\beta$:</strong> $\beta$ is the Lagrange multiplier that trades off rate vs distortion. It plays exactly the role of $s$ in the rate-distortion Lagrangian. Different $\beta$ values trace different points on the R(D) curve.</p>
                        <p><strong>(c) Extreme cases:</strong></p>
                        <ul>
                            <li><strong>$\beta = 0$:</strong> No rate penalty. The encoder can transmit unlimited information. Result: perfect reconstruction but $z$ contains everything about $x$ (standard autoencoder). Operates at $D = 0, R = \max$.</li>
                            <li><strong>$\beta \to \infty$:</strong> Rate overwhelmingly penalized. The encoder is forced to $q(z|x) \approx p(z)$ (transmit nothing). Result: $z$ is independent of $x$, decoder outputs the marginal mean. Operates at $R = 0, D = D_{\max}$.</li>
                        </ul>
                    </div>
                </div>

                <h3>Hard (Exercises 21-30)</h3>

                <!-- Exercise 21 -->
                <div class="exercise-item">
                    <h4>Exercise 21: Water-Filling for 4D Source</h4>
                    <p>A 4D Gaussian source has covariance eigenvalues $\lambda_1 = 16, \lambda_2 = 4, \lambda_3 = 1, \lambda_4 = 0.25$. Compute the total rate $R(D)$ for total distortion budgets $D = 5, 10, 15, 21.25$.</p>
                    <button class="solution-toggle" onclick="this.nextElementSibling.style.display = this.nextElementSibling.style.display === 'none' ? 'block' : 'none'">Show Solution</button>
                    <div class="solution-content" style="display: none;">
                        <p>For each $D$, find $\theta$ such that $\sum \min(\theta, \lambda_i) = D$.</p>

                        <p><strong>$D = 5$, try $\theta = 1$:</strong> $D_i = \min(1, 16) + \min(1, 4) + \min(1, 1) + \min(1, 0.25) = 1 + 1 + 1 + 0.25 = 3.25$. Too low.</p>
                        <p>Try $\theta = 2$: $2 + 2 + 1 + 0.25 = 5.25$. Too high. Try $\theta = 1.75$: $1.75 + 1.75 + 1 + 0.25 = 4.75$. Try $\theta = 1.875$: $1.875 + 1.875 + 1 + 0.25 = 5$. Correct!</p>
                        <p>$\theta = 1.875$. $R = \frac{1}{2}\log_2\frac{16}{1.875} + \frac{1}{2}\log_2\frac{4}{1.875} + 0 + 0 = \frac{1}{2}(3.093 + 1.093) = 2.093$ bits.</p>

                        <p><strong>$D = 10$, try $\theta = 4$:</strong> $4 + 4 + 1 + 0.25 = 9.25$. Try $\theta = 4.75$: $4.75 + 4 + 1 + 0.25 = 10$. Correct!</p>
                        <p>$R = \frac{1}{2}\log_2\frac{16}{4.75} = \frac{1}{2}(1.752) = 0.876$ bits.</p>

                        <p><strong>$D = 15$, try $\theta = 14.75$:</strong> Only $\lambda_1 = 16 > \theta$. $D_1 = 14.75, D_2 = 4, D_3 = 1, D_4 = 0.25$. Total $= 20$. Too high.</p>
                        <p>All components below threshold when $\theta \geq 16$: total $= 21.25 = \sum \lambda_i$. For $D = 15$: $\theta + 4 + 1 + 0.25 = 15$ when $4 < \theta < 16$, so $\theta = 9.75$.</p>
                        <p>$R = \frac{1}{2}\log_2\frac{16}{9.75} = \frac{1}{2}(0.715) = 0.357$ bits.</p>

                        <p><strong>$D = 21.25 = \sum\lambda_i$:</strong> $\theta \geq 16$, all components at max distortion. $R = 0$ bits.</p>
                    </div>
                </div>

                <!-- Exercise 22 -->
                <div class="exercise-item">
                    <h4>Exercise 22: Shannon Lower Bound</h4>
                    <p>The <strong>Shannon lower bound</strong> (SLB) states: $R(D) \geq h(X) - \frac{1}{2}\log_2(2\pi e D)$ for any source under MSE. (a) Prove this using the maximum entropy property of Gaussians. (b) For which sources is the SLB tight (i.e., equals $R(D)$)?</p>
                    <button class="solution-toggle" onclick="this.nextElementSibling.style.display = this.nextElementSibling.style.display === 'none' ? 'block' : 'none'">Show Solution</button>
                    <div class="solution-content" style="display: none;">
                        <p><strong>(a) Proof:</strong></p>
                        <p>$R(D) = \min I(X;Z) = \min [h(X) - h(X|Z)]$.</p>
                        <p>Since $h(X)$ is constant (doesn't depend on the encoder): $R(D) = h(X) - \max h(X|Z)$.</p>
                        <p>The constraint $\mathbb{E}[(X - \hat{X})^2] \leq D$ implies $\mathbb{E}[\text{Var}(X|Z)] \leq D$ (since optimal decoder uses conditional mean).</p>
                        <p>Among all distributions with variance $\leq D$, the Gaussian has maximum entropy: $h(X|Z) \leq \frac{1}{2}\log_2(2\pi e D)$.</p>
                        <p>Therefore: $R(D) \geq h(X) - \frac{1}{2}\log_2(2\pi e D)$. $\square$</p>
                        <p><strong>(b) Tightness:</strong> The SLB is tight when the conditional distribution $p(X|Z)$ is Gaussian for the optimal encoder. This happens when the source itself is Gaussian — the Gaussian R(D) formula $\frac{1}{2}\log_2\frac{\sigma^2}{D}$ exactly equals $h(X) - \frac{1}{2}\log_2(2\pi e D) = \frac{1}{2}\log_2(2\pi e\sigma^2) - \frac{1}{2}\log_2(2\pi e D) = \frac{1}{2}\log_2\frac{\sigma^2}{D}$.</p>
                    </div>
                </div>

                <!-- Exercise 23 -->
                <div class="exercise-item">
                    <h4>Exercise 23: Parametric R(D) via Lagrangian</h4>
                    <p>For the Gaussian source, the optimal encoder at Lagrange parameter $s$ satisfies $p(z|x) \propto q(z)\exp(-s(x-z)^2)$. (a) Show that for Gaussian $q(z)$, the resulting $p(z|x)$ is also Gaussian. (b) Find its mean and variance in terms of $s$ and $\sigma^2$. (c) Parametrically compute $R(s)$ and $D(s)$, then verify $R(D) = \frac{1}{2}\log_2\frac{\sigma^2}{D}$.</p>
                    <button class="solution-toggle" onclick="this.nextElementSibling.style.display = this.nextElementSibling.style.display === 'none' ? 'block' : 'none'">Show Solution</button>
                    <div class="solution-content" style="display: none;">
                        <p><strong>(a)</strong> If $q(z) = \mathcal{N}(0, \sigma_z^2)$, then $\log p(z|x) \propto -\frac{z^2}{2\sigma_z^2} - s(x-z)^2 = -\frac{z^2}{2\sigma_z^2} - sz^2 + 2sxz - sx^2$.</p>
                        <p>Collecting terms in $z$: $-(\frac{1}{2\sigma_z^2} + s)z^2 + 2sxz + \text{const}$. This is quadratic in $z$, so $p(z|x)$ is Gaussian. $\checkmark$</p>
                        <p><strong>(b)</strong> Completing the square: precision $= \frac{1}{\sigma_z^2} + 2s$, so variance $\sigma_{z|x}^2 = \frac{1}{1/\sigma_z^2 + 2s}$.</p>
                        <p>Mean: $\mu_{z|x} = \frac{2sx}{1/\sigma_z^2 + 2s} = \frac{2s\sigma_z^2}{1 + 2s\sigma_z^2}x$.</p>
                        <p>Self-consistency requires $\sigma_z^2 = \text{Var}(Z) = \mu_{z|x}^2\text{-terms} + \sigma_{z|x}^2$. After solving (using $\sigma_z^2 = \sigma^2 - D$ and $D = \sigma_{z|x}^2 \cdot \sigma^2 / \sigma_z^2$):</p>
                        <p><strong>(c)</strong> The parametric solution gives $D(s) = \frac{1}{2s}$ (in nats, with $s > \frac{1}{2\sigma^2}$) and $R(s) = \frac{1}{2}\ln(2s\sigma^2)$ nats. Eliminating $s$: $s = \frac{1}{2D}$, so $R = \frac{1}{2}\ln\frac{\sigma^2}{D}$ nats $= \frac{1}{2}\log_2\frac{\sigma^2}{D}$ bits. $\checkmark$</p>
                    </div>
                </div>

                <!-- Exercise 24 -->
                <div class="exercise-item">
                    <h4>Exercise 24: ELBO Decomposition and Rate-Distortion</h4>
                    <p>Starting from the VAE's Evidence Lower Bound (ELBO), show that $-\text{ELBO} = \underbrace{D_{KL}(q(z|x) \| p(z))}_{\text{rate}} + \underbrace{\mathbb{E}_q[-\log p(x|z)]}_{\text{distortion}}$, and prove that $D_{KL}(q(z|x) \| p(z)) \geq I_q(X; Z)$ where $I_q$ is the mutual information under the variational distribution.</p>
                    <button class="solution-toggle" onclick="this.nextElementSibling.style.display = this.nextElementSibling.style.display === 'none' ? 'block' : 'none'">Show Solution</button>
                    <div class="solution-content" style="display: none;">
                        <p><strong>ELBO decomposition:</strong></p>
                        <p>$\text{ELBO} = \mathbb{E}_{q(z|x)}[\log p(x|z)] - D_{KL}(q(z|x) \| p(z))$</p>
                        <p>$-\text{ELBO} = D_{KL}(q(z|x) \| p(z)) + \mathbb{E}_{q(z|x)}[-\log p(x|z)]$</p>
                        <p>This is exactly rate + distortion. $\checkmark$</p>

                        <p><strong>Proving $D_{KL}(q(z|x) \| p(z)) \geq I_q(X; Z)$:</strong></p>
                        <p>Taking expectations over $p(x)$:</p>
                        <p>$\mathbb{E}_{p(x)}[D_{KL}(q(z|x) \| p(z))] = \mathbb{E}_{p(x)}\left[\mathbb{E}_{q(z|x)}\left[\log\frac{q(z|x)}{p(z)}\right]\right]$</p>
                        <p>$= \mathbb{E}_{p(x)}\left[\mathbb{E}_{q(z|x)}\left[\log\frac{q(z|x)}{q(z)} + \log\frac{q(z)}{p(z)}\right]\right]$</p>
                        <p>where $q(z) = \mathbb{E}_{p(x)}[q(z|x)]$ is the aggregated posterior.</p>
                        <p>$= I_q(X; Z) + D_{KL}(q(z) \| p(z))$</p>
                        <p>Since $D_{KL}(q(z) \| p(z)) \geq 0$, we get $\mathbb{E}_{p(x)}[D_{KL}(q(z|x) \| p(z))] \geq I_q(X;Z)$. $\square$</p>
                        <p>The gap $D_{KL}(q(z) \| p(z))$ measures how far the aggregated posterior is from the prior — this is the "marginal KL" that causes the "hole problem" in VAEs.</p>
                    </div>
                </div>

                <!-- Exercise 25 -->
                <div class="exercise-item">
                    <h4>Exercise 25: Rate-Distortion for Exponential Source</h4>
                    <p>Let $X \sim \text{Exponential}(\lambda)$ with rate parameter $\lambda$ (mean $1/\lambda$, variance $1/\lambda^2$). Using the Shannon lower bound, find a lower bound for $R(D)$ under MSE distortion. Compare with the Gaussian $R(D)$ that has the same variance.</p>
                    <button class="solution-toggle" onclick="this.nextElementSibling.style.display = this.nextElementSibling.style.display === 'none' ? 'block' : 'none'">Show Solution</button>
                    <div class="solution-content" style="display: none;">
                        <p><strong>Shannon lower bound:</strong> $R(D) \geq h(X) - \frac{1}{2}\log_2(2\pi e D)$.</p>
                        <p>For $X \sim \text{Exp}(\lambda)$: $h(X) = 1 + \ln(1/\lambda) = 1 - \ln\lambda$ (in nats) $= \frac{1 - \ln\lambda}{\ln 2}$ (in bits).</p>
                        <p>Converting to bits: $h(X) = \log_2\frac{e}{\lambda}$.</p>
                        <p>SLB: $R(D) \geq \log_2\frac{e}{\lambda} - \frac{1}{2}\log_2(2\pi e D) = \frac{1}{2}\log_2\frac{e^2}{\lambda^2 \cdot 2\pi e D} = \frac{1}{2}\log_2\frac{e}{2\pi\lambda^2 D}$.</p>
                        <p><strong>Gaussian comparison:</strong> A Gaussian with variance $\sigma^2 = 1/\lambda^2$ has $R_G(D) = \frac{1}{2}\log_2\frac{1}{\lambda^2 D}$.</p>
                        <p>The SLB is $\frac{1}{2}\log_2\frac{e}{2\pi\lambda^2 D}$, while the Gaussian $R(D)$ is $\frac{1}{2}\log_2\frac{1}{\lambda^2 D}$.</p>
                        <p>Difference: $R_G(D) - \text{SLB} = \frac{1}{2}\log_2\frac{2\pi}{e} = \frac{1}{2}\log_2(2.31) = 0.604$ bits.</p>
                        <p>The exponential source's $R(D)$ lies between the SLB and the Gaussian $R(D)$. The exact $R(D)$ for the exponential doesn't have a simple closed form.</p>
                    </div>
                </div>

                <!-- Exercise 26 -->
                <div class="exercise-item">
                    <h4>Exercise 26: Successive Refinement</h4>
                    <p>A source is first encoded at rate $R_1$ with distortion $D_1$, then a <strong>refinement</strong> layer adds $R_2$ more bits to achieve distortion $D_2 < D_1$. Is the total rate $R_1 + R_2$ always equal to $R(D_2)$ (fully efficient)? A source is called <strong>successively refinable</strong> if this holds. (a) Prove that Gaussian sources are successively refinable. (b) Give an intuition for why some sources are not.</p>
                    <button class="solution-toggle" onclick="this.nextElementSibling.style.display = this.nextElementSibling.style.display === 'none' ? 'block' : 'none'">Show Solution</button>
                    <div class="solution-content" style="display: none;">
                        <p><strong>(a) Gaussian sources:</strong></p>
                        <p>At rate $R_1$: distortion $D_1 = \sigma^2 2^{-2R_1}$.</p>
                        <p>The reconstruction error $W_1 = X - \hat{X}_1 \sim \mathcal{N}(0, D_1)$ and is independent of $\hat{X}_1$.</p>
                        <p>The refinement layer encodes $W_1$ (a Gaussian source with variance $D_1$) at rate $R_2$ to achieve distortion $D_2 = D_1 \cdot 2^{-2R_2}$.</p>
                        <p>Total rate: $R_1 + R_2$. Total distortion: $D_2 = \sigma^2 2^{-2R_1} \cdot 2^{-2R_2} = \sigma^2 2^{-2(R_1+R_2)}$.</p>
                        <p>This matches $R(D_2) = R_1 + R_2$. The key property is that the error $W_1$ is Gaussian and independent of $\hat{X}_1$, so encoding $W_1$ is a fresh rate-distortion problem. $\checkmark$</p>
                        <p><strong>(b) Non-successively-refinable sources:</strong> If the error $W_1$ at the first stage is correlated with $\hat{X}_1$, or if its distribution depends on which codeword was used, then the refinement can't be treated independently. The first-stage encoder must "plan ahead" for refinement, but $R(D_1)$-optimal encoder doesn't know about future refinement. For some discrete sources, the optimal codebooks at different rates are structurally incompatible, causing a rate loss. Example: binary sources with certain parameters are not successively refinable.</p>
                    </div>
                </div>

                <!-- Exercise 27 -->
                <div class="exercise-item">
                    <h4>Exercise 27: Vector Quantization Advantage</h4>
                    <p>For a memoryless Gaussian source (i.i.d. samples), <strong>scalar quantization</strong> encodes each sample independently, while <strong>vector quantization</strong> encodes blocks of $n$ samples jointly. (a) Why can vector quantization outperform scalar quantization? (b) At $R = 1$ bit/sample with $\sigma^2 = 1$, scalar quantization achieves MSE $\approx 0.363$ (Exercise 15). What does $R(D)$ theory predict as the optimal distortion?</p>
                    <button class="solution-toggle" onclick="this.nextElementSibling.style.display = this.nextElementSibling.style.display === 'none' ? 'block' : 'none'">Show Solution</button>
                    <div class="solution-content" style="display: none;">
                        <p><strong>(a)</strong> Scalar quantization restricts the encoder to $p(z|x) = \prod_i p(z_i|x_i)$ — each sample encoded independently. Vector quantization allows arbitrary joint encoders $p(\mathbf{z}|\mathbf{x})$ that exploit correlations <em>within the code space</em>.</p>
                        <p>Even for i.i.d. sources (no correlation to exploit), VQ wins because it can use <strong>space-filling</strong>: VQ codebooks in high dimensions pack more efficiently (approaching sphere packing bounds). Scalar quantizers are restricted to rectangular lattices, which waste space.</p>
                        <p>Shannon's coding theorem guarantees that VQ with block length $n \to \infty$ achieves $R(D)$, while scalar quantization generally cannot.</p>
                        <p><strong>(b)</strong> $R(D)$ theory: $D(R=1) = \sigma^2 \cdot 2^{-2} = 0.25$.</p>
                        <p>Scalar quantization: $D = 0.363$.</p>
                        <p>Gap: $0.363 / 0.25 = 1.452$, or 1.62 dB. VQ can close this gap as block length increases.</p>
                    </div>
                </div>

                <!-- Exercise 28 -->
                <div class="exercise-item">
                    <h4>Exercise 28: Colored Gaussian Source</h4>
                    <p>A stationary Gaussian process has power spectral density $S(f) = \frac{1}{1 + (f/f_0)^2}$ (Lorentzian). The rate-distortion function for Gaussian processes is $R(D) = \int_{-W}^{W} \frac{1}{2}\log_2\frac{S(f)}{\theta} \, df$ summed over frequencies where $S(f) > \theta$, with $\theta$ chosen so that $\int \min(\theta, S(f))\, df = D$. Explain qualitatively: which frequencies get more bits? How does this relate to the multivariate water-filling solution?</p>
                    <button class="solution-toggle" onclick="this.nextElementSibling.style.display = this.nextElementSibling.style.display === 'none' ? 'block' : 'none'">Show Solution</button>
                    <div class="solution-content" style="display: none;">
                        <p>This is the <strong>continuous analog of water-filling</strong>. Instead of discrete eigenvalues $\lambda_i$, we have a continuous power spectrum $S(f)$. The water level $\theta$ plays the same role.</p>
                        <p><strong>Which frequencies get more bits?</strong> Frequencies with larger $S(f)$ (more power/variance) get more bits: $R(f) = \frac{1}{2}\log_2\frac{S(f)}{\theta}$ when $S(f) > \theta$.</p>
                        <p>For the Lorentzian $S(f) = \frac{1}{1+(f/f_0)^2}$:</p>
                        <ul>
                            <li>Low frequencies ($|f| \ll f_0$): $S(f) \approx 1$ (high power) → get the most bits</li>
                            <li>High frequencies ($|f| \gg f_0$): $S(f) \approx 0$ → get zero bits (below water level)</li>
                            <li>There's a cutoff frequency $f_c$ where $S(f_c) = \theta$; frequencies above $f_c$ are discarded</li>
                        </ul>
                        <p><strong>Connection to multivariate:</strong> By the KLT (Karhunen-Loève transform), the continuous process decomposes into independent frequency components — each with variance $S(f)$. This is exactly a continuous version of diagonalizing the covariance matrix. Water-filling across eigenvalues becomes water-filling across the power spectrum.</p>
                    </div>
                </div>

                <!-- Exercise 29 -->
                <div class="exercise-item">
                    <h4>Exercise 29: Source Coding Theorem (Converse)</h4>
                    <p>State and prove the converse of Shannon's lossy source coding theorem: if we encode at rate $R < R(D)$, then the distortion must exceed $D$. <em>Hint: Use the data processing inequality and the definition of $R(D)$.</em></p>
                    <button class="solution-toggle" onclick="this.nextElementSibling.style.display = this.nextElementSibling.style.display === 'none' ? 'block' : 'none'">Show Solution</button>
                    <div class="solution-content" style="display: none;">
                        <p><strong>Theorem (Converse):</strong> For any encoder-decoder pair $(f_n, g_n)$ operating on blocks of length $n$ with rate $\frac{1}{n}\log_2|M| = R$ (where $M$ is the codebook size), the distortion satisfies $\frac{1}{n}\sum_{i=1}^n \mathbb{E}[d(X_i, \hat{X}_i)] \geq D(R)$.</p>
                        <p><strong>Proof sketch:</strong></p>
                        <p>1. The encoder maps $X^n$ to an index $W \in \{1, \ldots, 2^{nR}\}$. By the data processing inequality: $I(X^n; \hat{X}^n) \leq I(X^n; W) \leq H(W) \leq nR$.</p>
                        <p>2. For i.i.d. sources: $I(X^n; \hat{X}^n) = \sum_{i=1}^n I(X_i; \hat{X}_i | X^{i-1}, \hat{X}^{i-1}, ...) \geq \sum_{i=1}^n I(X_i; \hat{X}_i)$ (conditioning reduces entropy, not MI for independent sources, but the key step uses single-letterization).</p>
                        <p>3. By definition of $R(D)$: $I(X_i; \hat{X}_i) \geq R(D_i)$ where $D_i = \mathbb{E}[d(X_i, \hat{X}_i)]$.</p>
                        <p>4. Combining: $nR \geq \sum_{i=1}^n R(D_i) \geq nR(\bar{D})$ by convexity of $R(\cdot)$, where $\bar{D} = \frac{1}{n}\sum D_i$.</p>
                        <p>5. Therefore $R \geq R(\bar{D})$, which means $\bar{D} \geq D(R)$. $\square$</p>
                    </div>
                </div>

                <!-- Exercise 30 -->
                <div class="exercise-item">
                    <h4>Exercise 30: Design Optimal Encoder for Gaussian Mixture</h4>
                    <p>A source is a mixture of two Gaussians: $p(x) = 0.5 \cdot \mathcal{N}(x; -3, 1) + 0.5 \cdot \mathcal{N}(x; 3, 1)$. (a) Qualitatively sketch the $R(D)$ curve. (b) At very low rate ($R \approx 1$ bit), what is the optimal encoder? (c) At high rate, how does $R(D)$ compare to a single Gaussian with the same variance? (d) Why can't we get a closed-form $R(D)$ for mixtures?</p>
                    <button class="solution-toggle" onclick="this.nextElementSibling.style.display = this.nextElementSibling.style.display === 'none' ? 'block' : 'none'">Show Solution</button>
                    <div class="solution-content" style="display: none;">
                        <p><strong>(a) Sketch:</strong> The mixture has $\text{Var}(X) = 1 + 9 = 10$ (within-component variance + between-component variance). At $R = 0$: $D_{\max} = 10$. The curve starts at $(10, 0)$ and decreases.</p>
                        <p>Crucially, the curve has a "kink" region around $R \approx 1$ bit where learning which cluster the point belongs to provides a large distortion reduction.</p>

                        <p><strong>(b) At $R \approx 1$ bit:</strong> The optimal encoder identifies which Gaussian component $x$ came from. With 1 bit, transmit $z = \text{sign}(x)$. The decoder outputs $\hat{x} = -3$ or $\hat{x} = +3$. Distortion $\approx \text{within-component variance} = 1$. This is a massive drop from $D_{\max} = 10$: one bit captures the cluster structure.</p>

                        <p><strong>(c) High rate:</strong> At high rate, we're encoding <em>within</em> each cluster. Each cluster is $\mathcal{N}(\pm 3, 1)$ with variance 1. The per-cluster $R(D) = \frac{1}{2}\log_2\frac{1}{D}$. Plus 1 bit for cluster ID. So $R(D) \approx 1 + \frac{1}{2}\log_2\frac{1}{D}$.</p>
                        <p>A single Gaussian with $\sigma^2 = 10$ would give $R(D) = \frac{1}{2}\log_2\frac{10}{D}$. At small $D$, the mixture is <em>more efficient</em> because its effective entropy is lower (structured data).</p>

                        <p><strong>(d) No closed form:</strong> The Gaussian $R(D)$ relies on the fact that Gaussians are "self-similar" — the optimal test channel produces Gaussian errors, and Gaussian + Gaussian = Gaussian. For mixtures, the optimal test channel doesn't preserve the mixture structure, and the resulting integrals don't simplify. One must use numerical methods (Blahut-Arimoto) or bounds.</p>
                    </div>
                </div>

                <!-- Navigation -->
                <div class="tutorial-nav">
                    <a href="../11-rnn/index.html" class="tutorial-nav-link prev">
                        <span class="nav-label">Previous</span>
                        <span class="nav-title">← RNNs</span>
                    </a>
                    <a href="../15-autoencoder/index.html" class="tutorial-nav-link next">
                        <span class="nav-label">Next</span>
                        <span class="nav-title">Autoencoders →</span>
                    </a>
                </div>
            </article>

        </main>

        <!-- TOC (Right Side) -->
        <aside class="toc-container" id="toc-panel">
            <h4 class="toc-title">Contents</h4>
            <nav class="toc-list">
                <a href="#lossy-compression" class="toc-link">Lossy Compression Problem</a>
                <a href="#framework" class="toc-link">The Framework</a>
                <a href="#mutual-information" class="toc-link">Mutual Information as Rate</a>
                <a href="#rate-distortion-function" class="toc-link">R(D) Function</a>
                <a href="#properties" class="toc-link">Properties of R(D)</a>
                <a href="#encoders" class="toc-link">Det. vs Stoch. Encoders</a>
                <a href="#gaussian" class="toc-link">Gaussian Source</a>
                <a href="#water-filling" class="toc-link">Water-Filling</a>
                <a href="#binary-source" class="toc-link">Binary Source</a>
                <a href="#blahut-arimoto" class="toc-link">Blahut-Arimoto</a>
                <a href="#distortion-rate" class="toc-link">D(R) Function</a>
                <a href="#connection-autoencoders" class="toc-link">Connection to AE/VAE</a>
            </nav>
        </aside>
    </div>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-brand">
                    <span class="logo-symbol">∇</span>
                    <span>ML Fundamentals</span>
                </div>
                <p class="footer-tagline">Deep understanding through first principles.</p>
            </div>
            <div class="footer-links">
                <a href="../../../index.html">Home</a>
                <a href="https://github.com/ml-entropy/ml-entropy.github.io" target="_blank">GitHub</a>
            </div>
        </div>
    </footer>

    <!-- Scripts -->
    <script src="../../../js/main.js"></script>
    <script>
        function showTab(tabName) {
            document.querySelectorAll('.article-content').forEach(el => el.style.display = 'none');
            document.querySelectorAll('.tutorial-tab').forEach(el => el.classList.remove('active'));

            document.getElementById(tabName).style.display = 'block';
            document.querySelector(`.tutorial-tab[onclick="showTab('${tabName}')"]`).classList.add('active');

            // Show/hide TOC based on tab
            const tocPanel = document.getElementById('toc-panel');
            if (tocPanel) {
                tocPanel.style.display = tabName === 'theory' ? '' : 'none';
            }

            // Re-render math
            if (typeof renderMathInElement === 'function') {
                renderMathInElement(document.getElementById(tabName), {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }
        }
    </script>
</body>
</html>
