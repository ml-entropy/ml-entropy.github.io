<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tacotron &amp; Attention-Based TTS | ML Fundamentals</title>
    <meta name="description" content="Tacotron, Tacotron 2, attention-based text-to-speech, location-sensitive attention, teacher forcing, attention alignment, and sequence-to-sequence speech synthesis.">

    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&family=Playfair+Display:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}, {left: '\\\\[', right: '\\\\]', display: true}, {left: '\\\\(', right: '\\\\)', display: false}], throwOnError: false});"></script>

    <!-- Styles -->
    <link rel="stylesheet" href="../../../css/main.css">
    <link rel="stylesheet" href="../../../css/components.css">
    <link rel="stylesheet" href="../../../css/sidebar.css">

    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>&#x221E;</text></svg>">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar" id="navbar">
        <div class="nav-container">
            <a href="../../../index.html" class="nav-logo">
                <span class="logo-symbol">&nabla;</span>
                <span class="logo-text">ML Fundamentals</span>
            </a>

            <button class="nav-toggle" id="navToggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>

            <div class="nav-menu" id="navMenu">
                <div class="nav-links">
                    <a href="../../../tutorials/ml/index.html" class="nav-link active">Machine Learning</a>
                    <a href="../../../tutorials/linear-algebra/index.html" class="nav-link">Linear Algebra</a>
                    <a href="../../../tutorials/calculus/index.html" class="nav-link">Calculus</a>
                    <a href="../../../tutorials/physics/index.html" class="nav-link">Physics</a>
                    <a href="../../../index.html#philosophy" class="nav-link">Philosophy</a>
                    <a href="../../../index.html#roadmap" class="nav-link">Roadmap</a>
                    <a href="https://github.com/ml-entropy/ml-entropy.github.io" class="nav-link" target="_blank">GitHub</a>
                </div>

                <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
                    <svg class="sun-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="5"/>
                        <path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"/>
                    </svg>
                    <svg class="moon-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                    </svg>
                </button>
            </div>
        </div>
    </nav>

    <!-- Tutorial Header -->
    <header class="tutorial-content-header">
        <div class="container">
            <nav class="breadcrumb">
                <a href="../../../index.html">Home</a>
                <span class="breadcrumb-separator">&rarr;</span>
                <a href="../index.html">Machine Learning</a>
                <span class="breadcrumb-separator">&rarr;</span>
                <span>Tacotron &amp; Attention TTS</span>
            </nav>

            <div class="tutorial-tabs">
                <a href="#theory" class="tutorial-tab active">Theory</a>
                <a href="#code" class="tutorial-tab">Code</a>
                <a href="#exercises" class="tutorial-tab">Exercises</a>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <div class="tutorial-wrapper">

        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <aside class="tutorial-sidebar">
            <div class="sidebar-section">
                <h3 class="sidebar-section-title">Machine Learning</h3>
                <nav class="sidebar-nav">
                        <a href="../00-probability/index.html" class="sidebar-link">00. Probability Foundations</a>
                    <a href="../04-logarithms/index.html" class="sidebar-link">01. Why Logarithms?</a>
                    <a href="../05-combinatorics/index.html" class="sidebar-link">02. Combinatorics</a>
                    <a href="../03-distributions/index.html" class="sidebar-link">03. Normal Distributions</a>
                    <a href="../01-entropy/index.html" class="sidebar-link">04. Entropy Fundamentals</a>
                    <a href="../02-cross-entropy/index.html" class="sidebar-link">05. Cross-Entropy</a>
                    <a href="../02-kl-divergence/index.html" class="sidebar-link">06. KL Divergence</a>
                    <a href="../14-entropy-connections/index.html" class="sidebar-link">07. Entropy Connections</a>
                    <a href="../06-backpropagation/index.html" class="sidebar-link">08. Backpropagation</a>
                    <a href="../07-regularization/index.html" class="sidebar-link">09. Regularization</a>
                    <a href="../08-batch-normalization/index.html" class="sidebar-link">10. Batch Normalization</a>
                    <a href="../09-learning-rate/index.html" class="sidebar-link">11. Learning Rate</a>
                    <a href="../10-cnn/index.html" class="sidebar-link">12. CNNs</a>
                    <a href="../11-rnn/index.html" class="sidebar-link">13. RNNs</a>
                    <a href="../34-rate-distortion/index.html" class="sidebar-link">14. Rate-Distortion Theory</a>
                    <a href="../15-autoencoder/index.html" class="sidebar-link">15. Autoencoders</a>
                    <a href="../13-variational-inference/index.html" class="sidebar-link">16. Variational Inference</a>
                    <a href="../12-vae/index.html" class="sidebar-link">17. VAE</a>
                    <a href="../16-inductive-bias/index.html" class="sidebar-link">18. Inductive Bias</a>
                    <a href="../17-architectural-biases/index.html" class="sidebar-link">19. Architectural Biases</a>
                    <a href="../18-designing-biases/index.html" class="sidebar-link">20. Designing Biases</a>
                    <a href="../19-fst-fundamentals/index.html" class="sidebar-link">21. FST Fundamentals</a>
                    <a href="../20-weighted-fsts/index.html" class="sidebar-link">22. Weighted FSTs</a>
                    <a href="../21-fst-libraries/index.html" class="sidebar-link">23. FST Libraries</a>
                    <a href="../22-fst-applications/index.html" class="sidebar-link">24. FST Applications</a>
                    <a href="../23-neural-symbolic/index.html" class="sidebar-link">25. Neural-Symbolic Hybrids</a>
                    <a href="../24-sequence-alignment/index.html" class="sidebar-link">26. Sequence Alignment</a>
                    <a href="../25-mas-algorithm/index.html" class="sidebar-link">27. MAS Algorithm</a>
                    <a href="../26-forced-alignment/index.html" class="sidebar-link">28. Forced Alignment & MFA</a>
                    <a href="../27-tts-fundamentals/index.html" class="sidebar-link">29. TTS Fundamentals</a>
                    <a href="../28-neural-vocoders/index.html" class="sidebar-link">30. Neural Vocoders</a>
                    <a href="../29-tacotron/index.html" class="sidebar-link active">31. Tacotron & Attention TTS</a>
                    <a href="../30-fastspeech/index.html" class="sidebar-link">32. FastSpeech & Non-AR TTS</a>
                    <a href="../31-glow-tts/index.html" class="sidebar-link">33. Glow-TTS & Flows</a>
                    <a href="../32-vits/index.html" class="sidebar-link">34. VITS: End-to-End TTS</a>
                    <a href="../33-bilingual-tts/index.html" class="sidebar-link">35. Bilingual TTS: RU+KY</a>
                </nav>
            </div>
            
            <div class="sidebar-section" style="margin-top: 2rem;">
                <h3 class="sidebar-section-title">Related Subjects</h3>
                <nav class="sidebar-nav">
                        <a href="../../linear-algebra/index.html" class="sidebar-link">Linear Algebra</a>
                    <a href="../../calculus/index.html" class="sidebar-link">Calculus</a>
                    <a href="../../physics/index.html" class="sidebar-link">Physics</a>
                </nav>
            </div>
        </aside>

        <!-- Main Article -->
        <main class="tutorial-main">

            <!-- ==================== THEORY TAB ==================== -->
            <article class="article-content" id="theory">

                <!-- Section 1: Sequence-to-Sequence TTS -->
                <h2 id="seq2seq-tts">Sequence-to-Sequence TTS</h2>

                <p>
                    The sequence-to-sequence (seq2seq) paradigm &mdash; an encoder that reads an input sequence and a decoder that generates an output sequence, connected by an attention mechanism &mdash; revolutionized machine translation starting with Sutskever et al. (2014) and Bahdanau et al. (2015). It was natural to ask: can we apply the same framework to text-to-speech? The answer, demonstrated by Tacotron (Wang et al., 2017), was a resounding yes &mdash; but with substantial complications that would define TTS research for years to come.
                </p>

                <p>
                    In machine translation, the input is a sequence of words in one language and the output is a sequence of words in another. The two sequences have roughly comparable lengths: a 20-word English sentence might translate to a 22-word French sentence. In TTS, the input is a sequence of text characters or phonemes (typically $L = 20\text{--}200$ tokens for a sentence), and the output is a sequence of mel-spectrogram frames (typically $T = 200\text{--}2000$ frames). The critical asymmetry is that $T \gg L$ &mdash; the output sequence is often <strong>10 to 20 times longer</strong> than the input.
                </p>

                <p>
                    This length disparity makes the attention problem dramatically harder. In translation, the attention mechanism learns a roughly one-to-one or one-to-few mapping between input and output tokens. In TTS, many consecutive output frames must attend to the <em>same</em> input token &mdash; a phoneme like /a/ might span 15 mel frames, meaning 15 consecutive decoder steps must all produce high attention weight on the same encoder position. Moreover, the mapping must be <strong>monotonic</strong>: speech unfolds left-to-right, so the attention should never jump backward (unlike translation, where word order can change between languages).
                </p>

                <div class="definition-box">
                    <div class="box-title">Definition: Sequence-to-Sequence TTS</div>
                    <p>
                        A <strong>sequence-to-sequence TTS</strong> system uses an encoder-attention-decoder architecture to directly map a text sequence $(x_1, \ldots, x_L)$ to an acoustic feature sequence $(y_1, \ldots, y_T)$, typically mel-spectrogram frames. The encoder produces hidden representations $h_1, \ldots, h_L$ from the text. At each decoder step $t$, an attention mechanism computes a context vector $c_t = \sum_{l=1}^{L} \alpha_{t,l} h_l$ by weighting the encoder outputs, and the decoder generates the next output frame $y_t$ conditioned on $c_t$ and previous outputs. The attention weights $\alpha_{t,l}$ form an alignment matrix of size $T \times L$ that implicitly learns the duration and ordering of each text token in the acoustic output.
                    </p>
                </div>

                <p>
                    The seq2seq approach to TTS is appealing because it is <strong>end-to-end</strong>: a single neural network replaces the entire traditional TTS pipeline of text analysis, linguistic feature extraction, duration modeling, acoustic modeling, and vocoding (except for the final waveform generation step). The attention mechanism simultaneously solves the alignment problem (which phoneme maps to which frames) and the duration problem (how long each phoneme lasts). No external aligner, no hand-crafted duration rules, no phoneme-level feature engineering &mdash; just text in, mel spectrogram out.
                </p>

                <p>
                    But this simplicity comes at a cost. The attention mechanism must discover the alignment from scratch during training, using only the reconstruction loss on mel frames as signal. There is no explicit supervision saying "phoneme 3 should map to frames 40&ndash;55." The model must learn this correspondence implicitly. When it succeeds, the results are remarkable. When it fails, the results are catastrophic: skipped words, repeated phrases, or unintelligible babbling. Understanding these failure modes &mdash; and the architectural innovations designed to mitigate them &mdash; is the central theme of this tutorial.
                </p>

                <div class="note-box">
                    <div class="box-title">Historical Context</div>
                    <p>
                        Before Tacotron, TTS systems were complex pipelines with many separately trained components. The state of the art was <strong>statistical parametric synthesis</strong> using HMMs or DNNs to predict vocoder parameters (F0, spectral envelope, aperiodicity) from linguistically rich input features (phoneme identity, position in syllable/word/phrase, stress, etc.). These systems required extensive feature engineering and produced robotic-sounding speech. Tacotron showed that a single neural network, trained end-to-end on &lt;text, audio&gt; pairs, could produce more natural speech than years of pipeline engineering.
                    </p>
                </div>

                <!-- Section 2: Tacotron 1 Architecture -->
                <h2 id="tacotron-architecture">Tacotron 1 Architecture</h2>

                <p>
                    Tacotron (Wang et al., 2017) introduced the first successful end-to-end seq2seq TTS system. Its architecture is built around a distinctive component called the <strong>CBHG module</strong> (Convolution Bank + Highway network + bidirectional GRU), used in both the encoder and the post-processing network. The overall architecture can be decomposed into five stages:
                </p>

                <p>
                    <strong>1. Character Embedding.</strong> Input characters are mapped to 256-dimensional embeddings. Unlike later systems that use phonemes, Tacotron 1 operates directly on characters, learning the grapheme-to-phoneme mapping implicitly.
                </p>

                <p>
                    <strong>2. CBHG Encoder.</strong> The character embeddings pass through a pre-net (two fully connected layers with dropout, acting as a bottleneck) and then through a CBHG module. The CBHG consists of:
                </p>

                <ul>
                    <li><strong>Convolution bank:</strong> $K$ sets of 1D convolutions with kernel sizes $1, 2, \ldots, K$ (where $K = 16$ in the encoder). Each kernel size captures patterns at a different scale &mdash; unigrams, bigrams, up to 16-grams of characters. The outputs are concatenated along the channel dimension.</li>
                    <li><strong>Max pooling:</strong> A stride-1 max pooling layer with width 2 that preserves the sequence length while providing local invariance.</li>
                    <li><strong>Projection convolutions:</strong> Two 1D convolution layers (with kernel sizes 3 and 3) that project the concatenated bank outputs back to a manageable dimension. Residual connections add the original input.</li>
                    <li><strong>Highway network:</strong> 4 layers of highway networks that allow the model to learn complex nonlinear transformations while maintaining gradient flow through gating mechanisms: $y = H(x) \cdot T(x) + x \cdot (1 - T(x))$, where $H$ is the transform, $T$ is the transfer gate, and $x$ is the input.</li>
                    <li><strong>Bidirectional GRU:</strong> A single-layer bidirectional GRU that captures long-range sequential dependencies. The forward and backward hidden states are concatenated, producing encoder outputs $h_1, \ldots, h_L$ of dimension $2d$.</li>
                </ul>

                <p>
                    <strong>3. Attention-Based Decoder.</strong> The decoder is an autoregressive RNN that generates mel-spectrogram frames one group at a time. At each step $t$:
                </p>

                <ol>
                    <li>The previous output frame passes through a <strong>pre-net</strong>: two fully connected layers (256 units each) with ReLU activation and 50% dropout. The pre-net acts as an information bottleneck that forces the decoder to rely on the attention context rather than simply copying the previous frame. This dropout is applied even at inference time &mdash; a deliberate design choice that adds slight variation and prevents the decoder from learning to ignore the attention.</li>
                    <li>The pre-net output is concatenated with the attention context vector and fed into a stack of 2 GRU layers (256 units each).</li>
                    <li>The GRU output is used to compute attention over the encoder states, producing a new context vector $c_t$.</li>
                    <li>The context vector and GRU output are concatenated and projected to predict $r$ mel-spectrogram frames simultaneously.</li>
                </ol>

                <p>
                    <strong>4. Reduction Factor $r$.</strong> A key innovation in Tacotron is predicting $r &gt; 1$ output frames per decoder step. With $r = 5$ (the default in Tacotron 1), each decoder step produces 5 consecutive mel frames. This has three benefits: (a) it reduces the number of decoder steps by a factor of $r$, making training and inference faster, (b) it reduces the length of the sequence the attention must align, making the attention problem easier (the attention matrix is $T/r \times L$ instead of $T \times L$), and (c) it improves training convergence because each decoder step gets supervision from $r$ frames simultaneously. The total number of decoder steps is:
                </p>

                $$\text{decoder steps} = \lceil T / r \rceil$$

                <p>
                    For a typical sentence with $T = 500$ mel frames and $r = 5$, this is 100 decoder steps instead of 500 &mdash; a much more manageable sequence for the attention mechanism.
                </p>

                <p>
                    <strong>5. CBHG Post-Net.</strong> The decoder's mel-spectrogram output is passed through a second CBHG module (the post-net) that predicts a <strong>linear-scale spectrogram</strong>. The linear spectrogram has higher frequency resolution than the mel spectrogram and can be converted to a waveform using the Griffin-Lim algorithm. The post-net serves as a refinement stage, correcting artifacts in the decoder output.
                </p>

                <div class="note-box">
                    <div class="box-title">Tacotron 1 Architecture (Simplified Diagram)</div>
<pre style="font-family: 'JetBrains Mono', monospace; font-size: 0.8rem; line-height: 1.4; overflow-x: auto;">
  Characters:  "Hello world"
       |
  [Char Embedding]  (256-dim)
       |
  [Pre-Net]  (FC-256-ReLU-Dropout -> FC-128-ReLU-Dropout)
       |
  [CBHG Encoder]
  | Conv Bank (K=16) -> MaxPool -> Projections -> Highway (x4) -> BiGRU |
       |
  Encoder outputs: h_1, ..., h_L
       |                          |
       v                          v
  [Attention]  <----------  [Decoder GRU x2]
       |                          |
       +--- context c_t ----------+
                                  |
                             [Linear Proj]
                                  |
                         r mel frames per step
                                  |
                          [CBHG Post-Net]
                                  |
                        Linear spectrogram
                                  |
                           [Griffin-Lim]
                                  |
                             Waveform
</pre>
                </div>

                <!-- Section 3: Location-Sensitive Attention -->
                <h2 id="attention-mechanism">Location-Sensitive Attention</h2>

                <p>
                    The attention mechanism is the most critical &mdash; and most fragile &mdash; component of a seq2seq TTS system. Standard content-based attention, as used in machine translation, computes the alignment between decoder state $s_t$ and encoder output $h_l$ purely from their content:
                </p>

                $$e_{t,l} = v^\top \tanh(W_s s_t + W_h h_l + b)$$

                $$\alpha_{t,l} = \frac{\exp(e_{t,l})}{\sum_{l'=1}^{L} \exp(e_{t,l'})}$$

                <p>
                    where $v$, $W_s$, $W_h$, and $b$ are learned parameters. The energy $e_{t,l}$ measures the compatibility between the decoder state (representing "what the decoder wants to generate next") and each encoder position (representing "what text content is available"). The softmax normalizes these energies into attention weights $\alpha_{t,l}$ that sum to 1, and the context vector is $c_t = \sum_l \alpha_{t,l} h_l$.
                </p>

                <p>
                    <strong>The problem with content-based attention for TTS.</strong> In speech synthesis, many encoder positions contain similar content (e.g., repeated phonemes, or different instances of common phonemes like /a/ or /t/). Content-based attention cannot distinguish between two occurrences of the same phoneme &mdash; they have identical encoder representations $h_l$ and thus identical energy scores. This leads to the attention "jumping" between positions or getting stuck. Furthermore, speech is monotonic: the attention should smoothly advance from left to right. Content-based attention has no mechanism to enforce this &mdash; it treats the encoder positions as an unordered set.
                </p>

                <p>
                    <strong>Location-sensitive attention</strong> (Chorowski et al., 2015) addresses these problems by conditioning the energy computation on the <em>previous</em> attention weights. The key idea is simple: convolve the previous attention weights $\alpha_{t-1}$ with learned filters to produce location features $f_{t,l}$, and add these to the energy computation:
                </p>

                <div class="math-derivation">
                    <div class="math-step">
                        <strong>Step 1: Compute location features from previous attention</strong>
                        $$f_t = F * \alpha_{t-1}$$
                        <p>where $F \in \mathbb{R}^{k \times d_f}$ is a bank of $d_f$ convolution filters of kernel size $k$, applied to the previous attention distribution $\alpha_{t-1} \in \mathbb{R}^L$. This produces location features $f_t \in \mathbb{R}^{L \times d_f}$ &mdash; one feature vector per encoder position.</p>
                    </div>
                    <div class="math-step">
                        <strong>Step 2: Compute energy with location features</strong>
                        $$e_{t,l} = v^\top \tanh(W_s s_t + W_h h_l + W_f f_{t,l} + b)$$
                        <p>The location features $f_{t,l}$ are projected by $W_f$ and added to the energy computation. This tells the attention mechanism <em>where it was looking before</em> and biases it toward nearby positions.</p>
                    </div>
                    <div class="math-step">
                        <strong>Step 3: Normalize to attention weights</strong>
                        $$\alpha_{t,l} = \frac{\exp(e_{t,l})}{\sum_{l'=1}^{L} \exp(e_{t,l'})}$$
                    </div>
                </div>

                <p>
                    The convolution filters in Step 1 effectively compute a local summary of the attention history around each encoder position. If position $l$ received high attention at the previous step, the convolution features for positions $l$ and $l+1$ will reflect this, encouraging the attention to either stay at $l$ (if the phoneme is not finished) or advance to $l+1$ (if it is time to move on). The kernel size $k$ determines how far the location signal propagates &mdash; typical values are $k = 31$ or $k = 32$, covering a wide neighborhood.
                </p>

                <p>
                    This mechanism elegantly encodes the inductive bias that speech alignment is <strong>monotonic and smooth</strong>. The attention does not need to discover this from scratch through the mel loss alone &mdash; the location features explicitly provide a "you were here last step" signal that strongly biases monotonic progression. In practice, location-sensitive attention dramatically improves alignment stability compared to pure content-based attention, though it does not eliminate failures entirely (as we will see in Section 6).
                </p>

                <div class="warning-box">
                    <div class="box-title">Why Location Matters for Monotonic Alignment</div>
                    <p>
                        Consider a sentence with two instances of the word "the": "the cat sat on the mat." Content-based attention produces identical energy scores for both "the" positions, because the encoder representations are the same. The model cannot distinguish which "the" it should attend to. Location-sensitive attention resolves this ambiguity: the convolution features encode <em>where the attention was at the previous step</em>, so even if two positions have identical content, the one closer to the current attention peak will receive higher energy. This is essential for correctly traversing repeated words and similar phonemes.
                    </p>
                </div>

                <!-- Section 4: Teacher Forcing and Exposure Bias -->
                <h2 id="teacher-forcing">Teacher Forcing and Exposure Bias</h2>

                <p>
                    Tacotron's decoder is <strong>autoregressive</strong>: at each step $t$, it generates output frame $y_t$ conditioned on the previous frame $y_{t-1}$. This creates a fundamental training dilemma. During training, we know the ground-truth previous frame $y_{t-1}^*$. Should we feed the model the ground truth, or its own prediction from the previous step?
                </p>

                <p>
                    <strong>Teacher forcing</strong> is the practice of feeding the ground-truth previous output $y_{t-1}^*$ as input to the decoder at step $t$, rather than the model's own prediction $\hat{y}_{t-1}$. This is standard practice in seq2seq training and has a clear advantage: it stabilizes training by ensuring the decoder always sees correct context, preventing error accumulation. The training loss is:
                </p>

                $$\mathcal{L} = \sum_{t=1}^{T/r} \| y_t^* - \text{Decoder}(y_{t-1}^*, c_t, s_{t-1}) \|^2$$

                <p>
                    where $y_t^*$ is the ground-truth mel frame and $c_t$ is the attention context. Each decoder step receives perfect input, so errors at one step do not corrupt future steps.
                </p>

                <p>
                    <strong>The exposure bias problem.</strong> At inference time, there is no ground truth &mdash; the model must use its own predictions $\hat{y}_{t-1}$ as input. If the model makes a small error at step $t$, the input to step $t+1$ is slightly wrong. This slightly wrong input leads to a slightly larger error at step $t+1$, which leads to an even larger error at step $t+2$, and so on. Errors compound exponentially through the autoregressive chain. The model was never exposed to its own errors during training, so it has no strategy for recovering from them.
                </p>

                <p>
                    This mismatch between training (seeing ground truth) and inference (seeing own predictions) is called <strong>exposure bias</strong>, and it is particularly severe in TTS because of the long output sequences. A typical utterance requires 100&ndash;400 decoder steps. Even a 1% error rate per step means a high probability of catastrophic accumulation over hundreds of steps.
                </p>

                <p>
                    <strong>Scheduled sampling</strong> (Bengio et al., 2015) is a mitigation strategy that gradually transitions from teacher forcing to free-running during training. At each decoder step, with probability $p$ we use the ground truth (teacher forcing), and with probability $1 - p$ we use the model's own prediction. The probability $p$ starts at 1.0 (pure teacher forcing) and is annealed toward 0 over the course of training, following a schedule such as:
                </p>

                $$p(i) = \max\left(\epsilon, k^i\right)$$

                <p>
                    where $i$ is the training step, $k &lt; 1$ is the decay rate, and $\epsilon$ is a minimum probability (e.g., 0.1) to always maintain some teacher forcing for stability. This exposes the model to its own errors during training, allowing it to learn recovery strategies.
                </p>

                <p>
                    <strong>The pre-net as implicit regularization.</strong> Tacotron's pre-net provides an alternative mitigation for exposure bias. The pre-net applies 50% dropout to the previous frame <em>even at inference time</em>, effectively adding noise to the autoregressive input. This has two effects: (1) it prevents the decoder from learning to rely too heavily on the exact values of the previous frame, making it more robust to prediction errors, and (2) it forces the decoder to rely more on the attention context vector $c_t$ (which comes from the encoder and is not noisy) for determining what to generate next. The pre-net dropout is arguably the most important design decision in Tacotron &mdash; without it, the decoder tends to ignore the attention entirely and simply copy the previous frame, producing monotone or looping output.
                </p>

                <div class="note-box">
                    <div class="box-title">The Autoregressive Generation Loop</div>
                    <p>
                        At inference time, Tacotron runs the following loop until a stop condition is met: (1) pass the previous output through the pre-net (with dropout), (2) concatenate with the previous context vector and feed into the GRU decoder, (3) compute attention over encoder outputs to get a new context vector, (4) predict $r$ mel frames and a stop token, (5) if the stop token exceeds a threshold (e.g., 0.5), stop generation, otherwise use the last predicted frame as input to step (1). The stop token is a scalar sigmoid output trained with binary cross-entropy to predict 1 at the final decoder step and 0 at all other steps. Getting the stop token right is critical &mdash; if it fires too early, the utterance is truncated; too late, and the model generates trailing garbage or silence.
                    </p>
                </div>

                <!-- Section 5: Tacotron 2 -->
                <h2 id="tacotron2">Tacotron 2</h2>

                <p>
                    Tacotron 2 (Shen et al., 2018) simplified and improved the original Tacotron architecture, achieving a <strong>Mean Opinion Score (MOS) of 4.53</strong> &mdash; the first system to reach near-human quality (natural speech scores approximately 4.58 MOS). The key changes were architectural simplification, a stronger attention mechanism, and replacing Griffin-Lim with a WaveNet vocoder.
                </p>

                <p>
                    <strong>Simplified encoder.</strong> The complex CBHG encoder was replaced with a much simpler architecture: 3 convolutional layers (each with 512 filters, kernel size 5, batch normalization, and ReLU activation) followed by a single bidirectional LSTM with 512 units (256 per direction). The convolutional layers capture local character patterns (analogous to the convolution bank in CBHG but simpler), while the BiLSTM captures long-range dependencies. This encoder is both simpler to implement and easier to train than CBHG.
                </p>

                <p>
                    <strong>Location-sensitive attention.</strong> Tacotron 2 uses the location-sensitive attention mechanism described in Section 3, with 32 convolution filters of kernel size 31. An important addition is an <strong>attention prior</strong> that initializes the attention weights to focus on the first encoder position, helping the model start alignment correctly. The attention weights are also used to compute a cumulative attention signal $\sum_{t'=1}^{t-1} \alpha_{t'}$, which is concatenated with $\alpha_{t-1}$ as input to the location convolution. This cumulative signal tells the model how much total attention each position has received so far, helping prevent the attention from attending to already-consumed positions.
                </p>

                <p>
                    <strong>Decoder.</strong> The decoder uses 2 unidirectional LSTM layers (each with 1024 units) instead of GRUs. The pre-net architecture is the same as Tacotron 1 (two FC layers with dropout). The reduction factor is $r = 1$ in the default configuration &mdash; each decoder step predicts a single mel frame. This is possible because the stronger attention mechanism and LSTM decoder can handle the longer sequence. A 5-layer convolutional post-net (each with 512 filters and kernel size 5) refines the mel-spectrogram prediction, adding a residual correction to the decoder output.
                </p>

                <p>
                    <strong>WaveNet vocoder.</strong> Instead of Griffin-Lim (which introduces audible artifacts), Tacotron 2 uses a modified WaveNet conditioned on the predicted mel spectrogram. This WaveNet uses 30 dilated causal convolution layers (3 dilation cycles of 10 layers each) with 512 residual channels. The mel spectrogram is upsampled to the audio sample rate using transposed convolutions and used as a local conditioning signal. This change alone accounts for a large share of the quality improvement over Tacotron 1, since Griffin-Lim was a major quality bottleneck.
                </p>

                <div class="note-box">
                    <div class="box-title">Architecture Comparison: Tacotron 1 vs Tacotron 2</div>
                    <table style="width: 100%; border-collapse: collapse; margin-top: 0.5rem; font-size: 0.9rem;">
                        <thead>
                            <tr style="border-bottom: 2px solid var(--color-border);">
                                <th style="text-align: left; padding: 0.5rem;">Component</th>
                                <th style="text-align: left; padding: 0.5rem;">Tacotron 1</th>
                                <th style="text-align: left; padding: 0.5rem;">Tacotron 2</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr style="border-bottom: 1px solid var(--color-border);">
                                <td style="padding: 0.5rem;"><strong>Input</strong></td>
                                <td style="padding: 0.5rem;">Characters</td>
                                <td style="padding: 0.5rem;">Characters (or phonemes)</td>
                            </tr>
                            <tr style="border-bottom: 1px solid var(--color-border);">
                                <td style="padding: 0.5rem;"><strong>Encoder</strong></td>
                                <td style="padding: 0.5rem;">CBHG (Conv bank + Highway + BiGRU)</td>
                                <td style="padding: 0.5rem;">3 Conv layers + BiLSTM</td>
                            </tr>
                            <tr style="border-bottom: 1px solid var(--color-border);">
                                <td style="padding: 0.5rem;"><strong>Attention</strong></td>
                                <td style="padding: 0.5rem;">Content-based (Bahdanau)</td>
                                <td style="padding: 0.5rem;">Location-sensitive + cumulative</td>
                            </tr>
                            <tr style="border-bottom: 1px solid var(--color-border);">
                                <td style="padding: 0.5rem;"><strong>Decoder RNN</strong></td>
                                <td style="padding: 0.5rem;">2-layer GRU (256 units)</td>
                                <td style="padding: 0.5rem;">2-layer LSTM (1024 units)</td>
                            </tr>
                            <tr style="border-bottom: 1px solid var(--color-border);">
                                <td style="padding: 0.5rem;"><strong>Reduction factor</strong></td>
                                <td style="padding: 0.5rem;">$r = 5$</td>
                                <td style="padding: 0.5rem;">$r = 1$ (default)</td>
                            </tr>
                            <tr style="border-bottom: 1px solid var(--color-border);">
                                <td style="padding: 0.5rem;"><strong>Post-net</strong></td>
                                <td style="padding: 0.5rem;">CBHG (predicts linear spec)</td>
                                <td style="padding: 0.5rem;">5-layer Conv (refines mel)</td>
                            </tr>
                            <tr style="border-bottom: 1px solid var(--color-border);">
                                <td style="padding: 0.5rem;"><strong>Vocoder</strong></td>
                                <td style="padding: 0.5rem;">Griffin-Lim</td>
                                <td style="padding: 0.5rem;">WaveNet (neural)</td>
                            </tr>
                            <tr>
                                <td style="padding: 0.5rem;"><strong>MOS</strong></td>
                                <td style="padding: 0.5rem;">~3.82</td>
                                <td style="padding: 0.5rem;"><strong>4.53</strong> (near-human)</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <p>
                    The training loss for Tacotron 2 combines the mel reconstruction loss (applied to both the decoder output and the post-net refined output) with the stop token loss:
                </p>

                $$\mathcal{L} = \| y^* - \hat{y}_{\text{decoder}} \|^2 + \| y^* - \hat{y}_{\text{postnet}} \|^2 + \lambda \cdot \text{BCE}(\text{stop}^*, \hat{\text{stop}})$$

                <p>
                    where BCE is binary cross-entropy and $\lambda$ balances the stop token loss. The dual mel loss (on both decoder and post-net outputs) ensures that the decoder produces a reasonable baseline that the post-net can refine, rather than relying on the post-net to do all the work.
                </p>

                <!-- Section 6: The Attention Alignment Problem -->
                <h2 id="attention-failures">The Attention Alignment Problem</h2>

                <p>
                    Despite Tacotron 2's impressive average quality, attention-based TTS systems suffer from a fundamental reliability problem. In practice, <strong>5&ndash;10% of synthesized utterances</strong> contain audible attention failures, even for well-trained models on in-domain text. For production systems that must serve millions of requests, a 5% failure rate is unacceptable. Understanding the failure modes is essential for appreciating why the field eventually moved beyond attention-based synthesis.
                </p>

                <p>
                    <strong>Failure Mode 1: Skipping.</strong> The attention jumps forward over one or more encoder positions, causing the corresponding text to be omitted from the speech. The listener hears a word or syllable missing. This occurs when the attention energy at the next position becomes large enough to "pull" the attention past intermediate positions. It is more common for short function words ("a", "the", "of") and unstressed syllables that produce weak encoder representations.
                </p>

                <p>
                    <strong>Failure Mode 2: Repeating.</strong> The attention gets stuck on one encoder position for too many decoder steps, or oscillates between two adjacent positions. The listener hears stuttering or repeated syllables. This occurs when the model cannot decide when to advance &mdash; the energy at the current position remains high while the energy at the next position remains low. It is particularly common for long vowels and emphasized words.
                </p>

                <p>
                    <strong>Failure Mode 3: Collapse.</strong> The attention distribution becomes diffuse (spreading across many encoder positions) or collapses to a degenerate distribution (attending to the same position regardless of decoder state). The result is unintelligible babbling. This typically occurs when the decoder hidden state loses track of its position in the sequence, often triggered by an unusual sequence of phonemes or a very long utterance.
                </p>

                <p>
                    <strong>Failure Mode 4: Trailing garbage.</strong> The stop token fails to fire at the end of the utterance, and the decoder continues generating frames beyond the text content. The attention may wrap around to the beginning of the encoder (producing repeated words) or spread uniformly (producing noise). This is a consequence of the stop token being a simple sigmoid output that must learn a complex decision boundary &mdash; "am I done?" &mdash; from a single scalar.
                </p>

                <div class="warning-box">
                    <div class="box-title">Visualizing Good vs. Bad Attention</div>
                    <p>
                        A well-aligned attention matrix shows a clear <strong>monotonic diagonal path</strong> from the bottom-left to the top-right. Each column (encoder position) receives attention for a contiguous block of rows (decoder steps), with sharp boundaries between segments. A failed attention matrix shows deviations: horizontal streaks (repeating), vertical gaps (skipping), diffuse bands (collapse), or the path wrapping around or going backward.
                    </p>
<pre style="font-family: 'JetBrains Mono', monospace; font-size: 0.75rem; line-height: 1.3; overflow-x: auto;">
  Good Attention:              Bad Attention (repeating + skipping):

  Decoder  ......####          Decoder  ......####
  steps    .....####.          steps    .....####.
  (time)   ....####..          (time)   ....####..
    |      ...####...            |      ...####...
    |      ...####...            |      ...####...  <-- stuck here
    |      ..####....            |      ...####...  <-- still stuck
    v      .####.....            v      .#.##.....  <-- skipped!
           ####......                   ##........
           Encoder positions ->         Encoder positions ->
</pre>
                    <p>
                        In practice, attention quality is assessed by computing the <strong>diagonal deviation</strong>: how far the attention peak at each decoder step deviates from the expected diagonal position. High diagonal deviation correlates strongly with perceptible synthesis errors.
                    </p>
                </div>

                <p>
                    <strong>Why does soft attention fail?</strong> The fundamental issue is that soft attention computes a <em>weighted average</em> over all encoder positions. For TTS alignment, we need a <em>hard decision</em>: "I am at position $l$ right now." The soft attention distribution is a probability distribution that must be unimodal and sharply peaked to produce clean speech. But the softmax function can only approximate a delta function &mdash; it always assigns some probability mass to incorrect positions. For short utterances, this imprecision is negligible. For long utterances (100+ encoder positions), the accumulated imprecision leads to drift and eventual failure.
                </p>

                <p>
                    The failure rate also increases with several factors: (1) utterance length (longer sequences give the attention more opportunities to fail), (2) out-of-domain text (unusual words, numbers, abbreviations that were underrepresented in training), (3) inference-time conditions (different speaker, recording environment, or language characteristics), and (4) lack of training data diversity. These fragilities motivated extensive research into more robust attention mechanisms.
                </p>

                <!-- Section 7: Attention Improvements -->
                <h2 id="attention-improvements">Attention Improvements</h2>

                <p>
                    The attention alignment problem in Tacotron spawned a rich line of research attempting to make attention more robust for TTS. Each approach encodes stronger inductive biases about the expected behavior of TTS alignment: monotonicity, locality, and smooth progression.
                </p>

                <p>
                    <strong>Forward attention</strong> (Zhang et al., 2018) constrains the attention to only move forward. At each decoder step, the attention peak can either stay at the current position or advance to the next position. Formally, the attention weights are computed using a forward algorithm analogous to HMM inference:
                </p>

                $$\alpha_{t,l} = \left( (1 - p_t) \cdot \alpha_{t-1,l} + p_t \cdot \alpha_{t-1,l-1} \right) \cdot \frac{\exp(e_{t,l})}{\sum_{l'} \exp(e_{t,l'})}$$

                <p>
                    where $p_t$ is a learned transition probability. This guarantees monotonic alignment by construction: the attention can never move backward. The transition probability $p_t$ allows the model to learn when to advance. This eliminates the wrapping and backward-jumping failure modes.
                </p>

                <p>
                    <strong>Gaussian Mixture Model (GMM) attention</strong> (Graves, 2013; adapted for TTS by Battenberg et al., 2020) replaces the softmax attention with a mixture of Gaussians. The attention distribution at step $t$ is:
                </p>

                $$\alpha_{t,l} = \sum_{j=1}^{J} w_{t,j} \cdot \mathcal{N}(l; \mu_{t,j}, \sigma_{t,j}^2)$$

                <p>
                    where $w_{t,j}$, $\mu_{t,j}$, and $\sigma_{t,j}$ are the weight, mean, and standard deviation of the $j$-th Gaussian component, all predicted by the decoder. Monotonicity is enforced by parameterizing the means as $\mu_{t,j} = \mu_{t-1,j} + \Delta_{t,j}$ where $\Delta_{t,j} &gt; 0$ (using a softplus activation). This ensures the Gaussian centers always move forward. GMM attention handles long sequences much better than softmax attention because the Gaussian parameterization naturally produces a peaked, localized distribution without requiring the softmax to approximate a delta function.
                </p>

                <p>
                    <strong>Stepwise monotonic attention</strong> (He et al., 2019) is a hard monotonic attention mechanism where the attention explicitly decides at each step: "stay at the current position or move to the next one." This is implemented as a Bernoulli random variable at each position, with the decision made left-to-right. At inference time, the attention selects the first position where the "move forward" probability exceeds a threshold. This produces perfectly hard, monotonic alignments with no possibility of skipping or repeating.
                </p>

                <p>
                    <strong>Diagonal-guided attention</strong> (Zhu et al., 2019) adds an explicit loss term that penalizes the attention for deviating from the diagonal:
                </p>

                $$\mathcal{L}_{\text{diag}} = \frac{1}{T \cdot L} \sum_{t=1}^{T} \sum_{l=1}^{L} \alpha_{t,l} \cdot \left( \frac{l}{L} - \frac{t}{T} \right)^2$$

                <p>
                    This loss penalizes attention weight placed far from the diagonal of the $T \times L$ attention matrix. Positions on the diagonal satisfy $l/L = t/T$ (proportional progress through both sequences). The quadratic penalty strongly discourages attention at positions far from the expected diagonal, providing a strong training signal for monotonic alignment. However, it assumes a roughly constant speech rate, which may not hold for all utterances.
                </p>

                <p>
                    <strong>Connection to Monotonic Alignment Search (MAS).</strong> The ultimate solution to the attention alignment problem was to <em>remove soft attention entirely</em>. Monotonic Alignment Search (Tutorial 26) computes the optimal hard monotonic alignment using dynamic programming, without any learned attention parameters. This alignment is then used to construct explicit durations for each text token, which are fed to a non-autoregressive decoder (as in Glow-TTS or VITS). MAS guarantees perfect monotonic alignment by construction, eliminates all attention failure modes, and produces alignments that improve as the model trains (unlike a fixed external aligner). The attention improvements described above can be seen as incremental steps toward the hard, monotonic alignment that MAS provides directly.
                </p>

                <div class="definition-box">
                    <div class="box-title">Definition: Monotonic Attention</div>
                    <p>
                        <strong>Monotonic attention</strong> is any attention mechanism that constrains the attention peak to move only forward (or stay) through the encoder positions as decoder steps progress. Formally, if $a_t = \arg\max_l \alpha_{t,l}$ is the attended position at step $t$, then monotonic attention enforces $a_t \geq a_{t-1}$ for all $t$. This is the natural constraint for speech synthesis, where text is spoken left-to-right. Monotonic attention eliminates backward-jumping and wrapping failure modes but may still suffer from stalling (the attention peak not advancing when it should).
                    </p>
                </div>

                <!-- Section 8: Tacotron's Legacy -->
                <h2 id="tacotron-legacy">Tacotron's Legacy</h2>

                <p>
                    Tacotron and Tacotron 2 were watershed moments in speech synthesis. Before Tacotron, the idea that a single neural network could convert text directly to natural-sounding speech seemed implausible. The best systems were complex pipelines of separately trained components, each requiring expert knowledge to design, train, and tune. Tacotron proved that end-to-end learning could not only match but <em>exceed</em> the quality of these engineered pipelines.
                </p>

                <p>
                    <strong>What Tacotron proved:</strong>
                </p>

                <ul>
                    <li><strong>End-to-end TTS is feasible.</strong> A single model can learn grapheme-to-phoneme conversion, prosody prediction, duration modeling, and acoustic feature generation simultaneously, without any linguistic feature engineering.</li>
                    <li><strong>Near-human quality is achievable.</strong> Tacotron 2's MOS of 4.53, compared to natural speech at 4.58, demonstrated that neural TTS could produce speech virtually indistinguishable from human recordings in controlled evaluations.</li>
                    <li><strong>Attention can discover alignment.</strong> The attention mechanism can learn the correspondence between text and acoustic features without any external alignment supervision, proving that the alignment signal in the mel reconstruction loss is sufficient (though not always reliable).</li>
                    <li><strong>The vocoder matters enormously.</strong> The jump from Griffin-Lim (Tacotron 1, MOS ~3.82) to WaveNet (Tacotron 2, MOS 4.53) showed that vocoder quality is often the bottleneck, not the acoustic model.</li>
                </ul>

                <p>
                    <strong>Tacotron's limitations:</strong>
                </p>

                <ul>
                    <li><strong>Slow autoregressive inference.</strong> Generating mel frames one at a time makes Tacotron 2 roughly 0.5x real-time for mel generation alone. Combined with the WaveNet vocoder, end-to-end synthesis is far below real-time.</li>
                    <li><strong>Attention failures.</strong> The 5&ndash;10% failure rate on unseen text makes Tacotron unreliable for production deployment without extensive post-hoc filtering and re-synthesis.</li>
                    <li><strong>Limited controllability.</strong> Prosody (pitch, duration, energy) emerges implicitly from the autoregressive process and cannot be directly controlled. There is no way to say "make this word louder" or "speak faster here."</li>
                    <li><strong>Data-hungry.</strong> Tacotron requires ~20 hours of high-quality, single-speaker studio recordings for good results &mdash; a significant data collection burden.</li>
                </ul>

                <p>
                    <strong>How Tacotron motivated what came next.</strong> Each of Tacotron's limitations directly motivated a subsequent line of research:
                </p>

                <ul>
                    <li>The slow inference speed motivated <strong>FastSpeech</strong> (Tutorial 31) and other non-autoregressive models that generate all mel frames in parallel.</li>
                    <li>The attention failures motivated <strong>Glow-TTS</strong> (Tutorial 32), which uses Monotonic Alignment Search instead of attention, and <strong>VITS</strong> (Tutorial 33), which eliminates the acoustic model / vocoder boundary entirely.</li>
                    <li>The controllability limitation motivated <strong>FastSpeech 2</strong>, which predicts explicit pitch, energy, and duration that can be modified at inference time.</li>
                    <li>The data requirements motivated research into few-shot and zero-shot TTS, multi-speaker models, and transfer learning approaches.</li>
                </ul>

                <p>
                    Tacotron's historical role is analogous to AlexNet in computer vision or the original Transformer in NLP: it demonstrated the fundamental feasibility of an approach that would be refined, improved, and eventually superseded by faster and more robust architectures &mdash; but the core insight (end-to-end neural TTS) remains the foundation of all modern systems.
                </p>

                <!-- Navigation -->
                <div class="tutorial-nav">
                    <a href="../28-neural-vocoders/index.html" class="tutorial-nav-link prev">
                        <span class="nav-label">Previous</span>
                        <span class="nav-title">&larr; Neural Vocoders</span>
                    </a>
                    <a href="../30-fastspeech/index.html" class="tutorial-nav-link next">
                        <span class="nav-label">Next</span>
                        <span class="nav-title">FastSpeech &amp; Non-AR TTS &rarr;</span>
                    </a>
                </div>

            </article>

            <!-- ==================== CODE TAB ==================== -->
            <article class="article-content" id="code" style="display: none;">
                <h2>Code Examples</h2>
                <p>Practical implementations of key Tacotron components. These examples demonstrate location-sensitive attention, the autoregressive decoder loop, and attention failure detection &mdash; the three most important algorithmic ideas in attention-based TTS.</p>

                <!-- Code Example 1: Location-Sensitive Attention -->
                <h3>1. Location-Sensitive Attention Module</h3>
                <p>PyTorch implementation of location-sensitive attention with convolution features. This is the core attention mechanism used in Tacotron 2, where the previous attention weights are convolved with learned filters to produce location features that bias the attention toward monotonic progression.</p>

<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F


class LocationSensitiveAttention(nn.Module):
    """
    Location-sensitive attention (Chorowski et al., 2015) as used in Tacotron 2.

    Computes:
        f_t = F * alpha_{t-1}           (location features)
        e_{t,l} = v^T tanh(W_s @ s_t + W_h @ h_l + W_f @ f_{t,l} + b)
        alpha_{t,l} = softmax(e_t)_l
        c_t = sum_l alpha_{t,l} * h_l   (context vector)

    Args:
        encoder_dim: Dimension of encoder outputs h_l
        decoder_dim: Dimension of decoder state s_t
        attention_dim: Hidden dimension for attention computation
        location_channels: Number of convolution filters for location features
        location_kernel_size: Kernel size for location convolution
    """

    def __init__(self, encoder_dim=512, decoder_dim=1024,
                 attention_dim=128, location_channels=32,
                 location_kernel_size=31):
        super().__init__()

        # Project decoder state: W_s @ s_t
        self.W_s = nn.Linear(decoder_dim, attention_dim, bias=False)

        # Project encoder outputs: W_h @ h_l (precomputed for efficiency)
        self.W_h = nn.Linear(encoder_dim, attention_dim, bias=False)

        # Project location features: W_f @ f_{t,l}
        self.location_conv = nn.Conv1d(
            in_channels=2,  # previous attention + cumulative attention
            out_channels=location_channels,
            kernel_size=location_kernel_size,
            padding=(location_kernel_size - 1) // 2,
            bias=False
        )
        self.W_f = nn.Linear(location_channels, attention_dim, bias=False)

        # Score vector: v^T @ tanh(...)
        self.v = nn.Linear(attention_dim, 1, bias=False)

        # Bias term
        self.b = nn.Parameter(torch.zeros(attention_dim))

    def forward(self, decoder_state, encoder_outputs, prev_attention,
                cumulative_attention, encoder_mask=None):
        """
        Args:
            decoder_state: (B, decoder_dim) current decoder hidden state
            encoder_outputs: (B, L, encoder_dim) encoder output sequence
            prev_attention: (B, L) attention weights from previous step
            cumulative_attention: (B, L) sum of all previous attention weights
            encoder_mask: (B, L) boolean mask (True = valid, False = padding)

        Returns:
            attention_weights: (B, L) normalized attention weights
            context: (B, encoder_dim) context vector
        """
        B, L, _ = encoder_outputs.shape

        # Step 1: Location features from previous attention
        # Stack previous and cumulative attention: (B, 2, L)
        attention_cat = torch.stack([prev_attention, cumulative_attention], dim=1)
        # Convolve: (B, 2, L) -> (B, location_channels, L)
        location_features = self.location_conv(attention_cat)
        # Transpose and project: (B, L, location_channels) -> (B, L, attention_dim)
        location_features = self.W_f(location_features.transpose(1, 2))

        # Step 2: Compute energy
        # W_s @ s_t: (B, attention_dim) -> (B, 1, attention_dim) for broadcasting
        decoder_proj = self.W_s(decoder_state).unsqueeze(1)
        # W_h @ h_l: (B, L, attention_dim) - can be precomputed
        encoder_proj = self.W_h(encoder_outputs)

        # e_{t,l} = v^T tanh(W_s s_t + W_h h_l + W_f f_{t,l} + b)
        energy = self.v(torch.tanh(
            decoder_proj + encoder_proj + location_features + self.b
        )).squeeze(-1)  # (B, L)

        # Apply mask (set padded positions to -inf before softmax)
        if encoder_mask is not None:
            energy = energy.masked_fill(~encoder_mask, float('-inf'))

        # Step 3: Softmax to get attention weights
        attention_weights = F.softmax(energy, dim=-1)  # (B, L)

        # Step 4: Context vector
        context = torch.bmm(
            attention_weights.unsqueeze(1),  # (B, 1, L)
            encoder_outputs                   # (B, L, encoder_dim)
        ).squeeze(1)  # (B, encoder_dim)

        return attention_weights, context


# ============================================================
# Demonstration
# ============================================================
torch.manual_seed(42)

B, L, T = 2, 50, 200  # batch=2, 50 encoder positions, 200 decoder steps
encoder_dim, decoder_dim = 512, 1024

attention = LocationSensitiveAttention(
    encoder_dim=encoder_dim,
    decoder_dim=decoder_dim,
    attention_dim=128,
    location_channels=32,
    location_kernel_size=31
)

print("=== Location-Sensitive Attention ===")
print(f"Parameters: {sum(p.numel() for p in attention.parameters()):,}")
print(f"Encoder dim: {encoder_dim}, Decoder dim: {decoder_dim}")
print(f"Sequence: {L} encoder positions, {T} decoder steps")
print()

# Simulate encoder outputs and decoder states
encoder_outputs = torch.randn(B, L, encoder_dim)
decoder_states = torch.randn(B, T, decoder_dim)

# Run attention step by step
prev_attn = torch.zeros(B, L)
prev_attn[:, 0] = 1.0  # Initialize attention at first position
cumul_attn = prev_attn.clone()

all_attention = []
for t in range(T):
    weights, context = attention(
        decoder_states[:, t], encoder_outputs,
        prev_attn, cumul_attn
    )
    all_attention.append(weights)
    cumul_attn = cumul_attn + weights
    prev_attn = weights

attention_matrix = torch.stack(all_attention, dim=1)  # (B, T, L)
print(f"Attention matrix shape: {tuple(attention_matrix.shape)}")
print(f"Context vector shape:   {tuple(context.shape)}")

# Check attention properties
peaks = attention_matrix[0].argmax(dim=-1)  # (T,)
monotonic = (peaks[1:] >= peaks[:-1]).float().mean()
print(f"Attention peak monotonicity: {monotonic:.1%}")
print(f"Peak range: [{peaks.min().item()}, {peaks.max().item()}] "
      f"(should span 0 to {L-1})")</code></pre>

                <!-- Code Example 2: Tacotron 2 Inference Pipeline -->
                <h3>2. Tacotron 2 Inference Pipeline</h3>
                <p>A simplified Tacotron 2 decoder loop demonstrating the autoregressive generation process: pre-net, LSTM decoder, location-sensitive attention, mel prediction, and stop token detection. This captures the essential inference logic without the full training infrastructure.</p>

<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F


class PreNet(nn.Module):
    """
    Tacotron 2 Pre-Net: two FC layers with dropout (applied at inference too).

    The dropout at inference is intentional -- it acts as a regularizer
    that prevents the decoder from simply copying the previous frame,
    forcing it to rely on the attention context.
    """

    def __init__(self, in_dim=80, hidden_dim=256, out_dim=256, dropout=0.5):
        super().__init__()
        self.fc1 = nn.Linear(in_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, out_dim)
        self.dropout = dropout

    def forward(self, x):
        # Dropout applied even during eval (inference)
        x = F.dropout(F.relu(self.fc1(x)), p=self.dropout, training=True)
        x = F.dropout(F.relu(self.fc2(x)), p=self.dropout, training=True)
        return x


class Tacotron2Decoder(nn.Module):
    """
    Simplified Tacotron 2 decoder for demonstration.

    Components:
    - Pre-net (FC + dropout, applied at inference too)
    - 2-layer unidirectional LSTM
    - Location-sensitive attention
    - Linear projection for mel output
    - Stop token prediction (sigmoid)
    """

    def __init__(self, encoder_dim=512, mel_dim=80, decoder_dim=1024,
                 prenet_dim=256, max_decoder_steps=1000):
        super().__init__()
        self.mel_dim = mel_dim
        self.decoder_dim = decoder_dim
        self.max_decoder_steps = max_decoder_steps

        # Pre-net
        self.prenet = PreNet(mel_dim, prenet_dim, prenet_dim)

        # Attention
        self.attention = LocationSensitiveAttention(
            encoder_dim=encoder_dim,
            decoder_dim=decoder_dim,
            attention_dim=128,
            location_channels=32,
            location_kernel_size=31
        )

        # LSTM decoder: input = prenet_output + context
        self.lstm1 = nn.LSTMCell(prenet_dim + encoder_dim, decoder_dim)
        self.lstm2 = nn.LSTMCell(decoder_dim, decoder_dim)

        # Output projections
        self.mel_proj = nn.Linear(decoder_dim + encoder_dim, mel_dim)
        self.stop_proj = nn.Linear(decoder_dim + encoder_dim, 1)

    def forward(self, encoder_outputs, encoder_mask=None):
        """
        Autoregressive inference loop.

        Args:
            encoder_outputs: (B, L, encoder_dim)
            encoder_mask: (B, L) boolean mask

        Returns:
            mel_outputs: (B, T, mel_dim) predicted mel spectrogram
            attention_weights: (B, T, L) attention alignment matrix
        """
        B, L, enc_dim = encoder_outputs.shape
        device = encoder_outputs.device

        # Initialize decoder states
        h1 = torch.zeros(B, self.decoder_dim, device=device)
        c1 = torch.zeros(B, self.decoder_dim, device=device)
        h2 = torch.zeros(B, self.decoder_dim, device=device)
        c2 = torch.zeros(B, self.decoder_dim, device=device)

        # Initialize attention
        prev_attn = torch.zeros(B, L, device=device)
        prev_attn[:, 0] = 1.0
        cumul_attn = prev_attn.clone()
        context = torch.zeros(B, enc_dim, device=device)

        # Initialize first frame (zeros = silence)
        current_frame = torch.zeros(B, self.mel_dim, device=device)

        mel_outputs = []
        all_attention = []

        for step in range(self.max_decoder_steps):
            # 1. Pre-net (with dropout, even at inference)
            prenet_out = self.prenet(current_frame)

            # 2. LSTM layers
            lstm_input = torch.cat([prenet_out, context], dim=-1)
            h1, c1 = self.lstm1(lstm_input, (h1, c1))
            h2, c2 = self.lstm2(h1, (h2, c2))

            # 3. Attention
            attn_weights, context = self.attention(
                h2, encoder_outputs, prev_attn, cumul_attn, encoder_mask
            )
            cumul_attn = cumul_attn + attn_weights
            prev_attn = attn_weights

            # 4. Predict mel frame and stop token
            decoder_context = torch.cat([h2, context], dim=-1)
            mel_frame = self.mel_proj(decoder_context)
            stop_logit = self.stop_proj(decoder_context).squeeze(-1)

            mel_outputs.append(mel_frame)
            all_attention.append(attn_weights)

            # 5. Check stop condition
            if torch.sigmoid(stop_logit).item() > 0.5:
                break

            # 6. Use predicted frame as next input
            current_frame = mel_frame

        mel_outputs = torch.stack(mel_outputs, dim=1)     # (B, T, mel_dim)
        all_attention = torch.stack(all_attention, dim=1)  # (B, T, L)

        return mel_outputs, all_attention


# ============================================================
# Demonstration
# ============================================================
torch.manual_seed(42)

B, L = 1, 30  # single utterance, 30 encoder positions
encoder_dim = 512

encoder_outputs = torch.randn(B, L, encoder_dim)
decoder = Tacotron2Decoder(
    encoder_dim=encoder_dim, mel_dim=80,
    decoder_dim=1024, max_decoder_steps=200
)
decoder.eval()

print("=== Tacotron 2 Inference ===")
print(f"Decoder parameters: {sum(p.numel() for p in decoder.parameters()):,}")

with torch.no_grad():
    mel_out, attn_out = decoder(encoder_outputs)

print(f"Generated mel shape:    {tuple(mel_out.shape)}")
print(f"Attention matrix shape: {tuple(attn_out.shape)}")
print(f"Decoder steps:          {mel_out.shape[1]}")

# Analyze attention
peaks = attn_out[0].argmax(dim=-1)
mono = (peaks[1:] >= peaks[:-1]).float().mean()
print(f"Attention monotonicity: {mono:.1%}")
print(f"Coverage: attended positions {peaks.min().item()} to {peaks.max().item()} "
      f"out of {L}")</code></pre>

                <!-- Code Example 3: Attention Failure Detection -->
                <h3>3. Attention Failure Detection</h3>
                <p>Code to analyze attention matrices for common failure modes. This utility computes entropy, diagonal deviation, and repeated-row metrics to automatically detect skipping, repeating, collapse, and other attention pathologies.</p>

<pre><code>import torch
import numpy as np


def attention_entropy(attention_matrix):
    """
    Compute per-step entropy of the attention distribution.

    Low entropy = peaked (good, attending to one position)
    High entropy = diffuse (bad, attending to many positions)

    Args:
        attention_matrix: (T, L) attention weights

    Returns:
        entropies: (T,) per-step entropy values
    """
    # Add small epsilon to avoid log(0)
    eps = 1e-8
    attn = attention_matrix.clamp(min=eps)
    entropies = -(attn * torch.log2(attn)).sum(dim=-1)
    return entropies


def diagonal_deviation(attention_matrix):
    """
    Measure how far the attention peak deviates from the expected diagonal.

    For perfect alignment, the peak at decoder step t should be at
    encoder position l = t * (L / T).

    Args:
        attention_matrix: (T, L) attention weights

    Returns:
        deviations: (T,) absolute deviation from expected diagonal position
        mean_deviation: scalar mean deviation
    """
    T, L = attention_matrix.shape
    peaks = attention_matrix.argmax(dim=-1).float()  # (T,)
    expected = torch.linspace(0, L - 1, T)  # expected diagonal positions
    deviations = (peaks - expected).abs()
    return deviations, deviations.mean().item()


def detect_repeating(attention_matrix, threshold=3):
    """
    Detect repeating (stuttering) by finding encoder positions
    that receive peak attention for too many consecutive steps.

    Args:
        attention_matrix: (T, L)
        threshold: max acceptable consecutive steps at same position

    Returns:
        repeats: list of (position, start_step, duration) tuples
    """
    peaks = attention_matrix.argmax(dim=-1).tolist()
    repeats = []
    current_pos = peaks[0]
    current_start = 0
    current_count = 1

    for t in range(1, len(peaks)):
        if peaks[t] == current_pos:
            current_count += 1
        else:
            if current_count > threshold:
                repeats.append((current_pos, current_start, current_count))
            current_pos = peaks[t]
            current_start = t
            current_count = 1

    if current_count > threshold:
        repeats.append((current_pos, current_start, current_count))

    return repeats


def detect_skipping(attention_matrix, max_jump=3):
    """
    Detect skipping by finding large forward jumps in attention peaks.

    Args:
        attention_matrix: (T, L)
        max_jump: maximum acceptable jump between consecutive peaks

    Returns:
        skips: list of (step, from_pos, to_pos, jump_size) tuples
    """
    peaks = attention_matrix.argmax(dim=-1).tolist()
    skips = []

    for t in range(1, len(peaks)):
        jump = peaks[t] - peaks[t - 1]
        if jump > max_jump:
            skips.append((t, peaks[t - 1], peaks[t], jump))

    return skips


def attention_coverage(attention_matrix):
    """
    Check which encoder positions received significant attention.
    Positions that were skipped entirely indicate missing content.

    Args:
        attention_matrix: (T, L)

    Returns:
        coverage: (L,) max attention received at each encoder position
        uncovered: list of encoder positions that received < 0.1 max attention
    """
    coverage = attention_matrix.max(dim=0).values  # (L,)
    threshold = 0.1
    uncovered = (coverage < threshold).nonzero(as_tuple=True)[0].tolist()
    return coverage, uncovered


def diagnose_attention(attention_matrix, verbose=True):
    """
    Run all diagnostics on an attention matrix and report findings.

    Args:
        attention_matrix: (T, L) attention weights (numpy or torch)

    Returns:
        report: dict with diagnostic results
    """
    if isinstance(attention_matrix, np.ndarray):
        attention_matrix = torch.from_numpy(attention_matrix)

    T, L = attention_matrix.shape
    report = {}

    # 1. Entropy analysis
    entropies = attention_entropy(attention_matrix)
    report['mean_entropy'] = entropies.mean().item()
    report['max_entropy'] = entropies.max().item()
    report['max_possible_entropy'] = np.log2(L)

    # 2. Diagonal deviation
    devs, mean_dev = diagonal_deviation(attention_matrix)
    report['mean_diagonal_deviation'] = mean_dev
    report['max_diagonal_deviation'] = devs.max().item()

    # 3. Repeating detection
    repeats = detect_repeating(attention_matrix, threshold=max(3, T // L))
    report['repeating_segments'] = repeats

    # 4. Skipping detection
    skips = detect_skipping(attention_matrix, max_jump=max(2, L // 10))
    report['skipping_events'] = skips

    # 5. Coverage analysis
    coverage, uncovered = attention_coverage(attention_matrix)
    report['uncovered_positions'] = uncovered
    report['coverage_ratio'] = 1 - len(uncovered) / L

    # 6. Overall diagnosis
    issues = []
    if report['mean_entropy'] > 0.5 * report['max_possible_entropy']:
        issues.append('HIGH_ENTROPY (diffuse attention, possible collapse)')
    if mean_dev > L * 0.15:
        issues.append('HIGH_DIAGONAL_DEVIATION (misaligned)')
    if len(repeats) > 0:
        issues.append(f'REPEATING ({len(repeats)} segments)')
    if len(skips) > 0:
        issues.append(f'SKIPPING ({len(skips)} jumps)')
    if len(uncovered) > 0:
        issues.append(f'MISSING_COVERAGE ({len(uncovered)} positions)')

    report['issues'] = issues
    report['is_healthy'] = len(issues) == 0

    if verbose:
        print(f"=== Attention Diagnosis ({T} steps x {L} positions) ===")
        print(f"Mean entropy:      {report['mean_entropy']:.3f} "
              f"(max possible: {report['max_possible_entropy']:.3f})")
        print(f"Diagonal deviation: {mean_dev:.2f} positions (mean), "
              f"{devs.max().item():.2f} (max)")
        print(f"Coverage:          {report['coverage_ratio']:.1%} "
              f"({L - len(uncovered)}/{L} positions covered)")
        print(f"Repeating:         {len(repeats)} segments")
        print(f"Skipping:          {len(skips)} events")
        print()
        if report['is_healthy']:
            print("DIAGNOSIS: Healthy alignment")
        else:
            print("DIAGNOSIS: Issues detected:")
            for issue in issues:
                print(f"  - {issue}")

    return report


# ============================================================
# Demonstration
# ============================================================
torch.manual_seed(42)

T, L = 150, 40  # 150 decoder steps, 40 encoder positions

# 1. Generate a GOOD attention matrix (clean diagonal)
print("=" * 60)
print("Test 1: Good attention (monotonic diagonal)")
print("=" * 60)
good_attn = torch.zeros(T, L)
for t in range(T):
    pos = min(int(t * L / T), L - 1)
    spread = 2
    for offset in range(-spread, spread + 1):
        p = pos + offset
        if 0 <= p < L:
            good_attn[t, p] = torch.exp(torch.tensor(-0.5 * offset ** 2))
good_attn = good_attn / good_attn.sum(dim=-1, keepdim=True)

good_report = diagnose_attention(good_attn)
print()

# 2. Generate a BAD attention matrix (repeating + skipping)
print("=" * 60)
print("Test 2: Bad attention (repeating and skipping)")
print("=" * 60)
bad_attn = good_attn.clone()
# Simulate repeating: stuck at position 15 for steps 50-80
for t in range(50, 80):
    bad_attn[t] = 0
    bad_attn[t, 15] = 1.0
# Simulate skipping: jump from position 20 to 30 at step 90
for t in range(80, 90):
    bad_attn[t] = 0
    bad_attn[t, 30] = 1.0
bad_attn = bad_attn / bad_attn.sum(dim=-1, keepdim=True).clamp(min=1e-8)

bad_report = diagnose_attention(bad_attn)
print()

# 3. Generate a COLLAPSED attention matrix (diffuse)
print("=" * 60)
print("Test 3: Collapsed attention (diffuse)")
print("=" * 60)
collapsed_attn = torch.ones(T, L) / L + torch.randn(T, L) * 0.01
collapsed_attn = F.softmax(collapsed_attn, dim=-1)

collapsed_report = diagnose_attention(collapsed_attn)</code></pre>

            </article>

            <!-- ==================== EXERCISES TAB ==================== -->
            <article class="article-content" id="exercises" style="display: none;">
                <h2>Exercises</h2>
                <p>Test your understanding of Tacotron, attention-based TTS, and the challenges of sequence-to-sequence speech synthesis. Exercises cover reduction factor math, attention properties, location features, stop token design, and attention failure analysis. Solutions are provided for self-study.</p>

                <div class="exercise-list">

                    <h3 style="margin-top: 1rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">Easy</h3>

                    <!-- Exercise 1 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">1. Reduction Factor Math</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>A Tacotron model uses reduction factor $r = 5$ and receives an input sentence that produces $T = 487$ mel-spectrogram frames. (a) How many decoder steps are required? (b) If the input text has $L = 42$ characters, what is the size of the attention matrix? (c) Compare this to the attention matrix size with $r = 1$. (d) If each decoder step takes 10 ms on a GPU, what is the total inference time for each reduction factor?</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a) Decoder steps with $r = 5$:</strong></p>
                                $$\text{steps} = \lceil T / r \rceil = \lceil 487 / 5 \rceil = \lceil 97.4 \rceil = 98 \text{ steps}$$
                                <p><strong>(b) Attention matrix size with $r = 5$:</strong></p>
                                <p>The attention matrix is $(\text{decoder steps}) \times L = 98 \times 42 = 4{,}116$ elements.</p>
                                <p><strong>(c) With $r = 1$:</strong></p>
                                <p>The attention matrix is $T \times L = 487 \times 42 = 20{,}454$ elements &mdash; about $5\times$ larger. The attention mechanism must maintain coherent alignment over $5\times$ more steps, making it significantly more likely to fail.</p>
                                <p><strong>(d) Inference time:</strong></p>
                                <ul>
                                    <li>$r = 5$: $98 \times 10\text{ ms} = 980\text{ ms} \approx 1.0\text{ s}$</li>
                                    <li>$r = 1$: $487 \times 10\text{ ms} = 4{,}870\text{ ms} \approx 4.9\text{ s}$</li>
                                </ul>
                                <p>The reduction factor provides a direct $\sim 5\times$ speedup in decoder inference time, at the cost of predicting $r$ frames simultaneously (which requires the model to capture intra-group dynamics in a single step).</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 2 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">2. Attention Matrix Properties</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Consider the attention weight matrix $A \in \mathbb{R}^{T \times L}$ for an utterance with $T = 300$ decoder steps and $L = 50$ encoder positions. (a) What constraint must each row satisfy? (b) For a well-aligned synthesis, what should the column sums $\sum_t \alpha_{t,l}$ approximately represent? (c) If the average speaking rate is 3 phonemes per second and the mel frame rate is 80 frames per second, what is the expected average column sum? (d) If column $l$ has a sum of 0, what does this mean perceptually?</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a) Row constraint:</strong></p>
                                <p>Each row must sum to 1: $\sum_{l=1}^{L} \alpha_{t,l} = 1$ for all $t$, because the attention weights are produced by a softmax and represent a probability distribution over encoder positions for each decoder step.</p>
                                <p><strong>(b) Column sums:</strong></p>
                                <p>The column sum $\sum_{t=1}^{T} \alpha_{t,l}$ represents the <strong>total attention received by encoder position $l$</strong>, which is approximately proportional to the <strong>duration</strong> (in decoder frames) of the phoneme at that position. If the attention were perfectly hard (one-hot at each step), the column sum would equal exactly the number of frames assigned to that phoneme.</p>
                                <p><strong>(c) Expected average column sum:</strong></p>
                                <p>At 80 frames/second and 3 phonemes/second, each phoneme lasts about $80/3 \approx 26.7$ frames on average. Alternatively: $T / L = 300 / 50 = 6.0$ frames per position. Both estimates differ because the first uses absolute rates while the second uses the given sequence lengths. The column sum must average to $T / L = 6.0$ since all rows sum to 1 and there are $T$ rows, so the total of all elements is $T$, giving an average column sum of $T / L$.</p>
                                <p><strong>(d) Zero column sum:</strong></p>
                                <p>If $\sum_t \alpha_{t,l} = 0$, encoder position $l$ received zero attention throughout the entire synthesis. The phoneme at position $l$ is completely absent from the generated speech &mdash; the word or sound is <strong>skipped</strong>. This is one of the most severe attention failure modes and results in missing content that listeners immediately notice.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 3 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">3. Location Feature Convolution</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>In location-sensitive attention, the previous attention weights $\alpha_{t-1} \in \mathbb{R}^L$ are convolved with filters of kernel size $k = 31$ and $d_f = 32$ output channels. (a) What is the shape of the convolution filter bank? (b) How many parameters does this convolution add? (c) What does a large kernel size $k$ allow the attention to "see"? (d) What would happen if $k = 1$ (pointwise convolution)?</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a) Filter bank shape:</strong></p>
                                <p>The convolution takes 2 input channels (previous attention + cumulative attention) and produces $d_f = 32$ output channels with kernel size $k = 31$. The filter bank shape is $(d_f, 2, k) = (32, 2, 31)$.</p>
                                <p><strong>(b) Parameters:</strong></p>
                                $$\text{params} = d_f \times 2 \times k = 32 \times 2 \times 31 = 1{,}984$$
                                <p>(No bias if bias=False, as is typical.) This is a very small number of parameters &mdash; a negligible addition to the overall model.</p>
                                <p><strong>(c) What large $k$ provides:</strong></p>
                                <p>With $k = 31$ and same-padding, the convolution at position $l$ can see the attention pattern in the range $[l - 15, l + 15]$. This means the attention can "see" its own history 15 encoder positions back and 15 positions ahead. Looking back helps it understand where it came from (for smooth continuation). Looking ahead helps it anticipate upcoming positions (for smooth transitions). This wide receptive field is important because phoneme durations vary &mdash; the attention might stay at one position for many steps, and the location features need enough context to distinguish "staying because the phoneme is long" from "stuck in a loop."</p>
                                <p><strong>(d) With $k = 1$ (pointwise):</strong></p>
                                <p>A pointwise convolution only sees the attention weight at each individual position, with no information about the spatial pattern. It could detect "this position had high attention" but not "the attention is moving from left to right" or "the attention peak is at position $l$ and position $l+1$ is next." This eliminates the location-awareness that makes the mechanism work. The attention would behave almost identically to content-based attention, losing the monotonicity bias. In practice, $k = 1$ provides negligible improvement over standard attention.</p>
                            </div>
                        </div>
                    </div>

                    <h3 style="margin-top: 2rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">Medium</h3>

                    <!-- Exercise 4 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">4. Stop Token Design</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>The Tacotron 2 stop token is a single sigmoid output trained with binary cross-entropy. The target is 0 for all steps except the last, where it is 1. (a) For an utterance with $T = 300$ decoder steps, what is the class imbalance ratio? (b) Why is this imbalance problematic? (c) Propose two strategies to mitigate the imbalance. (d) What happens if the stop token fires 20 steps too early? 20 steps too late?</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a) Class imbalance:</strong></p>
                                <p>There are 299 negative samples (stop = 0) and 1 positive sample (stop = 1). The imbalance ratio is $299:1$. For longer utterances (e.g., $T = 800$), it becomes $799:1$.</p>
                                <p><strong>(b) Why this is problematic:</strong></p>
                                <p>With such extreme imbalance, the model can achieve very low BCE loss by simply always predicting 0 (never stopping). The loss from the single positive example is overwhelmed by the loss from hundreds of negative examples. The model learns a strong bias toward "keep going" and becomes reluctant to stop, leading to trailing garbage at the end of utterances.</p>
                                <p><strong>(c) Mitigation strategies:</strong></p>
                                <ul>
                                    <li><strong>Positive weight scaling:</strong> Multiply the BCE loss for positive examples by a factor $w = T$ (or similar), so the single positive example contributes as much as all negative examples combined. PyTorch's <code>BCEWithLogitsLoss(pos_weight=...)</code> supports this directly.</li>
                                    <li><strong>Soft labeling:</strong> Instead of a hard 0/1 target, use a soft ramp: set the target to 1 for the last frame, 0.5 for the second-to-last, 0.25 for the third-to-last, etc. This gives the model a gradual "approaching the end" signal rather than a sudden jump from 0 to 1, making it easier to learn the transition.</li>
                                </ul>
                                <p><strong>(d) Consequences of early/late stopping:</strong></p>
                                <ul>
                                    <li><strong>20 steps too early:</strong> The last ~20 mel frames are missing, which at 80 frames/second corresponds to 250 ms of truncated audio. The listener hears the sentence cut off abruptly mid-word or mid-syllable. This is immediately noticeable and highly unnatural.</li>
                                    <li><strong>20 steps too late:</strong> The model generates 20 extra frames beyond the text content. The attention has no more text to attend to, so it typically either (a) spreads diffusely (producing noise/babble), (b) wraps to the beginning (repeating the first word), or (c) stays at the last position (producing a prolonged final sound). The listener hears garbage or an awkward trailing sound after the sentence ends.</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 5 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">5. Attention Entropy Analysis</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>The entropy of the attention distribution at decoder step $t$ is $H_t = -\sum_{l=1}^{L} \alpha_{t,l} \log_2 \alpha_{t,l}$. (a) What is the minimum possible entropy, and when does it occur? (b) What is the maximum possible entropy for $L = 50$? (c) If the attention is Gaussian-shaped centered at position $l_0$ with standard deviation $\sigma = 2$, estimate the entropy. (d) What entropy range would you expect for a well-trained Tacotron model?</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a) Minimum entropy:</strong></p>
                                <p>$H_{\min} = 0$ bits, occurring when the attention is perfectly peaked: $\alpha_{t,l_0} = 1$ for some $l_0$ and $\alpha_{t,l} = 0$ for all $l \neq l_0$. This is a delta function (hard attention) with zero uncertainty about which position is being attended.</p>
                                <p><strong>(b) Maximum entropy for $L = 50$:</strong></p>
                                <p>$H_{\max} = \log_2(L) = \log_2(50) \approx 5.64$ bits, occurring when the attention is uniform: $\alpha_{t,l} = 1/50$ for all $l$. This represents maximum uncertainty &mdash; the model has no idea where to attend.</p>
                                <p><strong>(c) Gaussian attention entropy:</strong></p>
                                <p>A continuous Gaussian with standard deviation $\sigma$ has entropy $H = \frac{1}{2} \log_2(2\pi e \sigma^2) = \frac{1}{2} \log_2(2\pi e \cdot 4) \approx \frac{1}{2} \log_2(34.16) \approx \frac{1}{2} \times 5.09 \approx 2.55$ bits. The discrete approximation on $L = 50$ positions will be close to this since $\sigma = 2$ covers about $4\sigma = 8$ positions with significant mass, which is well within the support. So the entropy is approximately 2.5 bits.</p>
                                <p><strong>(d) Expected range for well-trained Tacotron:</strong></p>
                                <p>For a well-trained model, the attention should be sharply peaked with $\sigma \approx 1\text{--}3$ positions. This gives entropy roughly $1.5\text{--}3.0$ bits. Values below 1 bit would indicate extremely sharp (almost hard) attention, which is possible but unusual for soft attention. Values above 4 bits (out of 5.64 max) would indicate problematic diffuse attention. A practical quality threshold is: mean entropy below $0.4 \times H_{\max} \approx 2.3$ bits indicates good alignment quality.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 6 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">6. Forward Attention Derivation</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Forward attention computes: $\alpha_{t,l} = ((1 - p_t) \cdot \alpha_{t-1,l} + p_t \cdot \alpha_{t-1,l-1}) \cdot \beta_{t,l}$ where $\beta_{t,l} = \text{softmax}(e_{t,\cdot})_l$ and $p_t$ is a transition probability. (a) Prove that $\sum_l \alpha_{t,l} = 1$ if $\sum_l \alpha_{t-1,l} = 1$ and $\sum_l \beta_{t,l} = 1$. (b) Why does this formulation guarantee monotonicity? (c) What happens when $p_t \approx 0$ for all $t$? When $p_t \approx 1$?</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a) Proof that $\sum_l \alpha_{t,l} = 1$:</strong></p>
                                <p>Note that the formula as written does <em>not</em> automatically sum to 1 &mdash; it requires renormalization. The raw (unnormalized) attention is $\tilde{\alpha}_{t,l} = ((1 - p_t) \cdot \alpha_{t-1,l} + p_t \cdot \alpha_{t-1,l-1}) \cdot \beta_{t,l}$. The term $q_{t,l} = (1 - p_t) \cdot \alpha_{t-1,l} + p_t \cdot \alpha_{t-1,l-1}$ is a valid probability distribution: $\sum_l q_{t,l} = (1 - p_t) \sum_l \alpha_{t-1,l} + p_t \sum_l \alpha_{t-1,l-1} = (1 - p_t) \cdot 1 + p_t \cdot 1 = 1$ (where we use $\alpha_{t-1,0} = 0$ for the boundary). But multiplying two distributions element-wise ($q_{t,l} \cdot \beta_{t,l}$) does not preserve normalization. So the actual implementation normalizes: $\alpha_{t,l} = \tilde{\alpha}_{t,l} / \sum_{l'} \tilde{\alpha}_{t,l'}$.</p>
                                <p><strong>(b) Monotonicity guarantee:</strong></p>
                                <p>The key insight is the recurrence $q_{t,l} = (1 - p_t) \cdot \alpha_{t-1,l} + p_t \cdot \alpha_{t-1,l-1}$. At each step, probability mass can only stay in place (the $(1 - p_t) \cdot \alpha_{t-1,l}$ term) or shift one position to the right (the $p_t \cdot \alpha_{t-1,l-1}$ term). There is no term involving $\alpha_{t-1,l+1}$ that would allow backward movement. This is exactly the forward algorithm of a left-to-right HMM with self-loops and forward transitions &mdash; it is structurally impossible for the attention peak to move backward.</p>
                                <p><strong>(c) Extreme values of $p_t$:</strong></p>
                                <ul>
                                    <li>$p_t \approx 0$: $q_{t,l} \approx \alpha_{t-1,l}$ &mdash; the attention stays in place. This corresponds to the model "holding" at the current phoneme, generating more frames for it. Useful for long phonemes.</li>
                                    <li>$p_t \approx 1$: $q_{t,l} \approx \alpha_{t-1,l-1}$ &mdash; the attention shifts right by one position. This corresponds to the model advancing to the next phoneme. If $p_t = 1$ for every step, the attention advances by one position per step, traversing the entire encoder sequence in $L$ steps.</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 7 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">7. Attention Alignment Metrics</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>You are building a production TTS system and need to automatically detect and re-synthesize utterances with poor attention alignment. (a) Define three quantitative metrics for attention quality. (b) For each metric, propose a threshold for flagging bad alignments. (c) How would you set these thresholds in practice? (d) What is the trade-off between a strict threshold (flagging many utterances) and a lenient one?</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a) Three metrics:</strong></p>
                                <ol>
                                    <li><strong>Mean attention entropy:</strong> $\bar{H} = \frac{1}{T} \sum_t H_t$ where $H_t = -\sum_l \alpha_{t,l} \log_2 \alpha_{t,l}$. Low entropy indicates sharp, confident alignment.</li>
                                    <li><strong>Diagonal deviation:</strong> $\bar{D} = \frac{1}{T} \sum_t |a_t - \frac{t \cdot L}{T}|$ where $a_t = \arg\max_l \alpha_{t,l}$. Measures how far the attention path deviates from a straight diagonal.</li>
                                    <li><strong>Maximum consecutive repetition:</strong> $R_{\max} = \max_l |\{t : a_t = l \text{ and } a_{t+1} = l\}|$. The longest run of consecutive steps attending to the same position. Excessive repetition indicates stuttering.</li>
                                </ol>
                                <p><strong>(b) Proposed thresholds:</strong></p>
                                <ul>
                                    <li>Mean entropy: flag if $\bar{H} > 0.35 \times \log_2(L)$</li>
                                    <li>Diagonal deviation: flag if $\bar{D} > 0.1 \times L$</li>
                                    <li>Max repetition: flag if $R_{\max} > 2 \times (T / L)$ (more than twice the expected average duration)</li>
                                </ul>
                                <p><strong>(c) Setting thresholds in practice:</strong></p>
                                <p>Synthesize a large set of utterances (e.g., 10,000) and have human evaluators rate a subset for alignment quality (binary: good/bad). For each metric, compute the ROC curve against the human labels and select the threshold that maximizes the F1 score or achieves a desired precision/recall trade-off. The thresholds should be validated on a held-out set and may need per-language or per-speaker calibration.</p>
                                <p><strong>(d) Threshold trade-off:</strong></p>
                                <p><strong>Strict thresholds</strong> (flag many utterances) maximize quality by catching most failures, but increase computation cost (more re-synthesis attempts) and latency. If the re-synthesis also fails, you may enter a costly loop. <strong>Lenient thresholds</strong> reduce re-synthesis cost but let some failures through to the user. The optimal point depends on the application: a news-reading system requires near-perfect quality (strict), while a casual assistant can tolerate occasional glitches (lenient).</p>
                            </div>
                        </div>
                    </div>

                    <h3 style="margin-top: 2rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">Hard</h3>

                    <!-- Exercise 8 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">8. Tacotron 2 vs Glow-TTS Comparison</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Compare Tacotron 2 and Glow-TTS across the following dimensions. For each, explain the fundamental architectural difference and its consequence: (a) Alignment mechanism, (b) Inference speed, (c) Robustness, (d) Output diversity, (e) Controllability. (f) In what scenario might Tacotron 2 still be preferred over Glow-TTS?</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a) Alignment:</strong></p>
                                <p>Tacotron 2 uses <strong>soft attention</strong> that learns alignment implicitly through the mel reconstruction loss. The attention is a learnable component that can fail. Glow-TTS uses <strong>Monotonic Alignment Search (MAS)</strong>, a DP algorithm that finds the optimal hard monotonic alignment during training. At inference, a duration predictor provides deterministic alignment. MAS guarantees monotonicity by construction; Tacotron's attention does not.</p>
                                <p><strong>(b) Inference speed:</strong></p>
                                <p>Tacotron 2 is <strong>autoregressive</strong>: $O(T)$ sequential steps, each requiring attention computation over all $L$ encoder positions. Glow-TTS is <strong>non-autoregressive</strong>: $O(1)$ sequential depth, generating all mel frames in parallel through an invertible flow. In practice, Glow-TTS is 15&ndash;50x faster than Tacotron 2 for mel generation.</p>
                                <p><strong>(c) Robustness:</strong></p>
                                <p>Tacotron 2 has a ~5&ndash;10% attention failure rate on unseen text (skipping, repeating, collapse). Glow-TTS has no attention and thus zero attention failures. Failures in Glow-TTS are limited to duration prediction errors (a word slightly too fast or slow), which are far less perceptible than missing or repeated words.</p>
                                <p><strong>(d) Output diversity:</strong></p>
                                <p>Tacotron 2 is inherently stochastic due to pre-net dropout at inference, producing slightly different outputs each run. This gives natural variation but also unpredictability. Glow-TTS samples from a Gaussian prior, and the temperature parameter controls diversity: temperature 0 gives deterministic output, temperature 0.667 (typical) gives natural variation, and higher temperatures give more diverse but potentially less natural output. Glow-TTS provides <em>controlled</em> diversity.</p>
                                <p><strong>(e) Controllability:</strong></p>
                                <p>Tacotron 2 offers essentially no explicit control over prosody. Speed, pitch, and emphasis emerge from the autoregressive process. Glow-TTS provides duration control (scale predicted durations) and some pitch control through the flow's latent space, though less than FastSpeech 2's explicit variance predictors.</p>
                                <p><strong>(f) When Tacotron 2 might be preferred:</strong></p>
                                <p>Tacotron 2 may be preferred for (1) expressive or emotional speech synthesis, where the autoregressive nature allows more nuanced prosody variation that deterministic duration models struggle to match, (2) very limited training data scenarios where the simpler training procedure (no MAS, no duration extraction) is advantageous, or (3) languages with complex prosody (e.g., tonal languages) where attention-based alignment may capture tonal patterns that a fixed duration model misses.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 9 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">9. Multi-Speaker Extension</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>You want to extend Tacotron 2 to support 100 speakers from a multi-speaker dataset. (a) Describe two approaches for conditioning the model on speaker identity. (b) Where in the architecture should the speaker embedding be injected? Justify your choice. (c) What happens to the attention mechanism in a multi-speaker model? Does it need modification? (d) How would you handle a new speaker with only 30 minutes of data, given a pre-trained 100-speaker model?</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a) Two approaches:</strong></p>
                                <ol>
                                    <li><strong>Lookup embedding:</strong> Assign a trainable embedding vector $e_s \in \mathbb{R}^d$ (e.g., $d = 256$) to each speaker $s$. The embedding is looked up by speaker ID during training. Simple and effective when speaker labels are available for all training data.</li>
                                    <li><strong>Speaker encoder:</strong> Train a separate neural network (e.g., a 3-layer LSTM on mel-spectrograms) that extracts a fixed-dimensional speaker embedding from a reference audio clip. This allows generalization to unseen speakers by providing a reference utterance at inference time. The speaker encoder can be pre-trained on a speaker verification task (e.g., GE2E loss) for better speaker discrimination.</li>
                                </ol>
                                <p><strong>(b) Where to inject:</strong></p>
                                <p>The speaker embedding should be injected at <strong>multiple points</strong>: (1) concatenated with the encoder output before attention (so the encoder representations are speaker-aware), (2) concatenated with the decoder input at each step (so the decoder can adjust its generation to the speaker), and (3) optionally as a conditioning input to the post-net. The most critical injection point is the decoder, because speaker-specific characteristics (pitch range, speaking rate, vocal quality) primarily manifest in the acoustic features generated by the decoder. Injecting only at the encoder is insufficient because the encoder processes text, which is speaker-independent.</p>
                                <p><strong>(c) Attention in multi-speaker models:</strong></p>
                                <p>The attention mechanism generally does <strong>not</strong> need modification, but it becomes harder to train. Different speakers have different speaking rates, so the attention must learn speaker-dependent alignment speeds. This can lead to slower convergence and more attention failures for speakers with unusual rates. A mitigation is to inject the speaker embedding into the attention energy computation: $e_{t,l} = v^\top \tanh(W_s s_t + W_h h_l + W_f f_{t,l} + W_e e_s + b)$, giving the attention explicit access to speaker identity when computing alignment.</p>
                                <p><strong>(d) Fine-tuning for a new speaker:</strong></p>
                                <p>With 30 minutes of data from a new speaker: (1) Initialize from the pre-trained 100-speaker model. (2) Add a new speaker embedding for the new speaker (randomly initialized or initialized as the average of existing embeddings). (3) Fine-tune the entire model (or just the decoder + speaker embedding) on the new speaker's data for 5k&ndash;20k steps with a reduced learning rate (e.g., $1 \times 10^{-4}$). (4) Monitor attention alignment quality on a validation set &mdash; attention failures may increase during fine-tuning if the learning rate is too high. The pre-trained model provides strong initialization for the encoder, attention, and decoder, allowing rapid adaptation to the new speaker's characteristics.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 10 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">10. Attention Failure Diagnosis</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>You deploy a Tacotron 2 system and observe the following attention matrices for three failed utterances. Diagnose each failure and propose a fix. (a) The attention matrix shows a sharp diagonal path for the first 60% of the utterance, then the attention peak stays at encoder position 35 (out of 50) for the remaining 40% of decoder steps. (b) The attention path is mostly diagonal but has three "gaps" &mdash; positions 12, 28, and 41 receive nearly zero total attention. (c) The attention is sharp and diagonal for 120 decoder steps, then suddenly jumps back to position 0 and traces the diagonal again (total: 240 steps for what should be 120).</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a) Diagnosis: Attention stalling / stop token failure</strong></p>
                                <p>The attention gets stuck at position 35 and cannot advance. This is likely because: (1) the content around position 35 creates an "attractor" in the energy function that the location features cannot overcome, or (2) the stop token should have fired near this point but the decoder state did not trigger it. The remaining 40% of decoder steps produce audio conditioned on the same encoder position, creating a prolonged repetition of the same sound. <strong>Fix:</strong> (1) Increase the weight of location features ($W_f$) to provide stronger forward pressure, (2) add a cumulative attention penalty that reduces energy at positions that have already received substantial attention, (3) use forward attention (which structurally prevents stalling by requiring the attention to advance).</p>
                                <p><strong>(b) Diagnosis: Skipping (missing words)</strong></p>
                                <p>Positions 12, 28, and 41 are skipped &mdash; the attention jumps over them. These positions likely correspond to short function words or unstressed syllables with weak encoder representations. The attention energy at adjacent positions is high enough to "pull" the attention past these weak positions. <strong>Fix:</strong> (1) Use phoneme input instead of characters (function words like "a" and "the" have very short character representations but longer phoneme sequences), (2) add a coverage loss $\mathcal{L}_{cov} = \sum_l \min(\alpha_{\cdot,l}, \text{threshold})$ that penalizes uncovered positions, (3) use diagonal-guided attention to enforce roughly uniform progression.</p>
                                <p><strong>(c) Diagnosis: Attention wrapping / text repetition</strong></p>
                                <p>The attention completes the entire text sequence, then wraps around to the beginning and repeats. This means the stop token failed to fire at step ~120. The decoder generates the entire utterance twice. This is a classic stop token failure, likely caused by: (1) the extreme class imbalance in stop token training (exercise 4), or (2) the decoder state at step 120 not being sufficiently different from mid-utterance states to trigger stopping. <strong>Fix:</strong> (1) Use position-aware stop token: concatenate the cumulative attention (which shows coverage of the full encoder sequence) with the decoder state for stop prediction, (2) add a hard maximum decoder length of $2 \times T_{\text{expected}}$ based on input length, (3) use the attention coverage signal &mdash; if $\sum_t \alpha_{t,L}$ exceeds a threshold (meaning the last encoder position has been substantially attended), force stop.</p>
                            </div>
                        </div>
                    </div>

                </div>
            </article>

        </main>

        <!-- TOC (Right Side) -->
        <aside class="toc-container">
            <h4 class="toc-title">Contents</h4>
            <nav class="toc-list">
                <a href="#seq2seq-tts" class="toc-link">Sequence-to-Sequence TTS</a>
                <a href="#tacotron-architecture" class="toc-link">Tacotron 1 Architecture</a>
                <a href="#attention-mechanism" class="toc-link">Location-Sensitive Attention</a>
                <a href="#teacher-forcing" class="toc-link">Teacher Forcing &amp; Exposure Bias</a>
                <a href="#tacotron2" class="toc-link">Tacotron 2</a>
                <a href="#attention-failures" class="toc-link">The Attention Alignment Problem</a>
                <a href="#attention-improvements" class="toc-link">Attention Improvements</a>
                <a href="#tacotron-legacy" class="toc-link">Tacotron's Legacy</a>
            </nav>
        </aside>
    </div>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-brand">
                    <span class="logo-symbol">&nabla;</span>
                    <span>ML Fundamentals</span>
                </div>
                <p class="footer-tagline">Deep understanding through first principles.</p>
            </div>
            <div class="footer-links">
                <a href="../../../index.html">Home</a>
                <a href="https://github.com/ml-entropy/ml-entropy.github.io" target="_blank">GitHub</a>
            </div>
        </div>
    </footer>

    <script src="../../../js/main.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            if (typeof renderMathInElement === 'function') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }

            const tabs = document.querySelectorAll('.tutorial-tab');
            const articles = document.querySelectorAll('.article-content');

            function switchTab(targetId) {
                if (!targetId || targetId === '#') targetId = '#theory';
                tabs.forEach(tab => {
                    if (tab.getAttribute('href') === targetId) {
                        tab.classList.add('active');
                    } else {
                        tab.classList.remove('active');
                    }
                });
                articles.forEach(article => {
                    const articleId = '#' + article.id;
                    if (articleId === targetId) {
                        article.style.display = 'block';
                    } else {
                        article.style.display = 'none';
                    }
                });
                if (typeof renderMathInElement === 'function') {
                    renderMathInElement(document.body, {
                        delimiters: [
                            {left: '$$', right: '$$', display: true},
                            {left: '$', right: '$', display: false},
                            {left: '\\[', right: '\\]', display: true},
                            {left: '\\(', right: '\\)', display: false}
                        ],
                        throwOnError: false
                    });
                }
                const toc = document.querySelector('.toc-container');
                if (toc) {
                    if (targetId === '#theory') {
                        toc.style.display = 'block';
                        setTimeout(() => toc.classList.add('visible'), 100);
                    } else {
                        toc.classList.remove('visible');
                        setTimeout(() => toc.style.display = 'none', 300);
                    }
                }
            }

            tabs.forEach(tab => {
                tab.addEventListener('click', (e) => {
                    e.preventDefault();
                    const targetId = tab.getAttribute('href');
                    history.pushState(null, null, targetId);
                    switchTab(targetId);
                });
            });

            window.addEventListener('popstate', () => {
                switchTab(window.location.hash);
            });

            switchTab(window.location.hash);
        });
    </script>
</body>
</html>