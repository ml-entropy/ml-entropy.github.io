<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Probability Distributions | ML Fundamentals</title>
    <meta name="description" content="Master the essential probability distributions for machine learning: Gaussian, Bernoulli, Categorical, and more.">
    
    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&family=Playfair+Display:wght@400;500;600;700&display=swap" rel="stylesheet">
    
    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}, {left: '\\\\[', right: '\\\\]', display: true}, {left: '\\\\(', right: '\\\\)', display: false}], throwOnError: false});"></script>
    
    <!-- Styles -->
    <link rel="stylesheet" href="../../../css/main.css">
    <link rel="stylesheet" href="../../../css/components.css">
    
    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>∞</text></svg>">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar" id="navbar">
        <div class="nav-container">
            <a href="../../../index.html" class="nav-logo">
                <span class="logo-symbol">∇</span>
                <span class="logo-text">ML Fundamentals</span>
            </a>
            
            <button class="nav-toggle" id="navToggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            
            <div class="nav-menu" id="navMenu">
                <div class="nav-links">
                    <div class="nav-dropdown">
                        <button class="nav-link dropdown-trigger">
                            Tutorials <span class="dropdown-arrow">▾</span>
                        </button>
                        <div class="dropdown-content">
                            <a href="../../ml/index.html" class="active">Machine Learning</a>
                            <a href="../../linear-algebra/index.html">Linear Algebra</a>
                            <a href="../../calculus/index.html">Calculus</a>
                            <a href="../../physics/index.html">Physics</a>
                        </div>
                    </div>
                    <a href="../../../index.html#philosophy" class="nav-link">Philosophy</a>
                    <a href="../../../index.html#roadmap" class="nav-link">Roadmap</a>
                    <a href="https://github.com/ml-entropy/ml-entropy.github.io" class="nav-link" target="_blank">GitHub</a>
                </div>
                
                <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
                    <svg class="sun-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="5"/>
                        <path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"/>
                    </svg>
                    <svg class="moon-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                    </svg>
                </button>
            </div>
        </div>
    </nav>

    <!-- Tutorial Header -->
    <header class="tutorial-content-header">
        <div class="container">
            <nav class="breadcrumb">
                <a href="../../../index.html">Home</a>
                <span class="breadcrumb-separator">→</span>
                <a href="../index.html">Machine Learning</a>
                <span class="breadcrumb-separator">→</span>
                <span>Probability Distributions</span>
            </nav>
            
            <h1>03. Probability Distributions</h1>
            <p class="lead">
                Every ML model makes assumptions about data distributions. Understanding the 
                Gaussian, Bernoulli, and exponential family unlocks the "why" behind loss functions 
                and model architectures.
            </p>
            
            <div class="tutorial-tabs">
                <a href="#theory" class="tutorial-tab active">Theory</a>
                <a href="#code" class="tutorial-tab">Code</a>
                <a href="#exercises" class="tutorial-tab">Exercises</a>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <main class="tutorial-article">
        <div class="container">
            <article class="article-content" id="theory">
                
                <!-- Section 1 -->
                <h2 id="gaussian">The Gaussian Distribution</h2>
                
                <p>
                    The <strong>Gaussian</strong> (or Normal) distribution is the most important 
                    distribution in ML. It appears everywhere due to the Central Limit Theorem.
                </p>
                
                <div class="definition-box">
                    <div class="box-title">Gaussian PDF</div>
                    <p style="margin-bottom: 0;">
                        $$\mathcal{N}(x | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$$
                        Parameters: mean $\mu$, variance $\sigma^2$
                    </p>
                </div>
                
                <h3>Why Gaussian?</h3>
                
                <ul>
                    <li><strong>Maximum Entropy:</strong> Among all distributions with fixed mean and variance, 
                        Gaussian has maximum entropy—it's the "least informative" assumption.</li>
                    <li><strong>Central Limit Theorem:</strong> Sum of many independent random variables → Gaussian.</li>
                    <li><strong>Conjugate Prior:</strong> Gaussian prior + Gaussian likelihood = Gaussian posterior.</li>
                </ul>
                
                <div class="math-derivation">
                    <div class="math-derivation-title">Gaussian → MSE Loss</div>
                    
                    <div class="math-step">
                        <div class="math-step-number">1</div>
                        <div class="math-step-content">
                            Assume $y = f(x) + \epsilon$ where $\epsilon \sim \mathcal{N}(0, \sigma^2)$
                        </div>
                    </div>
                    
                    <div class="math-step">
                        <div class="math-step-number">2</div>
                        <div class="math-step-content">
                            Log-likelihood: $\log p(y|x) = -\frac{(y - f(x))^2}{2\sigma^2} + \text{const}$
                        </div>
                    </div>
                    
                    <div class="math-step">
                        <div class="math-step-number">3</div>
                        <div class="math-step-content">
                            Maximizing log-likelihood = Minimizing $(y - f(x))^2$ = <strong>MSE loss!</strong>
                        </div>
                    </div>
                </div>
                
                <!-- Section 2 -->
                <h2 id="bernoulli">Bernoulli and Categorical</h2>
                
                <h3>Bernoulli Distribution</h3>
                
                <p>For binary outcomes (coin flips, binary classification):</p>
                
                <div class="math-block">
                    $$P(x | \mu) = \mu^x (1-\mu)^{1-x}, \quad x \in \{0, 1\}$$
                </div>
                
                <p>Mean: $\mathbb{E}[X] = \mu$, Variance: $\text{Var}(X) = \mu(1-\mu)$</p>
                
                <h3>Categorical Distribution</h3>
                
                <p>For $K$ mutually exclusive classes:</p>
                
                <div class="math-block">
                    $$P(\mathbf{x} | \boldsymbol{\mu}) = \prod_{k=1}^{K} \mu_k^{x_k}, \quad \text{where } \sum_k \mu_k = 1$$
                </div>
                
                <div class="math-derivation">
                    <div class="math-derivation-title">Bernoulli → Cross-Entropy Loss</div>
                    
                    <div class="math-step">
                        <div class="math-step-number">1</div>
                        <div class="math-step-content">
                            Model outputs $\hat{y} = \sigma(f(x))$ for probability of class 1
                        </div>
                    </div>
                    
                    <div class="math-step">
                        <div class="math-step-number">2</div>
                        <div class="math-step-content">
                            Log-likelihood: $\log p(y|x) = y\log\hat{y} + (1-y)\log(1-\hat{y})$
                        </div>
                    </div>
                    
                    <div class="math-step">
                        <div class="math-step-number">3</div>
                        <div class="math-step-content">
                            Negating gives <strong>binary cross-entropy loss!</strong>
                        </div>
                    </div>
                </div>
                
                <!-- Section 3 -->
                <h2 id="exponential-family">The Exponential Family</h2>
                
                <p>
                    Most useful distributions belong to the <strong>exponential family</strong>:
                </p>
                
                <div class="definition-box">
                    <div class="box-title">Exponential Family Form</div>
                    <p style="margin-bottom: 0;">
                        $$p(x | \boldsymbol{\eta}) = h(x) \exp\left(\boldsymbol{\eta}^T \mathbf{T}(x) - A(\boldsymbol{\eta})\right)$$
                        Where: $\boldsymbol{\eta}$ = natural parameters, $\mathbf{T}(x)$ = sufficient statistics, 
                        $A(\boldsymbol{\eta})$ = log-partition function
                    </p>
                </div>
                
                <p>Members include: Gaussian, Bernoulli, Categorical, Poisson, Gamma, Beta, Dirichlet.</p>
                
                <div class="note-box">
                    <div class="box-title">Why This Matters</div>
                    <p style="margin-bottom: 0;">
                        The exponential family has beautiful properties: conjugate priors exist, 
                        sufficient statistics compress data, and $\nabla_\eta A(\eta)$ gives the expected 
                        sufficient statistics. GLMs (Generalized Linear Models) are built on this foundation.
                    </p>
                </div>
                
                <!-- Section 4 -->
                <h2 id="multivariate">Multivariate Gaussian</h2>
                
                <p>For vectors $\mathbf{x} \in \mathbb{R}^D$:</p>
                
                <div class="math-block">
                    $$\mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \frac{1}{(2\pi)^{D/2}|\boldsymbol{\Sigma}|^{1/2}} \exp\left(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right)$$
                </div>
                
                <p>
                    The term $(\mathbf{x}-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})$ 
                    is the <strong>Mahalanobis distance</strong>—it accounts for correlations between dimensions.
                </p>
                
                <h3>Key Properties</h3>
                
                <ul>
                    <li><strong>Marginals are Gaussian:</strong> Any subset of variables is also Gaussian</li>
                    <li><strong>Conditionals are Gaussian:</strong> $p(x_1 | x_2)$ is Gaussian</li>
                    <li><strong>Linear transformations:</strong> $\mathbf{Ax} + \mathbf{b}$ is Gaussian if $\mathbf{x}$ is</li>
                </ul>
                
                <!-- Section 5 -->
                <h2 id="mixture-models">Mixture Models</h2>
                
                <p>
                    Complex distributions can be modeled as mixtures of simpler ones:
                </p>
                
                <div class="math-block">
                    $$p(x) = \sum_{k=1}^{K} \pi_k \, p_k(x)$$
                </div>
                
                <p>
                    <strong>Gaussian Mixture Models (GMMs)</strong> use Gaussian components and are 
                    trained with the EM algorithm. They're universal approximators—any continuous 
                    density can be approximated by a mixture of Gaussians.
                </p>
                
                <div class="warning-box">
                    <div class="box-title">Connection to Deep Learning</div>
                    <p style="margin-bottom: 0;">
                        VAEs and normalizing flows are modern ways to represent complex distributions. 
                        They're spiritual successors to mixture models, using neural networks to 
                        parameterize the components or transformations.
                    </p>
                </div>
                
                <!-- Navigation -->
                <div class="tutorial-nav">
                    <a href="../02-kl-divergence/index.html" class="tutorial-nav-link prev">
                        <span class="nav-label">Previous</span>
                        <span class="nav-title">← KL Divergence</span>
                    </a>
                    <a href="../04-logarithms/index.html" class="tutorial-nav-link next">
                        <span class="nav-label">Next</span>
                        <span class="nav-title">Logarithms in ML →</span>
                    </a>
                </div>
                
            </article>
        </div>
    </main>

    <!-- Table of Contents (floating) -->
    <aside class="toc-container">
        <h4 class="toc-title">Contents</h4>
        <nav class="toc-list">
            <a href="#gaussian" class="toc-link">Gaussian Distribution</a>
            <a href="#bernoulli" class="toc-link">Bernoulli & Categorical</a>
            <a href="#exponential-family" class="toc-link">Exponential Family</a>
            <a href="#multivariate" class="toc-link">Multivariate Gaussian</a>
            <a href="#mixture-models" class="toc-link">Mixture Models</a>
        </nav>
    </aside>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-brand">
                    <span class="logo-symbol">∇</span>
                    <span>ML Fundamentals</span>
                </div>
                <p class="footer-tagline">Deep understanding through first principles.</p>
            </div>
            <div class="footer-links">
                <a href="../../../index.html">Home</a>
                <a href="https://github.com/ml-entropy/ml-entropy.github.io" target="_blank">GitHub</a>
            </div>
        </div>
    </footer>

    <!-- Scripts -->
    <script src="../../../js/main.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            if (typeof renderMathInElement === 'function') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }
        });
    </script>
</body>
</html>
