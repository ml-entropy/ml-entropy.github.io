<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Neural Vocoders: WaveNet to HiFi-GAN | ML Fundamentals</title>
    <meta name="description" content="Neural vocoders from WaveNet to HiFi-GAN: autoregressive waveform generation, dilated causal convolutions, GAN-based vocoders, and real-time synthesis.">

    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&family=Playfair+Display:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}, {left: '\\\\[', right: '\\\\]', display: true}, {left: '\\\\(', right: '\\\\)', display: false}], throwOnError: false});"></script>

    <!-- Styles -->
    <link rel="stylesheet" href="../../../css/main.css">
    <link rel="stylesheet" href="../../../css/components.css">
    <link rel="stylesheet" href="../../../css/sidebar.css">

    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>&#x221E;</text></svg>">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar" id="navbar">
        <div class="nav-container">
            <a href="../../../index.html" class="nav-logo">
                <span class="logo-symbol">&nabla;</span>
                <span class="logo-text">ML Fundamentals</span>
            </a>

            <button class="nav-toggle" id="navToggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>

            <div class="nav-menu" id="navMenu">
                <div class="nav-links">
                    <a href="../../../tutorials/ml/index.html" class="nav-link active">Machine Learning</a>
                    <a href="../../../tutorials/linear-algebra/index.html" class="nav-link">Linear Algebra</a>
                    <a href="../../../tutorials/calculus/index.html" class="nav-link">Calculus</a>
                    <a href="../../../tutorials/physics/index.html" class="nav-link">Physics</a>
                    <a href="../../../index.html#philosophy" class="nav-link">Philosophy</a>
                    <a href="../../../index.html#roadmap" class="nav-link">Roadmap</a>
                    <a href="https://github.com/ml-entropy/ml-entropy.github.io" class="nav-link" target="_blank">GitHub</a>
                </div>

                <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
                    <svg class="sun-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="5"/>
                        <path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"/>
                    </svg>
                    <svg class="moon-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                    </svg>
                </button>
            </div>
        </div>
    </nav>

    <!-- Tutorial Header -->
    <header class="tutorial-content-header">
        <div class="container">
            <nav class="breadcrumb">
                <a href="../../../index.html">Home</a>
                <span class="breadcrumb-separator">&rarr;</span>
                <a href="../index.html">Machine Learning</a>
                <span class="breadcrumb-separator">&rarr;</span>
                <span>Neural Vocoders</span>
            </nav>

            <div class="tutorial-tabs">
                <a href="#theory" class="tutorial-tab active">Theory</a>
                <a href="#code" class="tutorial-tab">Code</a>
                <a href="#exercises" class="tutorial-tab">Exercises</a>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <div class="tutorial-wrapper">

        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <aside class="tutorial-sidebar">
            <div class="sidebar-section">
                <h3 class="sidebar-section-title">Machine Learning</h3>
                <nav class="sidebar-nav">
                        <a href="../00-probability/index.html" class="sidebar-link">00. Probability Foundations</a>
                    <a href="../04-logarithms/index.html" class="sidebar-link">01. Why Logarithms?</a>
                    <a href="../05-combinatorics/index.html" class="sidebar-link">02. Combinatorics</a>
                    <a href="../03-distributions/index.html" class="sidebar-link">03. Normal Distributions</a>
                    <a href="../01-entropy/index.html" class="sidebar-link">04. Entropy Fundamentals</a>
                    <a href="../02-cross-entropy/index.html" class="sidebar-link">05. Cross-Entropy</a>
                    <a href="../02-kl-divergence/index.html" class="sidebar-link">06. KL Divergence</a>
                    <a href="../14-entropy-connections/index.html" class="sidebar-link">07. Entropy Connections</a>
                    <a href="../06-backpropagation/index.html" class="sidebar-link">08. Backpropagation</a>
                    <a href="../07-regularization/index.html" class="sidebar-link">09. Regularization</a>
                    <a href="../08-batch-normalization/index.html" class="sidebar-link">10. Batch Normalization</a>
                    <a href="../09-learning-rate/index.html" class="sidebar-link">11. Learning Rate</a>
                    <a href="../10-cnn/index.html" class="sidebar-link">12. CNNs</a>
                    <a href="../11-rnn/index.html" class="sidebar-link">13. RNNs</a>
                    <a href="../34-rate-distortion/index.html" class="sidebar-link">14. Rate-Distortion Theory</a>
                    <a href="../15-autoencoder/index.html" class="sidebar-link">15. Autoencoders</a>
                    <a href="../13-variational-inference/index.html" class="sidebar-link">16. Variational Inference</a>
                    <a href="../12-vae/index.html" class="sidebar-link">17. VAE</a>
                    <a href="../16-inductive-bias/index.html" class="sidebar-link">18. Inductive Bias</a>
                    <a href="../17-architectural-biases/index.html" class="sidebar-link">19. Architectural Biases</a>
                    <a href="../18-designing-biases/index.html" class="sidebar-link">20. Designing Biases</a>
                    <a href="../19-fst-fundamentals/index.html" class="sidebar-link">21. FST Fundamentals</a>
                    <a href="../20-weighted-fsts/index.html" class="sidebar-link">22. Weighted FSTs</a>
                    <a href="../21-fst-libraries/index.html" class="sidebar-link">23. FST Libraries</a>
                    <a href="../22-fst-applications/index.html" class="sidebar-link">24. FST Applications</a>
                    <a href="../23-neural-symbolic/index.html" class="sidebar-link">25. Neural-Symbolic Hybrids</a>
                    <a href="../24-sequence-alignment/index.html" class="sidebar-link">26. Sequence Alignment</a>
                    <a href="../25-mas-algorithm/index.html" class="sidebar-link">27. MAS Algorithm</a>
                    <a href="../26-forced-alignment/index.html" class="sidebar-link">28. Forced Alignment & MFA</a>
                    <a href="../27-tts-fundamentals/index.html" class="sidebar-link">29. TTS Fundamentals</a>
                    <a href="../28-neural-vocoders/index.html" class="sidebar-link active">30. Neural Vocoders</a>
                    <a href="../29-tacotron/index.html" class="sidebar-link">31. Tacotron & Attention TTS</a>
                    <a href="../30-fastspeech/index.html" class="sidebar-link">32. FastSpeech & Non-AR TTS</a>
                    <a href="../31-glow-tts/index.html" class="sidebar-link">33. Glow-TTS & Flows</a>
                    <a href="../32-vits/index.html" class="sidebar-link">34. VITS: End-to-End TTS</a>
                    <a href="../33-bilingual-tts/index.html" class="sidebar-link">35. Bilingual TTS: RU+KY</a>
                </nav>
            </div>
            
            <div class="sidebar-section" style="margin-top: 2rem;">
                <h3 class="sidebar-section-title">Related Subjects</h3>
                <nav class="sidebar-nav">
                        <a href="../../linear-algebra/index.html" class="sidebar-link">Linear Algebra</a>
                    <a href="../../calculus/index.html" class="sidebar-link">Calculus</a>
                    <a href="../../physics/index.html" class="sidebar-link">Physics</a>
                </nav>
            </div>
        </aside>

        <!-- Main Article -->
        <main class="tutorial-main">

            <!-- ==================== THEORY TAB ==================== -->
            <article class="article-content" id="theory">

                <!-- Section 1: The Vocoder Problem -->
                <h2 id="vocoder-problem">The Vocoder Problem</h2>

                <p>
                    In a modern text-to-speech pipeline, an acoustic model (such as Tacotron or FastSpeech) predicts a <strong>mel spectrogram</strong> &mdash; a compact, perceptually motivated representation of sound that captures the spectral envelope of speech over time. But a mel spectrogram is not sound. To hear anything, we must convert this compact representation back into a raw audio waveform: a sequence of amplitude values sampled at 16,000, 22,050, or 24,000 times per second. The component that performs this conversion is called a <strong>vocoder</strong>.
                </p>

                <p>
                    The core challenge is one of massive upsampling. A mel spectrogram is computed using a Short-Time Fourier Transform (STFT) with a hop length &mdash; the number of audio samples between consecutive frames. If the hop length is 256 (a common choice at 22,050 Hz), then each mel frame corresponds to 256 audio samples. The vocoder must generate all 256 samples for every single mel frame, expanding a compact feature vector into a high-resolution temporal signal. For one second of audio at 22,050 Hz with hop length 256, the mel spectrogram has roughly 86 frames, but the waveform has 22,050 samples &mdash; a roughly <strong>256&times; upsampling</strong> factor.
                </p>

                <p>
                    Why not simply use classical signal processing? The mel spectrogram discards <strong>phase information</strong>. The STFT of a signal has both magnitude and phase at each frequency bin. The mel spectrogram keeps only a compressed version of the magnitude. Without phase, you cannot simply invert the transform. Classical algorithms like Griffin-Lim attempt to estimate the phase iteratively, but the results sound buzzy, metallic, and unnatural. The vocoder must <em>generate</em> plausible phase information from scratch &mdash; it is fundamentally a generative modeling problem.
                </p>

                <p>
                    Formally, the vocoder learns a conditional generative model:
                </p>

                $$p(\mathbf{x} \mid \mathbf{s})$$

                <p>
                    where $\mathbf{x} = (x_1, x_2, \ldots, x_T) \in \mathbb{R}^T$ is the raw waveform (with $T$ potentially in the tens of thousands) and $\mathbf{s} \in \mathbb{R}^{F \times M}$ is the mel spectrogram with $F$ frequency bins and $M$ frames. The model must produce waveforms that (1) are perceptually faithful to the mel spectrogram conditioning, (2) contain natural-sounding phase relationships, and (3) exhibit the fine temporal structure of real speech, including pitch harmonics, noise components, and transient consonantal bursts.
                </p>

                <div class="definition-box">
                    <div class="box-title">Definition: Neural Vocoder</div>
                    <p>
                        A <strong>neural vocoder</strong> is a neural network that models the conditional distribution $p(\mathbf{x} \mid \mathbf{s})$, where $\mathbf{x} = (x_1, \ldots, x_T)$ is a raw audio waveform and $\mathbf{s}$ is a conditioning signal (typically a mel spectrogram). The vocoder performs extreme upsampling &mdash; expanding each mel frame into $h$ audio samples (where $h$ is the hop length, typically 256) &mdash; while simultaneously generating plausible phase information that was discarded during mel spectrogram computation. The key quality metric is the <strong>Mean Opinion Score (MOS)</strong>, a perceptual rating from 1 (bad) to 5 (excellent) obtained from human listeners.
                    </p>
                </div>

                <p>
                    The history of neural vocoders is a story of the tension between quality and speed. WaveNet (2016) demonstrated that neural networks could produce stunningly natural audio, but it was far too slow for practical use. The subsequent five years saw a series of architectural innovations &mdash; from autoregressive RNNs to normalizing flows to GANs &mdash; that progressively closed the gap, culminating in HiFi-GAN (2020), which achieves near-WaveNet quality at hundreds of times real-time speed.
                </p>

                <!-- Section 2: WaveNet -->
                <h2 id="wavenet">WaveNet</h2>

                <p>
                    WaveNet (van den Oord et al., 2016) was the breakthrough that proved neural networks could generate raw audio waveforms of unprecedented quality. Its core idea is simple but powerful: model the waveform autoregressively, predicting each sample conditioned on all previous samples and the mel spectrogram.
                </p>

                <p>
                    The joint probability of the waveform factorizes as:
                </p>

                $$p(\mathbf{x} \mid \mathbf{s}) = \prod_{t=1}^{T} p(x_t \mid x_1, x_2, \ldots, x_{t-1}, \mathbf{s})$$

                <p>
                    At each time step $t$, WaveNet predicts a distribution over the next sample $x_t$ given the entire history of previous samples. This is a valid factorization by the chain rule of probability &mdash; no approximation is involved. The challenge is computational: for one second of 22 kHz audio, the model must run the forward pass 22,050 times sequentially, because each prediction depends on the previous one.
                </p>

                <h3>Dilated Causal Convolutions</h3>

                <p>
                    To condition on a long history efficiently, WaveNet uses <strong>dilated causal convolutions</strong>. A standard causal convolution with kernel size $K$ looks back $K-1$ samples. To see further into the past, you could stack more layers, but the receptive field grows only linearly with depth. Dilated convolutions solve this by spacing out the filter taps, allowing the receptive field to grow <em>exponentially</em> with depth.
                </p>

                <p>
                    A dilated convolution with dilation factor $d$ and kernel size $K$ applies the filter at positions spaced $d$ apart. For a 1D input $\mathbf{x}$ and filter $\mathbf{w}$, the dilated convolution is:
                </p>

                $$(x *_d w)[t] = \sum_{k=0}^{K-1} w[k] \cdot x[t - d \cdot k]$$

                <p>
                    WaveNet stacks layers with exponentially increasing dilation factors: $d = 1, 2, 4, 8, 16, 32, 64, 128, 256, 512$. This pattern is called a <strong>dilation cycle</strong>. With kernel size $K = 2$ and 10 layers in one cycle, the receptive field covers $2^{10} = 1024$ samples. Multiple cycles can be stacked for even larger receptive fields.
                </p>

                <p>
                    More precisely, for a stack of $L$ layers with dilation factors $d_l = 2^{l-1}$ (for $l = 1, \ldots, L$) and kernel size $K$, the receptive field is:
                </p>

                $$R = 1 + (K - 1) \sum_{l=1}^{L} d_l = 1 + (K - 1) \sum_{l=1}^{L} 2^{l-1} = 1 + (K - 1)(2^L - 1)$$

                <p>
                    For $K = 2$ and $L = 10$: $R = 1 + 1 \cdot (1024 - 1) = 1024$. For $K = 3$ and $L = 10$: $R = 1 + 2 \cdot 1023 = 2047$. If we stack $B$ dilation cycles, the total receptive field becomes:
                </p>

                $$R_{\text{total}} = 1 + B \cdot (K - 1)(2^L - 1)$$

                <p>
                    With $B = 3$ cycles, $K = 2$, $L = 10$: $R_{\text{total}} = 1 + 3 \cdot 1023 = 3070$ samples, or about 139 ms at 22 kHz &mdash; long enough to capture several pitch periods of human speech.
                </p>

                <h3>Gated Activation Units</h3>

                <p>
                    Each layer in WaveNet uses a <strong>gated activation unit</strong> inspired by gated recurrent networks. Instead of a simple nonlinearity like ReLU, the layer computes:
                </p>

                $$\mathbf{z} = \tanh(W_f * \mathbf{x}) \odot \sigma(W_g * \mathbf{x})$$

                <p>
                    where $W_f$ is the "filter" convolution weight, $W_g$ is the "gate" convolution weight, $*$ denotes (dilated) convolution, $\odot$ is element-wise multiplication, $\tanh$ is the hyperbolic tangent, and $\sigma$ is the sigmoid function. The $\tanh$ branch produces candidate activations in $[-1, 1]$, while the sigmoid "gate" controls how much of each activation to pass through. This gating mechanism allows the network to model both slowly varying and rapidly changing components of the waveform.
                </p>

                <h3>Residual and Skip Connections</h3>

                <p>
                    WaveNet uses both residual and skip connections. After the gated activation, the output is split into two paths: a <strong>residual connection</strong> that adds back to the input (enabling gradient flow through deep stacks), and a <strong>skip connection</strong> that feeds directly to the output layers. The skip connections from all layers are summed before passing through two $1 \times 1$ convolution layers and a softmax to produce the final output distribution.
                </p>

                <h3>&mu;-Law Quantization</h3>

                <p>
                    Raw audio samples are continuous values, but WaveNet models them as a <strong>categorical distribution</strong> over 256 discrete levels. The continuous waveform (typically in $[-1, 1]$) is first compressed using &mu;-law companding:
                </p>

                $$f(x) = \text{sign}(x) \frac{\ln(1 + \mu |x|)}{\ln(1 + \mu)}$$

                <p>
                    where $\mu = 255$. This nonlinear compression gives more resolution to quiet sounds (where human hearing is most sensitive) and less to loud sounds. The compressed values are then quantized to 256 levels, and the model outputs a softmax over these 256 classes. At synthesis time, the predicted class is converted back to a continuous value via the inverse &mu;-law transform.
                </p>

                <h3>Quality vs. Speed</h3>

                <p>
                    WaveNet achieved a MOS of approximately 4.2 &mdash; a dramatic improvement over previous parametric synthesis, and close to natural speech (~4.5 MOS). However, because each sample depends on the previous one, generation is inherently sequential. On a modern GPU, WaveNet generates audio at roughly <strong>0.02&times; real-time</strong>: it takes about 50 seconds to synthesize 1 second of audio. This made WaveNet impractical for production TTS systems, motivating the search for faster alternatives.
                </p>


                <!-- Section 3: The Speed Problem -->
                <h2 id="speed-problem">The Speed Problem</h2>

                <p>
                    WaveNet's quality was groundbreaking, but its speed was a showstopper for production systems. The fundamental issue is the autoregressive bottleneck: each of the ~22,000 samples per second must be generated one at a time, with a full forward pass through a deep network for each sample. Multiple research directions emerged to break this bottleneck while preserving quality.
                </p>

                <h3>WaveRNN (Kalchbrenner et al., 2018)</h3>

                <p>
                    WaveRNN replaces WaveNet's deep convolutional stack with a single-layer GRU (Gated Recurrent Unit). The key insight is that a recurrent network is already inherently autoregressive &mdash; each hidden state summarizes all past information &mdash; so the deep dilated stack is unnecessary. WaveRNN uses a <strong>dual softmax</strong> trick: instead of predicting all 16 bits of a sample at once (65,536 classes), it splits the prediction into coarse (8-bit) and fine (8-bit) components, each with only 256 classes. This reduces computation substantially.
                </p>

                <p>
                    With careful engineering (sparse matrix operations, efficient GRU kernels), WaveRNN achieves roughly <strong>4&times; real-time on a single CPU core</strong> &mdash; a 200&times; speedup over WaveNet. Quality is close to WaveNet (MOS ~4.0), making it the first neural vocoder suitable for on-device deployment.
                </p>

                <h3>WaveGlow (Prenger et al., 2019)</h3>

                <p>
                    WaveGlow takes a fundamentally different approach by using <strong>normalizing flows</strong>. Instead of autoregressive generation, WaveGlow learns an invertible transformation between a simple Gaussian distribution and the target waveform distribution. During training, the model learns to map waveforms to Gaussian noise (the "forward" flow). During synthesis, it maps Gaussian noise samples to waveforms (the "inverse" flow) &mdash; and this can be done <em>in parallel</em> for all samples simultaneously.
                </p>

                <p>
                    WaveGlow combines the affine coupling layers of Glow with WaveNet-like dilated convolutions. It achieves quality comparable to WaveNet (MOS ~4.0) and generates audio at roughly <strong>500 kHz on a GPU</strong> (about 22&times; real-time at 22 kHz), but requires a large model (~88M parameters) and significant GPU memory.
                </p>

                <h3>LPCNet (Valin &amp; Skoglund, 2019)</h3>

                <p>
                    LPCNet takes the opposite approach: instead of replacing DSP entirely with neural networks, it <em>combines</em> them. Linear Predictive Coding (LPC) is a classical DSP technique that models the vocal tract as a linear filter. LPCNet uses a small neural network to predict the <strong>residual</strong> &mdash; the part of the signal that LPC cannot predict &mdash; rather than the entire waveform. Because LPC handles most of the signal structure, the neural network can be very small (just two GRU layers) and still achieve excellent quality.
                </p>

                <p>
                    The result is extraordinary efficiency: LPCNet runs at <strong>real-time on a single CPU core</strong> with only ~3M parameters, making it suitable for embedded devices. The trade-off is slightly lower quality than pure neural vocoders for some speaker types.
                </p>

                <h3>Parallel WaveGAN (Yamamoto et al., 2020)</h3>

                <p>
                    Parallel WaveGAN uses <strong>knowledge distillation</strong> with adversarial training. A non-autoregressive student generator is trained to produce waveforms that are indistinguishable from real audio according to a discriminator network. The generator uses a WaveNet-like architecture but produces all samples in parallel. A multi-resolution STFT loss supplements the adversarial loss to stabilize training and improve spectral fidelity. This approach achieves high quality at roughly <strong>40&times; real-time on GPU</strong>.
                </p>


                <!-- Section 4: GAN-Based Vocoders -->
                <h2 id="gan-vocoders">GAN-Based Vocoders</h2>

                <p>
                    The application of Generative Adversarial Networks to vocoding turned out to be remarkably effective. The key insight is that a discriminator network can learn to distinguish real from generated audio based on perceptually relevant features &mdash; it acts as a learned perceptual loss function. This is far more effective than hand-designed spectral losses, because the discriminator can capture subtle artifacts (metallic timbres, buzzy harmonics, unnatural breathiness) that are hard to express mathematically but easy for humans (and discriminators) to detect.
                </p>

                <h3>Discriminator Design</h3>

                <p>
                    The design of the discriminator is critical for audio GANs. A single discriminator operating on the raw waveform tends to focus on either large-scale or small-scale patterns, missing multi-scale artifacts. Two complementary discriminator architectures emerged:
                </p>

                <p>
                    <strong>Multi-Scale Discriminator (MSD):</strong> The MSD operates on the waveform at multiple resolutions. The original waveform is processed by one sub-discriminator, then average-pooled by a factor of 2 for a second sub-discriminator, and pooled again by 2 for a third. Each sub-discriminator is a stack of 1D convolutions. This captures patterns at different time scales: the highest resolution catches fine pitch harmonics, while the lowest resolution catches broad spectral shape.
                </p>

                <p>
                    <strong>Multi-Period Discriminator (MPD):</strong> The MPD reshapes the 1D waveform into 2D by folding it with different period lengths. For a period $p$, the waveform $(x_1, x_2, \ldots, x_T)$ is reshaped into a 2D tensor of shape $(\lceil T/p \rceil, p)$. Each sub-discriminator then applies 2D convolutions. The periods are chosen to be <strong>prime numbers</strong> &mdash; typically [2, 3, 5, 7, 11] &mdash; to ensure that each sub-discriminator captures a unique set of periodic structures. Prime periods guarantee that different sub-discriminators have minimal overlap in the patterns they detect, because prime numbers share no common factors.
                </p>

                <h3>Training Objectives</h3>

                <p>
                    GAN vocoders use a combination of adversarial and reconstruction losses. The adversarial loss ensures perceptual quality, while reconstruction losses provide stable gradients and prevent mode collapse.
                </p>

                <p>
                    <strong>Feature matching loss:</strong> This loss encourages the generator to produce intermediate features (in the discriminator's hidden layers) that match those of real audio. For a discriminator $D$ with $L$ layers, the feature matching loss is:
                </p>

                $$\mathcal{L}_{fm} = \sum_{l=1}^{L} \frac{1}{N_l} \left\| D^{(l)}(\mathbf{x}) - D^{(l)}(\hat{\mathbf{x}}) \right\|_1$$

                <p>
                    where $D^{(l)}(\cdot)$ denotes the activations at layer $l$, $N_l$ is the number of elements in that layer's output, $\mathbf{x}$ is the real waveform, and $\hat{\mathbf{x}}$ is the generated waveform. Feature matching provides much smoother gradients than the adversarial loss alone, stabilizing training significantly.
                </p>

                <p>
                    <strong>Why GANs work so well for audio:</strong> Audio generation is inherently a one-to-many problem &mdash; many different waveforms can correspond to the same mel spectrogram (due to phase ambiguity). A regression loss like L1 or L2 on the waveform would average over these possibilities, producing blurry, washed-out audio. The adversarial loss avoids this by only requiring that the generated waveform look "realistic" to the discriminator, not that it match any specific target waveform sample-by-sample. The discriminator implicitly learns what natural audio "should" sound like, including proper phase relationships, harmonic structures, and noise characteristics.
                </p>


                <!-- Section 5: HiFi-GAN Architecture -->
                <h2 id="hifigan">HiFi-GAN Architecture</h2>

                <p>
                    HiFi-GAN (Kong et al., 2020) brought together the best ideas from prior work into a clean, efficient architecture that achieves both high fidelity and high speed. It consists of a convolutional generator with multi-receptive field fusion, paired with MPD and MSD discriminators.
                </p>

                <h3>Generator: Transposed Convolution Upsampling</h3>

                <p>
                    The generator takes a mel spectrogram as input and progressively upsamples it to the waveform resolution through a series of <strong>transposed convolution</strong> layers. Each transposed convolution increases the temporal resolution by a specific factor. The upsampling ratios are chosen so that their product equals the hop length. For example, with hop length 256:
                </p>

                $$256 = 8 \times 8 \times 2 \times 2$$

                <p>
                    So the generator uses four transposed convolution layers with strides [8, 8, 2, 2]. After each transposed convolution, a <strong>Multi-Receptive Field Fusion (MRF)</strong> block refines the upsampled signal. The channel dimension starts large (e.g., 512) and is halved after each upsampling stage: $512 \to 256 \to 128 \to 64 \to 1$ (the final channel is the mono waveform).
                </p>

                <h3>Multi-Receptive Field Fusion (MRF) Blocks</h3>

                <p>
                    The MRF block is HiFi-GAN's key architectural contribution. It consists of multiple <strong>residual blocks operating in parallel</strong>, each with a different kernel size and dilation pattern. For example, one MRF block might contain three residual blocks:
                </p>

                <ul>
                    <li><strong>ResBlock 1:</strong> kernel size 3, dilations [1, 3, 5]</li>
                    <li><strong>ResBlock 2:</strong> kernel size 7, dilations [1, 3, 5]</li>
                    <li><strong>ResBlock 3:</strong> kernel size 11, dilations [1, 3, 5]</li>
                </ul>

                <p>
                    Each residual block is a stack of dilated 1D convolutions with LeakyReLU activations and residual connections. The outputs of all parallel residual blocks are <strong>summed</strong> (and optionally averaged) to produce the MRF block's output. This fusion of multiple receptive fields allows the generator to simultaneously capture patterns at different time scales &mdash; short-range pitch harmonics, medium-range phoneme structures, and long-range prosodic patterns &mdash; within a single block.
                </p>

                <h3>Multi-Period Discriminator (MPD)</h3>

                <p>
                    HiFi-GAN's MPD uses five sub-discriminators with periods $p \in \{2, 3, 5, 7, 11\}$. For each period, the 1D waveform of length $T$ is reshaped into a 2D tensor of shape $(\lceil T/p \rceil, p)$ by padding with zeros if $T$ is not divisible by $p$. Each sub-discriminator is a stack of 2D convolutions that process this reshaped signal.
                </p>

                <p>
                    <strong>Why prime numbers?</strong> The choice of prime periods is deliberate. If we used powers of 2 like $\{2, 4, 8, 16, 32\}$, the period-4 discriminator would capture a strict subset of the patterns captured by the period-2 discriminator (since $4 = 2 \times 2$). Prime numbers ensure <strong>maximal diversity</strong>: the LCM of $\{2, 3, 5, 7, 11\}$ is $2310$, meaning the discriminators do not share common periodic patterns until period 2310 &mdash; far longer than typical speech structures. Each sub-discriminator thus provides unique, non-redundant gradient information to the generator.
                </p>

                <h3>Multi-Scale Discriminator (MSD)</h3>

                <p>
                    HiFi-GAN's MSD uses three sub-discriminators operating at three scales. The first processes the raw waveform, the second processes a 2&times; average-pooled version, and the third processes a 4&times; average-pooled version. Each sub-discriminator is a stack of 1D grouped convolutions followed by a final convolution that produces a scalar judgment. The pooling reduces temporal resolution while preserving spectral structure, encouraging each scale to focus on different frequency ranges.
                </p>

                <p>
                    Together, the MPD (5 sub-discriminators) and MSD (3 sub-discriminators) provide <strong>8 independent adversarial losses</strong> to the generator, each focusing on different aspects of audio quality. This ensemble approach is key to HiFi-GAN's high perceptual quality.
                </p>


                <!-- Section 6: HiFi-GAN Training -->
                <h2 id="hifigan-training">HiFi-GAN Training</h2>

                <p>
                    HiFi-GAN training follows the standard GAN paradigm of alternating generator and discriminator updates, augmented with carefully weighted auxiliary losses.
                </p>

                <h3>Generator Loss</h3>

                <p>
                    The total generator loss is a weighted sum of three components:
                </p>

                $$\mathcal{L}_G = \mathcal{L}_{adv}(G) + \lambda_{fm} \mathcal{L}_{fm}(G) + \lambda_{mel} \mathcal{L}_{mel}(G)$$

                <p>
                    <strong>Adversarial loss</strong> (least-squares GAN formulation): The generator tries to make the discriminator output 1 (real) for generated audio. Summing over all $K$ sub-discriminators (both MPD and MSD):
                </p>

                $$\mathcal{L}_{adv}(G) = \sum_{k=1}^{K} \mathbb{E}_{\mathbf{s}} \left[ \left( D_k(G(\mathbf{s})) - 1 \right)^2 \right]$$

                <p>
                    <strong>Feature matching loss:</strong> Summed over all sub-discriminators and all layers within each:
                </p>

                $$\mathcal{L}_{fm}(G) = \sum_{k=1}^{K} \sum_{l=1}^{L_k} \frac{1}{N_{k,l}} \left\| D_k^{(l)}(\mathbf{x}) - D_k^{(l)}(G(\mathbf{s})) \right\|_1$$

                <p>
                    <strong>Mel-spectrogram loss:</strong> An L1 loss between the mel spectrograms of real and generated audio. This auxiliary loss provides dense, stable gradients:
                </p>

                $$\mathcal{L}_{mel}(G) = \left\| \phi(\mathbf{x}) - \phi(G(\mathbf{s})) \right\|_1$$

                <p>
                    where $\phi(\cdot)$ is the function that extracts a mel spectrogram from a waveform. Note that $\phi$ involves an STFT followed by mel filterbank projection &mdash; both are differentiable operations, so gradients flow through this loss to the generator.
                </p>

                <p>
                    The loss weights are $\lambda_{fm} = 2$ and $\lambda_{mel} = 45$. The large mel loss weight reflects its importance for spectral fidelity: without it, the GAN can produce audio that sounds vaguely realistic but does not faithfully reproduce the target mel spectrogram's content (wrong vowels, missing consonants, etc.).
                </p>

                <h3>Discriminator Loss</h3>

                <p>
                    Each sub-discriminator is trained to classify real audio as 1 and generated audio as 0, using the least-squares GAN loss:
                </p>

                $$\mathcal{L}_D = \sum_{k=1}^{K} \mathbb{E}_{\mathbf{x}} \left[ \left( D_k(\mathbf{x}) - 1 \right)^2 \right] + \mathbb{E}_{\mathbf{s}} \left[ D_k(G(\mathbf{s}))^2 \right]$$

                <p>
                    The least-squares formulation (LS-GAN) is preferred over the original GAN loss ($\log D$ vs. $\log(1 - D)$) because it provides more informative gradients when the discriminator is confident, leading to more stable training.
                </p>

                <h3>Training Recipe</h3>

                <p>
                    HiFi-GAN is trained with the Adam optimizer ($\beta_1 = 0.8$, $\beta_2 = 0.99$) with an initial learning rate of $2 \times 10^{-4}$ and exponential decay (factor 0.999 per epoch). Training runs for approximately <strong>2.5 million steps</strong> (~400 epochs on LJSpeech) with a batch size of 16. Each training sample is a random 8192-sample segment (~0.37 seconds at 22 kHz).
                </p>

                <p>
                    The generator and discriminator are updated alternately: one generator update per one discriminator update. HiFi-GAN v1 (the highest quality variant) has approximately <strong>14M parameters</strong> in the generator and trains in about 4 days on a single V100 GPU. Smaller variants (v2 with ~1M and v3 with ~1.5M parameters) trade slightly lower quality for faster inference and are suitable for real-time applications.
                </p>


                <!-- Section 7: Universal vs Speaker-Specific -->
                <h2 id="universal-vocoders">Universal vs Speaker-Specific</h2>

                <p>
                    An important design decision when deploying a vocoder is whether to train it on data from a single speaker (speaker-specific) or on data from many speakers (universal). Each approach has distinct advantages and trade-offs.
                </p>

                <h3>Speaker-Specific Vocoders</h3>

                <p>
                    A speaker-specific vocoder is trained on recordings from a single speaker, typically 10&ndash;30 hours of high-quality studio recordings. Because the model only needs to learn the acoustic characteristics of one voice, it can achieve <strong>very high quality</strong> with relatively small model capacity. The vocoder learns the speaker's specific pitch range, vocal timbre, breathiness patterns, and micro-prosodic habits. For production TTS systems targeting a single voice (e.g., a virtual assistant), speaker-specific vocoders typically achieve the highest MOS.
                </p>

                <h3>Universal Vocoders</h3>

                <p>
                    A universal vocoder is trained on data from many speakers (hundreds or thousands) and is expected to synthesize any speaker's voice, including speakers <em>not seen during training</em>. This is essential for multi-speaker TTS systems, voice conversion, and any application where the target speaker changes. Universal vocoders must learn a much more complex mapping &mdash; they cannot memorize one voice's characteristics but must generalize across pitch ranges, timbres, accents, and recording conditions.
                </p>

                <p>
                    Training a universal vocoder requires larger datasets (e.g., LibriTTS with 585 hours from 2,456 speakers) and typically results in slightly lower quality per speaker compared to a dedicated speaker-specific model. However, the gap has narrowed considerably with modern architectures like HiFi-GAN. In practice, a well-trained universal HiFi-GAN achieves MOS within 0.1&ndash;0.2 of a speaker-specific one.
                </p>

                <h3>When to Use Which</h3>

                <ul>
                    <li><strong>Speaker-specific:</strong> Best when you have abundant high-quality data for a single target voice and maximum quality is the priority. Common in commercial TTS products with a fixed voice persona.</li>
                    <li><strong>Universal:</strong> Best when you need to support multiple speakers, when target speaker data is limited (fine-tune a universal vocoder with little data), or when building a research system that must generalize. Also essential for zero-shot TTS and voice cloning.</li>
                    <li><strong>Fine-tuning strategy:</strong> A common practical approach is to train a universal vocoder on a large multi-speaker dataset, then fine-tune it on the target speaker's data for 50k&ndash;100k steps. This combines the generalization of universal training with the precision of speaker-specific tuning.</li>
                </ul>


                <!-- Section 8: Vocoder Comparison -->
                <h2 id="vocoder-comparison">Vocoder Comparison</h2>

                <p>
                    The following table summarizes the major neural vocoders, comparing their quality, speed, model size, architectural type, and year of publication. Quality is measured by Mean Opinion Score (MOS) on a 1&ndash;5 scale, and speed is given as a real-time factor (RTF) where values above 1.0 mean faster than real-time.
                </p>

                <table style="width: 100%; border-collapse: collapse; margin: 1rem 0; font-size: 0.95em;">
                    <thead>
                        <tr style="border-bottom: 2px solid var(--color-border);">
                            <th style="text-align: left; padding: 0.5rem;">Model</th>
                            <th style="text-align: center; padding: 0.5rem;">Quality (MOS)</th>
                            <th style="text-align: center; padding: 0.5rem;">Speed (RTF)</th>
                            <th style="text-align: center; padding: 0.5rem;">Parameters</th>
                            <th style="text-align: center; padding: 0.5rem;">Year</th>
                            <th style="text-align: center; padding: 0.5rem;">Type</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.5rem;"><strong>WaveNet</strong></td>
                            <td style="text-align: center; padding: 0.5rem;">~4.2</td>
                            <td style="text-align: center; padding: 0.5rem;">0.02&times; (GPU)</td>
                            <td style="text-align: center; padding: 0.5rem;">~4M</td>
                            <td style="text-align: center; padding: 0.5rem;">2016</td>
                            <td style="text-align: center; padding: 0.5rem;">Autoregressive</td>
                        </tr>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.5rem;"><strong>WaveRNN</strong></td>
                            <td style="text-align: center; padding: 0.5rem;">~4.0</td>
                            <td style="text-align: center; padding: 0.5rem;">4&times; (CPU)</td>
                            <td style="text-align: center; padding: 0.5rem;">~4M</td>
                            <td style="text-align: center; padding: 0.5rem;">2018</td>
                            <td style="text-align: center; padding: 0.5rem;">Autoregressive</td>
                        </tr>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.5rem;"><strong>WaveGlow</strong></td>
                            <td style="text-align: center; padding: 0.5rem;">~4.0</td>
                            <td style="text-align: center; padding: 0.5rem;">22&times; (GPU)</td>
                            <td style="text-align: center; padding: 0.5rem;">~88M</td>
                            <td style="text-align: center; padding: 0.5rem;">2019</td>
                            <td style="text-align: center; padding: 0.5rem;">Flow-based</td>
                        </tr>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.5rem;"><strong>HiFi-GAN (v1)</strong></td>
                            <td style="text-align: center; padding: 0.5rem;">~4.3</td>
                            <td style="text-align: center; padding: 0.5rem;">120&times; (GPU)</td>
                            <td style="text-align: center; padding: 0.5rem;">~14M</td>
                            <td style="text-align: center; padding: 0.5rem;">2020</td>
                            <td style="text-align: center; padding: 0.5rem;">GAN</td>
                        </tr>
                        <tr>
                            <td style="padding: 0.5rem;"><strong>UnivNet</strong></td>
                            <td style="text-align: center; padding: 0.5rem;">~4.3</td>
                            <td style="text-align: center; padding: 0.5rem;">100&times; (GPU)</td>
                            <td style="text-align: center; padding: 0.5rem;">~14M</td>
                            <td style="text-align: center; padding: 0.5rem;">2022</td>
                            <td style="text-align: center; padding: 0.5rem;">GAN</td>
                        </tr>
                    </tbody>
                </table>

                <div class="note-box">
                    <div class="box-title">HiFi-GAN as the De Facto Standard</div>
                    <p>
                        As of 2024, <strong>HiFi-GAN</strong> remains the most widely used neural vocoder in both research and production. Its combination of high quality (~4.3 MOS), fast inference (100&times;+ real-time on GPU), reasonable model size (~14M parameters), and stable training makes it the default choice for most TTS systems. It is used as the vocoder component in VITS, Coqui TTS, ESPnet, Bark, and many other frameworks. While newer models like BigVGAN and Vocos offer incremental improvements, HiFi-GAN's maturity, extensive community support, and well-understood training dynamics make it the safest production choice. When building a new TTS system, starting with HiFi-GAN is almost always the right decision.
                    </p>
                </div>

                <!-- Tutorial Navigation -->
                <div class="tutorial-nav">
                    <a href="../27-tts-fundamentals/index.html" class="tutorial-nav-link prev">
                        <span class="nav-label">Previous</span>
                        <span class="nav-title">&larr; TTS Fundamentals</span>
                    </a>
                    <a href="../29-tacotron/index.html" class="tutorial-nav-link next">
                        <span class="nav-label">Next</span>
                        <span class="nav-title">Tacotron &amp; Attention TTS &rarr;</span>
                    </a>
                </div>

            </article>

            <!-- ==================== CODE TAB ==================== -->
            <article class="article-content" id="code" style="display: none;">
                <h2>Code Examples</h2>
                <p>Three implementations covering the core vocoder concepts: dilated causal convolutions from scratch, HiFi-GAN inference, and the Multi-Period Discriminator architecture.</p>

                <!-- Code Example 1: Dilated Causal Convolution from Scratch -->
                <h3>1. Dilated Causal Convolution from Scratch</h3>
                <p>A complete implementation of WaveNet-style dilated causal convolutions in PyTorch, including gated activations, skip connections, and receptive field calculation.</p>

<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F


class CausalConv1d(nn.Module):
    """
    A 1D causal convolution with dilation.

    Causal means the output at time t depends only on inputs at times &lt;= t.
    This is achieved by left-padding the input by (kernel_size - 1) * dilation.
    """

    def __init__(self, in_channels, out_channels, kernel_size, dilation=1):
        super().__init__()
        self.kernel_size = kernel_size
        self.dilation = dilation
        # Padding = (K-1) * d ensures causal: output[t] depends on input[t - (K-1)*d .. t]
        self.padding = (kernel_size - 1) * dilation
        self.conv = nn.Conv1d(
            in_channels, out_channels, kernel_size,
            dilation=dilation, padding=0  # we pad manually
        )

    def forward(self, x):
        # Left-pad with zeros for causality
        x = F.pad(x, (self.padding, 0))
        return self.conv(x)


class GatedResidualBlock(nn.Module):
    """
    One WaveNet residual block with:
    - Dilated causal convolution
    - Gated activation: tanh(W_f * x) * sigmoid(W_g * x)
    - Residual connection
    - Skip connection
    """

    def __init__(self, residual_channels, gate_channels, skip_channels,
                 kernel_size, dilation):
        super().__init__()
        # The dilated causal conv outputs 2x channels: filter + gate
        self.dilated_conv = CausalConv1d(
            residual_channels, 2 * gate_channels, kernel_size, dilation
        )
        # 1x1 conv to project back to residual channel size
        self.residual_conv = nn.Conv1d(gate_channels, residual_channels, 1)
        # 1x1 conv for skip connection
        self.skip_conv = nn.Conv1d(gate_channels, skip_channels, 1)

    def forward(self, x):
        """
        Args:
            x: (B, residual_channels, T)
        Returns:
            residual_out: (B, residual_channels, T) - input to next layer
            skip_out: (B, skip_channels, T) - accumulated for output
        """
        h = self.dilated_conv(x)

        # Split into filter and gate
        h_filter, h_gate = h.chunk(2, dim=1)

        # Gated activation: tanh(filter) * sigmoid(gate)
        h = torch.tanh(h_filter) * torch.sigmoid(h_gate)

        # Skip connection output
        skip_out = self.skip_conv(h)

        # Residual connection
        residual_out = self.residual_conv(h) + x

        return residual_out, skip_out


class DilatedCausalConvStack(nn.Module):
    """
    A full stack of WaveNet-style dilated causal convolution layers.

    Uses exponentially increasing dilations: 1, 2, 4, 8, ..., 2^(layers_per_cycle-1)
    repeated for num_cycles.
    """

    def __init__(self, residual_channels=64, gate_channels=64, skip_channels=64,
                 kernel_size=2, layers_per_cycle=10, num_cycles=3):
        super().__init__()
        self.kernel_size = kernel_size
        self.layers_per_cycle = layers_per_cycle
        self.num_cycles = num_cycles

        self.blocks = nn.ModuleList()
        for cycle in range(num_cycles):
            for layer in range(layers_per_cycle):
                dilation = 2 ** layer
                self.blocks.append(
                    GatedResidualBlock(
                        residual_channels, gate_channels, skip_channels,
                        kernel_size, dilation
                    )
                )

        # Output layers: two 1x1 convolutions with ReLU
        self.output_conv1 = nn.Conv1d(skip_channels, skip_channels, 1)
        self.output_conv2 = nn.Conv1d(skip_channels, 256, 1)  # 256 = mu-law levels

    def forward(self, x):
        """
        Args:
            x: (B, residual_channels, T)
        Returns:
            logits: (B, 256, T) - logits over 256 mu-law quantization levels
        """
        skip_sum = 0
        h = x

        for block in self.blocks:
            h, skip = block(h)
            skip_sum = skip_sum + skip

        # Output processing
        out = F.relu(skip_sum)
        out = F.relu(self.output_conv1(out))
        logits = self.output_conv2(out)

        return logits

    def receptive_field(self):
        """Calculate the receptive field of this stack."""
        total_layers = self.layers_per_cycle * self.num_cycles
        # Each cycle contributes (K-1) * (2^L - 1) to the receptive field
        per_cycle = (self.kernel_size - 1) * (2 ** self.layers_per_cycle - 1)
        return 1 + self.num_cycles * per_cycle


# ============================================================
# Demonstration
# ============================================================
model = DilatedCausalConvStack(
    residual_channels=32, gate_channels=32, skip_channels=32,
    kernel_size=2, layers_per_cycle=10, num_cycles=3
)

print("=== Dilated Causal Convolution Stack ===")
print(f"Kernel size: {model.kernel_size}")
print(f"Layers per cycle: {model.layers_per_cycle}")
print(f"Number of cycles: {model.num_cycles}")
print(f"Total layers: {model.layers_per_cycle * model.num_cycles}")
print(f"Receptive field: {model.receptive_field()} samples")
print(f"  At 22050 Hz: {model.receptive_field() / 22050 * 1000:.1f} ms")
print()

# Print dilation pattern
print("Dilation pattern:")
for cycle in range(model.num_cycles):
    dilations = [2**l for l in range(model.layers_per_cycle)]
    print(f"  Cycle {cycle}: {dilations}")
print()

# Forward pass test
x = torch.randn(1, 32, 8000)  # 1 batch, 32 channels, 8000 samples
logits = model(x)
print(f"Input shape:  {tuple(x.shape)}  (batch, channels, time)")
print(f"Output shape: {tuple(logits.shape)}  (batch, 256 mu-law bins, time)")
print(f"Parameters:   {sum(p.numel() for p in model.parameters()):,}")

# Verify causality: changing a future input should not affect past outputs
x2 = x.clone()
x2[:, :, 4000:] = 0  # zero out future
logits2 = model(x2)
diff = (logits[:, :, :4000] - logits2[:, :, :4000]).abs().max().item()
print(f"\nCausality check (should be 0.0): {diff}")</code></pre>

                <!-- Code Example 2: HiFi-GAN Inference -->
                <h3>2. HiFi-GAN Inference</h3>
                <p>Demonstration of HiFi-GAN generator architecture and inference. Shows the generator structure with transposed convolution upsampling and MRF blocks, and measures the real-time factor.</p>

<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F
import time


class ResBlock(nn.Module):
    """Residual block with dilated convolutions for HiFi-GAN MRF."""

    def __init__(self, channels, kernel_size, dilations):
        super().__init__()
        self.convs1 = nn.ModuleList()
        self.convs2 = nn.ModuleList()

        for d in dilations:
            padding = (kernel_size * d - d) // 2
            self.convs1.append(
                nn.Conv1d(channels, channels, kernel_size,
                          dilation=d, padding=padding)
            )
            self.convs2.append(
                nn.Conv1d(channels, channels, kernel_size,
                          dilation=1, padding=(kernel_size - 1) // 2)
            )

    def forward(self, x):
        for c1, c2 in zip(self.convs1, self.convs2):
            xt = F.leaky_relu(x, 0.1)
            xt = c1(xt)
            xt = F.leaky_relu(xt, 0.1)
            xt = c2(xt)
            x = x + xt
        return x


class MRFBlock(nn.Module):
    """Multi-Receptive Field Fusion block."""

    def __init__(self, channels, resblock_kernel_sizes, resblock_dilations):
        super().__init__()
        self.resblocks = nn.ModuleList()
        for k, d in zip(resblock_kernel_sizes, resblock_dilations):
            self.resblocks.append(ResBlock(channels, k, d))

    def forward(self, x):
        out = None
        for block in self.resblocks:
            if out is None:
                out = block(x)
            else:
                out = out + block(x)
        return out / len(self.resblocks)


class HiFiGANGenerator(nn.Module):
    """
    Simplified HiFi-GAN v1 generator.

    Architecture:
    mel (80, M) -> Conv1d -> [TransposedConv + MRF] x 4 -> Conv1d -> tanh -> waveform (1, M*256)
    """

    def __init__(self, mel_channels=80, upsample_initial_channel=512,
                 upsample_rates=(8, 8, 2, 2),
                 upsample_kernel_sizes=(16, 16, 4, 4),
                 resblock_kernel_sizes=(3, 7, 11),
                 resblock_dilations=((1, 3, 5), (1, 3, 5), (1, 3, 5))):
        super().__init__()

        self.upsample_rates = upsample_rates
        self.hop_length = 1
        for r in upsample_rates:
            self.hop_length *= r

        # Initial convolution
        self.conv_pre = nn.Conv1d(mel_channels, upsample_initial_channel, 7, padding=3)

        # Upsampling layers + MRF blocks
        self.ups = nn.ModuleList()
        self.mrfs = nn.ModuleList()
        ch = upsample_initial_channel
        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):
            self.ups.append(
                nn.ConvTranspose1d(ch, ch // 2, k, stride=u,
                                   padding=(k - u) // 2)
            )
            self.mrfs.append(
                MRFBlock(ch // 2, resblock_kernel_sizes, resblock_dilations)
            )
            ch = ch // 2

        # Final convolution
        self.conv_post = nn.Conv1d(ch, 1, 7, padding=3)

    def forward(self, mel):
        """
        Args:
            mel: (B, 80, M) mel spectrogram
        Returns:
            waveform: (B, 1, M * hop_length) audio waveform
        """
        x = self.conv_pre(mel)

        for up, mrf in zip(self.ups, self.mrfs):
            x = F.leaky_relu(x, 0.1)
            x = up(x)
            x = mrf(x)

        x = F.leaky_relu(x, 0.1)
        x = self.conv_post(x)
        x = torch.tanh(x)
        return x


# ============================================================
# Demonstration
# ============================================================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
generator = HiFiGANGenerator().to(device)

print("=== HiFi-GAN Generator ===")
print(f"Hop length (product of upsample rates): {generator.hop_length}")
print(f"Upsample rates: {generator.upsample_rates}")
print(f"Parameters: {sum(p.numel() for p in generator.parameters()):,}")
print(f"Device: {device}")
print()

# Simulate mel spectrogram input (1 second at 22050 Hz, hop=256 -> ~86 frames)
mel_frames = 86
mel = torch.randn(1, 80, mel_frames, device=device)

# Generate waveform
generator.eval()
with torch.no_grad():
    waveform = generator(mel)
print(f"Input mel shape:    {tuple(mel.shape)}  (batch=1, mels=80, frames={mel_frames})")
print(f"Output waveform:    {tuple(waveform.shape)}  (batch=1, channels=1, samples={waveform.shape[2]})")
print(f"Expected samples:   {mel_frames * generator.hop_length}")
print(f"Audio duration:     {waveform.shape[2] / 22050:.3f} seconds")
print()

# Measure speed (real-time factor)
num_runs = 20
with torch.no_grad():
    # Warm up
    for _ in range(5):
        _ = generator(mel)
    if device.type == "cuda":
        torch.cuda.synchronize()

    start = time.time()
    for _ in range(num_runs):
        _ = generator(mel)
    if device.type == "cuda":
        torch.cuda.synchronize()
    elapsed = time.time() - start

avg_time = elapsed / num_runs
audio_duration = waveform.shape[2] / 22050
rtf = audio_duration / avg_time

print(f"Average generation time: {avg_time * 1000:.2f} ms")
print(f"Audio duration:          {audio_duration * 1000:.2f} ms")
print(f"Real-time factor (RTF):  {rtf:.1f}x")</code></pre>

                <!-- Code Example 3: Multi-Period Discriminator -->
                <h3>3. Multi-Period Discriminator</h3>
                <p>Implementation of the Multi-Period Discriminator (MPD) from HiFi-GAN. The key idea is reshaping the 1D waveform into 2D using different prime-number periods, then applying 2D convolutions.</p>

<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F


class PeriodSubDiscriminator(nn.Module):
    """
    One sub-discriminator for a specific period p.

    Reshapes 1D waveform of length T into 2D tensor of shape (T//p, p),
    then applies a stack of 2D convolutions.
    """

    def __init__(self, period, channels=32, max_channels=512):
        super().__init__()
        self.period = period

        # Stack of 2D convolutions with increasing channels
        self.convs = nn.ModuleList()
        in_ch = 1
        for i in range(4):
            out_ch = min(channels * (2 ** (i + 1)), max_channels)
            self.convs.append(
                nn.Conv2d(in_ch, out_ch, kernel_size=(5, 1), stride=(3, 1),
                          padding=(2, 0))
            )
            in_ch = out_ch

        # Final conv to produce scalar output per patch
        self.conv_post = nn.Conv2d(in_ch, 1, kernel_size=(3, 1), padding=(1, 0))

    def forward(self, x):
        """
        Args:
            x: (B, 1, T) raw waveform

        Returns:
            score: final discriminator score (B, 1, H, 1)
            fmaps: list of intermediate feature maps (for feature matching loss)
        """
        B, C, T = x.shape
        fmaps = []

        # Pad waveform so length is divisible by period
        if T % self.period != 0:
            pad_len = self.period - (T % self.period)
            x = F.pad(x, (0, pad_len), "reflect")
            T = T + pad_len

        # Reshape 1D -> 2D: (B, 1, T) -> (B, 1, T//p, p)
        x = x.view(B, C, T // self.period, self.period)

        # Apply 2D convolutions
        for conv in self.convs:
            x = conv(x)
            x = F.leaky_relu(x, 0.1)
            fmaps.append(x)

        x = self.conv_post(x)
        fmaps.append(x)

        return x, fmaps


class MultiPeriodDiscriminator(nn.Module):
    """
    Multi-Period Discriminator (MPD) from HiFi-GAN.

    Uses prime-number periods [2, 3, 5, 7, 11] to ensure each
    sub-discriminator captures unique periodic structures.
    """

    def __init__(self, periods=(2, 3, 5, 7, 11)):
        super().__init__()
        self.periods = periods
        self.discriminators = nn.ModuleList([
            PeriodSubDiscriminator(p) for p in periods
        ])

    def forward(self, x):
        """
        Args:
            x: (B, 1, T) raw waveform

        Returns:
            scores: list of discriminator outputs (one per period)
            fmaps: list of feature map lists (for feature matching loss)
        """
        scores = []
        fmaps = []
        for disc in self.discriminators:
            score, fmap = disc(x)
            scores.append(score)
            fmaps.append(fmap)
        return scores, fmaps


def feature_matching_loss(fmaps_real, fmaps_fake):
    """
    Feature matching loss between real and fake feature maps.

    L_fm = sum over discriminators, sum over layers:
           (1/N_l) * ||D^l(x) - D^l(G(s))||_1
    """
    loss = 0
    for fmap_real, fmap_fake in zip(fmaps_real, fmaps_fake):
        for feat_real, feat_fake in zip(fmap_real, fmap_fake):
            loss += torch.mean(torch.abs(feat_real.detach() - feat_fake))
    return loss


def discriminator_loss(scores_real, scores_fake):
    """
    LS-GAN discriminator loss.
    L_D = sum_k [ E[(D_k(x) - 1)^2] + E[D_k(G(s))^2] ]
    """
    loss = 0
    for score_real, score_fake in zip(scores_real, scores_fake):
        loss += torch.mean((score_real - 1) ** 2) + torch.mean(score_fake ** 2)
    return loss


def generator_adversarial_loss(scores_fake):
    """
    LS-GAN generator loss.
    L_adv = sum_k E[(D_k(G(s)) - 1)^2]
    """
    loss = 0
    for score_fake in scores_fake:
        loss += torch.mean((score_fake - 1) ** 2)
    return loss


# ============================================================
# Demonstration
# ============================================================
mpd = MultiPeriodDiscriminator(periods=(2, 3, 5, 7, 11))

print("=== Multi-Period Discriminator ===")
print(f"Periods: {mpd.periods}")
print(f"Number of sub-discriminators: {len(mpd.discriminators)}")
print(f"Total parameters: {sum(p.numel() for p in mpd.parameters()):,}")
print()

# Test with a 1-second waveform
waveform_real = torch.randn(1, 1, 22050)   # real audio
waveform_fake = torch.randn(1, 1, 22050)   # generated audio

scores_real, fmaps_real = mpd(waveform_real)
scores_fake, fmaps_fake = mpd(waveform_fake)

print("Per-period output shapes:")
for i, (period, score) in enumerate(zip(mpd.periods, scores_real)):
    T = 22050
    pad_len = (period - T % period) % period
    reshaped_len = (T + pad_len) // period
    print(f"  Period {period:2d}: waveform reshaped to ({reshaped_len}, {period})"
          f"  ->  score shape {tuple(score.shape)}")

print()
d_loss = discriminator_loss(scores_real, scores_fake)
g_loss = generator_adversarial_loss(scores_fake)
fm_loss = feature_matching_loss(fmaps_real, fmaps_fake)

print(f"Discriminator loss:  {d_loss.item():.4f}")
print(f"Generator adv loss:  {g_loss.item():.4f}")
print(f"Feature match loss:  {fm_loss.item():.4f}")
print()
print("In HiFi-GAN, the full generator loss is:")
print("  L_G = L_adv + 2 * L_fm + 45 * L_mel")
print(f"  L_G = {g_loss.item():.4f} + 2 * {fm_loss.item():.4f} + 45 * L_mel")</code></pre>

            </article>

            <!-- ==================== EXERCISES TAB ==================== -->
            <article class="article-content" id="exercises" style="display: none;">
                <h2>Exercises</h2>
                <p>Test your understanding of neural vocoders, from basic signal processing concepts to advanced architectural design. Exercises cover receptive field calculation, upsampling, GAN training, and vocoder design. Solutions are provided for self-study.</p>

                <div class="exercise-list">

                    <h3 style="margin-top: 1rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">Easy</h3>

                    <!-- Exercise 1 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">1. Receptive Field Calculation</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>A WaveNet model uses kernel size $K = 3$, dilation factors that double each layer ($d_l = 2^{l-1}$ for $l = 1, \ldots, L$), and $L = 10$ layers per cycle with $B = 2$ cycles. (a) Calculate the receptive field of a single cycle. (b) Calculate the total receptive field. (c) How many milliseconds does this correspond to at 22,050 Hz? (d) Is this sufficient for modeling speech? How many pitch periods of a 100 Hz voice does it cover?</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a) Single cycle receptive field:</strong></p>
                                <p>The formula for one cycle with $L$ layers, kernel size $K$, and dilation $d_l = 2^{l-1}$:</p>
                                $$R_{\text{cycle}} = 1 + (K - 1) \sum_{l=1}^{L} 2^{l-1} = 1 + (K - 1)(2^L - 1)$$
                                <p>With $K = 3$, $L = 10$:</p>
                                $$R_{\text{cycle}} = 1 + 2 \times (2^{10} - 1) = 1 + 2 \times 1023 = 2047 \text{ samples}$$
                                <p><strong>(b) Total receptive field with $B = 2$ cycles:</strong></p>
                                $$R_{\text{total}} = 1 + B \times (K - 1)(2^L - 1) = 1 + 2 \times 2 \times 1023 = 4093 \text{ samples}$$
                                <p><strong>(c) Duration in milliseconds:</strong></p>
                                $$t = \frac{R_{\text{total}}}{f_s} = \frac{4093}{22050} \approx 0.1856 \text{ s} = 185.6 \text{ ms}$$
                                <p><strong>(d) Sufficiency for speech:</strong></p>
                                <p>A 100 Hz voice has a pitch period of $1/100 = 10$ ms. The receptive field covers $185.6 / 10 \approx 18.6$ pitch periods. This is more than sufficient to capture several pitch cycles, which is important for generating natural-sounding periodic waveforms. For lower voices (e.g., 80 Hz, period = 12.5 ms), it covers about 14.8 periods &mdash; still adequate. The receptive field also needs to capture consonant-to-vowel transitions, which are typically 20&ndash;50 ms, well within range.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 2 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">2. Upsampling Math</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>A mel spectrogram has 100 frames computed with hop length 256 at a sample rate of 22,050 Hz. (a) How many audio samples does the corresponding waveform have? (b) What is the audio duration in seconds? (c) The HiFi-GAN generator uses transposed convolutions with strides [8, 8, 2, 2]. Verify that these multiply to 256. (d) Propose an alternative set of 4 upsampling ratios that also multiply to 256. What are the trade-offs?</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a) Number of audio samples:</strong></p>
                                $$T = M \times h = 100 \times 256 = 25{,}600 \text{ samples}$$
                                <p>(In practice, the waveform may be slightly longer due to the STFT window size, but the hop-length relationship gives the primary count.)</p>
                                <p><strong>(b) Audio duration:</strong></p>
                                $$\text{duration} = \frac{T}{f_s} = \frac{25600}{22050} \approx 1.161 \text{ seconds}$$
                                <p><strong>(c) Verification:</strong></p>
                                $$8 \times 8 \times 2 \times 2 = 64 \times 4 = 256 \; \checkmark$$
                                <p><strong>(d) Alternative ratios:</strong> Several options exist:</p>
                                <ul>
                                    <li>$[4, 4, 4, 4] = 256$: Uniform upsampling. Each stage upsamples by 4&times;. Simpler architecture, but no stage captures very long-range patterns (each transposed convolution kernel "sees" a limited context).</li>
                                    <li>$[16, 4, 2, 2] = 256$: Front-loaded upsampling. The first stage does most of the work (16&times;), which means it operates on more mel-level context but produces a very large intermediate tensor early. Requires a large kernel in the first transposed conv ($k \geq 32$ typically, since $k \geq 2s$ is recommended).</li>
                                    <li>$[2, 2, 8, 8] = 256$: Back-loaded upsampling. The first stages operate at low resolution with many channels (fast), and the big upsampling happens later at fewer channels. May struggle to capture long-range dependencies at the high-resolution end.</li>
                                </ul>
                                <p>The HiFi-GAN choice of $[8, 8, 2, 2]$ balances these trade-offs: the two large upsamplings happen early (at high channel count, low temporal resolution), and the two small upsamplings refine the signal at high resolution.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 3 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">3. &mu;-Law Encoding</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>The &mu;-law companding formula is $f(x) = \text{sign}(x) \frac{\ln(1 + \mu|x|)}{\ln(1 + \mu)}$ with $\mu = 255$. (a) Compute $f(0.01)$, $f(0.1)$, $f(0.5)$, and $f(1.0)$. (b) What is the ratio $f(0.1) / f(0.01)$ compared to $0.1 / 0.01$? What does this tell you about the encoding? (c) Why does this nonlinear compression help WaveNet generate better audio?</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a) Computing &mu;-law values:</strong></p>
                                <p>With $\mu = 255$ and $\ln(256) \approx 5.5452$:</p>
                                <ul>
                                    <li>$f(0.01) = \frac{\ln(1 + 2.55)}{\ln(256)} = \frac{\ln(3.55)}{5.5452} = \frac{1.2669}{5.5452} \approx 0.228$</li>
                                    <li>$f(0.1) = \frac{\ln(1 + 25.5)}{5.5452} = \frac{\ln(26.5)}{5.5452} = \frac{3.2771}{5.5452} \approx 0.591$</li>
                                    <li>$f(0.5) = \frac{\ln(1 + 127.5)}{5.5452} = \frac{\ln(128.5)}{5.5452} = \frac{4.8553}{5.5452} \approx 0.876$</li>
                                    <li>$f(1.0) = \frac{\ln(1 + 255)}{5.5452} = \frac{\ln(256)}{5.5452} = 1.000$</li>
                                </ul>
                                <p><strong>(b) Ratio analysis:</strong></p>
                                <p>$f(0.1) / f(0.01) = 0.591 / 0.228 \approx 2.59$, compared to $0.1 / 0.01 = 10$.</p>
                                <p>In the linear domain, $0.1$ is 10 times larger than $0.01$. After &mu;-law compression, the ratio shrinks to only 2.59. This means &mu;-law <strong>compresses the dynamic range</strong> &mdash; it gives quiet sounds (near 0) much more representation than loud sounds (near 1). A 10:1 amplitude ratio becomes only 2.59:1 after compression.</p>
                                <p><strong>(c) Why this helps WaveNet:</strong></p>
                                <p>WaveNet quantizes the compressed signal to 256 levels. Without compression, quiet sounds would occupy only a few quantization levels (e.g., values $\pm 0.01$ map to just ~2&ndash;3 levels out of 256), producing audible quantization noise during quiet passages. With &mu;-law compression, quiet sounds are spread across many more quantization levels (~58 levels for $|x| &lt; 0.01$), preserving fine detail. Human hearing is approximately logarithmic in sensitivity, so &mu;-law's logarithmic compression matches our perceptual resolution. The result is lower quantization noise where it matters most: in quiet, perceptually sensitive regions.</p>
                            </div>
                        </div>
                    </div>

                    <h3 style="margin-top: 2rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">Medium</h3>

                    <!-- Exercise 4 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">4. GAN Loss Variants</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Compare three GAN loss formulations for vocoders: (a) Vanilla GAN: $\mathcal{L}_D = -\mathbb{E}[\log D(x)] - \mathbb{E}[\log(1 - D(G(s)))]$, (b) LS-GAN: $\mathcal{L}_D = \mathbb{E}[(D(x) - 1)^2] + \mathbb{E}[D(G(s))^2]$, (c) Hinge loss: $\mathcal{L}_D = \mathbb{E}[\max(0, 1 - D(x))] + \mathbb{E}[\max(0, 1 + D(G(s)))]$. For each, derive the generator loss. Which does HiFi-GAN use, and why?</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a) Vanilla GAN:</strong></p>
                                <p>Generator loss: $\mathcal{L}_G = -\mathbb{E}[\log D(G(s))]$ (or equivalently $\mathbb{E}[\log(1 - D(G(s)))]$, though the former gives stronger gradients early in training).</p>
                                <p>Problem: When $D$ is confident ($D(G(s)) \approx 0$), $\log D(G(s)) \to -\infty$, causing unstable gradients. Conversely, when $D$ is fooled ($D(G(s)) \approx 1$), the gradient vanishes. This leads to training instability, especially early on when the generator is poor.</p>

                                <p><strong>(b) LS-GAN (Least Squares):</strong></p>
                                <p>Generator loss: $\mathcal{L}_G = \mathbb{E}[(D(G(s)) - 1)^2]$</p>
                                <p>Advantage: The quadratic penalty provides <strong>non-zero gradients everywhere</strong>. Even when $D(G(s))$ is far from 1, the gradient is proportional to the distance, giving the generator a clear learning signal. No vanishing or exploding gradient issues from the loss function itself.</p>

                                <p><strong>(c) Hinge loss:</strong></p>
                                <p>Generator loss: $\mathcal{L}_G = -\mathbb{E}[D(G(s))]$</p>
                                <p>The discriminator is trained with a margin: once it correctly classifies real samples with margin $\geq 1$, it stops getting gradient from those samples. This prevents the discriminator from becoming "too confident" and provides more useful gradients to the generator.</p>

                                <p><strong>HiFi-GAN's choice:</strong> HiFi-GAN uses <strong>LS-GAN</strong>. The authors found it provides the best balance of training stability and audio quality. The smooth quadratic loss avoids the gradient issues of vanilla GAN, and unlike hinge loss, it continues to penalize the generator even when samples are close to realistic &mdash; pushing for higher fidelity. The continuous nature of the LS-GAN loss is particularly important for audio, where subtle differences in quality matter.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 5 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">5. MPD Period Selection</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>HiFi-GAN uses prime periods [2, 3, 5, 7, 11] for its MPD. (a) Compute the LCM of these periods. (b) Now compute the LCM of [2, 4, 8, 16, 32]. (c) Explain why the LCM matters for discriminator diversity. (d) Could you use [2, 3, 5, 7, 11, 13]? What would be the benefit and cost?</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a) LCM of prime periods:</strong></p>
                                <p>Since 2, 3, 5, 7, 11 are all prime, their LCM is simply their product:</p>
                                $$\text{LCM}(2, 3, 5, 7, 11) = 2 \times 3 \times 5 \times 7 \times 11 = 2310$$
                                <p><strong>(b) LCM of power-of-2 periods:</strong></p>
                                $$\text{LCM}(2, 4, 8, 16, 32) = 32$$
                                <p><strong>(c) Why LCM matters:</strong></p>
                                <p>The LCM tells us the shortest periodic pattern that <em>all</em> sub-discriminators would capture simultaneously. A larger LCM means the sub-discriminators have less overlap in the patterns they detect. With prime periods (LCM = 2310), patterns shorter than 2310 samples (~105 ms at 22 kHz) are captured by different combinations of sub-discriminators with minimal redundancy. With power-of-2 periods (LCM = 32), patterns with period 32 are detected by all five sub-discriminators simultaneously, wasting capacity. Worse, the period-4 discriminator captures a strict subset of the period-2 discriminator's patterns (every period-4 pattern is also a period-2 pattern), providing redundant gradient signal.</p>
                                <p><strong>(d) Adding period 13:</strong></p>
                                <p>$\text{LCM}(2, 3, 5, 7, 11, 13) = 2310 \times 13 = 30030$. The benefit is even more diverse periodic pattern detection. The cost is one additional sub-discriminator, increasing training time and memory by roughly $1/5 \approx 20\%$ for the MPD component. In practice, the improvement is marginal &mdash; 5 primes already capture most perceptually relevant periodic structures in speech. The diminishing returns do not justify the added computational cost, which is why HiFi-GAN stops at 5.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 6 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">6. Feature Matching Loss</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>The feature matching loss is $\mathcal{L}_{fm} = \sum_l \frac{1}{N_l} \|D^{(l)}(\mathbf{x}) - D^{(l)}(\hat{\mathbf{x}})\|_1$. (a) Why use L1 norm instead of L2? (b) Why is $D^{(l)}(\mathbf{x})$ detached from the computation graph (no gradient flows to the discriminator through this loss)? (c) How does feature matching relate to perceptual losses used in image generation? (d) What would happen if you trained with only feature matching loss and no adversarial loss?</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a) L1 vs L2:</strong></p>
                                <p>L1 norm ($|\cdot|$) produces sparser gradients and is more robust to outliers than L2 ($|\cdot|^2$). In audio, occasional large feature differences (e.g., at transient consonant boundaries) should not dominate the gradient. L2 would square these large differences, causing the model to over-prioritize matching a few outlier features at the expense of overall fidelity. L1 treats all differences linearly, leading to more balanced feature matching across all positions and layers.</p>
                                <p><strong>(b) Detaching the real features:</strong></p>
                                <p>If gradients from $\mathcal{L}_{fm}$ flowed through $D^{(l)}(\mathbf{x})$ to the discriminator, the discriminator would be trained to minimize the feature matching loss &mdash; which would push it toward producing identical features for real and fake audio. This directly contradicts the discriminator's objective (to distinguish real from fake). Detaching ensures the feature matching loss only updates the <em>generator</em>, pushing it to produce audio whose features match real audio in the discriminator's learned feature space.</p>
                                <p><strong>(c) Relation to perceptual losses:</strong></p>
                                <p>Feature matching is essentially a <strong>perceptual loss</strong> (also called style loss or content loss in image generation). In image generation, perceptual losses match features from a pretrained VGG network; in audio GANs, the discriminator itself serves as the "perceptual" network. The key advantage over image perceptual losses is that the discriminator is trained jointly and adapts its features to be relevant for the specific audio domain, rather than relying on a generic pretrained network.</p>
                                <p><strong>(d) Training with only feature matching:</strong></p>
                                <p>Without the adversarial loss, the generator receives no signal about whether its outputs are "realistic" in an absolute sense. Feature matching only tries to match feature statistics, not to fool the discriminator. The result would be audio that approximately matches the spectral envelope of real audio but lacks the fine perceptual quality that adversarial training provides. The audio would sound acceptable but slightly muffled or over-smoothed, similar to training with reconstruction losses alone. The adversarial loss is essential for pushing the generator to produce sharp, realistic audio details.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 7 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">7. WaveNet Speed Analysis</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>WaveNet generates audio autoregressively at ~0.02&times; real-time on a V100 GPU. (a) Calculate the exact time to generate 1 second of 22,050 Hz audio. (b) How many GFLOPS does a V100 provide, and what does this imply about per-sample computation? (c) If we reduce the sample rate to 8,000 Hz (telephone quality), how fast is WaveNet? (d) Why can't we simply batch the autoregressive steps to use GPU parallelism?</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a) Time to generate 1 second:</strong></p>
                                <p>At 0.02&times; real-time: $\text{time} = 1 / 0.02 = 50$ seconds to generate 1 second of audio. The model must produce 22,050 samples sequentially, so each sample takes $50 / 22050 \approx 2.27$ ms per sample.</p>
                                <p><strong>(b) GPU utilization:</strong></p>
                                <p>A V100 provides ~14 TFLOPS (FP32) = $14 \times 10^{12}$ operations per second. In 2.27 ms (one sample), the GPU performs $14 \times 10^{12} \times 0.00227 \approx 3.18 \times 10^{10}$ operations. WaveNet has ~4M parameters, and a forward pass involves roughly 20&ndash;40M multiply-add operations. This means the GPU is utilizing only about $40 \times 10^6 / (3.18 \times 10^{10}) \approx 0.0013 = 0.13\%$ of its peak throughput. The GPU is <em>massively underutilized</em> because each forward pass is too small to saturate the parallel compute units.</p>
                                <p><strong>(c) At 8,000 Hz:</strong></p>
                                <p>The per-sample computation does not change, so each sample still takes ~2.27 ms. For 1 second at 8 kHz: $8000 \times 0.00227 \approx 18.2$ seconds, giving RTF $\approx 0.055\times$. About 2.75&times; faster than at 22 kHz, but still far from real-time.</p>
                                <p><strong>(d) Why batching doesn't help:</strong></p>
                                <p>The autoregressive dependency $p(x_t | x_1, \ldots, x_{t-1})$ means sample $x_t$ cannot be computed until $x_{t-1}$ is known. There is no way to batch across time steps within a single utterance. You <em>can</em> batch across different utterances (process 32 utterances in parallel), but this only improves throughput, not latency &mdash; each individual utterance still takes 50 seconds. For real-time TTS, we need low latency for a single utterance, which autoregressive models fundamentally cannot provide.</p>
                            </div>
                        </div>
                    </div>

                    <h3 style="margin-top: 2rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">Hard</h3>

                    <!-- Exercise 8 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">8. Phase Reconstruction</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>The mel spectrogram discards phase information from the STFT. (a) Explain mathematically what phase information is and why it matters for waveform reconstruction. (b) Why does Griffin-Lim produce buzzy, metallic-sounding audio? (c) How do GAN vocoders like HiFi-GAN learn to generate phase implicitly, without an explicit phase prediction head? (d) Could you add an explicit phase prediction loss to HiFi-GAN? Would this help or hurt?</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a) What phase is:</strong></p>
                                <p>The STFT of a signal at frame $m$ and frequency bin $k$ is a complex number: $X_m[k] = |X_m[k]| \cdot e^{j\phi_m[k]}$ where $|X_m[k]|$ is the magnitude and $\phi_m[k] \in [-\pi, \pi]$ is the phase angle. The mel spectrogram is derived from $|X_m[k]|^2$ (the power spectrum) after mel filterbank projection, discarding all $\phi_m[k]$ values. Phase determines the <em>temporal alignment</em> of frequency components within each STFT frame. Two signals can have identical magnitude spectra but completely different waveforms if their phases differ. For reconstruction, the phase must satisfy <strong>consistency constraints</strong>: adjacent STFT frames must have phase values that, when the windows overlap-add, produce a coherent waveform. Random phase violates these constraints and produces clicks and artifacts.</p>
                                <p><strong>(b) Why Griffin-Lim sounds metallic:</strong></p>
                                <p>Griffin-Lim iteratively estimates phase by repeatedly applying the STFT and inverse STFT, projecting onto the constraint that the magnitude spectrum matches the target. It converges to a <em>locally consistent</em> phase, but this phase is not the natural phase of speech. Natural speech has specific phase relationships between harmonics (e.g., voiced sounds have approximately linear phase, corresponding to a shifted impulse). Griffin-Lim's solution typically has unnatural inter-harmonic phase relationships, producing the characteristic metallic/buzzy quality. It also converges slowly (100+ iterations) and the solution depends on the random initialization.</p>
                                <p><strong>(c) Implicit phase learning in GAN vocoders:</strong></p>
                                <p>HiFi-GAN never explicitly predicts phase. Instead, the generator produces a raw waveform directly, and the discriminators evaluate whether this waveform sounds natural. The discriminators implicitly learn to detect unnatural phase relationships: metallic harmonics, buzzy transients, and other phase-related artifacts cause the discriminator to output low scores. The generator is thus trained to produce waveforms with natural phase through the adversarial loss. The mel loss $\|\phi(x) - \phi(G(s))\|_1$ ensures spectral fidelity (correct magnitude), while the adversarial loss ensures perceptual quality (including natural phase). The convolutional generator architecture also has an inductive bias toward smooth, locally coherent waveforms, which naturally leads to reasonable phase.</p>
                                <p><strong>(d) Explicit phase prediction:</strong></p>
                                <p>Adding an explicit phase loss (e.g., $\|\text{phase}(x) - \text{phase}(G(s))\|$) would likely hurt. Phase is inherently ambiguous for the vocoding task: many valid phase sequences correspond to the same mel spectrogram and sound equally natural. Forcing the generator to match a specific target phase would over-constrain the problem, potentially causing the generator to produce artifacts while chasing the exact target phase values rather than producing perceptually natural audio. The adversarial approach is better because it only requires the output to <em>sound natural</em>, not to match a specific phase trajectory.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 9 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">9. Universal Vocoder Design</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Design a universal vocoder based on HiFi-GAN that can synthesize high-quality audio for speakers not seen during training. (a) What training data would you use? (b) What architectural modifications would improve generalization to unseen speakers? (c) How would you evaluate performance on unseen speakers? (d) Describe a fine-tuning strategy for adapting the universal vocoder to a new target speaker with only 5 minutes of data.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a) Training data:</strong></p>
                                <p>Use a large, diverse multi-speaker dataset such as LibriTTS (585 hours, 2,456 speakers) or a combination of LibriTTS + VCTK (109 speakers, diverse accents) + internal data. Key requirements: (1) speaker diversity (male/female, age range, accents, pitch ranges), (2) recording quality diversity (studio, semi-studio, mild noise), (3) balanced speaker representation (not dominated by a few speakers). Augment with pitch-shifted versions of training data to extend the pitch range coverage.</p>
                                <p><strong>(b) Architectural modifications:</strong></p>
                                <ul>
                                    <li><strong>Wider channel dimensions:</strong> Increase the generator channels (e.g., 512 &rarr; 768) to provide more capacity for modeling diverse speakers.</li>
                                    <li><strong>Larger MRF blocks:</strong> Add more kernel sizes (e.g., [3, 5, 7, 11, 13]) to capture a wider range of periodic structures across different pitch ranges.</li>
                                    <li><strong>Pitch-conditional normalization:</strong> Add a fundamental frequency (F0) input alongside the mel spectrogram. This helps the generator explicitly model pitch variations across speakers rather than having to infer pitch from mel features alone.</li>
                                    <li><strong>Deeper discriminators:</strong> Use larger discriminators with more layers to provide richer gradient information for diverse audio styles.</li>
                                </ul>
                                <p><strong>(c) Evaluation:</strong></p>
                                <p>Hold out a set of speakers (e.g., 100 speakers) never seen during training. For each held-out speaker, use ground-truth mel spectrograms and evaluate the vocoder's output with: (1) MOS listening tests comparing to ground truth, (2) Mel Cepstral Distortion (MCD) measuring spectral fidelity, (3) F0 RMSE measuring pitch accuracy, (4) speaker verification scores (cosine similarity of speaker embeddings between real and synthesized audio &mdash; should be high if the vocoder preserves speaker identity).</p>
                                <p><strong>(d) Fine-tuning strategy:</strong></p>
                                <p>With only 5 minutes of target speaker data: (1) Freeze the discriminators and fine-tune only the generator for 10k&ndash;20k steps with a reduced learning rate ($1 \times 10^{-5}$). (2) Use the mel loss with a higher weight ($\lambda_{mel} = 90$) to prevent overfitting while maintaining spectral fidelity. (3) After initial fine-tuning, unfreeze the discriminators and train jointly for 5k&ndash;10k additional steps at an even lower learning rate. (4) Monitor validation MCD on a held-out subset of the 5-minute data to detect overfitting. Total fine-tuning time: ~1&ndash;2 hours on a single GPU.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 10 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">10. Streaming Vocoder</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Design a vocoder for real-time streaming TTS, where mel spectrogram frames arrive one at a time and audio must be output with minimal latency. (a) What constraints does streaming add compared to offline vocoding? (b) How would you modify HiFi-GAN's generator for streaming? Consider the receptive field problem. (c) What is the minimum theoretical latency? (d) How would you handle the boundary artifacts between consecutive chunks?</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a) Streaming constraints:</strong></p>
                                <ul>
                                    <li><strong>Causal processing:</strong> The vocoder cannot use future mel frames. Standard HiFi-GAN convolutions are non-causal (they use padding on both sides, effectively looking at future context). All convolutions must be made causal.</li>
                                    <li><strong>Fixed memory:</strong> The model cannot buffer the entire utterance. Memory usage must be bounded regardless of utterance length.</li>
                                    <li><strong>Low latency:</strong> Each chunk of audio must be produced before the user perceives a delay. Perceptual threshold for noticeable delay is ~100&ndash;200 ms for interactive speech.</li>
                                    <li><strong>Consistent quality:</strong> Audio quality must be uniform across chunk boundaries &mdash; no clicks, pops, or timbral shifts at transitions.</li>
                                </ul>
                                <p><strong>(b) Modifying HiFi-GAN for streaming:</strong></p>
                                <ul>
                                    <li><strong>Causal convolutions:</strong> Replace all symmetric padding ($p = k//2$ on both sides) with left-only padding ($p = k-1$ on the left, 0 on the right). This ensures each output depends only on current and past inputs.</li>
                                    <li><strong>Chunk-based processing:</strong> Process mel frames in small chunks (e.g., 4&ndash;8 frames at a time). Each chunk produces $\text{chunk\_size} \times \text{hop\_length}$ audio samples.</li>
                                    <li><strong>Hidden state caching:</strong> Cache the convolution states (the left padding region) from the previous chunk. When processing the next chunk, use the cached states as the left context instead of zero padding. This is the convolutional equivalent of RNN hidden state propagation.</li>
                                    <li><strong>Receptive field management:</strong> The MRF blocks with large kernels and dilations have large receptive fields. For streaming, the cached state must be large enough to cover the full receptive field. Calculate the exact cache size needed for each layer: $\text{cache}_l = (\text{kernel\_size}_l - 1) \times \text{dilation}_l$.</li>
                                </ul>
                                <p><strong>(c) Minimum theoretical latency:</strong></p>
                                <p>The minimum latency is determined by: (1) chunk size: minimum 1 mel frame = $256 / 22050 \approx 11.6$ ms, (2) computation time: HiFi-GAN v1 processes ~120&times; real-time, so 11.6 ms of audio takes ~0.1 ms to compute, (3) buffering overhead: ~1&ndash;2 ms for I/O. Total minimum: ~13&ndash;15 ms. In practice, using chunks of 4 mel frames gives latency ~50 ms with more efficient GPU utilization (less overhead per chunk), which is well within the perceptual threshold.</p>
                                <p><strong>(d) Handling boundary artifacts:</strong></p>
                                <p>With proper state caching, there should be no boundary artifacts &mdash; the causal convolution output is mathematically identical whether computed in one pass or in chunks with cached states. However, two practical issues arise: (1) <strong>Transposed convolution boundaries:</strong> The stride in transposed convolutions can cause phase discontinuities if the chunk boundary falls at an awkward position. Solution: always chunk on mel frame boundaries (which align with the upsampling structure). (2) <strong>Warm-up period:</strong> The first chunk lacks past context. Solution: use a "look-ahead" of 2&ndash;4 mel frames before starting audio output, adding ~25&ndash;50 ms of initial latency but ensuring full receptive field coverage from the start.</p>
                            </div>
                        </div>
                    </div>

                </div>
            </article>

        </main>

        <!-- TOC (Right Side) -->
        <aside class="toc-container">
            <h4 class="toc-title">Contents</h4>
            <nav class="toc-list">
                <a href="#vocoder-problem" class="toc-link">The Vocoder Problem</a>
                <a href="#wavenet" class="toc-link">WaveNet</a>
                <a href="#speed-problem" class="toc-link">The Speed Problem</a>
                <a href="#gan-vocoders" class="toc-link">GAN-Based Vocoders</a>
                <a href="#hifigan" class="toc-link">HiFi-GAN Architecture</a>
                <a href="#hifigan-training" class="toc-link">HiFi-GAN Training</a>
                <a href="#universal-vocoders" class="toc-link">Universal vs Speaker-Specific</a>
                <a href="#vocoder-comparison" class="toc-link">Vocoder Comparison</a>
            </nav>
        </aside>
    </div>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-brand">
                    <span class="logo-symbol">&nabla;</span>
                    <span>ML Fundamentals</span>
                </div>
                <p class="footer-tagline">Deep understanding through first principles.</p>
            </div>
            <div class="footer-links">
                <a href="../../../index.html">Home</a>
                <a href="https://github.com/ml-entropy/ml-entropy.github.io" target="_blank">GitHub</a>
            </div>
        </div>
    </footer>

    <script src="../../../js/main.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            if (typeof renderMathInElement === 'function') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }

            const tabs = document.querySelectorAll('.tutorial-tab');
            const articles = document.querySelectorAll('.article-content');

            function switchTab(targetId) {
                if (!targetId || targetId === '#') targetId = '#theory';
                tabs.forEach(tab => {
                    if (tab.getAttribute('href') === targetId) {
                        tab.classList.add('active');
                    } else {
                        tab.classList.remove('active');
                    }
                });
                articles.forEach(article => {
                    const articleId = '#' + article.id;
                    if (articleId === targetId) {
                        article.style.display = 'block';
                    } else {
                        article.style.display = 'none';
                    }
                });
                if (typeof renderMathInElement === 'function') {
                    renderMathInElement(document.body, {
                        delimiters: [
                            {left: '$$', right: '$$', display: true},
                            {left: '$', right: '$', display: false},
                            {left: '\\[', right: '\\]', display: true},
                            {left: '\\(', right: '\\)', display: false}
                        ],
                        throwOnError: false
                    });
                }
                const toc = document.querySelector('.toc-container');
                if (toc) {
                    if (targetId === '#theory') {
                        toc.style.display = 'block';
                        setTimeout(() => toc.classList.add('visible'), 100);
                    } else {
                        toc.classList.remove('visible');
                        setTimeout(() => toc.style.display = 'none', 300);
                    }
                }
            }

            tabs.forEach(tab => {
                tab.addEventListener('click', (e) => {
                    e.preventDefault();
                    const targetId = tab.getAttribute('href');
                    history.pushState(null, null, targetId);
                    switchTab(targetId);
                });
            });

            window.addEventListener('popstate', () => {
                switchTab(window.location.hash);
            });

            switchTab(window.location.hash);
        });
    </script>
</body>
</html>