<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Exercises: Cross-Entropy - Opus Tutorials</title>
    <meta name="description" content="Practice exercises for Shannon entropy, Huffman coding, and information theory concepts.">
    
    <!-- Fonts (System fonts used via CSS) -->
    
    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}, {left: '\[', right: '\]', display: true}, {left: '\(', right: '\)', display: false}], throwOnError: false});"></script>
    
    <!-- Styles -->
    <link rel="stylesheet" href="../../../css/main.css">
    <link rel="stylesheet" href="../../../css/components.css">
    <link rel="stylesheet" href="../../../css/sidebar.css">
    <link rel="stylesheet" href="../../../css/style.css">
    
    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>∞</text></svg>">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar" id="navbar">
        <div class="nav-container">
            <a href="../../../index.html" class="nav-logo">
                <span class="logo-symbol">∇</span>
                <span class="logo-text">ML Fundamentals</span>
            </a>
            
            <button class="nav-toggle" id="navToggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            
            <div class="nav-menu" id="navMenu">
                <div class="nav-links">
                    <a href="../../../tutorials/ml/index.html" class="nav-link active">Machine Learning</a>
                    <a href="../../../tutorials/linear-algebra/index.html" class="nav-link">Linear Algebra</a>
                    <a href="../../../tutorials/calculus/index.html" class="nav-link">Calculus</a>
                    <a href="../../../tutorials/physics/index.html" class="nav-link">Physics</a>
                    <a href="../../../index.html#philosophy" class="nav-link">Philosophy</a>
                    <a href="../../../index.html#roadmap" class="nav-link">Roadmap</a>
                    <a href="https://github.com/ml-entropy/ml-entropy.github.io" class="nav-link" target="_blank">GitHub</a>
                </div>
                
                <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
                    <svg class="sun-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="5"/>
                        <path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"/>
                    </svg>
                    <svg class="moon-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                    </svg>
                </button>
            </div>
        </div>
    </nav>

    <!-- Tutorial Header -->
    <header class="tutorial-content-header">
        <div class="container">
            <nav class="breadcrumb">
                <a href="../../../index.html">Home</a>
                <span class="breadcrumb-separator">→</span>
                <a href="../index.html">Machine Learning</a>
                <span class="breadcrumb-separator">→</span>
                <span>Entropy Exercises</span>
            </nav>
            
            <h1>Cross-Entropy</h1>
            <p class="lead">
                Practice exercises to master entropy, surprise, and coding theory.
            </p>
            
            <div class="tutorial-tabs">
                <a href="index.html" class="tutorial-tab">Theory</a>
                <a href="index.html#code" class="tutorial-tab">Code</a>
                <a href="#" class="tutorial-tab active">Exercises</a>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <div class="tutorial-wrapper">
        <!-- Sidebar Navigation -->
        <aside class="tutorial-sidebar">
            <div class="sidebar-section">
                <h3 class="sidebar-section-title">Machine Learning</h3>
                <nav class="sidebar-nav">
                        <a href="../00-probability/index.html" class="sidebar-link">00. Probability Foundations</a>
                    <a href="../01-entropy/index.html" class="sidebar-link active">01. Entropy Fundamentals</a>
                    <a href="../02-kl-divergence/index.html" class="sidebar-link">02. KL Divergence</a>
                    <a href="../03-distributions/index.html" class="sidebar-link">03. Normal Distributions</a>
                    <a href="../04-logarithms/index.html" class="sidebar-link">04. Why Logarithms?</a>
                    <a href="../05-combinatorics/index.html" class="sidebar-link">05. Combinatorics</a>
                    <a href="../06-backpropagation/index.html" class="sidebar-link">06. Backpropagation</a>
                    <a href="../07-regularization/index.html" class="sidebar-link">07. Regularization</a>
                    <a href="../08-batch-normalization/index.html" class="sidebar-link">08. Batch Normalization</a>
                    <a href="../09-learning-rate/index.html" class="sidebar-link">09. Learning Rate</a>
                    <a href="../10-cnn/index.html" class="sidebar-link">10. CNNs</a>
                    <a href="../11-rnn/index.html" class="sidebar-link">11. RNNs</a>
                    <a href="../12-vae/index.html" class="sidebar-link">12. VAE</a>
                    <a href="../13-variational-inference/index.html" class="sidebar-link">13. Variational Inference</a>
                </nav>
            </div>
            
            <div class="sidebar-section" style="margin-top: 2rem;">
                <h3 class="sidebar-section-title">Related Subjects</h3>
                <nav class="sidebar-nav">
                        <a href="../../linear-algebra/index.html" class="sidebar-link">Linear Algebra</a>
                    <a href="../../calculus/index.html" class="sidebar-link">Calculus</a>
                    <a href="../../physics/index.html" class="sidebar-link">Physics</a>
                </nav>
            </div>
        </aside>

        <!-- Main Article -->
        <main class="tutorial-main">

            <section id="exercises">
                <h2 style="margin-bottom: 1.5rem;">Practice Exercises</h2>
                
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>A1. — Definition in Your Own Words</h3>
                        <span class="difficulty-badge diff-medium">Practice</span>
                    </div>
                    <p>Explain what Cross-Entropy measures from an information theory perspective. Why is it a useful metric for training a machine learning model?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>From an information theory perspective, Cross-Entropy is the average number of bits required to encode data coming from a true distribution $P$ when we use a code optimized for a different, approximate distribution $Q$.</p>

<p>It's useful for machine learning because it provides a measure of how "bad" our model's predictions ($Q$) are at describing the true data labels ($P$). A high cross-entropy means our model is a poor approximation of reality, resulting in a high loss. Minimizing cross-entropy forces the model's predictions to get closer to the true distribution.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>A2. — The Loss Formula</h3>
                        <span class="difficulty-badge diff-medium">Practice</span>
                    </div>
                    <p>In a multi-class classification setting, the true label is represented by a one-hot vector $y$ and the model's predictions by a probability vector $\hat{y}$. Show that the general Cross-Entropy formula $H(y, \hat{y}) = -\sum_i y_i \log \hat{y}_i$ simplifies to just the negative logarithm of the predicted probability for the correct class.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>Let the true label be class $k$. The one-hot vector $y$ will have $y_k = 1$ and $y_i = 0$ for all $i \neq k$.</p>

<p>The Cross-Entropy formula is:
<div class="math-block">$$ H(y, \hat{y}) = -\sum_{i=1}^{C} y_i \log \hat{y}_i $$</div>
where $C$ is the number of classes.</p>

<p>When we expand the sum, every term where $y_i = 0$ is multiplied by zero, so it disappears:
<div class="math-block">$$ = - (y_1 \log \hat{y}_1 + \dots + y_k \log \hat{y}_k + \dots + y_C \log \hat{y}_C) $$</div>
<div class="math-block">$$ = - (0 \cdot \log \hat{y}_1 + \dots + 1 \cdot \log \hat{y}_k + \dots + 0 \cdot \log \hat{y}_C) $$</div>
<div class="math-block">$$ = - \log \hat{y}_k $$</div>
This shows that the loss simplifies to the negative logarithm of the probability assigned to the single correct class.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>A3. — Manual Calculation</h3>
                        <span class="difficulty-badge diff-medium">Practice</span>
                    </div>
                    <p>A model is trained to classify images into three categories: {Apple, Banana, Cherry}. For an image of a Banana, the model outputs the following probabilities:
<ul>
<li>  $Q(\text{Apple}) = 0.2$</li>
<li>  $Q(\text{Banana}) = 0.5$</li>
<li>  $Q(\text{Cherry}) = 0.3$</li>
</ul></p>

<p>a) Write down the true probability distribution $P$.
b) Calculate the Cross-Entropy loss for this single prediction.
c) What would the loss be if the model was "perfectly confident" and correct, i.e., $Q(\text{Banana}) = 1$?
d) What would the loss be if the model was "perfectly confident" and incorrect, e.g., $Q(\text{Apple}) = 1$?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>a) <strong>True Distribution P</strong>: The image is a Banana, so the true distribution is a one-hot vector:
$P = \{\text{Apple: } 0, \text{Banana: } 1, \text{Cherry: } 0\}$</p>

<p>b) <strong>Cross-Entropy Loss</strong>:
Using the simplification from A2, the loss is just the negative log of the probability for the correct class (Banana).
<div class="math-block">$$ \text{Loss} = - \log(Q(\text{Banana})) = - \log(0.5) \approx \boxed{0.693} $$</div>
(Using natural log, which is standard in ML frameworks). If using log base 2, the answer would be 1 bit.</p>

<p>c) <strong>Perfectly Confident and Correct</strong>: If $Q(\text{Banana}) = 1$:
<div class="math-block">$$ \text{Loss} = - \log(1) = \boxed{0} $$</div>
A perfect prediction results in zero loss.</p>

<p>d) <strong>Perfectly Confident and Incorrect</strong>: If $Q(\text{Apple}) = 1$, then $Q(\text{Banana})$ must be 0 (or a very small number, for stability).
<div class="math-block">$$ \text{Loss} = - \log(Q(\text{Banana})) = - \log(0) = \boxed{\infty} $$</div>
The loss is infinite, indicating an extremely high penalty for being confidently wrong.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>A4. — Cross-Entropy vs. MSE</h3>
                        <span class="difficulty-badge diff-medium">Practice</span>
                    </div>
                    <p>Consider a binary classification problem where the true label is 1. A model with a sigmoid activation outputs a prediction $p$.
a) Write down the formula for Binary Cross-Entropy (BCE) loss.
b) Write down the formula for Mean Squared Error (MSE) loss.
c) Calculate the gradient of both the BCE and MSE loss with respect to the model's logit (the input to the sigmoid).
d) Explain why the BCE gradient is generally better for training than the MSE gradient, especially when the model's prediction is confidently wrong (e.g., $p$ is close to 0).</p>

<p>---</p>

<p>## Part B: Coding</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>a) <strong>BCE Loss</strong>: $L_{BCE} = - (y \log(p) + (1-y) \log(1-p))$. For $y=1$, this is $L_{BCE} = -\log(p)$.
b) <strong>MSE Loss</strong>: $L_{MSE} = (y - p)^2$. For $y=1$, this is $L_{MSE} = (1 - p)^2$.</p>

<p>c) <strong>Gradients</strong>: Let $z$ be the logit, so $p = \sigma(z) = 1 / (1 + e^{-z})$. The derivative of the sigmoid is $\sigma'(z) = \sigma(z)(1-\sigma(z)) = p(1-p)$.</p>

<ul>
<li>  <strong>BCE Gradient</strong>:</li>
</ul>
    <div class="math-block">$$ \frac{\partial L_{BCE}}{\partial z} = \frac{\partial L_{BCE}}{\partial p} \frac{\partial p}{\partial z} = \left(-\frac{1}{p}\right) \cdot (p(1-p)) = -(1-p) = p - 1 $$</div>
<ul>
<li>  <strong>MSE Gradient</strong>:</li>
</ul>
    <div class="math-block">$$ \frac{\partial L_{MSE}}{\partial z} = \frac{\partial L_{MSE}}{\partial p} \frac{\partial p}{\partial z} = (-2(1-p)) \cdot (p(1-p)) = -2(1-p)^2 p $$</div>

<p>d) <strong>Why BCE is better</strong>: Look at the gradients when the model is confidently wrong. If the true label is 1, a confidently wrong prediction means $p \to 0$.
<ul>
<li>  <strong>BCE Gradient</strong>: As $p \to 0$, the gradient $\frac{\partial L_{BCE}}{\partial z} \to -1$. It's a strong, constant signal to update the weights.</li>
<li>  <strong>MSE Gradient</strong>: As $p \to 0$, the gradient $\frac{\partial L_{MSE}}{\partial z} \to 0$. The gradient vanishes! The model learns extremely slowly when it needs to learn the most. This is known as the "vanishing gradient" problem for MSE in classification.</li>
</ul></p>

<p>---</p>

<p>## Part B: Coding Solutions</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>B1. — Cross-Entropy Function</h3>
                        <span class="difficulty-badge diff-medium">Practice</span>
                    </div>
                    <p>Write a Python function <code>cross_entropy(P, Q)</code> that takes two NumPy arrays representing the true and predicted distributions and returns the cross-entropy.
<ul>
<li>  Verify your function using the values from Exercise A3.</li>
<li>  What happens if a value in <code>Q</code> is exactly 0? How can you make your function numerically stable?</li>
</ul></p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <pre><code>python
import numpy as np

<p>def cross_entropy(P, Q):
    """
    Calculates the cross-entropy between two probability distributions.
    
    Args:
        P (np.array): The true distribution (one-hot).
        Q (np.array): The predicted distribution.
        
    Returns:
        float: The cross-entropy loss.
    """
    # Add a small epsilon for numerical stability to prevent log(0)
    epsilon = 1e-9
    Q = np.clip(Q, epsilon, 1. - epsilon)
    
    return -np.sum(P * np.log(Q))</p>

<p># Verification from A3
P_banana = np.array([0, 1, 0])
Q_banana = np.array([0.2, 0.5, 0.3])</p>

<p>loss = cross_entropy(P_banana, Q_banana)
print(f"Calculated Loss: {loss:.3f}")
print(f"Expected Loss: {-np.log(0.5):.3f}")</p>

<p># What happens if Q has a zero?
Q_zero = np.array([0.2, 0, 0.8])
# Without epsilon, this would cause a <code>RuntimeWarning: divide by zero in log</code>
# and result in <code>inf</code>. Our stable function handles it.
loss_stable = cross_entropy(P_banana, Q_zero)
print(f"Loss with a zero prediction (stable): {loss_stable:.3f}")
</code></pre></p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>B2. — Visualizing the Loss</h3>
                        <span class="difficulty-badge diff-medium">Practice</span>
                    </div>
                    <p>Create a plot to visualize the Cross-Entropy loss for a binary classification problem.
<ul>
<li>  The true label is 1.</li>
<li>  Plot the loss as the model's predicted probability for class 1 ranges from 0.01 to 0.99.</li>
<li>  On the same graph, plot the MSE loss.</li>
<li>  Label your axes and curves clearly. What does this visualization tell you about the behavior of the two loss functions?</li>
</ul></p>

<p>---</p>

<p>## Part C: Conceptual Deep Dive</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <pre><code>python
import matplotlib.pyplot as plt

<p># Predicted probabilities for the correct class (y=1)
p = np.linspace(0.01, 0.99, 100)</p>

<p># BCE Loss for y=1 is -log(p)
bce_loss = -np.log(p)</p>

<p># MSE Loss for y=1 is (1-p)^2
mse_loss = (1 - p)<em></em>2</p>

<p>plt.figure(figsize=(10, 6))
plt.plot(p, bce_loss, label='Cross-Entropy Loss', lw=2)
plt.plot(p, mse_loss, label='Mean Squared Error Loss', lw=2)
plt.xlabel("Predicted Probability for Correct Class (p)")
plt.ylabel("Loss")
plt.title("BCE vs. MSE Loss for a Correct Label of 1")
plt.legend()
plt.grid(True, linestyle='--', alpha=0.6)
plt.show()
</code></pre>
<strong>Interpretation</strong>: The visualization clearly shows that as the prediction <code>p</code> gets confidently wrong (approaches 0), the Cross-Entropy loss shoots up towards infinity, creating a powerful gradient. The MSE loss, in contrast, flattens out, leading to a vanishing gradient and slow learning.</p>

<p>---</p>

<p>## Part C: Conceptual Solutions</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>C1. — Label Smoothing</h3>
                        <span class="difficulty-badge diff-medium">Practice</span>
                    </div>
                    <p>Label smoothing is a regularization technique where the one-hot encoded labels are "softened." For example, for a 3-class problem, a true label of <code>[0, 1, 0]</code> might be changed to <code>[0.05, 0.9, 0.05]</code>.
a) How does this change the calculation of the Cross-Entropy loss?
b) What is the goal of label smoothing? How does it prevent a model from becoming "overconfident"?
c) What is the relationship between label smoothing and the KL Divergence between the model's predictions and the smoothed labels?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>a) <strong>How it changes the loss</strong>: Instead of having only one non-zero term in the cross-entropy sum, all terms will be non-zero. The model is now penalized for being too confident even in its correct predictions, as it must also assign some probability mass to the other classes. For $y_{smooth} = [0.05, 0.9, 0.05]$ and $\hat{y} = [0.1, 0.8, 0.1]$, the loss is now:
<code>-(0.05<em>log(0.1) + 0.9</em>log(0.8) + 0.05*log(0.1))</code> instead of just <code>-log(0.8)</code>.</p>

<p>b) <strong>Goal</strong>: The goal is to prevent the model from becoming overconfident. By forcing it to be "less sure" (i.e., never predicting a hard 0 or 1), it creates a softer decision boundary and can improve generalization and calibration. It regularizes the model by adding noise to the labels.</p>

<p>c) <strong>Relationship to KL Divergence</strong>: Minimizing the cross-entropy $H(y_{smooth}, \hat{y})$ is equivalent to minimizing the KL divergence $D_{KL}(y_{smooth} || \hat{y})$. Label smoothing, therefore, encourages the model's output distribution $\hat{y}$ to stay close to the smoothed target distribution, not a one-hot distribution.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>C2. — The Role of the Logarithm</h3>
                        <span class="difficulty-badge diff-medium">Practice</span>
                    </div>
                    <p>We've established that Cross-Entropy is $-\sum P(x) \log Q(x)$. Imagine an alternative universe where we defined a "Linear-Entropy" as $-\sum P(x) Q(x)$.
<ul>
<li>  Would this be a valid loss function?</li>
<li>  What desirable properties of the logarithmic loss would be lost? Think about the penalty for wrong predictions and the resulting gradients.</li>
</ul></p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <ul>
<li>  <strong>Is it a valid loss?</strong>: Yes, "Linear-Entropy" or $-\sum P(x) Q(x)$ would be a valid loss function in the sense that it is minimized when $Q(x)$ is 1 for the correct class (assuming $P$ is one-hot). This is related to the dot product, maximizing the alignment between the prediction and true vectors.</li>
</ul>

<ul>
<li>  <strong>What is lost?</strong>: The most critical property of the logarithm is lost: <strong>the infinite penalty for confident failure</strong>.</li>
<li>  With $-\log Q(x)$, if $Q(x) \to 0$, the loss $\to \infty$.</li>
<li>  With $-Q(x)$, if $Q(x) \to 0$, the loss $\to 0$.</li>
</ul>
    This is the opposite of what we want! A linear loss would not heavily penalize a model for being confidently wrong. The gradients would be small and constant, lacking the powerful error signal that makes cross-entropy so effective. The logarithmic scale correctly maps the multiplicative nature of probabilities to an additive, unbounded scale of "surprise" or "cost".
                        </div>
                    </details>
                </div>

                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>A5. — Focal Loss Calculation</h3>
                        <span class="difficulty-badge diff-medium">Practice</span>
                    </div>
                    <p>Focal Loss is defined as $FL(p_t) = -(1 - p_t)^\gamma \log(p_t)$, where $p_t$ is the probability of the true class.
                    <br>a) Calculate the Cross-Entropy loss ($\gamma=0$) for an "easy" example where $p_t = 0.9$.
                    <br>b) Calculate the Cross-Entropy loss for a "hard" example where $p_t = 0.1$.
                    <br>c) Calculate the Focal Loss with $\gamma=2$ for the same "easy" and "hard" examples.
                    <br>d) By what factor did the loss decrease for the easy example compared to the hard example when switching from CE to FL?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>a) <strong>Easy Example (p=0.9, CE)</strong>: $L_{CE} = - \ln(0.9) \approx 0.105$</p>
                            <p>b) <strong>Hard Example (p=0.1, CE)</strong>: $L_{CE} = - \ln(0.1) \approx 2.302$ (Ratio Hard/Easy $\approx 22$).</p>
                            <p>c) <strong>Focal Loss ($\gamma=2$)</strong>:
                            <br>- <strong>Easy</strong>: $L_{FL} = -(1 - 0.9)^2 \ln(0.9) = -(0.01) \cdot (-0.105) \approx 0.001$
                            <br>- <strong>Hard</strong>: $L_{FL} = -(1 - 0.1)^2 \ln(0.1) = -(0.81) \cdot (-2.302) \approx 1.865$</p>
                            <p>d) <strong>Comparison</strong>:
                            <br>- For the easy example, the loss dropped from $0.105$ to $0.001$ (factor of ~100x).
                            <br>- For the hard example, the loss dropped from $2.302$ to $1.865$ (factor of ~1.2x).
                            <br>- Focal Loss heavily penalizes/suppresses the easy examples, keeping the focus on the hard ones.</p>
                        </div>
                    </details>
                </div>

                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>B3. — Numerical Stability (LogSumExp)</h3>
                        <span class="difficulty-badge diff-hard">Challenge</span>
                    </div>
                    <p>Implement a function <code>stable_softmax_cross_entropy(logits, target_index)</code> that:
                    <br>1. Takes raw logits (not probabilities) as input.
                    <br>2. Computes the Cross-Entropy loss using the <strong>LogSumExp</strong> trick to avoid overflow/underflow.
                    <br>Formula: $\log(\sum e^{x_i}) = c + \log(\sum e^{x_i - c})$, where $c = \max(x)$.
                    <br>Loss: $-x_{target} + \log(\sum e^{x_j})$.
                    <br>3. Test it with <code>logits = [1000, 1001, 1002]</code> and <code>target_index = 1</code>.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
<pre><code>python
import numpy as np

def stable_softmax_cross_entropy(logits, target_index):
    # 1. LogSumExp Trick
    c = np.max(logits)
    # log(sum(exp(x))) = c + log(sum(exp(x - c)))
    log_sum_exp = c + np.log(np.sum(np.exp(logits - c)))
    
    # 2. Loss = -log(softmax(x_target))
    #        = - (x_target - log(sum(exp(x))))
    #        = -x_target + log_sum_exp
    loss = -logits[target_index] + log_sum_exp
    return loss

# Test
logits_large = np.array([1000.0, 1001.0, 1002.0])
target = 1

# Stable works
stable_loss = stable_softmax_cross_entropy(logits_large, target)
print(f"Stable Loss: {stable_loss:.4f}")
# Expected: ~1.4076 (similar to [0, 1, 2])
</code></pre>
                        </div>
                    </details>
                </div>

                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>D1. — Softmax Derivative</h3>
                        <span class="difficulty-badge diff-hard">Challenge</span>
                    </div>
                    <p>Derive the gradient of the Cross-Entropy loss with respect to the raw logits $z_i$, where $p_i = \text{Softmax}(z_i)$. show that $\frac{\partial L}{\partial z_i} = p_i - y_i$.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>Let $L = -\sum_k y_k \log p_k$. We want $\frac{\partial L}{\partial z_i}$.</p>
                            <p>Using the chain rule: $\frac{\partial L}{\partial z_i} = \sum_j \frac{\partial L}{\partial p_j} \frac{\partial p_j}{\partial z_i}$.</p>
                            <p>1. $\frac{\partial L}{\partial p_j} = -\frac{y_j}{p_j}$.</p>
                            <p>2. The derivative of Softmax is: $\frac{\partial p_j}{\partial z_i} = p_j(\delta_{ij} - p_i)$.</p>
                            <p>Combining terms:</p>
                            <div class="math-block">$$ \frac{\partial L}{\partial z_i} = -\sum_j \frac{y_j}{p_j} p_j(\delta_{ij} - p_i) $$</div>
                            <div class="math-block">$$ = -\sum_j y_j (\delta_{ij} - p_i) $$</div>
                            <div class="math-block">$$ = - (y_i(1 - p_i) + \sum_{j \neq i} y_j (-p_i)) $$</div>
                            <div class="math-block">$$ = - (y_i - y_i p_i - p_i \sum_{j \neq i} y_j) $$</div>
                            <p>Since $\sum_j y_j = 1$ (one-hot), then $\sum_{j \neq i} y_j = 1 - y_i$.</p>
                            <div class="math-block">$$ = - (y_i - y_i p_i - p_i(1 - y_i)) $$</div>
                            <div class="math-block">$$ = - (y_i - y_i p_i - p_i + y_i p_i) $$</div>
                            <div class="math-block">$$ = - (y_i - p_i) = p_i - y_i $$</div>
                        </div>
                    </details>
                </div>
            </section>
    
</main>
    </div>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-brand">
                    <span class="logo-symbol">∇</span>
                    <span>ML Fundamentals</span>
                </div>
                <p class="footer-tagline">Deep understanding through first principles.</p>
            </div>
            <div class="footer-links">
                <a href="../../../index.html">Home</a>
                <a href="https://github.com/ml-entropy/ml-entropy.github.io" target="_blank">GitHub</a>
            </div>
        </div>
    </footer>

    <!-- Scripts -->
    <script src="../../../js/main.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            if (typeof renderMathInElement === 'function') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\[', right: '\]', display: true},
                        {left: '\(', right: '\)', display: false}
                    ],
                    throwOnError: false
                });
            }
        });
    </script>
</body>
</html>
