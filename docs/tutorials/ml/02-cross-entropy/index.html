<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cross-Entropy Loss - Opus Tutorials</title>
    <meta name="description" content="Master Cross-Entropy Loss: The mathematical engine behind classification models. Detailed derivation, Python implementation, and intuitive visualizations.">
    
    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&family=Playfair+Display:wght@400;500;600;700&display=swap" rel="stylesheet">
    
    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}, {left: '\\[', right: '\\]', display: true}, {left: '\\(', right: '\\)', display: false}], throwOnError: false});"></script>
    
    <!-- Styles -->
    <link rel="stylesheet" href="../../../css/main.css">
    <link rel="stylesheet" href="../../../css/components.css">
    <link rel="stylesheet" href="../../../css/sidebar.css">
    
    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>∞</text></svg>">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar" id="navbar">
        <div class="nav-container">
            <a href="../../../index.html" class="nav-logo">
                <span class="logo-symbol">∇</span>
                <span class="logo-text">ML Fundamentals</span>
            </a>
            
            <button class="nav-toggle" id="navToggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            
            <div class="nav-menu" id="navMenu">
                <div class="nav-links">
                    <a href="../../../tutorials/ml/index.html" class="nav-link active">Machine Learning</a>
                    <a href="../../../tutorials/linear-algebra/index.html" class="nav-link">Linear Algebra</a>
                    <a href="../../../tutorials/calculus/index.html" class="nav-link">Calculus</a>
                    <a href="../../../tutorials/physics/index.html" class="nav-link">Physics</a>
                    <a href="../../../index.html#philosophy" class="nav-link">Philosophy</a>
                    <a href="../../../index.html#roadmap" class="nav-link">Roadmap</a>
                    <a href="https://github.com/ml-entropy/ml-entropy.github.io" class="nav-link" target="_blank">GitHub</a>
                </div>
                
                <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
                    <svg class="sun-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="5"/>
                        <path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"/>
                    </svg>
                    <svg class="moon-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                    </svg>
                </button>
            </div>
        </div>
    </nav>

    <!-- Tutorial Header -->
    <header class="tutorial-content-header">
        <div class="container">
            <nav class="breadcrumb">
                <a href="../../../index.html">Home</a>
                <span class="breadcrumb-separator">→</span>
                <a href="../index.html">Machine Learning</a>
                <span class="breadcrumb-separator">→</span>
                <span>Cross-Entropy</span>
            </nav>
            
            
            
            
            <div class="tutorial-tabs">
                <a href="#theory" class="tutorial-tab active">Theory</a>
                <a href="cross_entropy.ipynb" class="tutorial-tab">Code</a>
                <a href="exercises.html" class="tutorial-tab">Exercises</a>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <div class="tutorial-wrapper">
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <aside class="tutorial-sidebar">
            <div class="sidebar-section">
                <h3 class="sidebar-section-title">Machine Learning</h3>
                <nav class="sidebar-nav">
                        <a href="../00-probability/index.html" class="sidebar-link">00. Probability Foundations</a>
                    <a href="../01-entropy/index.html" class="sidebar-link">01. Entropy Fundamentals</a>
                    <a href="../02-cross-entropy/index.html" class="sidebar-link active">02. Cross-Entropy</a>
                    <a href="../02-kl-divergence/index.html" class="sidebar-link">03. KL Divergence</a>
                    <a href="../03-distributions/index.html" class="sidebar-link">04. Normal Distributions</a>
                    <a href="../04-logarithms/index.html" class="sidebar-link">05. Why Logarithms?</a>
                    <a href="../05-combinatorics/index.html" class="sidebar-link">06. Combinatorics</a>
                    <a href="../06-backpropagation/index.html" class="sidebar-link">07. Backpropagation</a>
                    <a href="../07-regularization/index.html" class="sidebar-link">08. Regularization</a>
                    <a href="../08-batch-normalization/index.html" class="sidebar-link">09. Batch Normalization</a>
                    <a href="../09-learning-rate/index.html" class="sidebar-link">10. Learning Rate</a>
                    <a href="../10-cnn/index.html" class="sidebar-link">11. CNNs</a>
                    <a href="../11-rnn/index.html" class="sidebar-link">12. RNNs</a>
                    <a href="../12-vae/index.html" class="sidebar-link">13. VAE</a>
                    <a href="../13-variational-inference/index.html" class="sidebar-link">14. Variational Inference</a>
                    <a href="../14-entropy-connections/index.html" class="sidebar-link">15. Entropy Connections</a>
                </nav>
            </div>
            
            <div class="sidebar-section" style="margin-top: 2rem;">
                <h3 class="sidebar-section-title">Related Subjects</h3>
                <nav class="sidebar-nav">
                        <a href="../../linear-algebra/index.html" class="sidebar-link">Linear Algebra</a>
                    <a href="../../calculus/index.html" class="sidebar-link">Calculus</a>
                    <a href="../../physics/index.html" class="sidebar-link">Physics</a>
                </nav>
            </div>
        </aside>

        <!-- Main Article -->
        <main class="tutorial-main">
            <section id="introduction">
                <h2>1. The Intuition: Calculating the Cost of Surprise</h2>
                <p>
                    Imagine you are a weather forecaster. You predict there is a <strong>90% chance of sun</strong> tomorrow.
                </p>
                <ul>
                    <li>If it shines, you are happy. Your model was right.</li>
                    <li>If it rains, you are shocked! Your model was wrong.</li>
                </ul>
                <p>
                    <strong>Cross-Entropy</strong> provides a mathematical way to quantify this "shock." It measures the distance between what you predicted (your probability distribution $Q$) and what actually happened (the true distribution $P$).
                </p>
                <div class="definition-box">
                    <strong>Definition:</strong> Cross-Entropy measures the average number of bits needed to identify an event from a distribution $P$, if we use a coding scheme optimized for a different distribution $Q$.
                </div>
            </section>

            <section id="history">
                <h2>3. Historical Context: From Steam Engines to AI</h2>
                <p>
                    The concept of entropy has a fascinating lineage, bridging two seemingly unrelated fields: 
                    <strong>Thermodynamics</strong> and <strong>Information Theory</strong>.
                </p>
                
                <h3>1865: Rudolf Clausius & Thermodynamics</h3>
                <p>
                    The story begins with <strong>Rudolf Clausius</strong>, who coined the term "entropy" to quantify the amount of 
                    thermal energy unavailable to do work. In simple terms, it measured the <strong>disorder</strong> of a physical system.
                </p>
                
                <h3>1870s: Ludwig Boltzmann & Microstates</h3>
                <p>
                    <strong>Ludwig Boltzmann</strong> gave entropy a statistical definition. He showed that entropy is related to the number of 
                    microscopic configurations (microstates) that correspond to a macroscopic state (like temperature). 
                    His famous equation, carved on his tombstone, is:
                </p>
                <div class="math-block">$$ S = k_B \ln W $$</div>
                
                <h3>1948: Claude Shannon & The Bit</h3>
                <p>
                    Decades later, <strong>Claude Shannon</strong> was working at Bell Labs on the problem of efficient communication. 
                    He needed a measure for the "uncertainty" in a message source. When he derived his formula, he allegedly consulted 
                    <strong>John von Neumann</strong>, who advised him:
                </p>
                <div class="note-box">
                    <p style="font-style: italic;">
                        "You should call it entropy, for two reasons. In the first place your uncertainty function has been used in statistical mechanics under that name, so it already has a name. In the second place, and more important, no one knows what entropy really is, so in a debate you will always have the advantage."
                    </p>
                </div>
                
                <h3>1951: Kullback & Leibler</h3>
                <p>
                    <strong>Solomon Kullback</strong> and <strong>Richard Leibler</strong> generalized Shannon's work to measure the difference 
                    between two probability distributions. Their measure, initially called "directed divergence," is what we now know as 
                    <strong>KL Divergence</strong>.
                </p>
                
                <h3>Modern Era: The Cross-Entropy Loss</h3>
                <p>
                    In the 1990s and 2000s, as neural networks gained popularity, researchers needed a differentiable loss function 
                    for classification. While Mean Squared Error (MSE) worked for regression, it caused "vanishing gradients" with 
                    softmax outputs. <strong>Cross-Entropy</strong> emerged as the standard because its gradient properties perfectly 
                    counteract the saturation of sigmoid/softmax functions, enabling deep learning models to learn efficiently.
                </p>
            </section>


            <section id="formula">
                <h2>3. The Formula</h2>
                <p>
                    For discrete probability distributions $P$ (the truth) and $Q$ (the prediction), the cross-entropy $H(P, Q)$ is defined as:
                </p>
                <div class="math-block">
                    $$ H(P, Q) = - \sum_{x} P(x) \log Q(x) $$
                </div>
                
                <h3>Breaking it Down</h3>
                <p>In supervised learning (classification), the "True Distribution" $P$ is usually a <strong>One-Hot Vector</strong>. If the image is a Cat, then:</p>
                <ul>
                    <li>$P(\text{Cat}) = 1$</li>
                    <li>$P(\text{Dog}) = 0$</li>
                    <li>$P(\text{Bird}) = 0$</li>
                </ul>
                <p>This simplifies the summation dramatically. Since $P(x)$ is 0 for all incorrect classes, those terms vanish. We are left with only the term for the correct class:</p>
                <div class="math-block">
                    $$ H(P, Q) = - \log Q(\text{Correct Class}) $$
                </div>
                <p>This is effectively the <strong>Negative Log Likelihood</strong>.</p>
            </section>

            <section id="visualization">
                <h2>4. Visualizing the Loss</h2>
                <p>Why do we use the logarithm? Why not just use $(1 - p)$?</p>
                <p>The logarithmic scale is crucial because it penalizes <strong>confident errors</strong> exponentially.</p>
                
                <!-- SVG Plot -->
                <div class="chart-container" style="display: flex; justify-content: center; margin: 2rem 0;">
                    <svg viewBox="0 0 500 300" style="max-width: 100%; height: auto; background: #fff; border: 1px solid #eee; border-radius: 8px;">
                        <!-- Grid -->
                        <defs>
                            <pattern id="grid" width="50" height="50" patternUnits="userSpaceOnUse">
                                <path d="M 50 0 L 0 0 0 50" fill="none" stroke="#f0f0f0" stroke-width="1"/>
                            </pattern>
                        </defs>
                        <rect width="500" height="300" fill="url(#grid)" />
                        
                        <!-- Axes -->
                        <line x1="50" y1="250" x2="450" y2="250" stroke="#333" stroke-width="2" /> <!-- X Axis -->
                        <line x1="50" y1="250" x2="50" y2="20" stroke="#333" stroke-width="2" /> <!-- Y Axis -->
                        
                        <!-- Labels -->
                        <text x="250" y="290" text-anchor="middle" font-family="Inter, sans-serif" font-size="14">Predicted Probability of Correct Class ($p$)</text>
                        <text x="20" y="150" text-anchor="middle" font-family="Inter, sans-serif" font-size="14" transform="rotate(-90 20,150)">Loss ($-\ln p$)</text>
                        
                        <!-- Ticks -->
                        <text x="50" y="270" text-anchor="middle" font-size="12">0</text>
                        <text x="250" y="270" text-anchor="middle" font-size="12">0.5</text>
                        <text x="450" y="270" text-anchor="middle" font-size="12">1.0</text>
                        
                        <!-- Curve: y = -ln(x) -->
                        <!-- Points roughly calculated: 
                             p=0.05 -> -ln(0.05) ~ 3.0
                             p=1.0  -> 0 
                        -->
                        <path d="M 60 20 Q 80 200 450 250" stroke="#ef4444" stroke-width="3" fill="none" />
                        
                        <!-- Annotation -->
                        <text x="100" y="50" font-family="Inter, sans-serif" font-size="14" fill="#ef4444" font-weight="bold">Infinite Penalty!</text>
                        <line x1="90" y1="55" x2="65" y2="30" stroke="#ef4444" stroke-width="1" marker-end="url(#arrow)" />
                    </svg>
                </div>

                <p>
                    As your prediction $p$ approaches 1 (certainty), the loss drops to 0. 
                    But as $p$ approaches 0 (you completely miss the truth), the loss shoots towards infinity.
                </p>
            </section>

            <section id="example">
                <h2>5. Concrete Example: Cat vs Dog vs Bird</h2>
                <p>Let's walk through a real calculation for a single image which is a <strong>Dog</strong>.</p>
                
                <table class="data-table" style="width: 100%; border-collapse: collapse; margin: 1rem 0;">
                    <thead>
                        <tr style="border-bottom: 2px solid #ddd; text-align: left;">
                            <th style="padding: 10px;">Class</th>
                            <th style="padding: 10px;">True Label ($y$)</th>
                            <th style="padding: 10px;">Model Pred ($p$)</th>
                            <th style="padding: 10px;">Log Pred ($\ln p$)</th>
                            <th style="padding: 10px;">Contribution ($-y \ln p$)</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td style="padding: 10px;">Cat</td>
                            <td style="padding: 10px;">0</td>
                            <td style="padding: 10px;">0.1</td>
                            <td style="padding: 10px;">-2.30</td>
                            <td style="padding: 10px;">0</td>
                        </tr>
                        <tr style="background-color: #f0fdf4;">
                            <td style="padding: 10px;"><strong>Dog</strong></td>
                            <td style="padding: 10px;"><strong>1</strong></td>
                            <td style="padding: 10px;"><strong>0.7</strong></td>
                            <td style="padding: 10px;"><strong>-0.36</strong></td>
                            <td style="padding: 10px;"><strong>0.36</strong></td>
                        </tr>
                        <tr>
                            <td style="padding: 10px;">Bird</td>
                            <td style="padding: 10px;">0</td>
                            <td style="padding: 10px;">0.2</td>
                            <td style="padding: 10px;">-1.61</td>
                            <td style="padding: 10px;">0</td>
                        </tr>
                    </tbody>
                    <tfoot>
                        <tr style="border-top: 2px solid #ddd; font-weight: bold;">
                            <td colspan="4" style="padding: 10px; text-align: right;">Total Loss:</td>
                            <td style="padding: 10px;">0.36</td>
                        </tr>
                    </tfoot>
                </table>
                <p>Even though the model assigned probability to Cat and Bird, those errors don't directly add to the sum because their target label is 0. However, because probabilities must sum to 1 (Softmax), increasing $p(\text{Cat})$ automatically decreases $p(\text{Dog})$, which <em>does</em> increase the loss.</p>
            </section>

            <section id="code">
                <h2>6. Python Implementation</h2>
                <p>Here is how you would implement this from scratch in Python using NumPy.</p>
                
<pre><code class="language-python">import numpy as np

def cross_entropy_loss(y_true, y_pred):
    """
    Computes cross-entropy loss.
    
    Args:
        y_true: One-hot encoded truth (e.g., [0, 1, 0])
        y_pred: Predicted probabilities (e.g., [0.1, 0.7, 0.2])
    """
    # 1. Clip predictions to avoid log(0) error
    epsilon = 1e-15
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    
    # 2. Compute cross-entropy
    # The sum technically includes 0 * log(p) terms which vanish
    loss = -np.sum(y_true * np.log(y_pred))
    
    return loss

# Test it
true_label = np.array([0, 1, 0])      # Dog
prediction = np.array([0.1, 0.7, 0.2]) # 70% sure it's Dog

print(f"Loss: {cross_entropy_loss(true_label, prediction):.4f}")
# Output: Loss: 0.3567
</code></pre>
            </section>

            <section id="pitfalls">
                <h2>7. Why Not Mean Squared Error (MSE)?</h2>
                <p>Beginners often ask: <em>"Why can't I just take the squared difference between 1 and 0.7?"</em></p>
                <div class="warning-box">
                    <div class="box-title">The Vanishing Gradient Problem</div>
                    <p>
                        If you use MSE with a Softmax/Sigmoid output, the gradients essentially die when the prediction is wrong but confident (e.g., predicting 0.0001 for the true class). 
                    </p>
                    <p>
                        Cross-Entropy cancels out the exponential term in the Softmax derivative, leaving a clean, linear gradient: 
                        $$ \frac{\partial L}{\partial z} = \hat{y} - y $$
                        This ensures the model learns quickly even when it is very wrong.
                    </p>
                </div>
            </section>
        
            <div class="tutorial-footer-summary" style="margin: 3rem 0; padding: 2rem; background: #f8f9fa; border-radius: 8px; border-left: 4px solid #3b82f6;">
                <h1>02. Cross-Entropy Loss</h1>
                <p class="lead">
                The heartbeat of modern classification models. We decode why "negative log probability" is the ultimate measure of prediction error.
            </p>
            </div>
            </main>

        <!-- TOC (Right Side) -->
        <aside class="toc-container">
            <h4 class="toc-title">Contents</h4>
            <nav class="toc-list">
                <a href="#introduction" class="toc-link">1. Intuition</a>
                <a href="#history" class="toc-link">2. History</a>
                <a href="#formula" class="toc-link">3. The Formula</a>
                <a href="#visualization" class="toc-link">4. Visualization</a>
                <a href="#example" class="toc-link">5. Concrete Example</a>
                <a href="#code" class="toc-link">6. Python Code</a>
                <a href="#pitfalls" class="toc-link">7. Why Not MSE?</a>
            </nav>
        </aside>
    </div>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-brand">
                    <span class="logo-symbol">∇</span>
                    <span>ML Fundamentals</span>
                </div>
                <p class="footer-tagline">Deep understanding through first principles.</p>
            </div>
            <div class="footer-links">
                <a href="../../../index.html">Home</a>
                <a href="https://github.com/ml-entropy/ml-entropy.github.io" target="_blank">GitHub</a>
            </div>
        </div>
    </footer>

    <!-- Scripts -->
    <script src="../../../js/main.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            if (typeof renderMathInElement === 'function') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }
        });
    </script>
</body>
</html>