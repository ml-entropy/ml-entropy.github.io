<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>KL Divergence | ML Fundamentals</title>
    <meta name="description" content="Understanding Kullback-Leibler divergence. Learn why it measures the difference between probability distributions and its role in ML.">
    
    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&family=Playfair+Display:wght@400;500;600;700&display=swap" rel="stylesheet">
    
    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}, {left: '\\\\[', right: '\\\\]', display: true}, {left: '\\\\(', right: '\\\\)', display: false}], throwOnError: false});"></script>
    
    <!-- Styles -->
    <link rel="stylesheet" href="../../../css/main.css">
    <link rel="stylesheet" href="../../../css/components.css">
    
    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>∞</text></svg>">
<link rel="stylesheet" href="../../../css/sidebar.css">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar" id="navbar">
        <div class="nav-container">
            <a href="../../../index.html" class="nav-logo">
                <span class="logo-symbol">∇</span>
                <span class="logo-text">ML Fundamentals</span>
            </a>
            
            <button class="nav-toggle" id="navToggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            
            <div class="nav-menu" id="navMenu">
                <div class="nav-links">
                    <a href="../../../tutorials/ml/index.html" class="nav-link active">Machine Learning</a>
                    <a href="../../../tutorials/linear-algebra/index.html" class="nav-link">Linear Algebra</a>
                    <a href="../../../tutorials/calculus/index.html" class="nav-link">Calculus</a>
                    <a href="../../../tutorials/physics/index.html" class="nav-link">Physics</a>
                    <a href="../../../index.html#philosophy" class="nav-link">Philosophy</a>
                    <a href="../../../index.html#roadmap" class="nav-link">Roadmap</a>
                    <a href="https://github.com/ml-entropy/ml-entropy.github.io" class="nav-link" target="_blank">GitHub</a>
                </div>
                
                <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
                    <svg class="sun-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="5"/>
                        <path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"/>
                    </svg>
                    <svg class="moon-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                    </svg>
                </button>
            </div>
        </div>
    </nav>

    <!-- Tutorial Header -->
    <header class="tutorial-content-header">
        <div class="container">
            <nav class="breadcrumb">
                <a href="../../../index.html">Home</a>
                <span class="breadcrumb-separator">→</span>
                <a href="../index.html">Machine Learning</a>
                <span class="breadcrumb-separator">→</span>
                <span>KL Divergence</span>
            </nav>
            
            
            
            
            <div class="tutorial-tabs">
                <a href="#theory" class="tutorial-tab active">Theory</a>
                <a href="#code" class="tutorial-tab">Code</a>
                <a href="#exercises" class="tutorial-tab">Exercises</a>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    
    <!-- Main Content -->
    <div class="tutorial-wrapper">
        
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <aside class="tutorial-sidebar">
            <div class="sidebar-section">
                <h3 class="sidebar-section-title">Machine Learning</h3>
                <nav class="sidebar-nav">
                        <a href="../00-probability/index.html" class="sidebar-link">00. Probability Foundations</a>
                    <a href="../04-logarithms/index.html" class="sidebar-link">01. Why Logarithms?</a>
                    <a href="../05-combinatorics/index.html" class="sidebar-link">02. Combinatorics</a>
                    <a href="../03-distributions/index.html" class="sidebar-link">03. Normal Distributions</a>
                    <a href="../01-entropy/index.html" class="sidebar-link">04. Entropy Fundamentals</a>
                    <a href="../02-cross-entropy/index.html" class="sidebar-link">05. Cross-Entropy</a>
                    <a href="../02-kl-divergence/index.html" class="sidebar-link active">06. KL Divergence</a>
                    <a href="../14-entropy-connections/index.html" class="sidebar-link">07. Entropy Connections</a>
                    <a href="../06-backpropagation/index.html" class="sidebar-link">08. Backpropagation</a>
                    <a href="../07-regularization/index.html" class="sidebar-link">09. Regularization</a>
                    <a href="../08-batch-normalization/index.html" class="sidebar-link">10. Batch Normalization</a>
                    <a href="../09-learning-rate/index.html" class="sidebar-link">11. Learning Rate</a>
                    <a href="../10-cnn/index.html" class="sidebar-link">12. CNNs</a>
                    <a href="../11-rnn/index.html" class="sidebar-link">13. RNNs</a>
                    <a href="../15-autoencoder/index.html" class="sidebar-link">14. Autoencoders</a>
                    <a href="../13-variational-inference/index.html" class="sidebar-link">15. Variational Inference</a>
                    <a href="../12-vae/index.html" class="sidebar-link">16. VAE</a>
                    <a href="../16-inductive-bias/index.html" class="sidebar-link">17. Inductive Bias</a>
                    <a href="../17-architectural-biases/index.html" class="sidebar-link">18. Architectural Biases</a>
                    <a href="../18-designing-biases/index.html" class="sidebar-link">19. Designing Biases</a>
                    <a href="../19-fst-fundamentals/index.html" class="sidebar-link">20. FST Fundamentals</a>
                    <a href="../20-weighted-fsts/index.html" class="sidebar-link">21. Weighted FSTs</a>
                    <a href="../21-fst-libraries/index.html" class="sidebar-link">22. FST Libraries</a>
                    <a href="../22-fst-applications/index.html" class="sidebar-link">23. FST Applications</a>
                    <a href="../23-neural-symbolic/index.html" class="sidebar-link">24. Neural-Symbolic Hybrids</a>
                </nav>
            </div>
            
            <div class="sidebar-section" style="margin-top: 2rem;">
                <h3 class="sidebar-section-title">Related Subjects</h3>
                <nav class="sidebar-nav">
                        <a href="../../linear-algebra/index.html" class="sidebar-link">Linear Algebra</a>
                    <a href="../../calculus/index.html" class="sidebar-link">Calculus</a>
                    <a href="../../physics/index.html" class="sidebar-link">Physics</a>
                </nav>
            </div>
        </aside>

        <!-- Main Article -->
        <main class="tutorial-main">
            
            <article class="article-content" id="theory">
                
                <!-- Section 1 -->
                <h2 id="intuition">The Intuition: Wrong Code, Extra Bits</h2>
                
                <p>
                    Imagine you're sending messages. If you know the true distribution $P$ of symbols, 
                    you can design an optimal code with average length $H(P)$ bits per symbol. 
                </p>
                
                <p>
                    But what if you <em>think</em> the distribution is $Q$ and design your code for that? 
                    You'll need extra bits on average. <strong>KL divergence measures these extra bits.</strong>
                </p>
                
                <div class="definition-box">
                    <div class="box-title">KL Divergence Definition</div>
                    <p style="margin-bottom: 0;">
                        The <strong>Kullback-Leibler divergence</strong> from $Q$ to $P$ is:
                        $$D_{KL}(P \| Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)} = \mathbb{E}_{P}\left[\log \frac{P(X)}{Q(X)}\right]$$
                    </p>
                </div>
                
                <p>
                    Think of it as: "The penalty for using model $Q$ when the truth is $P$."
                </p>

                <div class="note-box">
                    <div class="box-title">Relationship to Cross-Entropy</div>
                    <p style="margin-bottom: 0;">
                        KL Divergence is just the difference between the Cross-Entropy and the true Entropy:
                        $$D_{KL}(P \| Q) = H(P, Q) - H(P)$$
                        It measures the *avoidable* loss. Since $H(P)$ is a constant for a given dataset, minimizing Cross-Entropy is equivalent to minimizing KL Divergence.
                    </p>
                </div>

                <!-- Section 2 -->
                <h2 id="example">A Concrete Example: Biased Coin</h2>
                <p>
                    Let's make this tangible. Imagine a biased coin that we want to model.
                </p>
                <div class="example-box" style="background: var(--color-bg-alt); padding: 1.5rem; border-radius: 0.5rem; margin: 1.5rem 0;">
                    <p><strong>True Distribution (P):</strong></p>
                    <p>The coin is biased, landing Heads 70% of the time.</p>
                    <ul>
                        <li>$P(\text{Heads}) = 0.7$</li>
                        <li>$P(\text{Tails}) = 0.3$</li>
                    </ul>

                    <p><strong>Our Model (Q):</strong></p>
                    <p>We incorrectly believe the coin is fair.</p>
                    <ul>
                        <li>$Q(\text{Heads}) = 0.5$</li>
                        <li>$Q(\text{Tails}) = 0.5$</li>
                    </ul>

                    <p><strong>Calculating KL Divergence $D_{KL}(P \| Q)$:</strong></p>
                    <p>We sum over the two outcomes: Heads and Tails.</p>
                    <div class="math-block">
                        $$D_{KL}(P \| Q) = P(H)\log_2\frac{P(H)}{Q(H)} + P(T)\log_2\frac{P(T)}{Q(T)}$$
                        $$= 0.7 \cdot \log_2\frac{0.7}{0.5} + 0.3 \cdot \log_2\frac{0.3}{0.5}$$
                        $$= 0.7 \cdot \log_2(1.4) + 0.3 \cdot \log_2(0.6)$$
                        $$= 0.7 \cdot (0.485) + 0.3 \cdot (-0.737)$$
                        $$= 0.340 - 0.221 = \boxed{0.119 \text{ bits}}$$
                    </div>
                    <p><strong>Interpretation:</strong> By using the wrong model (fair coin), we will waste an average of <strong>0.119 extra bits</strong> for every coin flip we encode. This is the "cost" of our incorrect belief.</p>
                </div>

                <!-- Section 2 -->
                <h2 id="derivation">Deriving KL Divergence</h2>
                
                <p>
                    Let's derive KL divergence from the coding perspective:
                </p>
                
                <div class="math-derivation">
                    <div class="math-derivation-title">From Cross-Entropy to KL Divergence</div>
                    
                    <div class="math-step">
                        <div class="math-step-number">1</div>
                        <div class="math-step-content">
                            Optimal code for $Q$ has length $-\log Q(x)$ for symbol $x$
                        </div>
                    </div>
                    
                    <div class="math-step">
                        <div class="math-step-number">2</div>
                        <div class="math-step-content">
                            Expected length under $P$: $H(P,Q) = -\sum_x P(x) \log Q(x)$ (cross-entropy)
                        </div>
                    </div>
                    
                    <div class="math-step">
                        <div class="math-step-number">3</div>
                        <div class="math-step-content">
                            Extra bits beyond optimal: $D_{KL}(P\|Q) = H(P,Q) - H(P)$
                        </div>
                    </div>
                    
                    <div class="math-step">
                        <div class="math-step-number">4</div>
                        <div class="math-step-content">
                            Expanding:
                            $$D_{KL}(P\|Q) = -\sum_x P(x)\log Q(x) + \sum_x P(x)\log P(x)$$
                            $$= \sum_x P(x) \log\frac{P(x)}{Q(x)}$$
                        </div>
                    </div>
                </div>
                
                <!-- Section 3 -->
                <h2 id="properties">Properties of KL Divergence</h2>
                
                <h3>Non-Negativity (Gibbs' Inequality)</h3>
                
                <p>
                    KL divergence is always non-negative: $D_{KL}(P \| Q) \geq 0$, with equality iff $P = Q$.
                </p>
                
                <div class="math-derivation">
                    <div class="math-derivation-title">Proof Using Jensen's Inequality</div>
                    
                    <div class="math-step">
                        <div class="math-step-number">1</div>
                        <div class="math-step-content">
                            $-D_{KL}(P\|Q) = \sum_x P(x) \log\frac{Q(x)}{P(x)} = \mathbb{E}_P\left[\log\frac{Q(X)}{P(X)}\right]$
                        </div>
                    </div>
                    
                    <div class="math-step">
                        <div class="math-step-number">2</div>
                        <div class="math-step-content">
                            By Jensen's inequality (log is concave): $\mathbb{E}[\log Y] \leq \log \mathbb{E}[Y]$
                        </div>
                    </div>
                    
                    <div class="math-step">
                        <div class="math-step-number">3</div>
                        <div class="math-step-content">
                            $-D_{KL}(P\|Q) \leq \log \mathbb{E}_P\left[\frac{Q(X)}{P(X)}\right] = \log \sum_x Q(x) = \log 1 = 0$
                        </div>
                    </div>
                    
                    <div class="math-step">
                        <div class="math-step-number">4</div>
                        <div class="math-step-content">
                            Therefore: $D_{KL}(P\|Q) \geq 0$ ✓
                        </div>
                    </div>
                </div>
                
                <h3>Asymmetry</h3>
                
                <div class="warning-box">
                    <div class="box-title">KL Divergence is NOT a Distance</div>
                    <p style="margin-bottom: 0;">
                        $D_{KL}(P\|Q) \neq D_{KL}(Q\|P)$ in general. This asymmetry is important: 
                        "cost of using $Q$ for $P$" differs from "cost of using $P$ for $Q$." A true distance metric must be symmetric.
                    </p>
                </div>

                <div class="example-box" style="background: var(--color-bg-alt); padding: 1.5rem; border-radius: 0.5rem; margin: 1.5rem 0;">
                    <p><strong>Asymmetry Example (continued):</strong></p>
                    <p>Let's calculate the reverse KL, $D_{KL}(Q \| P)$, for our coin example:</p>
                    <div class="math-block">
                        $$D_{KL}(Q \| P) = Q(H)\log_2\frac{Q(H)}{P(H)} + Q(T)\log_2\frac{Q(T)}{P(T)}$$
                        $$= 0.5 \cdot \log_2\frac{0.5}{0.7} + 0.5 \cdot \log_2\frac{0.5}{0.3}$$
                        $$= 0.5 \cdot \log_2(0.714) + 0.5 \cdot \log_2(1.667)$$
                        $$= 0.5 \cdot (-0.485) + 0.5 \cdot (0.737)$$
                        $$= -0.243 + 0.369 = \boxed{0.126 \text{ bits}}$$
                    </div>
                    <p>As we can see, $0.119 \neq 0.126$. The "penalty" is different in each direction.</p>
                </div>

                <!-- Section 4 -->
                <h2 id="forward-vs-reverse">Forward vs Reverse KL</h2>
                
                <p>
                    Because KL Divergence is asymmetric, minimizing $D_{KL}(P \| Q)$ yields a very different result than minimizing $D_{KL}(Q \| P)$. This distinction is critical in fields like Variational Inference and Reinforcement Learning.
                </p>
                
                <h3>1. Forward KL: $D_{KL}(P \| Q)$ — "Mean-Seeking"</h3>
                
                <p>
                    <strong>The Penalty Mechanism (Zero-Avoiding):</strong><br>
                    Look at the term inside the expectation: $P(x) \log \frac{P(x)}{Q(x)}$.
                    If $P(x) > 0$ (an event is real) but $Q(x) \to 0$ (our model says it's impossible), the ratio explodes to infinity.
                </p>
                <p>
                    <strong>Result:</strong> To avoid this infinite penalty, our model $Q$ must "stretch" to cover <em>everywhere</em> that $P$ has probability mass. It cannot safely ignore any part of the true distribution. If $P$ is bimodal (has two peaks), $Q$ will try to bridge the gap between them, effectively averaging them out.
                </p>
                
                <h3>2. Reverse KL: $D_{KL}(Q \| P)$ — "Mode-Seeking"</h3>
                
                <p>
                    <strong>The Penalty Mechanism (Zero-Forcing):</strong><br>
                    Here, the term is $Q(x) \log \frac{Q(x)}{P(x)}$.
                    If $P(x) \to 0$ (an event is impossible in reality), then $Q(x)$ <strong>must</strong> also be 0. If $Q(x)$ assigns any probability to an impossible event, the ratio $Q/P$ explodes.
                </p>
                <p>
                    <strong>Result:</strong> The model $Q$ becomes "conservative." It will find a single peak (mode) of $P$ where it is safe and fit snugly inside it. It will happily ignore other peaks of $P$ because it pays no penalty for <em>missing</em> mass, only for placing mass in the wrong spot.
                </p>

                <h3>3. Visualizing the "Twin Peaks" Problem</h3>
                
                <p>
                    The best way to see this difference is when the true distribution $P$ is <strong>bimodal</strong> (has two peaks, like a camel's back), but our model $Q$ is unimodal (single Gaussian).
                </p>

                <div class="figure-container" style="text-align: center; margin: 2rem 0;">
                    <svg width="100%" height="300" viewBox="0 0 800 300" style="background: var(--color-bg-alt); border-radius: 8px;">
                        <defs>
                            <pattern id="grid" width="40" height="40" patternUnits="userSpaceOnUse">
                                <path d="M 40 0 L 0 0 0 40" fill="none" stroke="var(--color-border)" stroke-width="1" opacity="0.3"/>
                            </pattern>
                        </defs>
                        <rect width="800" height="300" fill="url(#grid)" />

                        <!-- P (Bimodal) -->
                        <path d="M 50.0 249.9 L 57.1 249.9 L 64.1 249.8 L 71.2 249.6 L 78.3 249.3 L 85.4 248.8 L 92.4 248.0 L 99.5 246.7 L 106.6 244.9 L 113.6 242.2 L 120.7 238.3 L 127.8 233.1 L 134.8 226.0 L 141.9 217.0 L 149.0 205.7 L 156.1 192.2 L 163.1 176.4 L 170.2 158.6 L 177.3 139.6 L 184.3 120.0 L 191.4 100.9 L 198.5 83.4 L 205.6 68.7 L 212.6 57.7 L 219.7 51.3 L 226.8 50.0 L 233.8 53.9 L 240.9 62.6 L 248.0 75.6 L 255.1 91.9 L 262.1 110.3 L 269.2 129.8 L 276.3 149.2 L 283.3 167.7 L 290.4 184.5 L 297.5 199.2 L 304.5 211.7 L 311.6 221.8 L 318.7 229.8 L 325.8 235.9 L 332.8 240.4 L 339.9 243.6 L 347.0 245.9 L 354.0 247.4 L 361.1 248.4 L 368.2 249.1 L 375.3 249.4 L 382.3 249.7 L 389.4 249.8 L 396.5 249.9 L 403.5 249.9 L 410.6 249.8 L 417.7 249.7 L 424.7 249.4 L 431.8 249.1 L 438.9 248.4 L 446.0 247.4 L 453.0 245.9 L 460.1 243.6 L 467.2 240.4 L 474.2 235.9 L 481.3 229.8 L 488.4 221.8 L 495.5 211.7 L 502.5 199.2 L 509.6 184.5 L 516.7 167.7 L 523.7 149.2 L 530.8 129.8 L 537.9 110.3 L 544.9 91.9 L 552.0 75.6 L 559.1 62.6 L 566.2 53.9 L 573.2 50.0 L 580.3 51.3 L 587.4 57.7 L 594.4 68.7 L 601.5 83.4 L 608.6 100.9 L 615.7 120.0 L 622.7 139.6 L 629.8 158.6 L 636.9 176.4 L 643.9 192.2 L 651.0 205.7 L 658.1 217.0 L 665.2 226.0 L 672.2 233.1 L 679.3 238.3 L 686.4 242.2 L 693.4 244.9 L 700.5 246.7 L 707.6 248.0 L 714.6 248.8 L 721.7 249.3 L 728.8 249.6 L 735.9 249.8 L 742.9 249.9 L 750.0 249.9" fill="rgba(59, 130, 246, 0.1)" stroke="#3b82f6" stroke-width="3" />

                        <!-- Q Forward (Mean-Seeking) -->
                        <path d="M 50.0 233.8 L 57.1 232.4 L 64.1 231.0 L 71.2 229.5 L 78.3 227.8 L 85.4 226.2 L 92.4 224.4 L 99.5 222.5 L 106.6 220.6 L 113.6 218.5 L 120.7 216.4 L 127.8 214.2 L 134.8 211.9 L 141.9 209.5 L 149.0 207.1 L 156.1 204.6 L 163.1 202.0 L 170.2 199.3 L 177.3 196.6 L 184.3 193.8 L 191.4 191.0 L 198.5 188.2 L 205.6 185.3 L 212.6 182.3 L 219.7 179.4 L 226.8 176.5 L 233.8 173.5 L 240.9 170.6 L 248.0 167.7 L 255.1 164.8 L 262.1 162.0 L 269.2 159.2 L 276.3 156.5 L 283.3 153.9 L 290.4 151.3 L 297.5 148.9 L 304.5 146.6 L 311.6 144.3 L 318.7 142.3 L 325.8 140.3 L 332.8 138.5 L 339.9 136.8 L 347.0 135.4 L 354.0 134.0 L 361.1 132.9 L 368.2 131.9 L 375.3 131.2 L 382.3 130.6 L 389.4 130.2 L 396.5 130.0 L 403.5 130.0 L 410.6 130.2 L 417.7 130.6 L 424.7 131.2 L 431.8 131.9 L 438.9 132.9 L 446.0 134.0 L 453.0 135.4 L 460.1 136.8 L 467.2 138.5 L 474.2 140.3 L 481.3 142.3 L 488.4 144.3 L 495.5 146.6 L 502.5 148.9 L 509.6 151.3 L 516.7 153.9 L 523.7 156.5 L 530.8 159.2 L 537.9 162.0 L 544.9 164.8 L 552.0 167.7 L 559.1 170.6 L 566.2 173.5 L 573.2 176.5 L 580.3 179.4 L 587.4 182.3 L 594.4 185.3 L 601.5 188.2 L 608.6 191.0 L 615.7 193.8 L 622.7 196.6 L 629.8 199.3 L 636.9 202.0 L 643.9 204.6 L 651.0 207.1 L 658.1 209.5 L 665.2 211.9 L 672.2 214.2 L 679.3 216.4 L 686.4 218.5 L 693.4 220.6 L 700.5 222.5 L 707.6 224.4 L 714.6 226.2 L 721.7 227.8 L 728.8 229.5 L 735.9 231.0 L 742.9 232.4 L 750.0 233.8" fill="none" stroke="#f59e0b" stroke-width="3" stroke-dasharray="8,8" />

                        <!-- Q Reverse (Mode-Seeking) -->
                        <path d="M 50.0 249.9 L 57.1 249.9 L 64.1 249.8 L 71.2 249.6 L 78.3 249.3 L 85.4 248.8 L 92.4 248.0 L 99.5 246.7 L 106.6 244.9 L 113.6 242.2 L 120.7 238.3 L 127.8 233.1 L 134.8 226.0 L 141.9 217.0 L 149.0 205.7 L 156.1 192.2 L 163.1 176.4 L 170.2 158.6 L 177.3 139.6 L 184.3 120.0 L 191.4 100.9 L 198.5 83.4 L 205.6 68.7 L 212.6 57.7 L 219.7 51.3 L 226.8 50.0 L 233.8 53.9 L 240.9 62.6 L 248.0 75.6 L 255.1 91.9 L 262.1 110.3 L 269.2 129.8 L 276.3 149.2 L 283.3 167.7 L 290.4 184.5 L 297.5 199.2 L 304.5 211.7 L 311.6 221.8 L 318.7 229.8 L 325.8 235.9 L 332.8 240.4 L 339.9 243.6 L 347.0 245.9 L 354.0 247.4 L 361.1 248.4 L 368.2 249.1 L 375.3 249.5 L 382.3 249.7 L 389.4 249.8 L 396.5 249.9 L 403.5 250.0 L 410.6 250.0 L 417.7 250.0 L 424.7 250.0 L 431.8 250.0 L 438.9 250.0 L 446.0 250.0 L 453.0 250.0 L 460.1 250.0 L 467.2 250.0 L 474.2 250.0 L 481.3 250.0 L 488.4 250.0 L 495.5 250.0 L 502.5 250.0 L 509.6 250.0 L 516.7 250.0 L 523.7 250.0 L 530.8 250.0 L 537.9 250.0 L 544.9 250.0 L 552.0 250.0 L 559.1 250.0 L 566.2 250.0 L 573.2 250.0 L 580.3 250.0 L 587.4 250.0 L 594.4 250.0 L 601.5 250.0 L 608.6 250.0 L 615.7 250.0 L 622.7 250.0 L 629.8 250.0 L 636.9 250.0 L 643.9 250.0 L 651.0 250.0 L 658.1 250.0 L 665.2 250.0 L 672.2 250.0 L 679.3 250.0 L 686.4 250.0 L 693.4 250.0 L 700.5 250.0 L 707.6 250.0 L 714.6 250.0 L 721.7 250.0 L 728.8 250.0 L 735.9 250.0 L 742.9 250.0 L 750.0 250.0" fill="none" stroke="#10b981" stroke-width="3" stroke-dasharray="4,4" />

                        <!-- Legends -->
                        <text x="50" y="30" font-family="JetBrains Mono, monospace" fill="#3b82f6" font-weight="bold">P (Truth)</text>
                        <text x="250" y="30" font-family="JetBrains Mono, monospace" fill="#f59e0b" font-weight="bold">Q (Forward KL)</text>
                        <text x="500" y="30" font-family="JetBrains Mono, monospace" fill="#10b981" font-weight="bold">Q (Reverse KL)</text>
                    </svg>
                    <figcaption style="margin-top: 1rem; color: #666; font-size: 0.9rem;">
                        <strong>Blue Area (P):</strong> The true bimodal distribution. <br>
                        <span style="color: #f59e0b;"><strong>Orange Dashed (Forward KL):</strong></span> Tries to cover both peaks, resulting in a wide, flat distribution in the middle. (Mean-Seeking)<br>
                        <span style="color: #10b981;"><strong>Green Dashed (Reverse KL):</strong></span> Locks onto the left peak, completely ignoring the right one to avoid penalty. (Mode-Seeking)
                    </figcaption>
                </div>

                <div class="intuition-steps">
                    <h3>Step-by-Step Minimization</h3>
                    
                    <div style="background: #fff8e1; padding: 1rem; border-left: 4px solid #f59e0b; margin-bottom: 1rem;">
                        <strong>Scenario 1: Forward KL (P || Q)</strong>
                        <ol style="margin-top: 0.5rem; padding-left: 1.5rem;">
                            <li><strong>Start:</strong> $Q$ is initialized somewhere in the middle.</li>
                            <li><strong>Check:</strong> $P$ has mass on the far left and far right.</li>
                            <li><strong>Penalty:</strong> "Hey $Q$, $P$ is non-zero here, but you are zero! Infinite penalty!"</li>
                            <li><strong>Update:</strong> $Q$ widens its variance desperately to ensure it is non-zero everywhere $P$ is.</li>
                            <li><strong>Result:</strong> $Q$ becomes a wide "blanket" that covers everything but captures no specific detail.</li>
                        </ol>
                    </div>

                    <div style="background: #e0f2f1; padding: 1rem; border-left: 4px solid #10b981;">
                        <strong>Scenario 2: Reverse KL (Q || P)</strong>
                        <ol style="margin-top: 0.5rem; padding-left: 1.5rem;">
                            <li><strong>Start:</strong> $Q$ is initialized in the middle.</li>
                            <li><strong>Check:</strong> $P$ is zero in the middle (the valley between peaks).</li>
                            <li><strong>Penalty:</strong> "Hey $Q$, you have mass here, but $P$ says this is impossible! Infinite penalty!"</li>
                            <li><strong>Update:</strong> $Q$ shrinks and moves to the nearest "safe" zone (one of the peaks).</li>
                            <li><strong>Result:</strong> $Q$ sits perfectly on one peak and pretends the other doesn't exist.</li>
                        </ol>
                    </div>
                </div>

                <h3>4. Deep Dive: Forward KL ($P \| Q$)</h3>
                
                <p>
                    Let's zoom in on Forward KL, the version used in <strong>Supervised Learning</strong> (via Cross-Entropy).
                </p>

                <div class="analogy-box" style="background: #fff3e0; padding: 1.5rem; border-radius: 8px; border-left: 5px solid #ff9800; margin: 1.5rem 0;">
                    <h4 style="margin-top: 0; color: #e65100;">Analogy: The Anxious Student</h4>
                    <p>
                        Imagine <strong>$P$ is the Teacher</strong> and <strong>$Q$ is the Student</strong>.
                    </p>
                    <ul>
                        <li>The Teacher ($P$) will randomly pick a topic for the final exam.</li>
                        <li>The Student ($Q$) must decide how much time to spend studying each topic.</li>
                        <li><strong>The Forward KL Penalty:</strong> If the Teacher asks a question about "Chapter 10" ($P(\text{Ch10}) > 0$), and the Student spent <strong>zero</strong> time studying it ($Q(\text{Ch10}) \approx 0$), the Student fails the entire course immediately (Infinite Loss).</li>
                    </ul>
                    <p>
                        <strong>Strategy:</strong> To stay safe, the Anxious Student ($Q$) spreads their study time across <em>every single chapter</em> the Teacher might possibly pick. They can't afford to ignore anything. This is why Forward KL is "Mean-Seeking" or "Inclusive."
                    </p>
                </div>

                <h4>The Math of "Infinite Penalty"</h4>
                <p>
                    Why is the penalty so severe? Let's plug in numbers.
                    Suppose an event $x$ happens rarely ($P(x) = 0.1$), but our model thinks it's impossible ($Q(x) = 10^{-9}$).
                </p>
                <div class="math-block">
                    $$ \text{Loss Contribution} = P(x) \log \frac{P(x)}{Q(x)} $$
                    $$ = 0.1 \cdot \log \frac{0.1}{10^{-9}} = 0.1 \cdot \log(10^8) \approx 0.1 \cdot 18.4 = \mathbf{1.84} $$
                </div>
                <p>
                    If $Q(x)$ drops to $10^{-50}$, the loss jumps linearly with the exponent. If $Q(x) \to 0$, the loss $\to \infty$.
                </p>

                <h4>Where do we use this?</h4>
                <ul>
                    <li><strong>Language Modeling (ChatGPT, BERT):</strong> We want the model to assign a non-zero probability to <em>every</em> valid English sentence. If a user types a valid sentence and the model assigns it probability 0, the model effectively "crashes" (perplexity explodes).</li>
                    <li><strong>Compression:</strong> If you assign a code length of $\infty$ (which corresponds to probability 0) to a symbol that actually appears in the file, you can never transmit that file.</li>
                </ul>

                <!-- Section 5 -->
                <h2 id="can-q-learn-bimodal">5. Can Forward KL Learn Bimodal Distributions?</h2>
                
                <p>
                    A common misconception is that "Forward KL cannot handle multimodal distributions." This is <strong>false</strong>. The outcome depends entirely on the capacity of your model $Q$.
                </p>

                <h3>Case A: $Q$ is a Single Gaussian (Under-parameterized)</h3>
                <p>
                    If $P$ has two peaks ("Twin Peaks") but we force $Q$ to be a single bell curve:
                </p>
                <ul>
                    <li><strong>Forward KL:</strong> $Q$ will stretch to cover both peaks. It learns the <em>average</em> (mean) and becomes very wide. It fails to capture the structure, but it successfully "covers" the data.</li>
                    <li><strong>Reverse KL:</strong> $Q$ will pick <em>one</em> peak and ignore the other. It captures the structure of one mode perfectly but misses the other entirely.</li>
                </ul>

                <h3>Case B: $Q$ is a Mixture Model or Neural Network (Flexible)</h3>
                <p>
                    If we allow $Q$ to be complex (e.g., a Mixture of two Gaussians, or a Flow-based model):
                </p>
                <ul>
                    <li><strong>Forward KL:</strong> $Q$ will split its mass and fit <strong>both peaks perfectly</strong>.</li>
                </ul>
                <p>
                    In fact, this is exactly how we train complex generative models! When we train a Neural Network with <strong>Cross-Entropy Loss</strong> (which is Forward KL), we are asking it to learn the complex, multimodal distribution of real-world data (images, text). Because the network is flexible enough, it succeeds.
                </p>
                <div class="note-box">
                    <div class="box-title">Key Takeaway</div>
                    <p style="margin-bottom: 0;">
                        The "Mean-Seeking" (blurry) behavior only happens when your model $Q$ is <strong>too simple</strong> to represent the complexity of reality $P$. If $Q$ is powerful enough, Forward KL is the gold standard for learning the true distribution.
                    </p>
                </div>

                <!-- Section 6 -->
                <h2 id="ml-applications">KL Divergence in Machine Learning</h2>
                
                <h3>Variational Inference</h3>
                
                <p>
                    We approximate intractable posteriors by minimizing:
                </p>
                
                <div class="math-block">
                    $$D_{KL}(q(\theta) \| p(\theta | \mathcal{D}))$$
                </div>
                
                <h3>Information Bottleneck</h3>
                
                <p>
                    Compression while preserving information uses KL terms for regularization.
                </p>
                
                <h3>Policy Optimization (RL)</h3>
                
                <p>
                    Trust region methods (TRPO, PPO) constrain policy updates using KL divergence 
                    to ensure stable learning.
                </p>
                
                <!-- Navigation -->
                
                        <!-- Exercises Section -->
            <section id="exercises" style="padding-top: 2rem; margin-top: 2rem; border-top: 1px solid #e5e7eb;">
                <h2 style="margin-bottom: 1.5rem;">30 Practice Exercises</h2>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>E1. Definition Recall</h3>
                        <span class="difficulty-badge diff-easy">Easy</span>
                    </div>
                    <p>Write the formula for the KL Divergence $D_{KL}(P || Q)$ for discrete probability distributions $P$ and $Q$.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>The definition is:</p>
    $$D_{KL}(P || Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)}$$
    <p>Or equivalently using expectation:</p>
    $$D_{KL}(P || Q) = \mathbb{E}_{x \sim P} \left[ \log \frac{P(x)}{Q(x)} \right]$$
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>E2. Non-Negativity</h3>
                        <span class="difficulty-badge diff-easy">Easy</span>
                    </div>
                    <p>What is the minimum possible value of KL Divergence? When is this value achieved?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>The minimum value is <strong>0</strong>.</p>
    <p>This occurs if and only if the two distributions are identical ($P(x) = Q(x)$ for all $x$).</p>
    <p>This property is known as <strong>Gibbs' Inequality</strong>.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>E3. Asymmetry</h3>
                        <span class="difficulty-badge diff-easy">Easy</span>
                    </div>
                    <p>True or False: $D_{KL}(P || Q) = D_{KL}(Q || P)$ for all $P, Q$.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p><strong>False.</strong></p>
    <p>KL Divergence is <strong>asymmetric</strong>. The "distance" from $P$ to $Q$ is not generally the same as from $Q$ to $P$.</p>
    <p>This is why it is called a "divergence" rather than a "distance" metric.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>E4. Simple Calculation</h3>
                        <span class="difficulty-badge diff-easy">Easy</span>
                    </div>
                    <p>Let $P = [0.5, 0.5]$ and $Q = [0.25, 0.75]$. Calculate $D_{KL}(P || Q)$ in bits.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>Using the formula:</p>
    $$D_{KL}(P || Q) = 0.5 \log_2 \frac{0.5}{0.25} + 0.5 \log_2 \frac{0.5}{0.75}$$
    $$= 0.5 \log_2(2) + 0.5 \log_2(2/3)$$
    $$= 0.5(1) + 0.5(1 - 1.585)$$
    $$= 0.5 - 0.2925 = 0.2075 \text{ bits}$$
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>E5. Relation to Cross-Entropy</h3>
                        <span class="difficulty-badge diff-easy">Easy</span>
                    </div>
                    <p>Express KL Divergence in terms of Cross-Entropy $H(P, Q)$ and Entropy $H(P)$.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>The relationship is:</p>
    $$D_{KL}(P || Q) = H(P, Q) - H(P)$$
    <p>This means KL Divergence is the difference between the cross-entropy (total cost) and the entropy (irreducible cost).</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>E6. Infinite Penalty</h3>
                        <span class="difficulty-badge diff-easy">Easy</span>
                    </div>
                    <p>What happens to $D_{KL}(P || Q)$ if there is an event $x$ such that $P(x) > 0$ but $Q(x) = 0$?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>The value goes to <strong>infinity</strong>.</p>
    <p>The term is $P(x) \log \frac{P(x)}{0}$. Since division by zero approaches infinity, the divergence explodes.</p>
    <p>This property forces $Q$ to be non-zero wherever $P$ is non-zero (absolute continuity).</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>E7. Identical Distributions</h3>
                        <span class="difficulty-badge diff-easy">Easy</span>
                    </div>
                    <p>Calculate $D_{KL}(P || P)$.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>$$D_{KL}(P || P) = \sum P(x) \log \frac{P(x)}{P(x)} = \sum P(x) \log(1)$$</p>
    <p>Since $\log(1) = 0$:</p>
    $$= \sum P(x) \cdot 0 = 0$$
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>E8. Forward vs Reverse Name</h3>
                        <span class="difficulty-badge diff-easy">Easy</span>
                    </div>
                    <p>Which direction corresponds to 'Forward KL': $D_{KL}(P || Q)$ or $D_{KL}(Q || P)$?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p><strong>$D_{KL}(P || Q)$</strong> is typically called Forward KL.</p>
    <p>In this context, $P$ is usually the "true" distribution and $Q$ is the "model."</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>E9. Zero-Forcing Behavior</h3>
                        <span class="difficulty-badge diff-easy">Easy</span>
                    </div>
                    <p>Which direction of KL Divergence encourages the model $Q$ to be 'Zero-Forcing' (Mode-Seeking)?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p><strong>Reverse KL: $D_{KL}(Q || P)$</strong>.</p>
    <p>If $P(x) = 0$, then $Q(x)$ must be 0 to avoid infinite penalty. This forces $Q$ to "hide" inside the support of $P$, leading to mode-seeking behavior.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>E10. Units</h3>
                        <span class="difficulty-badge diff-easy">Easy</span>
                    </div>
                    <p>If we use the natural logarithm ($\ln$), what are the units of KL Divergence?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>The units are <strong>nats</strong>.</p>
    <p>If we use base-2 logarithm ($\log_2$), the units are <strong>bits</strong>.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>M1. Bernoulli KL</h3>
                        <span class="difficulty-badge diff-medium">Medium</span>
                    </div>
                    <p>Derive the KL Divergence between two Bernoulli distributions $P \sim \text{Bern}(p)$ and $Q \sim \text{Bern}(q)$.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>The distributions are binary: $\{1, 0\}$ with probs $\{p, 1-p\}$ and $\{q, 1-q\}$.</p>
    $$D_{KL}(P || Q) = p \log \frac{p}{q} + (1-p) \log \frac{1-p}{1-q}$$
    <p>This is a fundamental formula often used in binary classification analysis.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>M2. Gaussian KL (Equal Variance)</h3>
                        <span class="difficulty-badge diff-medium">Medium</span>
                    </div>
                    <p>Calculate the KL divergence between two univariate Gaussians with different means $\mu_1, \mu_2$ but same variance $\sigma^2$.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>For $P \sim \mathcal{N}(\mu_1, \sigma^2)$ and $Q \sim \mathcal{N}(\mu_2, \sigma^2)$:</p>
    $$D_{KL}(P || Q) = \frac{1}{2\sigma^2} (\mu_1 - \mu_2)^2$$
    <p><strong>Intuition:</strong> It is simply the squared Euclidean distance between the means, scaled by the variance.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>M3. Gaussian KL (Equal Mean)</h3>
                        <span class="difficulty-badge diff-medium">Medium</span>
                    </div>
                    <p>Calculate the KL divergence between $P \sim \mathcal{N}(0, \sigma_1^2)$ and $Q \sim \mathcal{N}(0, \sigma_2^2)$.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>With means equal to 0:</p>
    $$D_{KL}(P || Q) = \ln \frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2}{2\sigma_2^2} - \frac{1}{2}$$
    <p>This measures the discrepancy solely due to the difference in spread (variance).</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>M4. Chain Rule for KL</h3>
                        <span class="difficulty-badge diff-medium">Medium</span>
                    </div>
                    <p>Write the Chain Rule for KL Divergence for joint distributions $P(x,y)$ and $Q(x,y)$.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>$$D_{KL}(P(x,y) || Q(x,y)) = D_{KL}(P(x) || Q(x)) + D_{KL}(P(y|x) || Q(y|x))$$</p>
    <p>Where the conditional term is expected over $P(x)$:</p>
    $$D_{KL}(P(y|x) || Q(y|x)) = \mathbb{E}_{x \sim P} [ D_{KL}(P(y|X=x) || Q(y|X=x)) ]$$
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>M5. Convexity</h3>
                        <span class="difficulty-badge diff-medium">Medium</span>
                    </div>
                    <p>Is $D_{KL}(P || Q)$ convex with respect to $Q$? Why is this important?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p><strong>Yes</strong>, it is convex with respect to both inputs (individually and jointly).</p>
    <p><strong>Importance:</strong> This means that when optimizing a model $Q$ to minimize Forward KL (like in Maximum Likelihood Estimation), there is a unique global minimum (for convex model families), making optimization stable.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>M6. Max Likelihood Equivalent</h3>
                        <span class="difficulty-badge diff-medium">Medium</span>
                    </div>
                    <p>Show that maximizing the likelihood of data is equivalent to minimizing the Forward KL divergence from the empirical distribution to the model.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>Let $\hat{P}$ be the empirical data distribution. Maximizing likelihood is maximizing $\sum \log Q(x_i)$.</p>
    <p>Minimizing Forward KL:</p>
    $$\min D_{KL}(\hat{P} || Q) = \min \sum \hat{P}(x) \log \frac{\hat{P}(x)}{Q(x)}$$
    $$= \min \left[ \sum \hat{P}(x) \log \hat{P}(x) - \sum \hat{P}(x) \log Q(x) \right]$$
    <p>The first term is constant (data entropy). Minimizing the negative second term is equivalent to <strong>maximizing</strong> $\sum \hat{P}(x) \log Q(x)$, which is the log-likelihood.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>M7. Pinsker's Inequality</h3>
                        <span class="difficulty-badge diff-medium">Medium</span>
                    </div>
                    <p>State Pinsker's Inequality. What does it bound?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>Pinsker's Inequality relates KL Divergence to the Total Variation Distance (TV):</p>
    $$D_{KL}(P || Q) \ge 2 \delta(P, Q)^2$$
    <p>Where $\delta(P, Q) = \sup_A |P(A) - Q(A)|$.</p>
    <p>It essentially states that if KL divergence is small, the distributions must be close in terms of absolute probability values (Total Variation).</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>M8. Evidence Lower Bound (ELBO)</h3>
                        <span class="difficulty-badge diff-medium">Medium</span>
                    </div>
                    <p>In Variational Inference, we maximize the ELBO. How does this relate to KL Divergence?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>The ELBO (Evidence Lower Bound) is defined as:</p>
    $$\text{ELBO} = \log P(x) - D_{KL}(Q(z) || P(z|x))$$
    <p>Since $\log P(x)$ is fixed for a given data point, <strong>maximizing the ELBO</strong> is mathematically identical to <strong>minimizing the Reverse KL divergence</strong> between the approximate posterior $Q(z)$ and the true posterior $P(z|x)$.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>M9. Why VAEs use Reverse KL</h3>
                        <span class="difficulty-badge diff-medium">Medium</span>
                    </div>
                    <p>Why do Variational Autoencoders (VAEs) use Reverse KL $D_{KL}(Q || P)$ instead of Forward KL?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>Because the true posterior $P(z|x)$ is intractable (we can't evaluate it).</p>
    <p>Forward KL requires taking an expectation over $P(z|x)$, which is impossible.</p>
    <p>Reverse KL requires taking an expectation over $Q(z)$ (our model), which we can easily sample from. This makes optimization computationally feasible.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>M10. Fisher Information</h3>
                        <span class="difficulty-badge diff-medium">Medium</span>
                    </div>
                    <p>How is KL Divergence related to Fisher Information for small changes in parameters?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>For two distributions $P_\theta$ and $P_{\theta+\delta}$ that are close in parameter space:</p>
    $$D_{KL}(P_\theta || P_{\theta+\delta}) \approx \frac{1}{2} \delta^T I(\theta) \delta$$
    <p>Where $I(\theta)$ is the Fisher Information Matrix.</p>
    <p>This means KL Divergence acts as a "quadratic distance" metric locally, defined by the curvature of the likelihood function.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>H1. Multivariate Gaussian KL</h3>
                        <span class="difficulty-badge diff-hard">Hard</span>
                    </div>
                    <p>Write the formula for KL Divergence between two $k$-dimensional Multivariate Gaussians $\mathcal{N}(\mu_0, \Sigma_0)$ and $\mathcal{N}(\mu_1, \Sigma_1)$.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>The general formula is:</p>
    $$D_{KL} = \frac{1}{2} \left[ \text{tr}(\Sigma_1^{-1}\Sigma_0) + (\mu_1 - \mu_0)^T \Sigma_1^{-1} (\mu_1 - \mu_0) - k + \ln \frac{|\Sigma_1|}{|\Sigma_0|} \right]$$
    <p>Where $\text{tr}$ is the trace and $|\Sigma|$ is the determinant.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>H2. Exponential Distributions</h3>
                        <span class="difficulty-badge diff-hard">Hard</span>
                    </div>
                    <p>Derive the KL Divergence between two Exponential distributions with rates $\lambda_1$ and $\lambda_2$.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>PDF: $p(x) = \lambda e^{-\lambda x}$.</p>
    $$D_{KL} = \int_0^\infty \lambda_1 e^{-\lambda_1 x} \ln \left( \frac{\lambda_1 e^{-\lambda_1 x}}{\lambda_2 e^{-\lambda_2 x}} \right) dx$$
    $$= \ln \frac{\lambda_1}{\lambda_2} + (\lambda_1 - \lambda_2) \int_0^\infty x \lambda_1 e^{-\lambda_1 x} dx$$
    <p>Since the integral is the expected value $1/\lambda_1$:</p>
    $$= \ln \frac{\lambda_1}{\lambda_2} + \frac{\lambda_1 - \lambda_2}{\lambda_1}$$
    $$= \ln \frac{\lambda_1}{\lambda_2} + \frac{\lambda_2}{\lambda_1} - 1$$
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>H3. Poisson Distributions</h3>
                        <span class="difficulty-badge diff-hard">Hard</span>
                    </div>
                    <p>Calculate $D_{KL}(P || Q)$ for Poisson distributions with parameters $\lambda_1$ and $\lambda_2$.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>PMF: $P(k) = e^{-\lambda} \lambda^k / k!$.</p>
    $$D_{KL} = \sum_{k=0}^\infty P(k) \ln \frac{e^{-\lambda_1} \lambda_1^k}{e^{-\lambda_2} \lambda_2^k}$$
    $$= \sum P(k) [ (\lambda_2 - \lambda_1) + k \ln \frac{\lambda_1}{\lambda_2} ]$$
    $$= (\lambda_2 - \lambda_1) + \mathbb{E}_P[k] \ln \frac{\lambda_1}{\lambda_2}$$
    $$= \lambda_2 - \lambda_1 + \lambda_1 \ln \frac{\lambda_1}{\lambda_2}$$
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>H4. Jensen-Shannon Divergence</h3>
                        <span class="difficulty-badge diff-hard">Hard</span>
                    </div>
                    <p>Define Jensen-Shannon (JS) Divergence and explain two advantages it has over KL Divergence.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p><strong>Definition:</strong> Let $M = \frac{1}{2}(P+Q)$.</p>
    $$D_{JS}(P || Q) = \frac{1}{2} D_{KL}(P || M) + \frac{1}{2} D_{KL}(Q || M)$$
    <p><strong>Advantages:</strong></p>
    <ol>
        <li><strong>Symmetric:</strong> $D_{JS}(P || Q) = D_{JS}(Q || P)$.</li>
        <li><strong>Bounded:</strong> $0 \le D_{JS} \le 1$ (if using base 2), whereas KL is unbounded.</li>
    </ol>
    <p>It is used in Generative Adversarial Networks (GANs).</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>H5. Jeffreys Divergence</h3>
                        <span class="difficulty-badge diff-hard">Hard</span>
                    </div>
                    <p>What is Jeffreys Divergence?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>Jeffreys Divergence is the symmetric version of KL Divergence:</p>
    $$D_J(P || Q) = D_{KL}(P || Q) + D_{KL}(Q || P)$$
    <p>It essentially sums the penalties in both directions, making it a symmetric measure of difference.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>H6. Sanov's Theorem</h3>
                        <span class="difficulty-badge diff-hard">Hard</span>
                    </div>
                    <p>What is the physical interpretation of KL Divergence provided by Sanov's Theorem?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>Sanov's Theorem states that the probability of observing a rare empirical distribution $\hat{P}$ when samples are drawn from a true distribution $Q$ decays exponentially with the KL Divergence:</p>
    $$P(\text{observe } \hat{P}) \approx e^{-n D_{KL}(\hat{P} || Q)}$$
    <p>This links KL Divergence to the probability of large deviations (rare events).</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>H7. Contrastive Divergence</h3>
                        <span class="difficulty-badge diff-hard">Hard</span>
                    </div>
                    <p>In training Energy-Based Models (like RBMs), we use Contrastive Divergence (CD). How is it related to KL?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>We want to minimize $D_{KL}(P_{data} || P_{model})$. The gradient of this involves an intractable term (sampling from the model equilibrium).</p>
    <p>Contrastive Divergence approximates this gradient by running a Markov Chain for only a few steps ($k$) instead of to convergence.</p>
    <p>It essentially approximates:</p>
    $$CD_k \approx D_{KL}(P_0 || P_\infty) - D_{KL}(P_k || P_\infty)$$
    <p>Where $P_0$ is data, $P_k$ is after $k$ steps, and $P_\infty$ is the model distribution.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>H8. Information Geometry</h3>
                        <span class="difficulty-badge diff-hard">Hard</span>
                    </div>
                    <p>In Information Geometry, what role does KL Divergence play?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>In Information Geometry, probability distributions form a Riemannian manifold.</p>
    <p>The KL Divergence defines the divergence function on this manifold.</p>
    <p>The second derivative of KL Divergence (the Hessian) gives the <strong>Fisher Information Matrix</strong>, which acts as the Riemannian metric tensor ($g_{ij}$). This allows us to measure distances on the statistical manifold.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>H9. Reverse KL and Mode Collapse</h3>
                        <span class="difficulty-badge diff-hard">Hard</span>
                    </div>
                    <p>Explain mathematically why minimizing Reverse KL $D_{KL}(Q || P)$ can lead to 'Mode Collapse' in multimodal distributions.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>If $P$ is multimodal (e.g., mixture of two Gaussians) and $Q$ is unimodal (single Gaussian):</p>
    <p>The cost $Q(x) \log \frac{Q(x)}{P(x)}$ punishes $Q$ heavily for being non-zero where $P$ is small (regions between modes).</p>
    <p>To avoid crossing the low-probability valley between modes, $Q$ effectively chooses to cover <strong>only one mode</strong> and sets $Q(x) \approx 0$ everywhere else.</p>
    <p>This is safer than trying to stretch across the valley, which would incur a massive penalty.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>H10. Mutual Information as KL</h3>
                        <span class="difficulty-badge diff-hard">Hard</span>
                    </div>
                    <p>Prove that Mutual Information $I(X; Y)$ is the KL Divergence between the joint distribution and the product of marginals.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>Definition of MI:</p>
    $$I(X; Y) = \sum_{x,y} P(x,y) \log \frac{P(x,y)}{P(x)P(y)}$$
    <p>Definition of KL:</p>
    $$D_{KL}(P(x,y) || P(x)P(y)) = \sum_{x,y} P(x,y) \log \frac{P(x,y)}{P(x)P(y)}$$
    <p>The expressions are identical.</p>
    <p><strong>Interpretation:</strong> MI measures how far the joint distribution is from being independent (factorized).</p>
                        </div>
                    </details>
                </div>
            </section>

<div class="tutorial-footer-summary" style="margin: 3rem 0; padding: 2rem; background: #f8f9fa; border-radius: 8px; border-left: 4px solid #3b82f6;">
                <h1>03. KL Divergence</h1>
                <p class="lead">
                How do we measure the "distance" between two probability distributions? 
                KL divergence tells us the extra bits needed when we use the wrong distribution—and 
                it's everywhere in machine learning.
            </p>
            </div>
                <div class="tutorial-nav">
                    <a href="../02-cross-entropy/index.html" class="tutorial-nav-link prev">
                        <span class="nav-label">Previous</span>
                        <span class="nav-title">← Cross-Entropy Loss</span>
                    </a>
                    <a href="../14-entropy-connections/index.html" class="tutorial-nav-link next">
                        <span class="nav-label">Next</span>
                        <span class="nav-title">Entropy Connections →</span>
                    </a>
                </div>
                
            </article>
        
        </main>

        <!-- TOC (Right Side) -->
        <aside class="toc-container">
        <h4 class="toc-title">Contents</h4>
        <nav class="toc-list">
            <a href="#intuition" class="toc-link">The Intuition</a>
            <a href="#derivation" class="toc-link">Derivation</a>
            <a href="#properties" class="toc-link">Properties</a>
            <a href="#forward-vs-reverse" class="toc-link">Forward vs Reverse</a>
            <a href="#ml-applications" class="toc-link">ML Applications</a>
        </nav>
    </aside>
    </div>
    

    <!-- Table of Contents (floating) -->
    

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-brand">
                    <span class="logo-symbol">∇</span>
                    <span>ML Fundamentals</span>
                </div>
                <p class="footer-tagline">Deep understanding through first principles.</p>
            </div>
            <div class="footer-links">
                <a href="../../../index.html">Home</a>
                <a href="https://github.com/ml-entropy/ml-entropy.github.io" target="_blank">GitHub</a>
            </div>
        </div>
    </footer>

    <!-- Scripts -->
    <script src="../../../js/main.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            if (typeof renderMathInElement === 'function') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }
        });
    </script>
</body>
</html>
