<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>KL Divergence | ML Fundamentals</title>
    <meta name="description" content="Understanding Kullback-Leibler divergence. Learn why it measures the difference between probability distributions and its role in ML.">
    
    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&family=Playfair+Display:wght@400;500;600;700&display=swap" rel="stylesheet">
    
    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}, {left: '\\\\[', right: '\\\\]', display: true}, {left: '\\\\(', right: '\\\\)', display: false}], throwOnError: false});"></script>
    
    <!-- Styles -->
    <link rel="stylesheet" href="../../../css/main.css">
    <link rel="stylesheet" href="../../../css/components.css">
    
    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>∞</text></svg>">
<link rel="stylesheet" href="../../../css/sidebar.css">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar" id="navbar">
        <div class="nav-container">
            <a href="../../../index.html" class="nav-logo">
                <span class="logo-symbol">∇</span>
                <span class="logo-text">ML Fundamentals</span>
            </a>
            
            <button class="nav-toggle" id="navToggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            
            <div class="nav-menu" id="navMenu">
                <div class="nav-links">
                    <a href="../../../tutorials/ml/index.html" class="nav-link active">Machine Learning</a>
                    <a href="../../../tutorials/linear-algebra/index.html" class="nav-link">Linear Algebra</a>
                    <a href="../../../tutorials/calculus/index.html" class="nav-link">Calculus</a>
                    <a href="../../../tutorials/physics/index.html" class="nav-link">Physics</a>
                    <a href="../../../index.html#philosophy" class="nav-link">Philosophy</a>
                    <a href="../../../index.html#roadmap" class="nav-link">Roadmap</a>
                    <a href="https://github.com/ml-entropy/ml-entropy.github.io" class="nav-link" target="_blank">GitHub</a>
                </div>
                
                <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
                    <svg class="sun-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="5"/>
                        <path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"/>
                    </svg>
                    <svg class="moon-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                    </svg>
                </button>
            </div>
        </div>
    </nav>

    <!-- Tutorial Header -->
    <header class="tutorial-content-header">
        <div class="container">
            <nav class="breadcrumb">
                <a href="../../../index.html">Home</a>
                <span class="breadcrumb-separator">→</span>
                <a href="../index.html">Machine Learning</a>
                <span class="breadcrumb-separator">→</span>
                <span>KL Divergence</span>
            </nav>
            
            
            
            
            <div class="tutorial-tabs">
                <a href="#theory" class="tutorial-tab active">Theory</a>
                <a href="#code" class="tutorial-tab">Code</a>
                <a href="#exercises" class="tutorial-tab">Exercises</a>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    
    <!-- Main Content -->
    <div class="tutorial-wrapper">
        
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <aside class="tutorial-sidebar">
            <div class="sidebar-section">
                <h3 class="sidebar-section-title">Machine Learning</h3>
                <nav class="sidebar-nav">
                        <a href="../00-probability/index.html" class="sidebar-link">00. Probability Foundations</a>
                    <a href="../01-entropy/index.html" class="sidebar-link">01. Entropy Fundamentals</a>
                    <a href="../15-cross-entropy/index.html" class="sidebar-link">02. Cross-Entropy</a>
                    <a href="../02-kl-divergence/index.html" class="sidebar-link active">03. KL Divergence</a>
                    <a href="../03-distributions/index.html" class="sidebar-link">04. Normal Distributions</a>
                    <a href="../04-logarithms/index.html" class="sidebar-link">05. Why Logarithms?</a>
                    <a href="../05-combinatorics/index.html" class="sidebar-link">06. Combinatorics</a>
                    <a href="../06-backpropagation/index.html" class="sidebar-link">07. Backpropagation</a>
                    <a href="../07-regularization/index.html" class="sidebar-link">08. Regularization</a>
                    <a href="../08-batch-normalization/index.html" class="sidebar-link">09. Batch Normalization</a>
                    <a href="../09-learning-rate/index.html" class="sidebar-link">10. Learning Rate</a>
                    <a href="../10-cnn/index.html" class="sidebar-link">11. CNNs</a>
                    <a href="../11-rnn/index.html" class="sidebar-link">12. RNNs</a>
                    <a href="../12-vae/index.html" class="sidebar-link">13. VAE</a>
                    <a href="../13-variational-inference/index.html" class="sidebar-link">14. Variational Inference</a>
                    <a href="../14-entropy-connections/index.html" class="sidebar-link">15. Entropy Connections</a>
                </nav>
            </div>
            
            <div class="sidebar-section" style="margin-top: 2rem;">
                <h3 class="sidebar-section-title">Related Subjects</h3>
                <nav class="sidebar-nav">
                        <a href="../../linear-algebra/index.html" class="sidebar-link">Linear Algebra</a>
                    <a href="../../calculus/index.html" class="sidebar-link">Calculus</a>
                    <a href="../../physics/index.html" class="sidebar-link">Physics</a>
                </nav>
            </div>
        </aside>

        <!-- Main Article -->
        <main class="tutorial-main">
            
            <article class="article-content" id="theory">
                
                <!-- Section 1 -->
                <h2 id="intuition">The Intuition: Wrong Code, Extra Bits</h2>
                
                <p>
                    Imagine you're sending messages. If you know the true distribution $P$ of symbols, 
                    you can design an optimal code with average length $H(P)$ bits per symbol. 
                </p>
                
                <p>
                    But what if you <em>think</em> the distribution is $Q$ and design your code for that? 
                    You'll need extra bits on average. <strong>KL divergence measures these extra bits.</strong>
                </p>
                
                <div class="definition-box">
                    <div class="box-title">KL Divergence Definition</div>
                    <p style="margin-bottom: 0;">
                        The <strong>Kullback-Leibler divergence</strong> from $Q$ to $P$ is:
                        $$D_{KL}(P \| Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)} = \mathbb{E}_{P}\left[\log \frac{P(X)}{Q(X)}\right]$$
                    </p>
                </div>
                
                <p>
                    Think of it as: "The penalty for using model $Q$ when the truth is $P$."
                </p>

                <div class="note-box">
                    <div class="box-title">Relationship to Cross-Entropy</div>
                    <p style="margin-bottom: 0;">
                        KL Divergence is just the difference between the Cross-Entropy and the true Entropy:
                        $$D_{KL}(P \| Q) = H(P, Q) - H(P)$$
                        It measures the *avoidable* loss. Since $H(P)$ is a constant for a given dataset, minimizing Cross-Entropy is equivalent to minimizing KL Divergence.
                    </p>
                </div>

                <!-- Section 2 -->
                <h2 id="example">A Concrete Example: Biased Coin</h2>
                <p>
                    Let's make this tangible. Imagine a biased coin that we want to model.
                </p>
                <div class="example-box" style="background: var(--color-bg-alt); padding: 1.5rem; border-radius: 0.5rem; margin: 1.5rem 0;">
                    <p><strong>True Distribution (P):</strong></p>
                    <p>The coin is biased, landing Heads 70% of the time.</p>
                    <ul>
                        <li>$P(\text{Heads}) = 0.7$</li>
                        <li>$P(\text{Tails}) = 0.3$</li>
                    </ul>

                    <p><strong>Our Model (Q):</strong></p>
                    <p>We incorrectly believe the coin is fair.</p>
                    <ul>
                        <li>$Q(\text{Heads}) = 0.5$</li>
                        <li>$Q(\text{Tails}) = 0.5$</li>
                    </ul>

                    <p><strong>Calculating KL Divergence $D_{KL}(P \| Q)$:</strong></p>
                    <p>We sum over the two outcomes: Heads and Tails.</p>
                    <div class="math-block">
                        $$D_{KL}(P \| Q) = P(H)\log_2\frac{P(H)}{Q(H)} + P(T)\log_2\frac{P(T)}{Q(T)}$$
                        $$= 0.7 \cdot \log_2\frac{0.7}{0.5} + 0.3 \cdot \log_2\frac{0.3}{0.5}$$
                        $$= 0.7 \cdot \log_2(1.4) + 0.3 \cdot \log_2(0.6)$$
                        $$= 0.7 \cdot (0.485) + 0.3 \cdot (-0.737)$$
                        $$= 0.340 - 0.221 = \boxed{0.119 \text{ bits}}$$
                    </div>
                    <p><strong>Interpretation:</strong> By using the wrong model (fair coin), we will waste an average of <strong>0.119 extra bits</strong> for every coin flip we encode. This is the "cost" of our incorrect belief.</p>
                </div>

                <!-- Section 2 -->
                <h2 id="derivation">Deriving KL Divergence</h2>
                
                <p>
                    Let's derive KL divergence from the coding perspective:
                </p>
                
                <div class="math-derivation">
                    <div class="math-derivation-title">From Cross-Entropy to KL Divergence</div>
                    
                    <div class="math-step">
                        <div class="math-step-number">1</div>
                        <div class="math-step-content">
                            Optimal code for $Q$ has length $-\log Q(x)$ for symbol $x$
                        </div>
                    </div>
                    
                    <div class="math-step">
                        <div class="math-step-number">2</div>
                        <div class="math-step-content">
                            Expected length under $P$: $H(P,Q) = -\sum_x P(x) \log Q(x)$ (cross-entropy)
                        </div>
                    </div>
                    
                    <div class="math-step">
                        <div class="math-step-number">3</div>
                        <div class="math-step-content">
                            Extra bits beyond optimal: $D_{KL}(P\|Q) = H(P,Q) - H(P)$
                        </div>
                    </div>
                    
                    <div class="math-step">
                        <div class="math-step-number">4</div>
                        <div class="math-step-content">
                            Expanding:
                            $$D_{KL}(P\|Q) = -\sum_x P(x)\log Q(x) + \sum_x P(x)\log P(x)$$
                            $$= \sum_x P(x) \log\frac{P(x)}{Q(x)}$$
                        </div>
                    </div>
                </div>
                
                <!-- Section 3 -->
                <h2 id="properties">Properties of KL Divergence</h2>
                
                <h3>Non-Negativity (Gibbs' Inequality)</h3>
                
                <p>
                    KL divergence is always non-negative: $D_{KL}(P \| Q) \geq 0$, with equality iff $P = Q$.
                </p>
                
                <div class="math-derivation">
                    <div class="math-derivation-title">Proof Using Jensen's Inequality</div>
                    
                    <div class="math-step">
                        <div class="math-step-number">1</div>
                        <div class="math-step-content">
                            $-D_{KL}(P\|Q) = \sum_x P(x) \log\frac{Q(x)}{P(x)} = \mathbb{E}_P\left[\log\frac{Q(X)}{P(X)}\right]$
                        </div>
                    </div>
                    
                    <div class="math-step">
                        <div class="math-step-number">2</div>
                        <div class="math-step-content">
                            By Jensen's inequality (log is concave): $\mathbb{E}[\log Y] \leq \log \mathbb{E}[Y]$
                        </div>
                    </div>
                    
                    <div class="math-step">
                        <div class="math-step-number">3</div>
                        <div class="math-step-content">
                            $-D_{KL}(P\|Q) \leq \log \mathbb{E}_P\left[\frac{Q(X)}{P(X)}\right] = \log \sum_x Q(x) = \log 1 = 0$
                        </div>
                    </div>
                    
                    <div class="math-step">
                        <div class="math-step-number">4</div>
                        <div class="math-step-content">
                            Therefore: $D_{KL}(P\|Q) \geq 0$ ✓
                        </div>
                    </div>
                </div>
                
                <h3>Asymmetry</h3>
                
                <div class="warning-box">
                    <div class="box-title">KL Divergence is NOT a Distance</div>
                    <p style="margin-bottom: 0;">
                        $D_{KL}(P\|Q) \neq D_{KL}(Q\|P)$ in general. This asymmetry is important: 
                        "cost of using $Q$ for $P$" differs from "cost of using $P$ for $Q$." A true distance metric must be symmetric.
                    </p>
                </div>

                <div class="example-box" style="background: var(--color-bg-alt); padding: 1.5rem; border-radius: 0.5rem; margin: 1.5rem 0;">
                    <p><strong>Asymmetry Example (continued):</strong></p>
                    <p>Let's calculate the reverse KL, $D_{KL}(Q \| P)$, for our coin example:</p>
                    <div class="math-block">
                        $$D_{KL}(Q \| P) = Q(H)\log_2\frac{Q(H)}{P(H)} + Q(T)\log_2\frac{Q(T)}{P(T)}$$
                        $$= 0.5 \cdot \log_2\frac{0.5}{0.7} + 0.5 \cdot \log_2\frac{0.5}{0.3}$$
                        $$= 0.5 \cdot \log_2(0.714) + 0.5 \cdot \log_2(1.667)$$
                        $$= 0.5 \cdot (-0.485) + 0.5 \cdot (0.737)$$
                        $$= -0.243 + 0.369 = \boxed{0.126 \text{ bits}}$$
                    </div>
                    <p>As we can see, $0.119 \neq 0.126$. The "penalty" is different in each direction.</p>
                </div>

                <!-- Section 4 -->
                <h2 id="forward-vs-reverse">Forward vs Reverse KL</h2>
                
                <p>
                    The two directions of KL divergence have very different behaviors:
                </p>
                
                <h3>Forward KL: $D_{KL}(P \| Q)$ — Mean-Seeking</h3>
                
                <p>
                    Minimizing this makes $Q$ cover everywhere $P$ has mass. If $P(x) > 0$, then 
                    $Q(x)$ better not be zero (infinite penalty!). Results in <em>mode-covering</em> behavior.
                </p>
                
                <h3>Reverse KL: $D_{KL}(Q \| P)$ — Mode-Seeking</h3>
                
                <p>
                    Minimizing this makes $Q$ zero wherever $P$ is zero. The approximation $Q$ will 
                    <em>lock onto one mode</em> of $P$ rather than spread out. Common in variational inference.
                </p>

                <div class="figure-container" style="text-align: center; margin: 2rem 0;">
                    <svg width="100%" height="300" viewBox="0 0 800 300" style="background: var(--color-bg-alt); border-radius: 8px;">
                        <defs>
                            <path id="p-curve" d="M 100 250 C 150 50, 250 50, 300 250" fill="none" stroke="var(--color-primary)" stroke-width="4"/>
                            <path id="q-curve-fwd" d="M 50 250 C 175 50, 225 50, 350 250" fill="none" stroke="var(--color-accent)" stroke-width="4" stroke-dasharray="8,8"/>
                            <path id="q-curve-rev" d="M 225 250 C 250 50, 250 50, 300 250" fill="none" stroke="var(--color-accent)" stroke-width="4" stroke-dasharray="8,8"/>
                        </defs>

                        <!-- Forward KL -->
                        <g>
                            <text x="200" y="30" font-family="Inter, sans-serif" font-size="20" fill="currentColor" text-anchor="middle">Forward KL: $D_{KL}(P \| Q)$</text>
                            <text x="200" y="55" font-family="Inter, sans-serif" font-size="16" fill="currentColor" text-anchor="middle" style="font-style: italic;">"Mean-seeking" / Mode-covering</text>
                            <use href="#p-curve"/>
                            <use href="#q-curve-fwd"/>
                            <text x="175" y="150" font-family="JetBrains Mono, monospace" font-size="18" fill="var(--color-primary)">P</text>
                            <text x="100" y="100" font-family="JetBrains Mono, monospace" font-size="18" fill="var(--color-accent)">Q</text>
                            <text x="200" y="280" font-family="Inter, sans-serif" font-size="14" fill="currentColor" text-anchor="middle">Q must be non-zero where P is non-zero.</text>
                        </g>

                        <!-- Divider -->
                        <line x1="400" y1="20" x2="400" y2="280" stroke="var(--color-border)" stroke-width="2"/>

                        <!-- Reverse KL -->
                        <g transform="translate(400, 0)">
                            <text x="200" y="30" font-family="Inter, sans-serif" font-size="20" fill="currentColor" text-anchor="middle">Reverse KL: $D_{KL}(Q \| P)$</text>
                            <text x="200" y="55" font-family="Inter, sans-serif" font-size="16" fill="currentColor" text-anchor="middle" style="font-style: italic;">"Mode-seeking"</text>
                            <use href="#p-curve"/>
                            <use href="#q-curve-rev"/>
                            <text x="175" y="150" font-family="JetBrains Mono, monospace" font-size="18" fill="var(--color-primary)">P</text>
                            <text x="260" y="100" font-family="JetBrains Mono, monospace" font-size="18" fill="var(--color-accent)">Q</text>
                            <text x="200" y="280" font-family="Inter, sans-serif" font-size="14" fill="currentColor" text-anchor="middle">Q is encouraged to be zero where P is zero.</text>
                        </g>
                    </svg>
                    <figcaption style="font-size: 0.9rem; color: var(--color-text-secondary); margin-top: 0.5rem;">
                        Visualizing the difference between Forward and Reverse KL Divergence when approximating a bimodal distribution P.
                    </figcaption>
                </div>

                <div class="note-box">
                    <div class="box-title">Practical Implication</div>
                    <p style="margin-bottom: 0;">
                        VAEs minimize reverse KL, which is why they sometimes produce blurry samples— 
                        the model hedges its bets rather than committing to sharp modes.
                    </p>
                </div>
                
                <!-- Section 5 -->
                <h2 id="ml-applications">KL Divergence in Machine Learning</h2>
                
                <h3>Variational Inference</h3>
                
                <p>
                    We approximate intractable posteriors by minimizing:
                </p>
                
                <div class="math-block">
                    $$D_{KL}(q(\theta) \| p(\theta | \mathcal{D}))$$
                </div>
                
                <h3>Information Bottleneck</h3>
                
                <p>
                    Compression while preserving information uses KL terms for regularization.
                </p>
                
                <h3>Policy Optimization (RL)</h3>
                
                <p>
                    Trust region methods (TRPO, PPO) constrain policy updates using KL divergence 
                    to ensure stable learning.
                </p>
                
                <!-- Navigation -->
                
            <div class="tutorial-footer-summary" style="margin: 3rem 0; padding: 2rem; background: #f8f9fa; border-radius: 8px; border-left: 4px solid #3b82f6;">
                <h1>03. KL Divergence</h1>
                <p class="lead">
                How do we measure the "distance" between two probability distributions? 
                KL divergence tells us the extra bits needed when we use the wrong distribution—and 
                it's everywhere in machine learning.
            </p>
            </div>
                <div class="tutorial-nav">
                    <a href="../01-entropy/index.html" class="tutorial-nav-link prev">
                        <span class="nav-label">Previous</span>
                        <span class="nav-title">← Information Entropy</span>
                    </a>
                    <a href="../03-distributions/index.html" class="tutorial-nav-link next">
                        <span class="nav-label">Next</span>
                        <span class="nav-title">Probability Distributions →</span>
                    </a>
                </div>
                
            </article>
        
        </main>

        <!-- TOC (Right Side) -->
        <aside class="toc-container">
        <h4 class="toc-title">Contents</h4>
        <nav class="toc-list">
            <a href="#intuition" class="toc-link">The Intuition</a>
            <a href="#derivation" class="toc-link">Derivation</a>
            <a href="#properties" class="toc-link">Properties</a>
            <a href="#forward-vs-reverse" class="toc-link">Forward vs Reverse</a>
            <a href="#ml-applications" class="toc-link">ML Applications</a>
        </nav>
    </aside>
    </div>
    

    <!-- Table of Contents (floating) -->
    

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-brand">
                    <span class="logo-symbol">∇</span>
                    <span>ML Fundamentals</span>
                </div>
                <p class="footer-tagline">Deep understanding through first principles.</p>
            </div>
            <div class="footer-links">
                <a href="../../../index.html">Home</a>
                <a href="https://github.com/ml-entropy/ml-entropy.github.io" target="_blank">GitHub</a>
            </div>
        </div>
    </footer>

    <!-- Scripts -->
    <script src="../../../js/main.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            if (typeof renderMathInElement === 'function') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }
        });
    </script>
</body>
</html>
