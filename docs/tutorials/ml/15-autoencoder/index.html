<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Autoencoders | ML Fundamentals</title>
    <meta name="description" content="Learn autoencoders from first principles: compression, information bottleneck, sparse, denoising, and contractive variants. The foundation for VAEs.">

    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&family=Playfair+Display:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}, {left: '\\\\[', right: '\\\\]', display: true}, {left: '\\\\(', right: '\\\\)', display: false}], throwOnError: false});"></script>

    <!-- Styles -->
    <link rel="stylesheet" href="../../../css/main.css">
    <link rel="stylesheet" href="../../../css/components.css">

    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>∞</text></svg>">
<link rel="stylesheet" href="../../../css/sidebar.css">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar" id="navbar">
        <div class="nav-container">
            <a href="../../../index.html" class="nav-logo">
                <span class="logo-symbol">∇</span>
                <span class="logo-text">ML Fundamentals</span>
            </a>

            <button class="nav-toggle" id="navToggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>

            <div class="nav-menu" id="navMenu">
                <div class="nav-links">
                    <a href="../../../tutorials/ml/index.html" class="nav-link active">Machine Learning</a>
                    <a href="../../../tutorials/linear-algebra/index.html" class="nav-link">Linear Algebra</a>
                    <a href="../../../tutorials/calculus/index.html" class="nav-link">Calculus</a>
                    <a href="../../../tutorials/physics/index.html" class="nav-link">Physics</a>
                    <a href="../../../index.html#philosophy" class="nav-link">Philosophy</a>
                    <a href="../../../index.html#roadmap" class="nav-link">Roadmap</a>
                    <a href="https://github.com/ml-entropy/ml-entropy.github.io" class="nav-link" target="_blank">GitHub</a>
                </div>

                <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
                    <svg class="sun-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="5"/>
                        <path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"/>
                    </svg>
                    <svg class="moon-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                    </svg>
                </button>
            </div>
        </div>
    </nav>

    <!-- Tutorial Header -->
    <header class="tutorial-content-header">
        <div class="container">
            <nav class="breadcrumb">
                <a href="../../../index.html">Home</a>
                <span class="breadcrumb-separator">→</span>
                <a href="../index.html">Machine Learning</a>
                <span class="breadcrumb-separator">→</span>
                <span>Autoencoders</span>
            </nav>

            <div class="tutorial-tabs">
                <a href="#theory" class="tutorial-tab active">Theory</a>
                <a href="#code" class="tutorial-tab">Code</a>
                <a href="#exercises" class="tutorial-tab">Exercises</a>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <div class="tutorial-wrapper">
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <aside class="tutorial-sidebar">
            <div class="sidebar-section">
                <h3 class="sidebar-section-title">Machine Learning</h3>
                <nav class="sidebar-nav">
                        <a href="../00-probability/index.html" class="sidebar-link">00. Probability Foundations</a>
                    <a href="../01-logarithms/index.html" class="sidebar-link">01. Why Logarithms?</a>
                    <a href="../02-combinatorics/index.html" class="sidebar-link">02. Combinatorics</a>
                    <a href="../03-distributions/index.html" class="sidebar-link">03. Normal Distributions</a>
                    <a href="../04-entropy/index.html" class="sidebar-link">04. Entropy Fundamentals</a>
                    <a href="../05-cross-entropy/index.html" class="sidebar-link">05. Cross-Entropy</a>
                    <a href="../06-kl-divergence/index.html" class="sidebar-link">06. KL Divergence</a>
                    <a href="../07-entropy-connections/index.html" class="sidebar-link">07. Entropy Connections</a>
                    <a href="../08-backpropagation/index.html" class="sidebar-link">08. Backpropagation</a>
                    <a href="../09-regularization/index.html" class="sidebar-link">09. Regularization</a>
                    <a href="../10-batch-normalization/index.html" class="sidebar-link">10. Batch Normalization</a>
                    <a href="../11-learning-rate/index.html" class="sidebar-link">11. Learning Rate</a>
                    <a href="../12-cnn/index.html" class="sidebar-link">12. CNNs</a>
                    <a href="../13-rnn/index.html" class="sidebar-link">13. RNNs</a>
                    <a href="../14-rate-distortion/index.html" class="sidebar-link">14. Rate-Distortion Theory</a>
                    <a href="../15-autoencoder/index.html" class="sidebar-link active">15. Autoencoders</a>
                    <a href="../16-variational-inference/index.html" class="sidebar-link">16. Variational Inference</a>
                    <a href="../17-vae/index.html" class="sidebar-link">17. VAE</a>
                    <a href="../18-inductive-bias/index.html" class="sidebar-link">18. Inductive Bias</a>
                    <a href="../19-architectural-biases/index.html" class="sidebar-link">19. Architectural Biases</a>
                    <a href="../20-designing-biases/index.html" class="sidebar-link">20. Designing Biases</a>
                    <a href="../21-fst-fundamentals/index.html" class="sidebar-link">21. FST Fundamentals</a>
                    <a href="../22-weighted-fsts/index.html" class="sidebar-link">22. Weighted FSTs</a>
                    <a href="../23-fst-libraries/index.html" class="sidebar-link">23. FST Libraries</a>
                    <a href="../24-fst-applications/index.html" class="sidebar-link">24. FST Applications</a>
                    <a href="../25-neural-symbolic/index.html" class="sidebar-link">25. Neural-Symbolic Hybrids</a>
                    <a href="../26-sequence-alignment/index.html" class="sidebar-link">26. Sequence Alignment</a>
                    <a href="../27-mas-algorithm/index.html" class="sidebar-link">27. MAS Algorithm</a>
                    <a href="../28-forced-alignment/index.html" class="sidebar-link">28. Forced Alignment & MFA</a>
                    <a href="../29-tts-fundamentals/index.html" class="sidebar-link">29. TTS Fundamentals</a>
                    <a href="../30-neural-vocoders/index.html" class="sidebar-link">30. Neural Vocoders</a>
                    <a href="../31-tacotron/index.html" class="sidebar-link">31. Tacotron & Attention TTS</a>
                    <a href="../32-fastspeech/index.html" class="sidebar-link">32. FastSpeech & Non-AR TTS</a>
                    <a href="../33-glow-tts/index.html" class="sidebar-link">33. Glow-TTS & Flows</a>
                    <a href="../34-vits/index.html" class="sidebar-link">34. VITS: End-to-End TTS</a>
                    <a href="../35-bilingual-tts/index.html" class="sidebar-link">35. Bilingual TTS: RU+KY</a>
                </nav>
            </div>
            
            <div class="sidebar-section" style="margin-top: 2rem;">
                <h3 class="sidebar-section-title">Related Subjects</h3>
                <nav class="sidebar-nav">
                        <a href="../../linear-algebra/index.html" class="sidebar-link">Linear Algebra</a>
                    <a href="../../calculus/index.html" class="sidebar-link">Calculus</a>
                    <a href="../../physics/index.html" class="sidebar-link">Physics</a>
                </nav>
            </div>
        </aside>

        <!-- Main Article -->
        <main class="tutorial-main">

            <article class="article-content" id="theory">

                <h1>14. Autoencoders</h1>
                <p class="lead" style="color: var(--color-text-secondary); font-size: 1.125rem; margin-bottom: 2rem;">
                    Autoencoders learn to compress data into a lower-dimensional representation and reconstruct it.
                    We derive the architecture, understand the information bottleneck, and explore variants:
                    sparse, denoising, and contractive autoencoders.
                </p>

                <!-- Section 1 -->
                <h2 id="why-autoencoders">Why Autoencoders?</h2>

                <p>
                    Raw data lives in very high-dimensional spaces, but the <strong>intrinsic dimensionality</strong>
                    is often much lower. A 64×64 grayscale image has 4,096 dimensions, but the space of "meaningful"
                    images is a tiny manifold within $\mathbb{R}^{4096}$.
                </p>

                <div class="definition-box">
                    <div class="box-title">Autoencoder Architecture</div>
                    <p style="margin-bottom: 0;">
                        <strong>Encoder $f_\theta$:</strong> Maps input $x \in \mathbb{R}^n$ to latent code $z \in \mathbb{R}^d$ where $d \ll n$<br>
                        <strong>Decoder $g_\phi$:</strong> Maps latent code back to reconstruction $\hat{x} \in \mathbb{R}^n$<br>
                        <strong>Objective:</strong> Minimize $\|x - \hat{x}\|^2$ — the bottleneck forces learning what matters
                    </p>
                </div>

                <p>
                    A single-layer linear autoencoder learns the same subspace as PCA.
                    The power of autoencoders comes from <strong>nonlinear</strong> encodings via deep networks.
                </p>

                <!-- Section 2 -->
                <h2 id="mathematics">The Mathematics</h2>

                <p>
                    The encoder and decoder are parameterized as neural networks:
                </p>

                <div class="math-block">
                    $$z = f_\theta(x) = \sigma(Wx + b)$$
                </div>

                <div class="math-block">
                    $$\hat{x} = g_\phi(z) = \sigma'(W'z + b')$$
                </div>

                <p>For continuous data, we use <strong>MSE loss</strong>:</p>

                <div class="math-block">
                    $$\mathcal{L}_{MSE} = \|x - \hat{x}\|^2 = \sum_{j=1}^n (x_j - \hat{x}_j)^2$$
                </div>

                <p>For binary data, we use <strong>binary cross-entropy</strong> (maximum likelihood):</p>

                <div class="math-block">
                    $$\mathcal{L}_{BCE} = -\sum_{j=1}^n \left[x_j \log \hat{x}_j + (1 - x_j) \log(1 - \hat{x}_j)\right]$$
                </div>

                <div class="note-box">
                    <div class="box-title">Tied Weights</div>
                    <p>
                        Setting decoder weights $W' = W^T$ halves parameters and acts as regularization.
                        The decoder is forced to use the transpose of the encoder's learned features.
                    </p>
                    <p style="margin-bottom: 0;">
                        <strong>Why does this help?</strong> With untied weights, the encoder and decoder can "conspire" — the encoder maps data to an arbitrary code using $W$, and the decoder inverts it using an independent $W'$. The code need not be meaningful. Tying forces $W' = W^T$, which means the encoder and decoder share the same linear subspace. The columns of $W$ are used for encoding and the rows of $W$ (= columns of $W^T$) for decoding, so the latent space must be interpretable in both directions. This prevents degenerate solutions and pushes the learned features toward principal directions of the data.
                    </p>
                </div>

                <!-- Section 3 -->
                <h2 id="information-bottleneck">The Information Bottleneck</h2>

                <p>
                    The encoder performs <strong>lossy compression</strong>. With $d &lt; n$, information must be lost.
                    The autoencoder learns <em>which</em> information to keep. To understand this precisely,
                    we need three information-theoretic quantities.
                </p>

                <h3>Entropy $H(X)$</h3>

                <div class="math-block">
                    $$H(X) = -\sum_x p(x) \log p(x)$$
                </div>

                <p>
                    <strong>Intuition:</strong> $H(X)$ is the total uncertainty or average surprise in $X$. For our data,
                    $H(X)$ is fixed — it's how much information the images inherently contain.
                    (We covered this in detail in the <a href="../04-entropy/index.html">Entropy Fundamentals</a> tutorial.)
                </p>

                <h3>Conditional Entropy $H(X|Z)$</h3>

                <div class="math-block">
                    $$H(X|Z) = -\sum_{x,z} p(x,z) \log p(x|z)$$
                </div>

                <p>
                    <strong>Intuition:</strong> $H(X|Z)$ is the uncertainty remaining in $X$ after observing the latent code $Z$.
                    If the autoencoder is perfect, $H(X|Z) = 0$ — knowing $Z$ tells us everything about $X$.
                    If the encoding is useless, $H(X|Z) = H(X)$ — $Z$ tells us nothing.
                </p>

                <h3>Mutual Information $I(X; Z)$</h3>

                <div class="definition-box">
                    <div class="box-title">Mutual Information</div>
                    <p>
                        $$I(X; Z) = H(X) - H(X|Z)$$
                    </p>
                    <p style="margin-bottom: 0;">
                        $I(X; Z)$ measures how many bits of information about $X$ are preserved in $Z$.
                        The autoencoder's reconstruction loss is a proxy for maximizing $I(X; Z)$ — the better the
                        reconstruction, the more information $Z$ retained.
                    </p>
                </div>

                <p>
                    <strong>Key insight:</strong> Since $H(X)$ is fixed for our dataset, minimizing $H(X|Z)$
                    (which corresponds to minimizing reconstruction error) is equivalent to maximizing $I(X; Z)$.
                </p>

                <h3>Shannon's Rate-Distortion Theory</h3>

                <p>
                    We've seen that the bottleneck forces lossy compression and that the autoencoder maximizes $I(X; Z)$ within a capacity budget. But what is the <em>theoretical limit</em> of this trade-off? How few bits can we get away with for a given reconstruction quality? Shannon's rate-distortion theory gives the exact answer.
                </p>

                <p>
                    Imagine you need to describe a photograph over a phone call. If you can say 1000 words, you can describe it precisely. With 10 words, you can convey the gist: "a cat on a red couch." With 1 word: "cat." Each reduction in description length (rate) forces more loss of detail (distortion). Shannon's rate-distortion theory tells us the <em>fundamental limit</em>: for a given level of acceptable distortion, what is the absolute minimum number of bits needed?
                </p>

                <div class="definition-box">
                    <div class="box-title">The Core Setup</div>
                    <p>
                        <strong>Source:</strong> A random variable $X$ with distribution $p(x)$ (e.g., images from a dataset).<br>
                        <strong>Encoder:</strong> Maps $X$ to a compressed representation $Z$ via a conditional distribution $p(z|x)$.<br>
                        <strong>Decoder:</strong> Reconstructs $\hat{X}$ from $Z$ via $p(\hat{x}|z)$.<br>
                        <strong>Distortion measure:</strong> $d(x, \hat{x})$ is a function chosen by the practitioner to quantify how bad the reconstruction is. The most common choice is MSE: $d(x, \hat{x}) = \|x - \hat{x}\|^2$. For discrete data (e.g., text), Hamming distance $d(x, \hat{x}) = \sum_j \mathbf{1}[x_j \neq \hat{x}_j]$ counts the number of disagreeing symbols. The choice of $d$ shapes the entire rate-distortion trade-off.<br>
                        <strong>Rate:</strong> The mutual information $I(X; Z)$ — the number of bits the encoder "communicates" about $X$ through $Z$.
                    </p>
                    <p style="margin-bottom: 0;">
                        Why mutual information as the rate? Because $I(X; Z)$ measures exactly how many bits of information about $X$ are preserved in $Z$. Shannon's source coding theorem proves that you need at least $I(X;Z)$ bits to represent the encoder output.
                    </p>
                </div>

                <div class="math-derivation">
                    <div class="math-derivation-title">The Rate-Distortion Function $R(D)$</div>

                    <div class="math-step">
                        <div class="math-step-number">1</div>
                        <div class="math-step-content">
                            <strong>The question:</strong> What is the minimum number of bits (rate $R$) needed to describe
                            a source $X$ such that the expected distortion $\mathbb{E}[d(X, \hat{X})]$ is at most $D$?
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">2</div>
                        <div class="math-step-content">
                            <strong>Rate-distortion function:</strong>
                            $$R(D) = \min_{p(z|x): \mathbb{E}[d(x,\hat{x})] \leq D} I(X; Z)$$
                            <p><strong>Notation:</strong> The colon "$:$" reads as "such that." The expression $\min_{p(z|x): \mathbb{E}[d(x,\hat{x})] \leq D}$ means "minimize over all $p(z|x)$ <em>such that</em> $\mathbb{E}[d(x,\hat{x})] \leq D$." The part before the colon is the optimization variable; the part after is the constraint.</p>
                            <p>Read this carefully: we search over all possible encoders $p(z|x)$ — every conceivable way to compress $X$ into $Z$ — subject to the constraint that the average distortion stays within budget $D$. Among all such encoders, we pick the one that transmits the <em>fewest bits</em> $I(X;Z)$. This minimum is $R(D)$.</p>
                            <p style="margin-bottom: 0;">No encoder-decoder pair — no matter how clever — can achieve distortion $\leq D$ while transmitting fewer than $R(D)$ bits. This is the <strong>fundamental limit of lossy compression</strong>.</p>
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">3</div>
                        <div class="math-step-content">
                            <strong>Why minimize over $p(z|x)$?</strong>

                            <p>The optimization variable in $R(D)$ is the <strong>encoder</strong> $p(z|x)$. But what exactly is $p(z|x)$, and why is it written as a probability distribution rather than a function?</p>

                            <p><strong>$f(x)$ vs $p(z|x)$: Deterministic vs Stochastic Encoders</strong></p>

                            <p>A <strong>deterministic encoder</strong> is a function $z = f(x)$ that maps each input to exactly one code. For example, rounding to the nearest integer: $f(3.7) = 4$, always. Given the same input, you always get the same output — no randomness involved.</p>

                            <p>A <strong>stochastic encoder</strong> is a conditional probability distribution $p(z|x)$ that, given input $x$, can produce <em>different</em> codes $z$ with different probabilities. For example, given $x = 3.7$, a stochastic encoder might output $z = 4$ with probability 0.7 and $z = 3$ with probability 0.3. Each time you encode $x = 3.7$, you might get a different $z$.</p>

                            <p>The key insight: <strong>$f(x)$ is a special case of $p(z|x)$</strong>. When $f(x) = c$ for some fixed value $c$, the corresponding distribution is:</p>

                            $$p(z|x) = \delta(z - f(x))$$

                            <p><strong>What is a delta function?</strong> The Dirac delta function $\delta(z - c)$ is a distribution that puts <em>all</em> its probability mass at the single point $z = c$ and zero everywhere else. Think of it as an infinitely tall, infinitely narrow spike at $c$:</p>

                            <ul>
                                <li>$\delta(z - c) = 0$ for all $z \neq c$</li>
                                <li>$\int_{-\infty}^{\infty} \delta(z - c) \, dz = 1$ (total probability is 1)</li>
                                <li>$\int_{-\infty}^{\infty} g(z) \, \delta(z - c) \, dz = g(c)$ (picks out the value at $c$)</li>
                            </ul>

                            <p>So $p(z|x) = \delta(z - f(x))$ means: "given $x$, the code $z$ is <em>definitely</em> $f(x)$, with 100% probability." No uncertainty. This is what makes the encoder deterministic.</p>

                            <p><strong>What is a "soft distribution"?</strong> In contrast, a <strong>soft distribution</strong> spreads probability across multiple values instead of concentrating it all at one point. For example, a Gaussian encoder $p(z|x) = \mathcal{N}(z; \, \mu(x), \, \sigma^2)$ centers around $\mu(x)$ but allows $z$ to take nearby values with declining probability. The distribution is "soft" because it has width — it's a smooth bell curve instead of a sharp spike.</p>

                            <p><strong>How the encoder determines both rate and distortion:</strong></p>

                            <ul>
                                <li><strong>Rate</strong> $I(X; Z)$ is mutual information between input and code. A deterministic one-to-one encoder preserves all information: $I(X;Z) = H(X)$, which is the maximum rate (expensive). A stochastic encoder that adds noise to the encoding <em>reduces</em> $I(X;Z)$ — the noise destroys some information, so fewer bits need to be transmitted. More noise = lower rate.</li>
                                <li><strong>Distortion</strong> $\mathbb{E}[d(x, \hat{x})]$ depends on how well we can reconstruct. More noise in the encoder means the decoder has less information to work with, so reconstruction gets worse. More noise = higher distortion.</li>
                            </ul>

                            <p style="margin-bottom: 0;">The encoder <em>controls the trade-off</em>: adding noise to $p(z|x)$ reduces rate (good) but increases distortion (bad). The $R(D)$ optimization finds the <em>optimal amount</em> of stochasticity — the encoder that achieves the target distortion $D$ while transmitting the fewest bits. Sometimes the optimal encoder is stochastic: by slightly randomizing similar inputs into the same code, it saves bits at a small distortion cost. This is exactly the principle behind the VAE encoder $q(z|x) = \mathcal{N}(\mu(x), \sigma^2(x))$, which uses a learned Gaussian (stochastic) encoder.</p>
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">4</div>
                        <div class="math-step-content">
                            <strong>Properties of the $R(D)$ curve:</strong>
                            <ul style="margin-bottom: 0;">
                                <li><strong>Monotonically decreasing:</strong> More bits (higher rate) always allows less distortion. You can't make reconstruction worse by sending more information.</li>
                                <li><strong>Convex:</strong> The marginal benefit of each additional bit decreases. The first few bits capture the most important structure; later bits capture diminishing details.</li>
                                <li><strong>At $R = 0$:</strong> No information transmitted. The decoder can only output a fixed value $c$. To find the best constant, minimize $\mathbb{E}[\|X - c\|^2]$: take the derivative, set it to zero, and get $c = \mathbb{E}[X]$. The resulting distortion is $D = \text{Var}(X)$.</li>
                                <li><strong>At $D = 0$:</strong> Perfect reconstruction. For continuous sources, this requires $R \to \infty$ (infinite bits). For discrete sources with $|\mathcal{X}|$ outcomes, $R(0) = H(X) \leq \log_2|\mathcal{X}|$ bits.</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <h3>The Gaussian Case: A Worked Example</h3>

                <p>
                    For a Gaussian source $X \sim \mathcal{N}(0, \sigma^2)$ with MSE distortion, Shannon derived the <em>exact</em> rate-distortion function. This is one of the few cases with a closed-form solution, and it builds powerful intuition for the general case.
                </p>

                <div class="math-derivation">
                    <div class="math-derivation-title">Deriving $R(D)$ for a Gaussian Source</div>

                    <div class="math-step">
                        <div class="math-step-number">1</div>
                        <div class="math-step-content">
                            <strong>The optimal "test channel":</strong> Consider encoding $X$ by adding noise: $Z = X + N$ where $N \sim \mathcal{N}(0, \sigma_N^2)$ is independent of $X$. The decoder uses the MMSE estimator:
                            $$\hat{X} = \mathbb{E}[X|Z] = \frac{\sigma^2}{\sigma^2 + \sigma_N^2} Z$$
                            Why does the decoder scale $Z$? Because $Z = X + N$ is a noisy version of $X$. The factor $\frac{\sigma^2}{\sigma^2 + \sigma_N^2}$ shrinks $Z$ toward zero, "hedging" against the noise. When noise is small ($\sigma_N^2 \ll \sigma^2$), the factor is near 1 (trust the channel). When noise is large, the factor is near 0 (ignore the channel, output the mean).
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">2</div>
                        <div class="math-step-content">
                            <strong>Compute the distortion.</strong> The MSE of the MMSE estimator is the conditional variance:
                            $$D = \text{Var}(X|Z) = \sigma^2 - \frac{\sigma^4}{\sigma^2 + \sigma_N^2} = \frac{\sigma^2 \sigma_N^2}{\sigma^2 + \sigma_N^2}$$
                            Solving for the noise variance: $\sigma_N^2 = \frac{D\sigma^2}{\sigma^2 - D}$ (valid for $D < \sigma^2$).
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">3</div>
                        <div class="math-step-content">
                            <strong>Compute the rate.</strong> Since $X$ and $N$ are independent Gaussians, $Z \sim \mathcal{N}(0, \sigma^2 + \sigma_N^2)$. The mutual information is:
                            $$I(X;Z) = h(Z) - h(Z|X) = h(Z) - h(N) = \frac{1}{2}\log_2\frac{\sigma^2 + \sigma_N^2}{\sigma_N^2}$$
                        </div>
                    </div>

                    <div class="note-box" style="margin: 1rem 0;">
                        <div class="box-title">Notation: $h(\cdot)$ vs $H(\cdot)$</div>
                        <p style="margin-bottom: 0;">
                            Lowercase $h$ denotes <strong>differential entropy</strong> for continuous random variables: $h(Z) = -\int p(z) \log p(z) \, dz$. Uppercase $H$ denotes <strong>discrete entropy</strong>: $H(X) = -\sum_x p(x) \log p(x)$. For a Gaussian $Z \sim \mathcal{N}(\mu, \sigma^2)$, the differential entropy is $h(Z) = \frac{1}{2}\log_2(2\pi e \sigma^2)$. The step $h(Z|X) = h(N)$ follows because given $X$, the only randomness in $Z = X + N$ comes from $N$.
                        </p>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">4</div>
                        <div class="math-step-content">
                            <strong>Substitute $\sigma_N^2$ in terms of $D$:</strong>
                            $$\frac{\sigma^2 + \sigma_N^2}{\sigma_N^2} = 1 + \frac{\sigma^2}{\sigma_N^2} = 1 + \frac{\sigma^2 - D}{D} = \frac{\sigma^2}{D}$$
                            $$\boxed{R(D) = \frac{1}{2}\log_2\frac{\sigma^2}{D}} \quad \text{for } 0 < D \leq \sigma^2$$
                            Shannon proved this is optimal: no encoding scheme can do better.
                        </div>
                    </div>
                </div>

                <div class="note-box">
                    <div class="box-title">Reading the Gaussian $R(D)$ Formula</div>
                    <p>The formula $R(D) = \frac{1}{2}\log_2(\sigma^2 / D)$ says:</p>
                    <ul>
                        <li><strong>Every halving of distortion costs $\frac{1}{2}$ bit.</strong> From $D = 4$ to $D = 2$ to $D = 1$: each step adds 0.5 bits. This is a universal property of Gaussian sources.</li>
                        <li><strong>The ratio $\sigma^2/D$ is the signal-to-noise ratio.</strong> $R(D) = \frac{1}{2}\log_2(\text{SNR})$ — you need half a bit per unit of SNR in the log scale.</li>
                        <li><strong>At $D = \sigma^2$:</strong> $R = 0$. The "reconstruction" is just the mean (zero). Distortion equals the source variance.</li>
                    </ul>
                    <p><strong>Concrete example:</strong> For $\sigma^2 = 4$:</p>
                    <table style="width: 100%; border-collapse: collapse; margin: 0.5rem 0;">
                        <tr style="border-bottom: 2px solid var(--color-border);">
                            <th style="padding: 0.3rem 0.5rem; text-align: center;">$D$</th>
                            <th style="padding: 0.3rem 0.5rem; text-align: center;">$R(D)$ bits</th>
                            <th style="padding: 0.3rem 0.5rem; text-align: left;">Interpretation</th>
                        </tr>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.3rem 0.5rem; text-align: center;">4.0</td>
                            <td style="padding: 0.3rem 0.5rem; text-align: center;">0</td>
                            <td style="padding: 0.3rem 0.5rem;">No bits — just output the mean</td>
                        </tr>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.3rem 0.5rem; text-align: center;">2.0</td>
                            <td style="padding: 0.3rem 0.5rem; text-align: center;">0.5</td>
                            <td style="padding: 0.3rem 0.5rem;">Half a bit: "above or below average?"</td>
                        </tr>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.3rem 0.5rem; text-align: center;">1.0</td>
                            <td style="padding: 0.3rem 0.5rem; text-align: center;">1.0</td>
                            <td style="padding: 0.3rem 0.5rem;">One bit: which quartile?</td>
                        </tr>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.3rem 0.5rem; text-align: center;">0.25</td>
                            <td style="padding: 0.3rem 0.5rem; text-align: center;">2.0</td>
                            <td style="padding: 0.3rem 0.5rem;">Two bits: which of 4 buckets?</td>
                        </tr>
                        <tr>
                            <td style="padding: 0.3rem 0.5rem; text-align: center;">0.0625</td>
                            <td style="padding: 0.3rem 0.5rem; text-align: center;">3.0</td>
                            <td style="padding: 0.3rem 0.5rem;">Three bits: which of 8 buckets?</td>
                        </tr>
                    </table>
                    <p style="margin-bottom: 0;">Each additional bit roughly doubles the precision of the reconstruction.</p>
                </div>

                <h3>Multivariate Gaussian: The Water-Filling Principle</h3>

                <p>
                    For a multivariate Gaussian $X \sim \mathcal{N}(0, \Sigma)$ with eigenvalues $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_n$ (the principal component variances), the rate-distortion function has a beautiful "water-filling" solution:
                </p>

                $$R(D) = \sum_{i=1}^n \max\left(0, \frac{1}{2}\log_2\frac{\lambda_i}{\theta}\right)$$

                <p>
                    where $\theta$ is chosen so that $\sum_{i=1}^n \min(\lambda_i, \theta) = D$. The parameter $\theta$ acts as a "water level." Imagine the eigenvalues as pillars of different heights, and you pour water to level $\theta$:
                </p>

                <ul>
                    <li><strong>Components where $\lambda_i > \theta$:</strong> These get encoded with $\frac{1}{2}\log_2(\lambda_i/\theta)$ bits each. The distortion contribution is $\theta$ (the water level).</li>
                    <li><strong>Components where $\lambda_i \leq \theta$:</strong> These are <em>discarded entirely</em> (zero bits). Their full variance $\lambda_i$ contributes to distortion. These components contain less information than the encoding cost would justify.</li>
                </ul>

                <div class="warning-box">
                    <div class="box-title">Why This Matters for Autoencoders</div>
                    <p>The water-filling solution explains exactly what a linear autoencoder does. With bottleneck dimension $d$, it keeps the top $d$ principal components and discards the rest. This is optimal for Gaussian data! For non-Gaussian data, nonlinear autoencoders can do better — they can exploit non-linear structure that PCA misses.</p>
                    <p style="margin-bottom: 0;">The bottleneck dimension $d$ implicitly sets the rate: a $d$-dimensional continuous bottleneck can transmit at most a certain number of bits per sample (depending on the precision of the activations). Training finds the best encoding within this budget.</p>
                </div>

                <div class="definition-box">
                    <div class="box-title">Rate-Distortion Trade-off Summary</div>
                    <p>
                        <strong>Rate:</strong> Bits needed to represent $z$ (limited by bottleneck dimension $d$)<br>
                        <strong>Distortion:</strong> Reconstruction error $\|x - \hat{x}\|^2$<br>
                        <strong>Trade-off:</strong> Smaller bottleneck &rarr; more compression &rarr; higher distortion
                    </p>
                    <p style="margin-bottom: 0;">
                        An autoencoder with bottleneck dimension $d=2$ on MNIST (784-dim) achieves roughly 99.7% compression
                        — the network must learn which 0.3% of information matters most.
                    </p>
                </div>

                <!-- Section 4 -->
                <h2 id="undercomplete-overcomplete">Undercomplete vs Overcomplete</h2>

                <p>
                    <strong>Undercomplete ($d &lt; n$):</strong> The bottleneck alone forces compression. Simple and effective,
                    but may be too restrictive for complex data.
                </p>

                <p>
                    <strong>Overcomplete ($d \geq n$):</strong> The network can learn the identity function with zero loss!
                    We must add explicit regularization to prevent trivial solutions.
                </p>

                <!-- Section 5 -->
                <h2 id="sparse-autoencoders">Sparse Autoencoders</h2>

                <p>
                    Add a sparsity penalty so most latent units are inactive for any given input:
                </p>

                <div class="math-block">
                    $$\mathcal{L} = \|x - \hat{x}\|^2 + \lambda \sum_{j=1}^d |z_j|$$
                </div>

                <p>
                    For a more principled approach, use the <strong>KL divergence sparsity penalty</strong>.
                    Define the average activation $\hat{\rho}_j = \frac{1}{N}\sum_{i=1}^N z_j^{(i)}$ and
                    penalize deviation from target sparsity $\rho$:
                </p>

                <div class="math-block">
                    $$\Omega_{sparse} = \sum_{j=1}^d D_{KL}(\rho \| \hat{\rho}_j) = \sum_{j=1}^d \left[\rho \log \frac{\rho}{\hat{\rho}_j} + (1-\rho)\log\frac{1-\rho}{1-\hat{\rho}_j}\right]$$
                </div>

                <div class="note-box">
                    <div class="box-title">Why Sparsity?</div>
                    <p style="margin-bottom: 0;">
                        Sparse codes enable overcomplete representations, produce interpretable features
                        (each unit learns a distinct pattern), and connect to biological neural coding where
                        cortical neurons have sparse firing patterns.
                    </p>
                </div>

                <!-- Section 6 -->
                <h2 id="denoising-autoencoders">Denoising Autoencoders</h2>

                <p>
                    A denoising autoencoder flips the usual setup: instead of compressing clean data, we deliberately <strong>corrupt the input</strong> and train the network to recover the original. This forces the network to learn the underlying structure of the data — not just memorize a mapping.
                </p>

                <div class="math-derivation">
                    <div class="math-derivation-title">Denoising Training Procedure</div>

                    <div class="math-step">
                        <div class="math-step-number">1</div>
                        <div class="math-step-content">
                            <strong>Corrupt:</strong> $\tilde{x} = \text{corrupt}(x)$ (Gaussian noise, masking, or salt-and-pepper). For Gaussian: $\tilde{x} = x + \epsilon$ where $\epsilon \sim \mathcal{N}(0, \sigma^2 I)$.
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">2</div>
                        <div class="math-step-content">
                            <strong>Encode:</strong> $z = f_\theta(\tilde{x})$ — the encoder sees the <em>corrupted</em> input.
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">3</div>
                        <div class="math-step-content">
                            <strong>Decode:</strong> $\hat{x} = g_\phi(z)$ — the decoder tries to produce the <em>clean</em> original.
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">4</div>
                        <div class="math-step-content">
                            <strong>Loss:</strong> $\mathcal{L} = \|x - \hat{x}\|^2$ — compare to the <em>original</em> $x$, not the corrupted $\tilde{x}$. This is the key: the target is the clean data.
                        </div>
                    </div>
                </div>

                <p>
                    Why does corruption help? A regular autoencoder can "cheat" by memorizing the identity function. But a denoising autoencoder cannot simply pass the input through, because the input is corrupted — copying $\tilde{x}$ to the output would reproduce the noise. To minimize $\|x - \hat{x}\|^2$, the network must learn what clean data looks like, essentially learning the <em>data manifold</em>.
                </p>

                <h3>The Optimal Denoising Function</h3>

                <p>
                    What is the best possible denoiser? Given a noisy observation $\tilde{x}$, many different clean images $x$ could have produced it (by adding different noise realizations). The best we can do, in the MSE sense, is to output the <strong>average</strong> of all clean images that could have generated $\tilde{x}$:
                </p>

                <div class="definition-box">
                    <div class="box-title">Optimal Denoiser = Conditional Expectation</div>
                    <p>The function that minimizes $\mathbb{E}[\|X - g(\tilde{X})\|^2]$ over all possible functions $g$ is:</p>
                    $$g^*(\tilde{x}) = \mathbb{E}[X \mid \tilde{X} = \tilde{x}]$$
                    <p style="margin-bottom: 0;">This is the <strong>posterior mean</strong>: "given that I observed the noisy version $\tilde{x}$, what is the expected value of the original clean $x$?" Geometrically, the conditional expectation is the "center of mass" of the posterior distribution $p(x|\tilde{x})$ — the point in data space that minimizes the average squared distance to all plausible clean images, weighted by their probability.</p>
                </div>

                <p>
                    <strong>Intuition:</strong> Suppose you see a blurry, noisy photo of a face. The noisy pixel values are consistent with many possible clean faces. The conditional expectation $\mathbb{E}[X | \tilde{X} = \tilde{x}]$ averages over all plausible clean faces weighted by how likely each one is. This is the safest guess — it minimizes the expected squared error.
                </p>

                <p>
                    <strong>Why conditional expectation, not just the most likely $x$?</strong> The MAP (maximum a posteriori) estimate $\arg\max_x p(x|\tilde{x})$ finds the single most probable clean image. But for MSE loss, the conditional mean is provably optimal. Consider: if two clean images $x_1$ and $x_2$ are equally likely given $\tilde{x}$, the MSE-optimal output is their midpoint $\frac{x_1 + x_2}{2}$, not either one. This explains why neural denoisers sometimes produce slightly blurry outputs — they are correctly averaging over multiple plausible reconstructions.
                </p>

                <h3>What is the Score Function?</h3>

                <p>
                    The <strong>score function</strong> of a probability distribution $p(x)$ is the gradient of its log-density:
                </p>

                $$s(x) = \nabla_x \log p(x)$$

                <div class="definition-box">
                    <div class="box-title">Score Function: Geometric Meaning</div>
                    <p>The score $\nabla_x \log p(x)$ is a <strong>vector field</strong> that points in the direction of increasing probability density. At any point $x$ in the data space:</p>
                    <ul>
                        <li><strong>Direction:</strong> The score points toward nearby regions of higher density — toward the "center" of the data distribution.</li>
                        <li><strong>Magnitude:</strong> Large when $x$ is far from high-density regions (strong pull back). Small near density peaks (already at a good location).</li>
                        <li><strong>At a mode:</strong> $\nabla_x \log p(x) = 0$ — the score vanishes at peaks of the distribution.</li>
                    </ul>
                    <p style="margin-bottom: 0;">
                        Think of it as a "force field" that pushes any point toward where data is likely to be. If you follow the score arrows, you climb toward the modes of $p(x)$.
                    </p>
                </div>

                <p>
                    <strong>Why is the score useful?</strong> The score function tells us about $p(x)$ without requiring us to compute $p(x)$ itself. Computing $p(x)$ requires the intractable normalization constant $Z = \int p^*(x)dx$ (where $p(x) = p^*(x)/Z$). But the score cancels $Z$:
                </p>

                $$\nabla_x \log p(x) = \nabla_x \log p^*(x) - \nabla_x \log Z = \nabla_x \log p^*(x)$$

                <p>
                    So the score gives us the direction toward high-density regions without ever needing to compute the normalization constant. This makes it computationally attractive for generative modeling.
                </p>

                <h3>Tweedie's Formula: Connecting Denoising to the Score</h3>

                <p>
                    Here is the remarkable connection: the optimal denoiser directly tells us the score function. This result, known as <strong>Tweedie's formula</strong>, is the theoretical foundation of modern diffusion models. The derivation has three steps: (1) write the noisy distribution $p_\sigma(\tilde{x})$ as a mixture of Gaussians centered on clean data points, (2) compute the score $\nabla_{\tilde{x}} \log p_\sigma(\tilde{x})$ using the quotient rule, and (3) recognize that the result is exactly the optimal denoising direction scaled by $1/\sigma^2$.
                </p>

                <div class="math-derivation">
                    <div class="math-derivation-title">Tweedie's Formula: Denoising = Score Estimation</div>

                    <div class="math-step">
                        <div class="math-step-number">1</div>
                        <div class="math-step-content">
                            <strong>Setup:</strong> We add Gaussian noise to clean data: $\tilde{X} = X + \epsilon$ where $\epsilon \sim \mathcal{N}(0, \sigma^2 I)$. The noisy distribution is $p_\sigma(\tilde{x}) = \int \mathcal{N}(\tilde{x}; x, \sigma^2 I) \, p(x) \, dx$ — a smoothed version of $p(x)$.
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">2</div>
                        <div class="math-step-content">
                            <strong>The score of the noisy distribution:</strong> We compute $\nabla_{\tilde{x}} \log p_\sigma(\tilde{x})$ using the identity $\nabla \log f = \frac{\nabla f}{f}$:
                            $$\nabla_{\tilde{x}} \log p_\sigma(\tilde{x}) = \frac{\int \nabla_{\tilde{x}} \mathcal{N}(\tilde{x}; x, \sigma^2 I) \cdot p(x) \, dx}{p_\sigma(\tilde{x})}$$
                            Since $\nabla_{\tilde{x}} \log \mathcal{N}(\tilde{x}; x, \sigma^2 I) = \frac{x - \tilde{x}}{\sigma^2}$, this becomes:
                            $$= \frac{1}{\sigma^2} \int (x - \tilde{x}) \cdot p(x | \tilde{x}) \, dx = \frac{\mathbb{E}[X|\tilde{X} = \tilde{x}] - \tilde{x}}{\sigma^2}$$
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">3</div>
                        <div class="math-step-content">
                            <strong>Tweedie's formula:</strong> Since $g^*(\tilde{x}) = \mathbb{E}[X|\tilde{X} = \tilde{x}]$ (the optimal denoiser), we get:
                            $$\boxed{\nabla_{\tilde{x}} \log p_\sigma(\tilde{x}) = \frac{g^*(\tilde{x}) - \tilde{x}}{\sigma^2}}$$
                            <p>In words: the <strong>score of the noisy distribution equals the optimal denoising direction, scaled by $1/\sigma^2$</strong>.</p>
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">4</div>
                        <div class="math-step-content">
                            <strong>Reading the formula:</strong>
                            <ul style="margin-bottom: 0;">
                                <li>$g^*(\tilde{x}) - \tilde{x}$ is the <strong>denoising direction</strong>: the vector from the noisy point to the estimated clean point. It points "toward the data."</li>
                                <li>Dividing by $\sigma^2$ normalizes by the noise level. More noise $\Rightarrow$ divide by a larger number $\Rightarrow$ smaller score magnitude (less confidence in the direction).</li>
                                <li>A trained denoising autoencoder with output $g(\tilde{x}) \approx g^*(\tilde{x})$ gives us a score estimator: $\nabla_{\tilde{x}} \log p_\sigma(\tilde{x}) \approx \frac{g(\tilde{x}) - \tilde{x}}{\sigma^2}$.</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <div class="warning-box">
                    <div class="box-title">Connection to Diffusion Models (DDPM, Score SDE)</div>
                    <p>
                        This connection is <em>the</em> foundational idea behind modern diffusion models. The training objective of DDPM (Denoising Diffusion Probabilistic Models) and Score SDE is essentially: train a neural network to denoise data at many noise levels $\sigma_1 > \sigma_2 > \cdots > \sigma_L$. By Tweedie's formula, this simultaneously estimates the score function $\nabla_x \log p_\sigma(x)$ at each noise level.
                    </p>
                    <p style="margin-bottom: 0;">
                        To generate new samples, start from pure noise $x_T \sim \mathcal{N}(0, I)$ and iteratively "denoise" using the learned score — each step moves $x_t$ toward higher-density regions of the data distribution. This is called <strong>Langevin dynamics</strong>: $x_{t-1} = x_t + \eta \nabla_x \log p_{\sigma_t}(x_t) + \sqrt{2\eta}\epsilon$. The denoising autoencoder, a simple idea from 2008, turned out to be the key insight behind the AI image generation revolution.
                    </p>
                </div>

                <!-- Section 7 -->
                <h2 id="contractive-autoencoders">Contractive Autoencoders</h2>

                <p>
                    Penalize the encoder's Jacobian to encourage locally invariant representations:
                </p>

                <div class="math-block">
                    $$\mathcal{L} = \|x - \hat{x}\|^2 + \lambda \left\|\frac{\partial f_\theta(x)}{\partial x}\right\|_F^2$$
                </div>

                <p>
                    The Frobenius norm of the Jacobian measures how sensitive the encoding is to input perturbations.
                    Minimizing it makes $z$ locally invariant, pulling the representation toward a lower-dimensional manifold.
                </p>

                <h3>Comparison of Regularized Autoencoders</h3>

                <table style="width: 100%; border-collapse: collapse; margin: 1rem 0;">
                    <thead>
                        <tr style="border-bottom: 2px solid var(--color-border);">
                            <th style="padding: 0.75rem; text-align: left;">Variant</th>
                            <th style="padding: 0.75rem; text-align: left;">Regularizer</th>
                            <th style="padding: 0.75rem; text-align: left;">Effect</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.75rem;">Sparse</td>
                            <td style="padding: 0.75rem;">$\lambda \sum|z_j|$</td>
                            <td style="padding: 0.75rem;">Few active units per input</td>
                        </tr>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.75rem;">Denoising</td>
                            <td style="padding: 0.75rem;">Corrupt input</td>
                            <td style="padding: 0.75rem;">Learn to denoise; score matching</td>
                        </tr>
                        <tr>
                            <td style="padding: 0.75rem;">Contractive</td>
                            <td style="padding: 0.75rem;">$\lambda \|J\|_F^2$</td>
                            <td style="padding: 0.75rem;">Locally invariant encodings</td>
                        </tr>
                    </tbody>
                </table>

                <!-- Section 8 -->
                <h2 id="applications">Applications</h2>

                <h3>Dimensionality Reduction</h3>
                <p>Use encoder output $z$ for visualization (2D/3D), clustering, or classification.</p>

                <h3>Anomaly Detection</h3>
                <p>
                    Train on "normal" data. Anomalies produce high reconstruction error:
                </p>
                <div class="math-block">
                    $$\text{anomaly score}(x) = \|x - g_\phi(f_\theta(x))\|^2$$
                </div>

                <h3>Pretraining</h3>
                <p>
                    Layer-wise autoencoder pretraining was historically crucial for deep networks.
                    The principle of learning useful representations remains central to modern self-supervised learning.
                </p>

                <!-- Section 9 -->
                <h2 id="from-ae-to-vae">From Autoencoders to VAEs</h2>

                <p>
                    Standard autoencoders learn <strong>deterministic</strong> encodings: $z = f_\theta(x)$.
                    This means the latent space may have "holes" and we can't meaningfully sample new data.
                </p>

                <div class="definition-box">
                    <div class="box-title">The VAE Solution (Next Tutorial)</div>
                    <p style="margin-bottom: 0;">
                        VAEs make encoding <strong>probabilistic</strong>: $q_\phi(z|x) = \mathcal{N}(\mu_\phi(x), \sigma_\phi^2(x))$<br>
                        Adding KL regularization gives a smooth, continuous latent space that enables generation.
                    </p>
                </div>

                <table style="width: 100%; border-collapse: collapse; margin: 1rem 0;">
                    <thead>
                        <tr style="border-bottom: 2px solid var(--color-border);">
                            <th style="padding: 0.75rem; text-align: left;">Autoencoder</th>
                            <th style="padding: 0.75rem; text-align: left;">VAE</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.75rem;">$z = f(x)$ (point)</td>
                            <td style="padding: 0.75rem;">$z \sim q(z|x)$ (distribution)</td>
                        </tr>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.75rem;">Bottleneck regularizes</td>
                            <td style="padding: 0.75rem;">KL divergence regularizes</td>
                        </tr>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.75rem;">Can't generate</td>
                            <td style="padding: 0.75rem;">Can generate</td>
                        </tr>
                        <tr>
                            <td style="padding: 0.75rem;">Reconstruction only</td>
                            <td style="padding: 0.75rem;">Reconstruction + KL</td>
                        </tr>
                    </tbody>
                </table>

                <!-- Navigation -->
                <div class="tutorial-nav">
                    <a href="../14-rate-distortion/index.html" class="tutorial-nav-link prev">
                        <span class="nav-label">Previous</span>
                        <span class="nav-title">← Rate-Distortion Theory</span>
                    </a>
                    <a href="../16-variational-inference/index.html" class="tutorial-nav-link next">
                        <span class="nav-label">Next</span>
                        <span class="nav-title">Variational Inference →</span>
                    </a>
                </div>

            </article>

            <!-- ==================== CODE TAB ==================== -->
            <article class="article-content" id="code" style="display: none;">
                <h2>Python Code Examples</h2>
                <p>Three complete, runnable examples demonstrating autoencoder variants on MNIST using PyTorch: a basic autoencoder, a sparse autoencoder with KL sparsity penalty, and a denoising autoencoder.</p>

                <!-- Code Example 1 -->
                <h3>1. Basic Autoencoder on MNIST</h3>
                <p>A fully-connected autoencoder that compresses 784-dimensional MNIST images to a 32-dimensional latent space and reconstructs them. Includes the encoder, decoder, training loop, and visualization of reconstructions.</p>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np

# ---- Model ----
class Autoencoder(nn.Module):
    def __init__(self, input_dim=784, latent_dim=32):
        super().__init__()
        # Encoder: 784 -> 256 -> 128 -> 32
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, latent_dim),
            nn.ReLU()
        )
        # Decoder: 32 -> 128 -> 256 -> 784
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Linear(256, input_dim),
            nn.Sigmoid()  # output in [0, 1] for pixel values
        )

    def forward(self, x):
        z = self.encoder(x)
        x_hat = self.decoder(z)
        return x_hat, z

# ---- Data ----
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Lambda(lambda x: x.view(-1))  # flatten 28x28 -> 784
])

train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)

# ---- Training ----
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = Autoencoder(input_dim=784, latent_dim=32).to(device)
optimizer = optim.Adam(model.parameters(), lr=1e-3)
criterion = nn.MSELoss()

num_epochs = 20
for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    for batch_x, _ in train_loader:
        batch_x = batch_x.to(device)

        x_hat, z = model(batch_x)
        loss = criterion(x_hat, batch_x)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item() * batch_x.size(0)

    avg_loss = total_loss / len(train_dataset)
    if (epoch + 1) % 5 == 0:
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.6f}")

# ---- Evaluate and Visualize ----
model.eval()
with torch.no_grad():
    test_x, _ = next(iter(test_loader))
    test_x = test_x.to(device)
    recon_x, latent_z = model(test_x)

n = 10  # number of images to display
fig, axes = plt.subplots(2, n, figsize=(15, 3))
for i in range(n):
    # Original
    axes[0, i].imshow(test_x[i].cpu().view(28, 28), cmap='gray')
    axes[0, i].axis('off')
    if i == 0:
        axes[0, i].set_title('Original', fontsize=10)

    # Reconstruction
    axes[1, i].imshow(recon_x[i].cpu().view(28, 28), cmap='gray')
    axes[1, i].axis('off')
    if i == 0:
        axes[1, i].set_title('Reconstructed', fontsize=10)

plt.suptitle(f'Basic Autoencoder (latent_dim=32, MSE={avg_loss:.4f})')
plt.tight_layout()
plt.savefig('basic_autoencoder_results.png', dpi=150)
plt.show()

print(f"\nCompression ratio: {784}/{32} = {784/32:.1f}x")
print(f"Information retained: {32}/{784} = {32/784*100:.1f}%")</code></pre>

                <!-- Code Example 2 -->
                <h3>2. Sparse Autoencoder with KL Sparsity Penalty</h3>
                <p>This extends the basic autoencoder with a KL divergence sparsity penalty that encourages most hidden units to be inactive for any given input. The target sparsity $\rho$ controls how sparse the latent activations should be.</p>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

# ---- KL Sparsity Penalty ----
def kl_divergence_sparsity(rho, rho_hat):
    """
    KL divergence between Bernoulli(rho) and Bernoulli(rho_hat).
    rho: target sparsity (scalar, e.g. 0.05)
    rho_hat: average activation per unit (tensor of shape [latent_dim])
    Returns: sum of KL divergences across all latent units
    """
    # Clamp to avoid log(0)
    rho_hat = torch.clamp(rho_hat, 1e-6, 1 - 1e-6)
    kl = rho * torch.log(rho / rho_hat) + (1 - rho) * torch.log((1 - rho) / (1 - rho_hat))
    return kl.sum()

# ---- Model ----
class SparseAutoencoder(nn.Module):
    def __init__(self, input_dim=784, latent_dim=128):
        super().__init__()
        # Overcomplete: latent_dim > input bottleneck (but we add sparsity)
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Linear(256, latent_dim),
            nn.Sigmoid()  # activations in [0,1] for KL sparsity
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 256),
            nn.ReLU(),
            nn.Linear(256, input_dim),
            nn.Sigmoid()
        )

    def forward(self, x):
        z = self.encoder(x)
        x_hat = self.decoder(z)
        return x_hat, z

# ---- Data ----
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Lambda(lambda x: x.view(-1))
])
train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)

# ---- Training ----
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = SparseAutoencoder(input_dim=784, latent_dim=128).to(device)
optimizer = optim.Adam(model.parameters(), lr=1e-3)

rho = 0.05          # target sparsity: 5% average activation
beta = 3.0          # sparsity penalty weight
num_epochs = 20

for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    total_recon = 0
    total_sparse = 0

    for batch_x, _ in train_loader:
        batch_x = batch_x.to(device)

        x_hat, z = model(batch_x)

        # Reconstruction loss
        recon_loss = nn.functional.mse_loss(x_hat, batch_x)

        # Sparsity penalty: average activation across the batch
        rho_hat = z.mean(dim=0)  # shape: [latent_dim]
        sparse_loss = kl_divergence_sparsity(rho, rho_hat)

        # Total loss
        loss = recon_loss + beta * sparse_loss

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item() * batch_x.size(0)
        total_recon += recon_loss.item() * batch_x.size(0)
        total_sparse += sparse_loss.item() * batch_x.size(0)

    n = len(train_dataset)
    if (epoch + 1) % 5 == 0:
        print(f"Epoch [{epoch+1}/{num_epochs}] "
              f"Total: {total_loss/n:.4f}  "
              f"Recon: {total_recon/n:.6f}  "
              f"Sparsity: {total_sparse/n:.4f}")

# ---- Analyze Sparsity ----
model.eval()
with torch.no_grad():
    sample_x, _ = next(iter(train_loader))
    sample_x = sample_x.to(device)
    _, z = model(sample_x)

    avg_activation = z.mean(dim=0)  # per-unit average
    print(f"\nTarget sparsity (rho): {rho}")
    print(f"Achieved avg activation: {avg_activation.mean().item():.4f}")
    print(f"Units with avg activation < 0.1: {(avg_activation < 0.1).sum().item()}/{z.shape[1]}")
    print(f"Units with avg activation < 0.01: {(avg_activation < 0.01).sum().item()}/{z.shape[1]}")

    # Show activation histogram
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))

    axes[0].hist(avg_activation.cpu().numpy(), bins=50, edgecolor='black')
    axes[0].axvline(x=rho, color='red', linestyle='--', label=f'target rho={rho}')
    axes[0].set_xlabel('Average Activation')
    axes[0].set_ylabel('Number of Units')
    axes[0].set_title('Per-Unit Average Activation Distribution')
    axes[0].legend()

    # Show a single sample's activations (should be sparse)
    single_z = z[0].cpu().numpy()
    axes[1].bar(range(len(single_z)), single_z, width=1.0)
    axes[1].set_xlabel('Latent Unit Index')
    axes[1].set_ylabel('Activation')
    axes[1].set_title(f'Single Sample Activations ({(single_z > 0.1).sum()} active units)')

    plt.tight_layout()
    plt.savefig('sparse_autoencoder_analysis.png', dpi=150)
    plt.show()</code></pre>

                <!-- Code Example 3 -->
                <h3>3. Denoising Autoencoder</h3>
                <p>A denoising autoencoder trained to reconstruct clean MNIST images from noisy inputs. Gaussian noise is added during training, and the model learns to remove it. This implicitly learns the data manifold structure.</p>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np

# ---- Model ----
class DenoisingAutoencoder(nn.Module):
    def __init__(self, input_dim=784, latent_dim=64):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, latent_dim),
            nn.ReLU()
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Linear(256, input_dim),
            nn.Sigmoid()
        )

    def forward(self, x):
        z = self.encoder(x)
        x_hat = self.decoder(z)
        return x_hat, z

def add_gaussian_noise(x, noise_factor=0.5):
    """Add Gaussian noise and clamp to [0, 1]."""
    noisy = x + noise_factor * torch.randn_like(x)
    return torch.clamp(noisy, 0.0, 1.0)

# ---- Data ----
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Lambda(lambda x: x.view(-1))
])
train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)

# ---- Training ----
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = DenoisingAutoencoder(input_dim=784, latent_dim=64).to(device)
optimizer = optim.Adam(model.parameters(), lr=1e-3)
criterion = nn.MSELoss()

noise_factor = 0.5
num_epochs = 20

for epoch in range(num_epochs):
    model.train()
    total_loss = 0

    for batch_x, _ in train_loader:
        batch_x = batch_x.to(device)

        # Step 1: Corrupt the input
        noisy_x = add_gaussian_noise(batch_x, noise_factor)

        # Step 2-3: Encode noisy input, decode
        x_hat, z = model(noisy_x)

        # Step 4: Loss compares to CLEAN input (not noisy)
        loss = criterion(x_hat, batch_x)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item() * batch_x.size(0)

    avg_loss = total_loss / len(train_dataset)
    if (epoch + 1) % 5 == 0:
        print(f"Epoch [{epoch+1}/{num_epochs}], Denoising Loss: {avg_loss:.6f}")

# ---- Evaluate: Denoise Test Images ----
model.eval()
with torch.no_grad():
    test_x, _ = next(iter(test_loader))
    test_x = test_x.to(device)
    noisy_test = add_gaussian_noise(test_x, noise_factor)
    denoised, _ = model(noisy_test)

n = 10
fig, axes = plt.subplots(3, n, figsize=(15, 4.5))
for i in range(n):
    # Original
    axes[0, i].imshow(test_x[i].cpu().view(28, 28), cmap='gray')
    axes[0, i].axis('off')
    if i == 0:
        axes[0, i].set_title('Original', fontsize=10)

    # Noisy
    axes[1, i].imshow(noisy_test[i].cpu().view(28, 28), cmap='gray')
    axes[1, i].axis('off')
    if i == 0:
        axes[1, i].set_title('Noisy', fontsize=10)

    # Denoised
    axes[2, i].imshow(denoised[i].cpu().view(28, 28), cmap='gray')
    axes[2, i].axis('off')
    if i == 0:
        axes[2, i].set_title('Denoised', fontsize=10)

plt.suptitle(f'Denoising Autoencoder (noise_factor={noise_factor}, loss={avg_loss:.4f})')
plt.tight_layout()
plt.savefig('denoising_autoencoder_results.png', dpi=150)
plt.show()

# ---- Compare: noise levels ----
print("\nDenoising performance at different noise levels:")
for nf in [0.1, 0.3, 0.5, 0.7, 1.0]:
    with torch.no_grad():
        noisy = add_gaussian_noise(test_x, nf)
        recon, _ = model(noisy)
        mse = nn.functional.mse_loss(recon, test_x).item()
        print(f"  noise_factor={nf:.1f}: MSE={mse:.6f}")</code></pre>

            </article>

            <!-- ==================== EXERCISES TAB ==================== -->
            <article class="article-content" id="exercises" style="display: none;">
                <h2>Exercises</h2>
                <p>Test your understanding of autoencoder theory, from linear autoencoders and PCA equivalence to information-theoretic foundations and advanced regularization techniques. Solutions are provided for self-study.</p>

                <div class="exercise-group">

                    <h3 style="margin-top: 1rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">Easy</h3>

                    <!-- Exercise 1 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">1. Linear Autoencoder = PCA</span>
                        </div>
                        <div class="exercise-body">
                            <p>Consider a linear autoencoder with encoder $z = Wx$ and decoder $\hat{x} = W^T z$ (no activation functions, tied weights), trained with MSE loss $\mathcal{L} = \frac{1}{N}\sum_{i=1}^N \|x^{(i)} - W^T W x^{(i)}\|^2$. Show that the optimal $W \in \mathbb{R}^{d \times n}$ has rows equal to the top-$d$ eigenvectors of the data covariance matrix $\Sigma = \frac{1}{N}\sum_{i=1}^N x^{(i)} (x^{(i)})^T$.</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p>We want to minimize $\mathcal{L} = \frac{1}{N}\sum_i \|x^{(i)} - W^T W x^{(i)}\|^2 = \mathbb{E}[\|x - W^T Wx\|^2]$.</p>
                                <p>Let $P = W^T W$. This is a projection matrix (onto the column space of $W^T$). The loss becomes $\mathbb{E}[\|x - Px\|^2] = \mathbb{E}[\|(I - P)x\|^2]$.</p>
                                <p>Expanding: $\mathbb{E}[\|(I-P)x\|^2] = \text{tr}((I-P)\Sigma(I-P)^T) = \text{tr}(\Sigma) - 2\text{tr}(P\Sigma) + \text{tr}(P\Sigma P^T)$.</p>
                                <p>Since $P$ is an orthogonal projection ($P^2 = P$, $P = P^T$), we have $P\Sigma P^T = P\Sigma P$, and the loss simplifies to $\text{tr}(\Sigma) - \text{tr}(P\Sigma)$.</p>
                                <p>Minimizing the loss is equivalent to maximizing $\text{tr}(P\Sigma) = \text{tr}(W^T W \Sigma)$. Writing $W$ with rows $w_1, \ldots, w_d$ (orthonormal), this equals $\sum_{k=1}^d w_k^T \Sigma w_k$.</p>
                                <p>By the Rayleigh quotient, each $w_k^T \Sigma w_k$ is maximized when $w_k$ is an eigenvector of $\Sigma$, and the sum is maximized by choosing the top-$d$ eigenvectors. Therefore $\Sigma w_k = \lambda_k w_k$ for the $d$ largest eigenvalues $\lambda_1 \geq \cdots \geq \lambda_d$.</p>
                                <p>This is exactly PCA: the rows of the optimal $W$ are the principal components, and $z = Wx$ gives the PCA projection.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 2 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">2. Reconstruction Error Calculation</span>
                        </div>
                        <div class="exercise-body">
                            <p>Given a 1-layer autoencoder with $W = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \end{pmatrix}$, $b = 0$, $W' = W^T$, $b' = 0$ (no activation function), compute the latent code and reconstruction for $x = (3, 4, 5)^T$. What information is lost?</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>Encoding:</strong> $z = Wx = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \end{pmatrix} \begin{pmatrix} 3 \\ 4 \\ 5 \end{pmatrix} = \begin{pmatrix} 3 \\ 4 \end{pmatrix}$</p>
                                <p><strong>Decoding:</strong> $\hat{x} = W^T z = \begin{pmatrix} 1 & 0 \\ 0 & 1 \\ 0 & 0 \end{pmatrix} \begin{pmatrix} 3 \\ 4 \end{pmatrix} = \begin{pmatrix} 3 \\ 4 \\ 0 \end{pmatrix}$</p>
                                <p><strong>What is lost:</strong> The third component ($x_3 = 5$) is completely lost. The encoder projects onto the first two coordinates, discarding the third dimension entirely.</p>
                                <p><strong>Reconstruction error:</strong> $\|x - \hat{x}\|^2 = (3-3)^2 + (4-4)^2 + (5-0)^2 = 0 + 0 + 25 = 25$</p>
                                <p>This illustrates how the bottleneck ($d=2 < n=3$) forces information loss. A smarter autoencoder might learn a different projection that distributes the error more evenly, but with this fixed $W$, all error concentrates on the discarded dimension.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 3 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">3. Undercomplete vs Overcomplete</span>
                        </div>
                        <div class="exercise-body">
                            <p>An autoencoder has input dimension $n = 100$ and latent dimension $d = 150$. (a) Is this undercomplete or overcomplete? (b) What pathological solution can it learn? (c) Name two regularization strategies to prevent this.</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> This is <strong>overcomplete</strong> because $d = 150 > n = 100$. The latent space is higher-dimensional than the input.</p>
                                <p><strong>(b)</strong> The network can learn the <strong>identity mapping</strong>: the encoder simply copies the input (plus padding with zeros), and the decoder copies it back. This achieves zero reconstruction loss without learning any useful features. Formally, the encoder can set $z = (x, 0_{50})$ and the decoder can read out the first 100 components.</p>
                                <p><strong>(c)</strong> Two regularization strategies:</p>
                                <ul>
                                    <li><strong>Sparse autoencoder:</strong> Add a sparsity penalty ($L_1$ or KL divergence) on the latent activations. This forces most latent units to be inactive for any given input, preventing the identity solution even though $d > n$.</li>
                                    <li><strong>Denoising autoencoder:</strong> Corrupt the input before encoding. The identity mapping on corrupted inputs would produce corrupted outputs, so the network must learn the data structure to denoise.</li>
                                </ul>
                                <p>A third option is the <strong>contractive autoencoder</strong>, which penalizes the Frobenius norm of the encoder's Jacobian, discouraging sensitivity to input perturbations.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 4 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">4. Activation Function Effects</span>
                        </div>
                        <div class="exercise-body">
                            <p>Given encoder weights $W = \begin{pmatrix} 0.5 & -0.3 \\ 0.2 & 0.8 \end{pmatrix}$, bias $b = (0.1, -0.1)^T$, and input $x = (1, 2)^T$:
                            (a) Compute $z$ with sigmoid activation $\sigma(a) = 1/(1+e^{-a})$.
                            (b) Compute $z$ with ReLU activation $\text{ReLU}(a) = \max(0, a)$.
                            (c) Which activation allows negative latent values? Why does this matter?</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p>Pre-activation: $a = Wx + b = (0.5 \cdot 1 + (-0.3) \cdot 2 + 0.1,\; 0.2 \cdot 1 + 0.8 \cdot 2 + (-0.1))^T = (0.5 - 0.6 + 0.1,\; 0.2 + 1.6 - 0.1)^T = (0.0, 1.7)^T$.</p>
                                <p><strong>(a) Sigmoid:</strong> $z = (\sigma(0.0), \sigma(1.7))^T = (0.5, 0.846)^T$. Both values lie in $(0,1)$.</p>
                                <p><strong>(b) ReLU:</strong> $z = (\max(0, 0.0), \max(0, 1.7))^T = (0.0, 1.7)^T$. One unit is dead (exactly zero), the other is unbounded.</p>
                                <p><strong>(c)</strong> Neither allows negative values. But if pre-activations were negative, ReLU would output exactly 0 (dead neuron), while sigmoid would output values in $(0, 0.5)$. This matters because dead ReLU neurons lose all gradient flow, while sigmoid always passes some gradient. For latent spaces, ReLU creates exact sparsity (zeros) while sigmoid creates soft activations.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 5 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">5. Compression Ratio Analysis</span>
                        </div>
                        <div class="exercise-body">
                            <p>An autoencoder maps $28 \times 28$ grayscale images (8 bits/pixel) to a 16-dimensional latent vector stored as 32-bit floats. (a) Compute the raw compression ratio. (b) A JPEG at quality 10 compresses the same image to ~800 bytes. Which is better? (c) At what bottleneck dimension does the autoencoder match JPEG's ratio?</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> Input: $28 \times 28 \times 8 = 6272$ bits $= 784$ bytes. Latent: $16 \times 32 = 512$ bits $= 64$ bytes. Ratio $= 784/64 = 12.25:1$.</p>
                                <p><strong>(b)</strong> JPEG: $784/800 \approx 0.98:1$ (barely compresses; JPEG overhead dominates for tiny images). The autoencoder achieves much better compression. BUT this comparison is misleading: the AE can only reconstruct images from its training distribution (e.g., MNIST digits), while JPEG works on any image.</p>
                                <p><strong>(c)</strong> To match JPEG's ~800 bytes: $800 = d \times 4$ bytes, so $d = 200$. But this exceeds the input dimension (784 pixels), showing that for tiny images JPEG's fixed overhead makes it inefficient.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 6 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">6. Binary Cross-Entropy as Log-Likelihood</span>
                        </div>
                        <div class="exercise-body">
                            <p>For normalized pixel values $x_i \in [0,1]$ modeled as independent Bernoulli random variables with parameter $\hat{x}_i$ (decoder output), derive the negative log-likelihood and show it equals the BCE loss: $-\sum_i [x_i \log \hat{x}_i + (1-x_i)\log(1-\hat{x}_i)]$.</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p>For a Bernoulli distribution: $p(x_i | \hat{x}_i) = \hat{x}_i^{x_i} (1-\hat{x}_i)^{1-x_i}$.</p>
                                <p>For independent pixels: $p(x|\hat{x}) = \prod_i p(x_i|\hat{x}_i) = \prod_i \hat{x}_i^{x_i}(1-\hat{x}_i)^{1-x_i}$.</p>
                                <p>Taking the negative log:</p>
                                $$-\log p(x|\hat{x}) = -\sum_i [x_i \log \hat{x}_i + (1-x_i)\log(1-\hat{x}_i)]$$
                                <p>This is exactly BCE! So minimizing BCE = maximizing the likelihood of the decoder model. This is why BCE is preferred over MSE when pixels are bounded in $[0,1]$: it correctly models the data-generating distribution. MSE implicitly assumes Gaussian noise, which can assign non-zero probability to values outside $[0,1]$.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 7 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">7. Latent Space Interpolation</span>
                        </div>
                        <div class="exercise-body">
                            <p>Given two images $x_1$ (digit "3") and $x_2$ (digit "7") with latent codes $z_1$ and $z_2$:
                            (a) Write the formula for linear interpolation $z(\alpha) = (1-\alpha)z_1 + \alpha z_2$ for $\alpha \in [0,1]$.
                            (b) What should $\text{decode}(z(0.5))$ look like?
                            (c) Name one problem with linear interpolation in high-dimensional latent spaces and one alternative.</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> $z(\alpha) = (1-\alpha)z_1 + \alpha z_2$. At $\alpha = 0$: $z(0) = z_1$. At $\alpha = 1$: $z(1) = z_2$.</p>
                                <p><strong>(b)</strong> $z(0.5) = \frac{z_1 + z_2}{2}$. The decoded image should look like a smooth blend between "3" and "7" — possibly a character with features of both digits. In a well-trained AE with a smooth latent space, this might look like an "8" or an ambiguous character.</p>
                                <p><strong>(c)</strong> Problem: In high dimensions, the midpoint $(z_1 + z_2)/2$ may lie in a low-density region of the latent space (the "soap bubble" effect: most mass of a high-dimensional Gaussian lies on a shell, not at the center). The decoder hasn't seen latent codes like this during training, producing blurry or nonsensical output.</p>
                                <p>Alternative: spherical linear interpolation (slerp):</p>
                                $$z(\alpha) = \frac{\sin((1-\alpha)\theta)}{\sin\theta} z_1 + \frac{\sin(\alpha\theta)}{\sin\theta} z_2$$
                                <p>where $\cos\theta = \frac{z_1 \cdot z_2}{\|z_1\|\|z_2\|}$. This keeps $\|z(\alpha)\|$ roughly constant, staying on the shell where the data lives.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 8 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">8. Depth vs Width Trade-off</span>
                        </div>
                        <div class="exercise-body">
                            <p>Compare two autoencoders, both with bottleneck $d=32$:
                            Shallow: $784 \to 32 \to 784$ (one encoder layer, one decoder layer).
                            Deep: $784 \to 256 \to 64 \to 32 \to 64 \to 256 \to 784$ (three encoder layers, three decoder layers).
                            (a) Count parameters for each (ignore biases). (b) Which learns better representations for MNIST? Why?</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a) Shallow:</strong> Encoder: $784 \times 32 = 25{,}088$. Decoder: $32 \times 784 = 25{,}088$. Total: $50{,}176$.</p>
                                <p><strong>Deep:</strong> Encoder: $784 \times 256 + 256 \times 64 + 64 \times 32 = 200{,}704 + 16{,}384 + 2{,}048 = 219{,}136$. Decoder (symmetric): also $219{,}136$. Total: $438{,}272$.</p>
                                <p><strong>(b)</strong> The deep AE learns better representations despite being ~8.7x more parameters, because: (1) the shallow AE performs a single linear+nonlinear transformation, similar to PCA with a nonlinearity — it can only capture simple patterns; (2) the deep AE performs hierarchical feature extraction: early layers detect edges, middle layers detect parts (strokes), and the bottleneck captures digit identity; (3) the gradual dimension reduction ($784 \to 256 \to 64 \to 32$) is gentler than the abrupt $784 \to 32$, making optimization easier. This is why practical autoencoders are always deep.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 9 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">9. Training Diagnostics</span>
                        </div>
                        <div class="exercise-body">
                            <p>Diagnose each scenario:
                            (a) Training loss = 0.001, test loss = 0.15, latent dim = 128, dataset = 1000 MNIST images.
                            (b) Training loss = test loss = 0.08, latent dim = 2, dataset = 60000 MNIST images.
                            (c) Training loss drops quickly to 0, latent dim = 800, input dim = 784.
                            For each: identify the problem and propose a fix.</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a) Overfitting.</strong> The model memorized the training data but doesn't generalize. With only 1000 samples and $d = 128$, the AE has far too much capacity. Fixes: reduce latent dimension (try $d = 16$ or $d = 32$), add regularization (dropout, weight decay, or use a sparse/contractive AE), or collect more data.</p>
                                <p><strong>(b) Underfitting.</strong> Both losses are similar (no overfitting), but 0.08 MSE on MNIST means blurry reconstructions. The bottleneck $d = 2$ is too small to capture the complexity of 10 digit classes with their variations. Fix: increase latent dimension to $d = 16$ or $d = 32$.</p>
                                <p><strong>(c) The identity function problem.</strong> With $d = 800 > n = 784$, the AE can learn the identity mapping without learning any useful features. Training loss = 0 is suspicious, not good. Fix: either reduce $d < 784$ (undercomplete) or add regularization: sparsity penalty, noise injection, or contractive penalty.</p>
                            </div>
                        </div>
                    </div>

                    <h3 style="margin-top: 2rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">Medium</h3>

                    <!-- Exercise 4 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">10. KL Sparsity Gradient</span>
                        </div>
                        <div class="exercise-body">
                            <p>Derive $\frac{\partial}{\partial \hat{\rho}_j} D_{KL}(\rho \| \hat{\rho}_j)$ where $D_{KL}(\rho \| \hat{\rho}_j) = \rho \log\frac{\rho}{\hat{\rho}_j} + (1-\rho)\log\frac{1-\rho}{1-\hat{\rho}_j}$, and show it equals $-\frac{\rho}{\hat{\rho}_j} + \frac{1-\rho}{1-\hat{\rho}_j}$. Discuss: what happens when $\hat{\rho}_j > \rho$? When $\hat{\rho}_j < \rho$?</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p>Starting from $D_{KL}(\rho \| \hat{\rho}_j) = \rho \log\rho - \rho\log\hat{\rho}_j + (1-\rho)\log(1-\rho) - (1-\rho)\log(1-\hat{\rho}_j)$:</p>
                                <p>Taking the derivative with respect to $\hat{\rho}_j$:</p>
                                $$\frac{\partial}{\partial \hat{\rho}_j} D_{KL} = -\frac{\rho}{\hat{\rho}_j} - (1-\rho) \cdot \frac{-1}{1-\hat{\rho}_j} = -\frac{\rho}{\hat{\rho}_j} + \frac{1-\rho}{1-\hat{\rho}_j}$$
                                <p><strong>When $\hat{\rho}_j > \rho$</strong> (unit is too active): The gradient is positive. Since we are minimizing the KL divergence, gradient descent will decrease $\hat{\rho}_j$, pushing it back toward $\rho$. The positive gradient means "the average activation is too high; reduce it."</p>
                                <p><strong>When $\hat{\rho}_j < \rho$</strong> (unit is too inactive): The gradient is negative. Gradient descent will increase $\hat{\rho}_j$, pushing it toward $\rho$. The negative gradient means "the average activation is too low; increase it."</p>
                                <p><strong>At equilibrium</strong> $\hat{\rho}_j = \rho$: The gradient is $-\frac{\rho}{\rho} + \frac{1-\rho}{1-\rho} = -1 + 1 = 0$, confirming that $\hat{\rho}_j = \rho$ is the stationary point. This is also the global minimum of $D_{KL}(\rho \| \hat{\rho}_j)$ since KL divergence is non-negative and equals zero iff the distributions match.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 5 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">11. Tied Weights Constraint</span>
                        </div>
                        <div class="exercise-body">
                            <p>If $W' = W^T$ (tied weights) and $W \in \mathbb{R}^{d \times n}$, how many free parameters does the autoencoder have compared to untied weights? For $n = 784, d = 32$, compute both counts. (Include bias terms: encoder bias $b \in \mathbb{R}^d$, decoder bias $b' \in \mathbb{R}^n$.)</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>Tied weights:</strong></p>
                                <ul>
                                    <li>Encoder weight $W \in \mathbb{R}^{d \times n}$: $d \times n$ parameters</li>
                                    <li>Decoder weight $W' = W^T$: 0 additional parameters (shared)</li>
                                    <li>Encoder bias $b \in \mathbb{R}^d$: $d$ parameters</li>
                                    <li>Decoder bias $b' \in \mathbb{R}^n$: $n$ parameters</li>
                                    <li><strong>Total:</strong> $dn + d + n$</li>
                                </ul>
                                <p><strong>Untied weights:</strong></p>
                                <ul>
                                    <li>Encoder weight $W \in \mathbb{R}^{d \times n}$: $d \times n$ parameters</li>
                                    <li>Decoder weight $W' \in \mathbb{R}^{n \times d}$: $n \times d$ parameters</li>
                                    <li>Encoder bias $b \in \mathbb{R}^d$: $d$ parameters</li>
                                    <li>Decoder bias $b' \in \mathbb{R}^n$: $n$ parameters</li>
                                    <li><strong>Total:</strong> $2dn + d + n$</li>
                                </ul>
                                <p><strong>For $n = 784, d = 32$:</strong></p>
                                <ul>
                                    <li>Tied: $784 \times 32 + 32 + 784 = 25{,}088 + 32 + 784 = 25{,}904$</li>
                                    <li>Untied: $2 \times 784 \times 32 + 32 + 784 = 50{,}176 + 32 + 784 = 50{,}992$</li>
                                </ul>
                                <p>Tied weights use roughly half the parameters ($25{,}904$ vs $50{,}992$), which acts as a form of regularization and can help prevent overfitting, especially with limited training data.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 6 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">12. Denoising Optimality</span>
                        </div>
                        <div class="exercise-body">
                            <p>Show that for the MSE loss with noise $\tilde{x} = x + \epsilon$ where $\epsilon \sim \mathcal{N}(0, \sigma^2 I)$, the optimal denoising function is $g^*(\tilde{x}) = \mathbb{E}[X | \tilde{X} = \tilde{x}]$.</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p>We want to find $g^*$ that minimizes the expected MSE:</p>
                                $$\mathcal{L}(g) = \mathbb{E}_{X, \tilde{X}}[\|X - g(\tilde{X})\|^2]$$
                                <p>We can decompose this using the law of iterated expectations, conditioning on $\tilde{X}$:</p>
                                $$\mathcal{L}(g) = \mathbb{E}_{\tilde{X}}\left[\mathbb{E}_{X|\tilde{X}}[\|X - g(\tilde{X})\|^2 \mid \tilde{X}]\right]$$
                                <p>For any fixed $\tilde{x}$, the inner expectation $\mathbb{E}[\|X - c\|^2 | \tilde{X} = \tilde{x}]$ over a constant $c = g(\tilde{x})$ is minimized when $c = \mathbb{E}[X | \tilde{X} = \tilde{x}]$. This is a standard result: the conditional mean minimizes the conditional MSE.</p>
                                <p><strong>Proof of the inner claim:</strong> Let $\mu = \mathbb{E}[X | \tilde{X} = \tilde{x}]$. Then:</p>
                                $$\mathbb{E}[\|X - c\|^2 | \tilde{X}] = \mathbb{E}[\|X - \mu + \mu - c\|^2 | \tilde{X}]$$
                                $$= \mathbb{E}[\|X - \mu\|^2 | \tilde{X}] + 2\mathbb{E}[(X - \mu)^T(\mu - c) | \tilde{X}] + \|\mu - c\|^2$$
                                <p>The cross term vanishes because $\mathbb{E}[X - \mu | \tilde{X}] = 0$ by definition of conditional expectation. The first term is the irreducible conditional variance (does not depend on $c$). The third term $\|\mu - c\|^2 \geq 0$ is minimized when $c = \mu$.</p>
                                <p>Since this holds for each $\tilde{x}$, the optimal $g^*(\tilde{x}) = \mathbb{E}[X | \tilde{X} = \tilde{x}]$ minimizes the overall loss.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 7 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">13. Information Content of Bottleneck</span>
                        </div>
                        <div class="exercise-body">
                            <p>A 2-dimensional bottleneck uses 32-bit floats per dimension. What is the maximum number of bits per sample? Why is the effective rate typically much lower?</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>Maximum bits:</strong> $2 \times 32 = 64$ bits per sample. Each of the 2 dimensions is stored as a 32-bit float, so the raw storage capacity is 64 bits.</p>
                                <p><strong>Why the effective rate is much lower:</strong></p>
                                <p>The autoencoder maps inputs to a low-dimensional manifold within $\mathbb{R}^2$. The effective information content is determined by the <em>entropy</em> of the latent distribution $p(z)$, not the bit-width of the representation.</p>
                                <ul>
                                    <li><strong>Concentrated distribution:</strong> If the encoder maps all MNIST digits to a small region of $\mathbb{R}^2$ (say, 10 clusters for 10 digit classes), the effective entropy is $H(Z) \approx \log_2 10 \approx 3.3$ bits — far less than 64.</li>
                                    <li><strong>Smooth manifold:</strong> The encoder learns a continuous mapping, so $z$ values are not spread uniformly across the full 32-bit float range. Most of the $2^{64}$ possible bit patterns are never produced.</li>
                                    <li><strong>Correlation:</strong> The two latent dimensions may be correlated, further reducing the effective information.</li>
                                    <li><strong>Precision vs information:</strong> Using 32-bit floats provides numerical precision for gradient-based optimization, but the meaningful information content is determined by the signal-to-noise ratio of the latent activations, which is much less than 32 bits per dimension.</li>
                                </ul>
                                <p>A rough estimate: if each latent dimension effectively uses about 4-8 bits of meaningful precision (based on the spread of activations relative to reconstruction sensitivity), the effective rate is roughly 8-16 bits per sample, not 64.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 14 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">14. L1 vs KL Sparsity</span>
                        </div>
                        <div class="exercise-body">
                            <p>Compare two sparsity penalties for an overcomplete AE ($d=256$, $n=784$): (a) $\Omega_{L_1} = \lambda \sum_j |z_j|$ and (b) $\Omega_{KL} = \beta \sum_j D_{KL}(\rho \| \hat{\rho}_j)$ with target $\rho = 0.05$. Show that $L_1$ pushes activations to exactly zero while KL pushes them toward $\rho$. Which produces sparser representations? Which is more biologically plausible?</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>For $L_1$:</strong> The subgradient $\frac{\partial |z_j|}{\partial z_j} = \text{sign}(z_j)$ for $z_j \neq 0$. At $z_j = 0$, the subgradient includes 0, so zero is a stable point. The penalty creates a "dead zone" around zero — any activation with gradient magnitude less than $\lambda$ gets pushed to exactly zero. This is the proximal operator effect: the optimal solution has $z_j = 0$ for many units.</p>
                                <p><strong>For KL:</strong> $\frac{\partial D_{KL}}{\partial \hat{\rho}_j} = 0$ when $\hat{\rho}_j = \rho = 0.05$. The equilibrium is not zero but the target sparsity $\rho$. Each unit is encouraged to be active $5\%$ of the time across the dataset.</p>
                                <p>$L_1$ produces sparser representations (exact zeros vs. small-but-nonzero values). However, KL sparsity is more biologically plausible: real neurons have low but nonzero baseline firing rates, and the "5% active" interpretation matches sparse coding theories in neuroscience (Olshausen &amp; Field, 1996).</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 15 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">15. Convolutional Autoencoder Dimensions</span>
                        </div>
                        <div class="exercise-body">
                            <p>An encoder has: Conv2d(1, 16, 3, stride=2, padding=1) $\to$ ReLU $\to$ Conv2d(16, 32, 3, stride=2, padding=1) $\to$ ReLU $\to$ Flatten $\to$ Linear(?, 64). For a $28 \times 28$ input: (a) Compute the spatial dimensions after each conv layer using $\lfloor(n + 2p - k)/s\rfloor + 1$. (b) What is the "?" in the Linear layer? (c) Total encoder parameters (with biases).</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> After Conv1: $\lfloor(28 + 2 - 3)/2\rfloor + 1 = \lfloor 27/2 \rfloor + 1 = 13 + 1 = 14$. Output: 16 channels $\times$ 14 $\times$ 14.</p>
                                <p>After Conv2: $\lfloor(14 + 2 - 3)/2\rfloor + 1 = \lfloor 13/2 \rfloor + 1 = 6 + 1 = 7$. Output: 32 channels $\times$ 7 $\times$ 7.</p>
                                <p><strong>(b)</strong> After flatten: $32 \times 7 \times 7 = 1568$. So the Linear layer is Linear(1568, 64).</p>
                                <p><strong>(c)</strong> Conv1: $1 \times 16 \times 3 \times 3 + 16 = 144 + 16 = 160$. Conv2: $16 \times 32 \times 3 \times 3 + 32 = 4608 + 32 = 4640$. Linear: $1568 \times 64 + 64 = 100{,}352 + 64 = 100{,}416$. Total: $160 + 4640 + 100{,}416 = 105{,}216$.</p>
                                <p>Note: the linear layer dominates (~95% of parameters). This motivates fully convolutional autoencoders that replace it with global average pooling.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 16 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">16. Symmetric Decoder Design</span>
                        </div>
                        <div class="exercise-body">
                            <p>For the encoder in Exercise 15, design a symmetric decoder. (a) Write the decoder architecture using ConvTranspose2d to go from 64-dim latent back to $28 \times 28$. (b) What is the "checkerboard artifact" problem with transposed convolutions? (c) Propose an alternative.</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> Decoder: Linear(64, 1568) $\to$ Reshape to $32 \times 7 \times 7$ $\to$ ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1) $\to$ ReLU $\to$ ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1) $\to$ Sigmoid.</p>
                                <p>Verification: ConvT1: $(7-1)\times 2 - 2\times 1 + 3 + 1 = 14$. Output: $16 \times 14 \times 14$. ConvT2: $(14-1)\times 2 - 2\times 1 + 3 + 1 = 28$. Output: $1 \times 28 \times 28$. &#10003;</p>
                                <p><strong>(b)</strong> Checkerboard artifacts: ConvTranspose2d with stride $> 1$ produces overlapping patterns in the output. When stride doesn't evenly divide the kernel size, some output pixels receive contributions from more kernel elements than others, creating a grid-like pattern. For kernel=3, stride=2, every other pixel gets 2 contributions while adjacent ones get 1.</p>
                                <p><strong>(c)</strong> Alternative: Upsample + Conv2d. Replace each ConvTranspose2d with: nn.Upsample(scale_factor=2, mode='bilinear') followed by nn.Conv2d with stride=1. This separates the upsampling (bilinear interpolation) from the learned filtering (convolution), eliminating checkerboard artifacts. This is the approach used in many modern architectures (e.g., U-Net decoders).</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 17 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">17. Manifold Dimension Theorem</span>
                        </div>
                        <div class="exercise-body">
                            <p>Suppose data lies exactly on a $k$-dimensional linear subspace $V \subset \mathbb{R}^n$ (so $x = U\alpha$ where $U \in \mathbb{R}^{n \times k}$ has orthonormal columns and $\alpha \in \mathbb{R}^k$). Prove: (a) A linear AE with $d \geq k$ can achieve zero reconstruction error. (b) A linear AE with $d < k$ cannot achieve zero error. (c) What changes for a nonlinear manifold?</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> Since $x = U\alpha \in V$, the data covariance $\Sigma = U \text{diag}(\sigma_1^2, \ldots, \sigma_k^2) U^T$ has rank $k$, with $k$ positive eigenvalues and $n-k$ zero eigenvalues. A linear AE with $d \geq k$ can set $W = U^T$ (the top-$d$ eigenvectors of $\Sigma$, which include all $k$ non-zero directions). Then $P = W^T W = UU^T$ is the projection onto $V$, and since $x \in V$: $\hat{x} = Px = x$. Error = 0.</p>
                                <p><strong>(b)</strong> If $d < k$, the projection $P = W^T W$ has rank $d < k$, so its column space is a strict subspace of $V$. There exist $x \in V$ not in this subspace, giving $Px \neq x$. Formally, the error is $\sum_{i=d+1}^k \sigma_i^2 > 0$ (the sum of discarded eigenvalues).</p>
                                <p><strong>(c)</strong> For nonlinear manifolds, a linear AE with $d = k$ may fail because it can only capture linear subspaces. But a nonlinear AE (with sufficient depth and width) can parameterize a nonlinear encoder $f: \mathbb{R}^n \to \mathbb{R}^k$ that maps the manifold to $\mathbb{R}^k$ bijectively, and a decoder $g: \mathbb{R}^k \to \mathbb{R}^n$ that inverts it. Universal approximation theorems guarantee such $f, g$ exist. This is why nonlinear AEs outperform PCA on real data.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 18 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">18. Dropout as Implicit Ensemble</span>
                        </div>
                        <div class="exercise-body">
                            <p>Applying dropout with rate $p$ to the $d$-dimensional latent layer: $\tilde{z}_i = z_i \cdot m_i / (1-p)$ where $m_i \sim \text{Bernoulli}(1-p)$. (a) How many distinct "sub-autoencoders" does this create? (b) Show the scaling factor $1/(1-p)$ ensures $\mathbb{E}[\tilde{z}_i] = z_i$. (c) How is this different from a denoising autoencoder?</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> Each of the $d$ latent units is either on or off, giving $2^d$ possible binary masks. Each mask defines a different sub-autoencoder that uses a subset of the latent dimensions. For $d = 32$, that's $2^{32} \approx 4.3 \times 10^9$ sub-networks.</p>
                                <p><strong>(b)</strong> $\mathbb{E}[\tilde{z}_i] = \mathbb{E}[z_i \cdot m_i / (1-p)] = z_i \cdot \mathbb{E}[m_i] / (1-p) = z_i \cdot (1-p)/(1-p) = z_i$. The scaling ensures that at test time (no dropout), the expected latent values match training, so no rescaling is needed.</p>
                                <p><strong>(c)</strong> Key differences: (1) Dropout corrupts the latent code $z$ (internal representation), while denoising AEs corrupt the input $x$. (2) Dropout applies multiplicative Bernoulli noise (binary masking), while denoising AEs typically apply additive Gaussian noise. (3) Dropout trains the decoder to be robust to missing features, while denoising trains the encoder to be robust to noisy inputs. (4) Dropout can be seen as Bayesian approximate inference (Gal &amp; Ghahramani, 2016), while denoising connects to score matching.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 19 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">19. Backpropagation Through the Decoder</span>
                        </div>
                        <div class="exercise-body">
                            <p>For a single-layer decoder $\hat{x} = \sigma(W_2 z + b_2)$ with sigmoid activation, MSE loss $\mathcal{L} = \|x - \hat{x}\|^2$: (a) Derive $\frac{\partial \mathcal{L}}{\partial z}$ (the gradient that flows back to the encoder). (b) When does this gradient vanish? What does this mean for training?</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> Let $a = W_2 z + b_2$ (pre-activation), $\hat{x} = \sigma(a)$.</p>
                                <p>$\frac{\partial \mathcal{L}}{\partial \hat{x}_i} = -2(x_i - \hat{x}_i)$</p>
                                <p>$\frac{\partial \hat{x}_i}{\partial a_i} = \hat{x}_i(1 - \hat{x}_i)$</p>
                                <p>$\frac{\partial a_i}{\partial z_j} = (W_2)_{ij}$</p>
                                <p>By chain rule:</p>
                                $$\frac{\partial \mathcal{L}}{\partial z_j} = \sum_i \frac{\partial \mathcal{L}}{\partial \hat{x}_i} \cdot \frac{\partial \hat{x}_i}{\partial a_i} \cdot \frac{\partial a_i}{\partial z_j} = -2 \sum_i (x_i - \hat{x}_i) \hat{x}_i(1-\hat{x}_i) (W_2)_{ij}$$
                                <p>In matrix form: $\frac{\partial \mathcal{L}}{\partial z} = -2 W_2^T [\delta \odot \hat{x} \odot (1-\hat{x})]$ where $\delta = x - \hat{x}$.</p>
                                <p><strong>(b)</strong> The gradient vanishes when: (1) $\hat{x}_i \approx x_i$ (reconstruction is already good — this is the desired equilibrium); (2) $\hat{x}_i \approx 0$ or $\hat{x}_i \approx 1$ (sigmoid saturation): the factor $\hat{x}_i(1-\hat{x}_i) \approx 0$ kills the gradient regardless of reconstruction quality. This is the vanishing gradient problem with sigmoid decoders. Fix: use ReLU intermediate layers or skip connections.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 20 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">20. Wasserstein Autoencoder Motivation</span>
                        </div>
                        <div class="exercise-body">
                            <p>In a VAE, we minimize $D_{KL}(q(z|x) \| p(z))$ for each input $x$, matching each individual posterior to the prior. In a Wasserstein AE (WAE), we instead match the aggregated posterior $q(z) = \mathbb{E}_{p(x)}[q(z|x)]$ to $p(z)$. (a) Why is matching the aggregated posterior weaker than matching individual posteriors? (b) Show with an example: let $p(z) = \mathcal{N}(0, 1)$ and $q(z|x)$ be point masses at $z = f(x)$. When does $q(z)$ match $p(z)$? (c) What is the practical benefit?</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> Matching individual posteriors: $D_{KL}(q(z|x) \| p(z)) = 0$ for all $x$ means $q(z|x) = p(z)$ for every $x$. This forces the encoder to ignore the input entirely! In practice, the VAE trades off between this term and reconstruction, but the KL penalty still pushes toward ignoring the input. Matching the aggregate: $q(z) = p(z)$ only requires that the overall distribution of latent codes matches the prior, not that each individual input maps to the prior. This is a much weaker constraint.</p>
                                <p><strong>(b)</strong> If $q(z|x) = \delta(z - f(x))$ (deterministic encoder), then $q(z) = p_{f(X)}(z)$, the push-forward of $p(x)$ through $f$. This matches $p(z) = \mathcal{N}(0,1)$ when $f$ is a measure-preserving map: e.g., $f = \Phi^{-1} \circ F_X$ where $F_X$ is the data CDF and $\Phi^{-1}$ is the standard normal quantile function. The encoder just needs to rearrange the data distribution into a Gaussian — without adding noise!</p>
                                <p><strong>(c)</strong> Practical benefit: WAE allows deterministic encoders (no sampling noise, no reparameterization trick needed), producing sharper reconstructions than VAE. The latent space is still regularized (covers the prior), but individual encodings are precise, not blurred.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 21 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">21. Memorization vs Generalization</span>
                        </div>
                        <div class="exercise-body">
                            <p>An autoencoder has a single hidden layer with $d$ units and is trained on $N$ samples in $\mathbb{R}^n$ with ReLU activations. (a) Derive a condition on $d$ such that the AE can perfectly memorize all $N$ training samples (zero training loss) without learning any generalizable features. (b) Why is this bad? Give a concrete example.</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> A ReLU network with $d$ hidden units can represent a piecewise-linear function with at most $d$ linear regions. For the encoder $f: \mathbb{R}^n \to \mathbb{R}^d$, each of the $N$ training points $x^{(i)}$ can be mapped to a unique latent code $z^{(i)} = f(x^{(i)})$ as long as the encoder has enough capacity.</p>
                                <p>The decoder $g: \mathbb{R}^d \to \mathbb{R}^n$ needs to map each $z^{(i)}$ back to $x^{(i)}$. This is an interpolation problem: fitting $N$ points in $\mathbb{R}^d$ to $\mathbb{R}^n$. A single-layer ReLU network can memorize $N$ arbitrary input-output pairs if $d \geq N$ (each hidden unit can "specialize" to one training sample using indicator-like activation patterns).</p>
                                <p>So the condition is $d \geq N$: with at least as many hidden units as training samples, perfect memorization is possible.</p>
                                <p><strong>(b)</strong> This is bad because the AE acts as a lookup table: for training point $x^{(7)}$, the encoder stores a unique code, and the decoder retrieves the original. For any input not seen during training, the output is meaningless. Example: train an AE with $d = 1000$ on $N = 500$ MNIST digits. It achieves zero training loss but produces garbage for any new digit, even from the same distribution. The latent space has no structure — nearby latent codes don't correspond to similar images. The model has learned nothing about "digit-ness," only about these specific 500 images.</p>
                            </div>
                        </div>
                    </div>

                    <h3 style="margin-top: 2rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">Hard</h3>

                    <!-- Exercise 8 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">22. Contractive Jacobian Computation</span>
                        </div>
                        <div class="exercise-body">
                            <p>For encoder $f(x) = \sigma(Wx + b)$ with sigmoid activation $\sigma$, derive the Jacobian $J_{ij} = \frac{\partial z_i}{\partial x_j}$ and show that $\|J\|_F^2 = \sum_i z_i^2(1-z_i)^2 \sum_j W_{ij}^2$.</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p>The encoder computes $z_i = \sigma(a_i)$ where $a_i = \sum_j W_{ij} x_j + b_i$ is the pre-activation.</p>
                                <p><strong>Step 1: Jacobian entry.</strong> By the chain rule:</p>
                                $$J_{ij} = \frac{\partial z_i}{\partial x_j} = \frac{\partial \sigma(a_i)}{\partial a_i} \cdot \frac{\partial a_i}{\partial x_j} = \sigma'(a_i) \cdot W_{ij}$$
                                <p><strong>Step 2: Sigmoid derivative.</strong> For the sigmoid function, $\sigma'(a) = \sigma(a)(1 - \sigma(a)) = z_i(1 - z_i)$. Therefore:</p>
                                $$J_{ij} = z_i(1 - z_i) W_{ij}$$
                                <p><strong>Step 3: Frobenius norm.</strong></p>
                                $$\|J\|_F^2 = \sum_{i} \sum_{j} J_{ij}^2 = \sum_{i} \sum_{j} [z_i(1-z_i)]^2 W_{ij}^2$$
                                $$= \sum_{i} z_i^2(1-z_i)^2 \sum_{j} W_{ij}^2$$
                                <p>The last step factors because $z_i(1-z_i)$ does not depend on $j$.</p>
                                <p><strong>Interpretation:</strong> The contractive penalty $\|J\|_F^2$ has two factors for each hidden unit $i$:</p>
                                <ul>
                                    <li>$z_i^2(1-z_i)^2$: the squared sigmoid derivative, which is largest when $z_i \approx 0.5$ (on the linear part of the sigmoid) and smallest when $z_i \approx 0$ or $z_i \approx 1$ (saturated). Units in saturation are already locally invariant.</li>
                                    <li>$\sum_j W_{ij}^2$: the squared norm of the $i$-th row of $W$. Large weights amplify input perturbations.</li>
                                </ul>
                                <p>The penalty thus encourages either small weights (like $L_2$ regularization) or saturated activations (which create flat regions in the encoding function).</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 9 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">23. Rate-Distortion for Gaussian Source</span>
                        </div>
                        <div class="exercise-body">
                            <p>For a Gaussian source $X \sim \mathcal{N}(0, \sigma^2)$ with MSE distortion, verify that $R(D) = \frac{1}{2}\log_2 \frac{\sigma^2}{D}$ by computing $I(X; Z)$ for the "test channel" $Z = X + N$ where $N \sim \mathcal{N}(0, \sigma_N^2)$ is independent noise, with $\hat{X} = \frac{\sigma^2}{\sigma^2 + \sigma_N^2} Z$ being the MMSE estimator. Set the distortion equal to $D$ and solve for $I(X;Z)$.</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>Step 1: Compute the distortion.</strong> The MMSE estimator of $X$ given $Z = X + N$ is:</p>
                                $$\hat{X} = \mathbb{E}[X|Z] = \frac{\sigma^2}{\sigma^2 + \sigma_N^2} Z$$
                                <p>The MSE distortion is:</p>
                                $$D = \mathbb{E}[(X - \hat{X})^2] = \text{Var}(X|Z) = \sigma^2 - \frac{\sigma^4}{\sigma^2 + \sigma_N^2} = \frac{\sigma^2 \sigma_N^2}{\sigma^2 + \sigma_N^2}$$
                                <p><strong>Step 2: Solve for $\sigma_N^2$ in terms of $D$.</strong></p>
                                $$D(\sigma^2 + \sigma_N^2) = \sigma^2 \sigma_N^2 \implies D\sigma^2 + D\sigma_N^2 = \sigma^2 \sigma_N^2 \implies \sigma_N^2 = \frac{D\sigma^2}{\sigma^2 - D}$$
                                <p><strong>Step 3: Compute $I(X; Z)$.</strong> Since $X$ and $N$ are independent Gaussians, $Z \sim \mathcal{N}(0, \sigma^2 + \sigma_N^2)$. Using $I(X;Z) = h(Z) - h(Z|X) = h(Z) - h(N)$:</p>
                                $$h(Z) = \frac{1}{2}\log_2(2\pi e(\sigma^2 + \sigma_N^2))$$
                                $$h(N) = \frac{1}{2}\log_2(2\pi e \sigma_N^2)$$
                                $$I(X;Z) = \frac{1}{2}\log_2\frac{\sigma^2 + \sigma_N^2}{\sigma_N^2}$$
                                <p><strong>Step 4: Substitute $\sigma_N^2$.</strong></p>
                                $$\frac{\sigma^2 + \sigma_N^2}{\sigma_N^2} = 1 + \frac{\sigma^2}{\sigma_N^2} = 1 + \frac{\sigma^2(\sigma^2 - D)}{D\sigma^2} = 1 + \frac{\sigma^2 - D}{D} = \frac{\sigma^2}{D}$$
                                <p>Therefore:</p>
                                $$I(X; Z) = \frac{1}{2}\log_2\frac{\sigma^2}{D} = R(D)$$
                                <p>This confirms the rate-distortion function. The Gaussian test channel achieves the theoretical minimum: no encoding scheme can do better than $R(D) = \frac{1}{2}\log_2\frac{\sigma^2}{D}$ bits for distortion $D$.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 10 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">24. Score Matching Connection</span>
                        </div>
                        <div class="exercise-body">
                            <p>Show that the denoising autoencoder loss with Gaussian noise $\epsilon \sim \mathcal{N}(0, \sigma^2 I)$ is equivalent to learning the score function $\nabla_x \log p(x)$ up to a constant. Start from $\mathcal{L} = \mathbb{E}_{x, \epsilon}[\|g(\tilde{x}) - x\|^2]$ and derive the connection $\nabla_{\tilde{x}} \log p(\tilde{x}) \approx \frac{g^*(\tilde{x}) - \tilde{x}}{\sigma^2}$.</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>Step 1: Optimal denoiser via Tweedie's formula.</strong> From Exercise 6, we know $g^*(\tilde{x}) = \mathbb{E}[X | \tilde{X} = \tilde{x}]$. For the Gaussian noise model $\tilde{X} = X + \epsilon$ with $\epsilon \sim \mathcal{N}(0, \sigma^2 I)$:</p>
                                <p>By Bayes' rule: $p(x|\tilde{x}) = \frac{p(\tilde{x}|x) p(x)}{p(\tilde{x})}$ where $p(\tilde{x}|x) = \mathcal{N}(\tilde{x}; x, \sigma^2 I)$.</p>
                                <p><strong>Step 2: Compute the score of the noisy distribution.</strong></p>
                                $$\nabla_{\tilde{x}} \log p(\tilde{x}) = \nabla_{\tilde{x}} \log \int p(\tilde{x}|x) p(x) dx = \frac{\int \nabla_{\tilde{x}} p(\tilde{x}|x) p(x) dx}{p(\tilde{x})}$$
                                <p>Since $p(\tilde{x}|x) = \mathcal{N}(\tilde{x}; x, \sigma^2 I)$, we have $\nabla_{\tilde{x}} \log p(\tilde{x}|x) = \frac{x - \tilde{x}}{\sigma^2}$, so $\nabla_{\tilde{x}} p(\tilde{x}|x) = p(\tilde{x}|x) \cdot \frac{x - \tilde{x}}{\sigma^2}$.</p>
                                <p>Substituting:</p>
                                $$\nabla_{\tilde{x}} \log p(\tilde{x}) = \frac{1}{p(\tilde{x})} \int \frac{x - \tilde{x}}{\sigma^2} p(\tilde{x}|x) p(x) dx = \frac{1}{\sigma^2}\left(\int x \cdot p(x|\tilde{x}) dx - \tilde{x}\right)$$
                                $$= \frac{\mathbb{E}[X|\tilde{X} = \tilde{x}] - \tilde{x}}{\sigma^2} = \frac{g^*(\tilde{x}) - \tilde{x}}{\sigma^2}$$
                                <p>This is <strong>Tweedie's formula</strong>: the score of the noisy distribution equals the optimal denoising direction scaled by $1/\sigma^2$.</p>
                                <p><strong>Step 3: The connection.</strong> A trained denoising autoencoder with output $g(\tilde{x}) \approx g^*(\tilde{x})$ implicitly provides an estimate of the score:</p>
                                $$\nabla_{\tilde{x}} \log p_\sigma(\tilde{x}) \approx \frac{g(\tilde{x}) - \tilde{x}}{\sigma^2}$$
                                <p>As $\sigma \to 0$, $p_\sigma(\tilde{x}) \to p(x)$ and we recover the score of the clean data distribution. This is the foundation of <strong>score-based generative models</strong> and <strong>diffusion models</strong> (DDPM, Score SDE), which train denoisers at multiple noise levels $\sigma$ to estimate the score function across scales.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 25 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">25. VQ-VAE: Vector Quantized Latent Codes</span>
                        </div>
                        <div class="exercise-body">
                            <p>In VQ-VAE, the encoder outputs $z_e \in \mathbb{R}^d$, which is quantized to the nearest codebook vector: $z_q = e_k$ where $k = \arg\min_j \|z_e - e_j\|^2$, with codebook $\{e_j\}_{j=1}^K$. The loss is:</p>
                            $$\mathcal{L} = \|x - \hat{x}\|^2 + \|\text{sg}[z_e] - z_q\|^2 + \beta\|z_e - \text{sg}[z_q]\|^2$$
                            <p>where $\text{sg}[\cdot]$ is stop-gradient. (a) Explain what each of the three loss terms does. (b) Since $\arg\min$ is not differentiable, how does the gradient flow from $\hat{x}$ to the encoder? Derive the "straight-through estimator." (c) Compute the information-theoretic capacity: with $K$ codebook vectors and a sequence of $L$ latent positions, how many bits can be stored?</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> Term 1: $\|x - \hat{x}\|^2$ — reconstruction loss. Trains the decoder to reconstruct $x$ from $z_q$, and (via straight-through) the encoder to produce useful $z_e$.</p>
                                <p>Term 2: $\|\text{sg}[z_e] - z_q\|^2$ — codebook loss. Moves codebook vectors $e_k$ toward the encoder outputs $z_e$. The stop-gradient on $z_e$ means this only updates the codebook, not the encoder.</p>
                                <p>Term 3: $\beta\|z_e - \text{sg}[z_q]\|^2$ — commitment loss. Encourages the encoder output $z_e$ to stay close to the chosen codebook vector. Prevents $z_e$ from fluctuating wildly and "jumping" between codebook entries. $\beta$ is typically 0.25.</p>
                                <p><strong>(b)</strong> The quantization $z_q = e_k$ is non-differentiable. The straight-through estimator simply copies the gradient from $z_q$ to $z_e$:</p>
                                $$\frac{\partial \mathcal{L}}{\partial z_e} \approx \frac{\partial \mathcal{L}}{\partial z_q}$$
                                <p>In code: $z_q = z_e + \text{sg}[z_q - z_e]$. During forward pass, this equals $z_q$. During backward pass, $\text{sg}[\cdot]$ has zero gradient, so $\frac{\partial z_q}{\partial z_e} = 1$. The decoder's gradient flows unchanged to the encoder.</p>
                                <p><strong>(c)</strong> Each latent position selects one of $K$ codebook vectors, encoding $\log_2 K$ bits. With $L$ positions: total capacity $= L \log_2 K$ bits. For $K = 512, L = 32$: $32 \times \log_2 512 = 32 \times 9 = 288$ bits. This is an exact, discrete capacity — unlike continuous latent spaces where the effective bit rate depends on precision.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 26 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">26. Information Bottleneck Lagrangian</span>
                        </div>
                        <div class="exercise-body">
                            <p>The Information Bottleneck (IB) objective minimizes $I(X;Z)$ while preserving $I(Z;Y)$: $\mathcal{L}_{IB} = I(X;Z) - \beta I(Z;Y)$ where $Y$ is a target variable. (a) Using calculus of variations, derive the self-consistent equation for the optimal encoder $p(z|x)$. (b) Show that the optimal $p(z|x) \propto p(z) \exp(-\beta D_{KL}(p(y|x) \| p(y|z)))$. (c) Explain how $\beta$ controls the rate-relevance trade-off.</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> We minimize $\mathcal{L} = I(X;Z) - \beta I(Z;Y)$ over $p(z|x)$ subject to normalization $\sum_z p(z|x) = 1$.</p>
                                $$I(X;Z) = \sum_{x,z} p(x) p(z|x) \log \frac{p(z|x)}{p(z)}$$
                                $$I(Z;Y) = \sum_{z,y} p(z,y) \log \frac{p(y|z)}{p(y)}$$
                                <p>where $p(z) = \sum_x p(x) p(z|x)$ and $p(y|z) = \sum_x p(y|x) p(x|z)$. Using a Lagrange multiplier $\lambda(x)$ for normalization:</p>
                                $$\frac{\delta}{\delta p(z|x)} \left[\mathcal{L} - \sum_x \lambda(x) \sum_z p(z|x)\right] = 0$$
                                <p><strong>(b)</strong> Taking the functional derivative and solving (noting that $p(z)$ and $p(y|z)$ depend on $p(z|x)$, requiring self-consistency):</p>
                                $$p(z|x) = \frac{p(z)}{Z(\beta, x)} \exp\left(-\beta D_{KL}(p(y|x) \| p(y|z))\right)$$
                                <p>where $Z(\beta, x) = \sum_z p(z) \exp(-\beta D_{KL}(p(y|x) \| p(y|z)))$ is the normalization constant.</p>
                                <p>Intuition: The encoder $p(z|x)$ assigns $x$ to cluster $z$ proportionally to: (1) the cluster's size $p(z)$ (prior); (2) how similar $x$'s label distribution $p(y|x)$ is to cluster $z$'s label distribution $p(y|z)$. The KL divergence penalizes assigning $x$ to a cluster whose predictions differ from the true labels.</p>
                                <p><strong>(c)</strong> $\beta \to 0$: $p(z|x) \propto p(z)$ — the encoder ignores the input entirely. $I(X;Z) = 0$ (perfect compression), $I(Z;Y) = 0$ (no information preserved). $\beta \to \infty$: the encoder becomes deterministic, mapping each $x$ to the $z$ with the best-matching $p(y|z)$. $I(X;Z)$ is maximized, $I(Z;Y) \approx I(X;Y)$ (all relevant information preserved). Intermediate $\beta$: trades off compression against predictive power. This traces out the information curve, analogous to the rate-distortion curve.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 27 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">27. Contractive AE with ReLU</span>
                        </div>
                        <div class="exercise-body">
                            <p>Repeat the Jacobian analysis of Exercise 22 (Contractive Jacobian) for ReLU activation $\sigma(a) = \max(0, a)$. (a) Derive $J_{ij}$ and show $\|J\|_F^2 = \sum_i \mathbb{1}[a_i > 0] \sum_j W_{ij}^2$. (b) Why is this harder to optimize than the sigmoid case? (c) Is the penalty still useful? What does it actually penalize?</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> With ReLU, $\sigma'(a_i) = \mathbb{1}[a_i > 0]$ (1 if pre-activation is positive, 0 otherwise).</p>
                                $$J_{ij} = \sigma'(a_i) \cdot W_{ij} = \mathbb{1}[a_i > 0] \cdot W_{ij}$$
                                $$\|J\|_F^2 = \sum_i \sum_j [\mathbb{1}[a_i > 0]]^2 W_{ij}^2 = \sum_i \mathbb{1}[a_i > 0] \sum_j W_{ij}^2$$
                                <p>Since $\mathbb{1}[a_i > 0]^2 = \mathbb{1}[a_i > 0]$ (the indicator is 0 or 1).</p>
                                <p><strong>(b)</strong> Two problems: (1) The gradient of the indicator $\mathbb{1}[a_i > 0]$ is zero almost everywhere (and undefined at $a_i = 0$). The penalty can only reduce $\|J\|_F^2$ by shrinking $W_{ij}$ (like weight decay) for active units, not by moving units toward saturation. (2) The penalty is piecewise constant in $a_i$: it doesn't smoothly encourage any particular activation level. With sigmoid, the $z_i^2(1-z_i)^2$ factor smoothly encourages saturation ($z_i \to 0$ or $1$), providing a richer optimization landscape.</p>
                                <p><strong>(c)</strong> Yes, still useful but acts differently. For active units ($a_i > 0$), the penalty is $\sum_j W_{ij}^2$ — pure weight decay on the $i$-th row of $W$. For dead units ($a_i \leq 0$), the penalty is 0, so they contribute nothing. The penalty thus: (1) shrinks weights of active units, reducing input sensitivity; (2) doesn't affect dead units at all; (3) indirectly encourages sparsity by making it "free" (zero penalty) to kill a unit. With ReLU, the contractive AE behaves more like sparse AE + weight decay than the sigmoid version's contraction toward saturated states.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 28 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">28. Optimal Transport View of Autoencoders</span>
                        </div>
                        <div class="exercise-body">
                            <p>The autoencoder objective $\min_{f,g} \mathbb{E}_{p(x)}[\|x - g(f(x))\|^2]$ can be viewed through optimal transport. (a) In the Monge formulation, optimal transport finds a map $T: \mathbb{R}^n \to \mathbb{R}^n$ minimizing $\mathbb{E}[\|x - T(x)\|^2]$ subject to $T_{\#}p = q$ (pushforward constraint). Show that an autoencoder computes a map $T = g \circ f$ with the implicit constraint $T_{\#}p \approx p$. (b) The key difference: autoencoder maps factor through a bottleneck. Show that if $f: \mathbb{R}^n \to \mathbb{R}^d$ with $d < n$, then $T = g \circ f$ can only produce outputs on a $d$-dimensional manifold. (c) Interpret: what does the autoencoder "transport" and where?</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> Define $T(x) = g(f(x)) = \hat{x}$. The AE loss $\mathbb{E}[\|x - T(x)\|^2]$ is the quadratic transport cost from $p(x)$ to $T_{\#}p$. If the AE achieves zero loss, $T(x) = x$ for $p$-a.e. $x$, meaning $T_{\#}p = p$ exactly. In practice, $T_{\#}p \approx p$ — the reconstruction distribution approximates the data distribution. But unlike standard OT, we don't specify the target measure; the objective itself drives $T_{\#}p$ toward $p$.</p>
                                <p><strong>(b)</strong> Since $f$ maps to $\mathbb{R}^d$ and $g$ maps back to $\mathbb{R}^n$, the image of $T = g \circ f$ is $\text{Im}(g) \subseteq \mathbb{R}^n$. By the rank theorem, if $g$ is smooth and $d < n$, then $\text{Im}(g)$ has measure zero in $\mathbb{R}^n$ — it's a $d$-dimensional manifold (assuming $g$ is an immersion). Therefore $T_{\#}p$ is a distribution supported on a $d$-dimensional manifold, even if $p$ has full $n$-dimensional support.</p>
                                <p><strong>(c)</strong> The autoencoder "transports" each data point $x$ to its nearest point $\hat{x}$ on the learned manifold $\mathcal{M} = g(\mathbb{R}^d)$. The manifold is optimized so that the total transport cost (sum of squared distances from data to manifold) is minimized. This is a nonlinear generalization of PCA, where PCA finds the best $d$-dimensional hyperplane and the AE finds the best $d$-dimensional manifold. The "transport plan" is deterministic: each $x$ maps to exactly one $\hat{x} = g(f(x))$, which is the autoencoder's version of projection onto the manifold.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 29 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">29. Posterior Collapse in VAEs</span>
                        </div>
                        <div class="exercise-body">
                            <p>In a VAE, the ELBO is $\mathcal{L} = \mathbb{E}_{q(z|x)}[\log p(x|z)] - D_{KL}(q(z|x) \| p(z))$. (a) Show that if the decoder $p(x|z)$ is a powerful autoregressive model (e.g., PixelCNN) that ignores $z$, then the optimal encoder is $q(z|x) = p(z)$. This is "posterior collapse." (b) Compute the ELBO when posterior collapse occurs. (c) Name three remedies and explain the mechanism of each.</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> If the decoder ignores $z$: $p(x|z) = p(x)$ for all $z$. Then the reconstruction term becomes $\mathbb{E}_{q(z|x)}[\log p(x|z)] = \mathbb{E}_{q(z|x)}[\log p(x)] = \log p(x)$, which doesn't depend on $q(z|x)$. The only term involving $q$ is $-D_{KL}(q(z|x) \| p(z))$, which is maximized (least negative) when $q(z|x) = p(z)$, giving $D_{KL} = 0$. So the optimal encoder outputs the prior regardless of $x$.</p>
                                <p><strong>(b)</strong> With $q(z|x) = p(z)$: $\mathcal{L} = \log p(x) - 0 = \log p(x)$. The ELBO equals the log-evidence. This is tight, but trivial: the latent variable is unused. The model degenerates to $p(x)$ alone.</p>
                                <p><strong>(c)</strong> Three remedies:</p>
                                <p>(1) <strong>KL annealing (warm-up):</strong> start training with $\beta = 0$ in $\beta \cdot D_{KL}$ and slowly increase to 1. This lets the encoder learn useful representations before the KL penalty pushes toward collapse. Mechanism: the reconstruction term dominates early, establishing encoder-decoder communication.</p>
                                <p>(2) <strong>Free bits (Kingma et al.):</strong> use $\max(\delta, D_{KL})$ instead of $D_{KL}$, ensuring each latent dimension uses at least $\delta$ nats. Mechanism: directly prevents the KL from reaching zero.</p>
                                <p>(3) <strong>Weaken the decoder:</strong> use a simpler decoder (MLP instead of PixelCNN) that cannot model $p(x)$ without $z$. Mechanism: forces $p(x|z) \neq p(x)$, making the reconstruction term depend on $z$, so the encoder must transmit information.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 31 (NEW) -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">31. Rate-Distortion: Water-Filling by Hand</span>
                        </div>
                        <div class="exercise-body">
                            <p>A 3-dimensional Gaussian source has covariance eigenvalues $\lambda_1 = 8, \lambda_2 = 2, \lambda_3 = 0.5$. (a) Compute $R(D)$ for $D = 2.5$ using the water-filling formula. How many components are kept? (b) Compute $R(D)$ for $D = 9$. What does the encoder output? (c) At what distortion $D$ does the encoder switch from keeping 2 components to keeping 1? (d) Sketch the $R(D)$ curve and mark the points from (a), (b), (c).</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> $D = 2.5$. We need $\sum_i \min(\lambda_i, \theta) = D$. Try $\theta = 1$: $\min(8,1) + \min(2,1) + \min(0.5,1) = 1 + 1 + 0.5 = 2.5 = D$. So $\theta = 1$.</p>
                                <p>Components kept: $\lambda_1 = 8 > 1$ (keep), $\lambda_2 = 2 > 1$ (keep), $\lambda_3 = 0.5 \leq 1$ (discard). <strong>Two components kept.</strong></p>
                                <p>$R(2.5) = \frac{1}{2}\log_2\frac{8}{1} + \frac{1}{2}\log_2\frac{2}{1} + 0 = \frac{3}{2} + \frac{1}{2} = 2$ bits.</p>
                                <p><strong>(b)</strong> $D = 9$. Total variance $= 8 + 2 + 0.5 = 10.5$, and $D = 9 < 10.5$. Try $\theta = 3$: $\min(8,3) + \min(2,3) + \min(0.5,3) = 3 + 2 + 0.5 = 5.5 \neq 9$. Try $\theta = 8$: $8 + 2 + 0.5 = 10.5 \neq 9$. We need only $\lambda_1$ kept. So $\theta$ satisfies $\theta + 2 + 0.5 = 9 \Rightarrow \theta = 6.5$. Check: $\lambda_1 = 8 > 6.5$ (keep), $\lambda_2 = 2 < 6.5$ (discard), $\lambda_3 = 0.5 < 6.5$ (discard). One component kept.</p>
                                <p>$R(9) = \frac{1}{2}\log_2\frac{8}{6.5} \approx \frac{1}{2}(0.299) \approx 0.15$ bits. With so much allowed distortion, barely any information is transmitted.</p>
                                <p><strong>(c)</strong> The transition from 2 components to 1 occurs when $\theta = \lambda_2 = 2$. At this point: $D = \min(8,2) + 2 + 0.5 = 2 + 2 + 0.5 = 4.5$. So at $D = 4.5$, the second component is just barely discarded.</p>
                                <p><strong>(d)</strong> The $R(D)$ curve is piecewise: a steep segment for $D \in [0, 0.5]$ (all 3 components), a moderate segment for $D \in [0.5, 4.5]$ (2 components), and a gentle segment for $D \in [4.5, 10.5]$ (1 component). At $D = 10.5$, $R = 0$.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 32 (NEW) -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">32. Rate-Distortion vs Autoencoder Bottleneck</span>
                        </div>
                        <div class="exercise-body">
                            <p>A dataset has intrinsic dimensionality 5, embedded in $\mathbb{R}^{100}$. You train autoencoders with bottleneck dimensions $d = 2, 5, 10, 50$. (a) Which bottleneck dimension corresponds to operating on the $R(D)$ curve, and which operates above it? (b) Can an autoencoder with $d = 2$ ever achieve lower distortion than one with $d = 5$ on this data? Why or why not? (c) Can an autoencoder with $d = 50$ achieve lower distortion than one with $d = 5$? What would this mean about the "intrinsic dimensionality" claim? (d) How does the nonlinearity of the autoencoder relate to the gap between its performance and the $R(D)$ bound?</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> The $R(D)$ curve is the theoretical lower bound — no real system can go below it. All autoencoders operate <em>on or above</em> the curve. A perfectly trained autoencoder with the right architecture operates close to $R(D)$; a poorly trained one operates well above it (more distortion than necessary for the given rate). No autoencoder operates below $R(D)$.</p>
                                <p><strong>(b)</strong> No. With $d = 2$, the encoder can transmit at most as much information as 2 continuous dimensions allow. With $d = 5$, it can transmit strictly more. Since the data has 5 intrinsic dimensions, the $d = 5$ autoencoder can (in principle) achieve zero distortion, while the $d = 2$ autoencoder must discard 3 dimensions of variation, incurring irreducible distortion.</p>
                                <p><strong>(c)</strong> In principle, $d = 50$ cannot achieve lower distortion than $d = 5$ on data that truly has 5 intrinsic dimensions, because $d = 5$ already suffices for zero distortion. In practice, $d = 50$ might achieve slightly lower <em>training</em> distortion if the $d = 5$ autoencoder hasn't converged or has optimization issues. If $d = 50$ achieves <em>significantly</em> lower distortion, it means the data has more than 5 intrinsic dimensions (the original claim was wrong), or the data contains noise that the overcomplete autoencoder is memorizing.</p>
                                <p><strong>(d)</strong> A linear autoencoder is equivalent to PCA, which achieves the $R(D)$ bound for Gaussian data. For non-Gaussian data, PCA is suboptimal — there exists nonlinear structure that a linear encoder cannot exploit. A nonlinear autoencoder can capture this structure, operating closer to the true $R(D)$ curve. The gap between a linear and nonlinear autoencoder's performance is a measure of the non-Gaussian structure in the data.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 33 (NEW) -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">33. Rate-Distortion for Uniform Source</span>
                        </div>
                        <div class="exercise-body">
                            <p>Consider a uniform source $X \sim \text{Uniform}[0, 1]$ with MSE distortion. (a) What is the distortion at $R = 0$ (zero bits)? (b) If we use 1 bit (two reconstruction levels), what is the optimal placement of the two reconstruction points and the resulting distortion? (c) Generalize: with $n$ bits ($2^n$ reconstruction levels uniformly spaced), show that $D = \frac{1}{12 \cdot 4^n}$. (d) Derive $R(D) = \frac{1}{2}\log_2\frac{1}{12D}$ for $D \leq \frac{1}{12}$ and compare to the Gaussian case.</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> At $R = 0$, the decoder outputs a constant $\hat{x} = c$. The MSE is $\mathbb{E}[(X - c)^2]$, minimized at $c = \mathbb{E}[X] = 1/2$. The distortion is $\text{Var}(X) = 1/12 \approx 0.0833$.</p>
                                <p><strong>(b)</strong> With 1 bit, we partition $[0,1]$ into 2 intervals. The optimal partition is $[0, 1/2)$ and $[1/2, 1]$, with reconstruction points at their midpoints: $\hat{x}_1 = 1/4$ and $\hat{x}_2 = 3/4$. The distortion is $\mathbb{E}[(X - \hat{X})^2] = 2 \int_0^{1/2} (x - 1/4)^2 dx = 2 \cdot \frac{1}{48} = \frac{1}{48} \approx 0.0208$. This is $1/4$ of the zero-rate distortion — one bit reduces distortion by a factor of 4.</p>
                                <p><strong>(c)</strong> With $2^n$ uniform intervals of width $\Delta = 2^{-n}$, reconstruction points at interval midpoints: $D = \mathbb{E}[(X - \hat{X})^2] = \frac{\Delta^2}{12} = \frac{2^{-2n}}{12} = \frac{1}{12 \cdot 4^n}$. Each additional bit reduces distortion by a factor of 4 (6 dB), same as for Gaussian sources.</p>
                                <p><strong>(d)</strong> Solving $D = \frac{1}{12 \cdot 4^n}$ for $n$: $4^n = \frac{1}{12D}$, so $n = \frac{1}{2}\log_2\frac{1}{12D}$. Thus $R(D) = \frac{1}{2}\log_2\frac{1}{12D}$ for $D \leq 1/12$. Comparing to Gaussian $R(D) = \frac{1}{2}\log_2\frac{\sigma^2}{D}$: the uniform source with $\sigma^2 = 1/12$ gives $R(D) = \frac{1}{2}\log_2\frac{1/12}{D}$, which is identical. This is not a coincidence: the rate-distortion function depends only on the variance for MSE distortion in many cases, though the achievability (which code achieves the bound) differs.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 34 (NEW) -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">34. Score Function: Intuition and Computation</span>
                        </div>
                        <div class="exercise-body">
                            <p>(a) Compute the score function $\nabla_x \log p(x)$ for a 1D Gaussian $p(x) = \mathcal{N}(x; \mu, \sigma^2)$. What direction does it point when $x > \mu$? When $x < \mu$? At $x = \mu$? (b) Compute the score for a mixture of two Gaussians: $p(x) = \frac{1}{2}\mathcal{N}(x; -3, 1) + \frac{1}{2}\mathcal{N}(x; 3, 1)$. What is the score at $x = 0$? At $x = 3$? (c) Explain intuitively why the score is useful for generation but the density $p(x)$ is not directly computable. (d) If a denoising autoencoder is trained with $\sigma = 2$ on data from this mixture, what does the denoiser output at $\tilde{x} = 0$? At $\tilde{x} = 5$?</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> $\log p(x) = -\frac{(x-\mu)^2}{2\sigma^2} + \text{const}$. Score: $\nabla_x \log p(x) = -\frac{x - \mu}{\sigma^2}$.</p>
                                <ul>
                                    <li>When $x > \mu$: score is negative (points left, toward the mean).</li>
                                    <li>When $x < \mu$: score is positive (points right, toward the mean).</li>
                                    <li>At $x = \mu$: score is zero (at the mode, no preferred direction).</li>
                                </ul>
                                <p>The score always points toward the mean — it's a "restoring force" pulling $x$ back to high-density regions.</p>
                                <p><strong>(b)</strong> For the mixture: $s(x) = \frac{-\frac{1}{2}(x+3)e^{-(x+3)^2/2} - \frac{1}{2}(x-3)e^{-(x-3)^2/2}}{\frac{1}{2}e^{-(x+3)^2/2} + \frac{1}{2}e^{-(x-3)^2/2}}$.</p>
                                <p>At $x = 0$: by symmetry, the two components contribute equal and opposite forces. Score $= 0$. (The midpoint between two equal modes is a saddle point of $\log p$.) At $x = 3$: the right mode dominates. Score $\approx -(3-3)/1 = 0$ (near the mode). More precisely, the left mode contributes a tiny positive pull, so the score is slightly positive (pulled slightly toward $x = 0$, but negligibly).</p>
                                <p><strong>(c)</strong> Computing $p(x)$ requires the normalization constant $Z = \int p^*(x) dx$, which is intractable for high-dimensional distributions (e.g., images). But the score $\nabla_x \log p(x) = \nabla_x \log p^*(x)$ cancels $Z$ because $\nabla_x \log Z = 0$ (Z is a constant). So we can estimate the score from unnormalized densities or from denoising, without ever computing $Z$. For generation, the score is sufficient: Langevin dynamics uses only the score to move samples toward high-density regions.</p>
                                <p><strong>(d)</strong> The optimal denoiser output is $g^*(\tilde{x}) = \mathbb{E}[X | \tilde{X} = \tilde{x}]$. At $\tilde{x} = 0$: by symmetry, $X = -3$ and $X = 3$ are equally likely given $\tilde{x} = 0$ (with $\sigma = 2$). So $g^*(0) = \frac{1}{2}(-3) + \frac{1}{2}(3) = 0$. The denoiser outputs 0 — the average of two modes, not either mode. This is the "blurring" effect of MSE-optimal denoising.</p>
                                <p>At $\tilde{x} = 5$: the right mode ($x = 3$) is much more likely than the left ($x = -3$). The posterior is dominated by $x \approx 3$, so $g^*(5) \approx \frac{\sigma_X^2}{\sigma_X^2 + \sigma^2}(5 - 3) + 3 \approx 3.4$ (pulled toward 3 but not all the way, because $\tilde{x} = 5$ is also consistent with $x$ values larger than 3).</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 35 (NEW) -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">35. Denoising at Different Noise Levels</span>
                        </div>
                        <div class="exercise-body">
                            <p>Consider data on a 1D manifold (a circle) in 2D: $x = (\cos\theta, \sin\theta)$ with $\theta \sim \text{Uniform}[0, 2\pi)$. A denoising autoencoder is trained with noise level $\sigma$. (a) For very small $\sigma \to 0$, what does the optimal denoiser do geometrically? (b) For very large $\sigma \to \infty$, what does the optimal denoiser output? (c) For intermediate $\sigma \approx 0.5$, describe qualitatively what the optimal denoiser does for a point $\tilde{x}$ that is 0.5 units from the circle. (d) Explain why training at multiple noise levels (as in diffusion models) is more useful than training at a single level.</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> For $\sigma \to 0$: the noisy point $\tilde{x}$ is very close to the circle. The posterior $p(x|\tilde{x})$ concentrates on the nearest point on the circle. The denoiser acts as a <strong>projection onto the manifold</strong>: $g^*(\tilde{x}) \approx \text{proj}_{\text{circle}}(\tilde{x})$. This is the geometric interpretation: small-noise denoising = manifold projection.</p>
                                <p><strong>(b)</strong> For $\sigma \to \infty$: the noise completely overwhelms the signal. The posterior $p(x|\tilde{x}) \approx p(x)$ regardless of $\tilde{x}$ (the observation is uninformative). The denoiser outputs $g^*(\tilde{x}) \approx \mathbb{E}[X] = (0, 0)$ — the center of the circle. All noisy inputs map to the same point.</p>
                                <p><strong>(c)</strong> For $\sigma \approx 0.5$: the posterior $p(x|\tilde{x})$ is concentrated on an arc of the circle near $\tilde{x}$, but not a single point. The denoiser outputs the centroid of this arc. For a point 0.5 units outside the circle, the output is on or near the circle but slightly pulled toward the center (because the arc's centroid is inside the circle, not on it). The output is "shrunk" toward $(0,0)$ compared to pure projection — a form of regularization.</p>
                                <p><strong>(d)</strong> Different noise levels capture different scales of data structure:</p>
                                <ul>
                                    <li><strong>Large $\sigma$:</strong> The denoiser learns the global structure — where the data is centered, its overall shape. The score at large $\sigma$ points toward the data from far away.</li>
                                    <li><strong>Medium $\sigma$:</strong> The denoiser learns the manifold's geometry — its curvature, the distance between clusters. The score resolves which mode a point belongs to.</li>
                                    <li><strong>Small $\sigma$:</strong> The denoiser learns fine local details — texture, edges, precise manifold position. The score provides precise manifold projection.</li>
                                </ul>
                                <p>Training at a single noise level only learns one scale. Diffusion models train at all scales and then denoise iteratively from large $\sigma$ to small $\sigma$, progressively refining from global structure to fine details. This coarse-to-fine approach is why diffusion models produce such high-quality samples.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 36 (NEW) -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">36. Tweedie's Formula: Verification for Gaussian Data</span>
                        </div>
                        <div class="exercise-body">
                            <p>Verify Tweedie's formula explicitly for Gaussian data. Let $X \sim \mathcal{N}(\mu, \tau^2)$ and noise $\epsilon \sim \mathcal{N}(0, \sigma^2)$, so $\tilde{X} = X + \epsilon$. (a) Compute $p(\tilde{x})$ by marginalizing out $X$. (b) Compute the score $\nabla_{\tilde{x}} \log p(\tilde{x})$ directly. (c) Compute the optimal denoiser $g^*(\tilde{x}) = \mathbb{E}[X|\tilde{X}=\tilde{x}]$ using Bayes' rule for Gaussians. (d) Verify that $\nabla_{\tilde{x}} \log p(\tilde{x}) = \frac{g^*(\tilde{x}) - \tilde{x}}{\sigma^2}$. (e) Interpret: when $\tau^2 \gg \sigma^2$, what does the denoiser do? When $\tau^2 \ll \sigma^2$?</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> Since $\tilde{X} = X + \epsilon$ with independent Gaussians: $\tilde{X} \sim \mathcal{N}(\mu, \tau^2 + \sigma^2)$. So $p(\tilde{x}) = \mathcal{N}(\tilde{x}; \mu, \tau^2 + \sigma^2)$.</p>
                                <p><strong>(b)</strong> $\log p(\tilde{x}) = -\frac{(\tilde{x}-\mu)^2}{2(\tau^2+\sigma^2)} + \text{const}$. Score: $\nabla_{\tilde{x}} \log p(\tilde{x}) = -\frac{\tilde{x} - \mu}{\tau^2 + \sigma^2}$.</p>
                                <p><strong>(c)</strong> By Bayes for Gaussians (posterior of a Gaussian prior with Gaussian likelihood): $X | \tilde{X} = \tilde{x} \sim \mathcal{N}\left(\frac{\tau^2 \tilde{x} + \sigma^2 \mu}{\tau^2 + \sigma^2}, \frac{\tau^2 \sigma^2}{\tau^2 + \sigma^2}\right)$.</p>
                                <p>So $g^*(\tilde{x}) = \mathbb{E}[X|\tilde{X}=\tilde{x}] = \frac{\tau^2 \tilde{x} + \sigma^2 \mu}{\tau^2 + \sigma^2}$.</p>
                                <p><strong>(d)</strong> Compute $\frac{g^*(\tilde{x}) - \tilde{x}}{\sigma^2}$:</p>
                                $$\frac{g^*(\tilde{x}) - \tilde{x}}{\sigma^2} = \frac{1}{\sigma^2}\left(\frac{\tau^2 \tilde{x} + \sigma^2 \mu}{\tau^2+\sigma^2} - \tilde{x}\right) = \frac{1}{\sigma^2} \cdot \frac{\tau^2\tilde{x} + \sigma^2\mu - (\tau^2+\sigma^2)\tilde{x}}{\tau^2+\sigma^2}$$
                                $$= \frac{1}{\sigma^2} \cdot \frac{\sigma^2(\mu - \tilde{x})}{\tau^2+\sigma^2} = \frac{\mu - \tilde{x}}{\tau^2+\sigma^2} = -\frac{\tilde{x}-\mu}{\tau^2+\sigma^2}$$
                                <p>This equals the score from (b). Tweedie's formula verified.</p>
                                <p><strong>(e)</strong> When $\tau^2 \gg \sigma^2$ (low noise relative to data variance): $g^*(\tilde{x}) \approx \tilde{x}$ — the denoiser barely changes the input, because the noise is small compared to the signal. When $\tau^2 \ll \sigma^2$ (high noise): $g^*(\tilde{x}) \approx \mu$ — the denoiser ignores the noisy observation and outputs the prior mean, because the noise has destroyed all information about $X$. The interpolation factor $\frac{\tau^2}{\tau^2+\sigma^2}$ is a signal-to-noise ratio that determines how much to trust the observation.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 30 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">30. Denoising Score Matching to Diffusion Models</span>
                        </div>
                        <div class="exercise-body">
                            <p>Extend Exercise 24 (Score Matching) to multiple noise levels. Consider $L$ noise levels $\sigma_1 > \sigma_2 > \cdots > \sigma_L > 0$ and the multi-scale denoising score matching objective:</p>
                            $$\mathcal{L} = \sum_{i=1}^L \lambda(\sigma_i) \mathbb{E}_{x \sim p(x)} \mathbb{E}_{\tilde{x} \sim \mathcal{N}(x, \sigma_i^2 I)} \left[\left\|s_\theta(\tilde{x}, \sigma_i) - \nabla_{\tilde{x}} \log p_{\sigma_i}(\tilde{x}|x)\right\|^2\right]$$
                            <p>(a) Show that $\nabla_{\tilde{x}} \log p_{\sigma_i}(\tilde{x}|x) = \frac{x - \tilde{x}}{\sigma_i^2}$ and simplify the objective. (b) With $\lambda(\sigma_i) = \sigma_i^2$, show this is equivalent to training a denoiser: $\epsilon_\theta(\tilde{x}, \sigma_i) \approx \epsilon$ where $\tilde{x} = x + \sigma_i \epsilon$. (c) Explain how this connects to DDPM: as $L \to \infty$ and we take $\sigma_i^2 = \bar{\sigma}^2(t_i)$ for a continuous noise schedule $t \in [0, T]$, the discrete sum becomes an integral and the iterative denoising process becomes the reverse of a stochastic differential equation.</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> Since $p(\tilde{x}|x) = \mathcal{N}(\tilde{x}; x, \sigma_i^2 I)$:</p>
                                $$\log p(\tilde{x}|x) = -\frac{\|\tilde{x} - x\|^2}{2\sigma_i^2} - \frac{n}{2}\log(2\pi\sigma_i^2)$$
                                $$\nabla_{\tilde{x}} \log p(\tilde{x}|x) = -\frac{\tilde{x} - x}{\sigma_i^2} = \frac{x - \tilde{x}}{\sigma_i^2}$$
                                <p>Substituting into the objective:</p>
                                $$\mathcal{L} = \sum_{i=1}^L \lambda(\sigma_i) \mathbb{E}\left[\left\|s_\theta(\tilde{x}, \sigma_i) - \frac{x - \tilde{x}}{\sigma_i^2}\right\|^2\right]$$
                                <p><strong>(b)</strong> Let $\epsilon = \frac{\tilde{x} - x}{\sigma_i} \sim \mathcal{N}(0, I)$, so $x - \tilde{x} = -\sigma_i \epsilon$ and $\frac{x - \tilde{x}}{\sigma_i^2} = \frac{-\epsilon}{\sigma_i}$.</p>
                                <p>Reparametrize: $s_\theta(\tilde{x}, \sigma_i) = \frac{-\epsilon_\theta(\tilde{x}, \sigma_i)}{\sigma_i}$.</p>
                                <p>With $\lambda(\sigma_i) = \sigma_i^2$:</p>
                                $$\mathcal{L} = \sum_{i=1}^L \sigma_i^2 \mathbb{E}\left[\left\|\frac{-\epsilon_\theta}{\sigma_i} - \frac{-\epsilon}{\sigma_i}\right\|^2\right] = \sum_{i=1}^L \mathbb{E}\left[\|\epsilon_\theta(\tilde{x}, \sigma_i) - \epsilon\|^2\right]$$
                                <p>This is exactly the DDPM loss: predict the noise $\epsilon$ that was added.</p>
                                <p><strong>(c)</strong> As $L \to \infty$ with a continuous schedule $\sigma^2(t)$: the forward process $x_t = x_0 + \sigma(t)\epsilon$ becomes an Ito SDE: $dx_t = f(x_t, t)dt + g(t)dw_t$ where $g(t) = \left(\frac{d\sigma^2(t)}{dt}\right)^{1/2}$ and $f$ depends on the schedule parameterization. The discrete denoising iterations converge to the reverse-time SDE:</p>
                                $$dx = [f(x,t) - g^2(t)\nabla_x \log p_t(x)]dt + g(t)d\bar{w}$$
                                <p>where $\bar{w}$ is a reverse-time Brownian motion. The trained score network $s_\theta \approx \nabla_x \log p_t$ provides the drift correction. This is Anderson's (1982) reverse-time SDE theorem applied to generative modeling — the foundation of Score SDE (Song et al., 2021).</p>
                            </div>
                        </div>
                    </div>

                </div>
            </article>

        </main>

        <!-- TOC (Right Side) -->
        <aside class="toc-container">
        <h4 class="toc-title">Contents</h4>
        <nav class="toc-list">
            <a href="#why-autoencoders" class="toc-link">Why Autoencoders?</a>
            <a href="#mathematics" class="toc-link">The Mathematics</a>
            <a href="#information-bottleneck" class="toc-link">Information Bottleneck</a>
            <a href="#undercomplete-overcomplete" class="toc-link">Undercomplete vs Overcomplete</a>
            <a href="#sparse-autoencoders" class="toc-link">Sparse Autoencoders</a>
            <a href="#denoising-autoencoders" class="toc-link">Denoising Autoencoders</a>
            <a href="#contractive-autoencoders" class="toc-link">Contractive Autoencoders</a>
            <a href="#applications" class="toc-link">Applications</a>
            <a href="#from-ae-to-vae" class="toc-link">From AE to VAE</a>
        </nav>
    </aside>
    </div>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-brand">
                    <span class="logo-symbol">∇</span>
                    <span>ML Fundamentals</span>
                </div>
                <p class="footer-tagline">Deep understanding through first principles.</p>
            </div>
            <div class="footer-links">
                <a href="../../../index.html">Home</a>
                <a href="https://github.com/ml-entropy/ml-entropy.github.io" target="_blank">GitHub</a>
            </div>
        </div>
    </footer>

    <!-- Scripts -->
    <script src="../../../js/main.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            if (typeof renderMathInElement === 'function') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }

            const tabs = document.querySelectorAll('.tutorial-tab');
            const articles = document.querySelectorAll('.article-content');

            function switchTab(targetId) {
                if (!targetId || targetId === '#') targetId = '#theory';
                tabs.forEach(tab => {
                    if (tab.getAttribute('href') === targetId) {
                        tab.classList.add('active');
                    } else {
                        tab.classList.remove('active');
                    }
                });
                articles.forEach(article => {
                    const articleId = '#' + article.id;
                    if (articleId === targetId) {
                        article.style.display = 'block';
                    } else {
                        article.style.display = 'none';
                    }
                });
                if (typeof renderMathInElement === 'function') {
                    renderMathInElement(document.body, {
                        delimiters: [
                            {left: '$$', right: '$$', display: true},
                            {left: '$', right: '$', display: false},
                            {left: '\\[', right: '\\]', display: true},
                            {left: '\\(', right: '\\)', display: false}
                        ],
                        throwOnError: false
                    });
                }
                const toc = document.querySelector('.toc-container');
                if (toc) {
                    if (targetId === '#theory') {
                        toc.style.display = 'block';
                        setTimeout(() => toc.classList.add('visible'), 100);
                    } else {
                        toc.classList.remove('visible');
                        setTimeout(() => toc.style.display = 'none', 300);
                    }
                }
            }

            tabs.forEach(tab => {
                tab.addEventListener('click', (e) => {
                    e.preventDefault();
                    const targetId = tab.getAttribute('href');
                    history.pushState(null, null, targetId);
                    switchTab(targetId);
                });
            });

            window.addEventListener('popstate', () => {
                switchTab(window.location.hash);
            });

            switchTab(window.location.hash);
        });
    </script>
</body>
</html>
