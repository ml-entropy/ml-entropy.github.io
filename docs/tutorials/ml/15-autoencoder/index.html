<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Autoencoders | ML Fundamentals</title>
    <meta name="description" content="Learn autoencoders from first principles: compression, information bottleneck, sparse, denoising, and contractive variants. The foundation for VAEs.">

    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&family=Playfair+Display:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}, {left: '\\\\[', right: '\\\\]', display: true}, {left: '\\\\(', right: '\\\\)', display: false}], throwOnError: false});"></script>

    <!-- Styles -->
    <link rel="stylesheet" href="../../../css/main.css">
    <link rel="stylesheet" href="../../../css/components.css">

    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>∞</text></svg>">
<link rel="stylesheet" href="../../../css/sidebar.css">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar" id="navbar">
        <div class="nav-container">
            <a href="../../../index.html" class="nav-logo">
                <span class="logo-symbol">∇</span>
                <span class="logo-text">ML Fundamentals</span>
            </a>

            <button class="nav-toggle" id="navToggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>

            <div class="nav-menu" id="navMenu">
                <div class="nav-links">
                    <a href="../../../tutorials/ml/index.html" class="nav-link active">Machine Learning</a>
                    <a href="../../../tutorials/linear-algebra/index.html" class="nav-link">Linear Algebra</a>
                    <a href="../../../tutorials/calculus/index.html" class="nav-link">Calculus</a>
                    <a href="../../../tutorials/physics/index.html" class="nav-link">Physics</a>
                    <a href="../../../index.html#philosophy" class="nav-link">Philosophy</a>
                    <a href="../../../index.html#roadmap" class="nav-link">Roadmap</a>
                    <a href="https://github.com/ml-entropy/ml-entropy.github.io" class="nav-link" target="_blank">GitHub</a>
                </div>

                <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
                    <svg class="sun-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="5"/>
                        <path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"/>
                    </svg>
                    <svg class="moon-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                    </svg>
                </button>
            </div>
        </div>
    </nav>

    <!-- Tutorial Header -->
    <header class="tutorial-content-header">
        <div class="container">
            <nav class="breadcrumb">
                <a href="../../../index.html">Home</a>
                <span class="breadcrumb-separator">→</span>
                <a href="../index.html">Machine Learning</a>
                <span class="breadcrumb-separator">→</span>
                <span>Autoencoders</span>
            </nav>

            <div class="tutorial-tabs">
                <a href="#theory" class="tutorial-tab active">Theory</a>
                <a href="#code" class="tutorial-tab">Code</a>
                <a href="#exercises" class="tutorial-tab">Exercises</a>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <div class="tutorial-wrapper">
        <!-- Sidebar Navigation -->
        <aside class="tutorial-sidebar">
            <div class="sidebar-section">
                <h3 class="sidebar-section-title">Machine Learning</h3>
                <nav class="sidebar-nav">
                        <a href="../00-probability/index.html" class="sidebar-link">00. Probability Foundations</a>
                    <a href="../04-logarithms/index.html" class="sidebar-link">01. Why Logarithms?</a>
                    <a href="../05-combinatorics/index.html" class="sidebar-link">02. Combinatorics</a>
                    <a href="../03-distributions/index.html" class="sidebar-link">03. Normal Distributions</a>
                    <a href="../01-entropy/index.html" class="sidebar-link">04. Entropy Fundamentals</a>
                    <a href="../02-cross-entropy/index.html" class="sidebar-link">05. Cross-Entropy</a>
                    <a href="../02-kl-divergence/index.html" class="sidebar-link">06. KL Divergence</a>
                    <a href="../14-entropy-connections/index.html" class="sidebar-link">07. Entropy Connections</a>
                    <a href="../06-backpropagation/index.html" class="sidebar-link">08. Backpropagation</a>
                    <a href="../07-regularization/index.html" class="sidebar-link">09. Regularization</a>
                    <a href="../08-batch-normalization/index.html" class="sidebar-link">10. Batch Normalization</a>
                    <a href="../09-learning-rate/index.html" class="sidebar-link">11. Learning Rate</a>
                    <a href="../10-cnn/index.html" class="sidebar-link">12. CNNs</a>
                    <a href="../11-rnn/index.html" class="sidebar-link">13. RNNs</a>
                    <a href="../15-autoencoder/index.html" class="sidebar-link active">14. Autoencoders</a>
                    <a href="../13-variational-inference/index.html" class="sidebar-link">15. Variational Inference</a>
                    <a href="../12-vae/index.html" class="sidebar-link">16. VAE</a>
                    <a href="../16-inductive-bias/index.html" class="sidebar-link">17. Inductive Bias</a>
                    <a href="../17-architectural-biases/index.html" class="sidebar-link">18. Architectural Biases</a>
                    <a href="../18-designing-biases/index.html" class="sidebar-link">19. Designing Biases</a>
                    <a href="../19-fst-fundamentals/index.html" class="sidebar-link">20. FST Fundamentals</a>
                    <a href="../20-weighted-fsts/index.html" class="sidebar-link">21. Weighted FSTs</a>
                    <a href="../21-fst-libraries/index.html" class="sidebar-link">22. FST Libraries</a>
                    <a href="../22-fst-applications/index.html" class="sidebar-link">23. FST Applications</a>
                    <a href="../23-neural-symbolic/index.html" class="sidebar-link">24. Neural-Symbolic Hybrids</a>
                    <a href="../24-sequence-alignment/index.html" class="sidebar-link">25. Sequence Alignment</a>
                    <a href="../25-mas-algorithm/index.html" class="sidebar-link">26. MAS Algorithm</a>
                    <a href="../26-forced-alignment/index.html" class="sidebar-link">27. Forced Alignment & MFA</a>
                </nav>
            </div>
            
            <div class="sidebar-section" style="margin-top: 2rem;">
                <h3 class="sidebar-section-title">Related Subjects</h3>
                <nav class="sidebar-nav">
                        <a href="../../linear-algebra/index.html" class="sidebar-link">Linear Algebra</a>
                    <a href="../../calculus/index.html" class="sidebar-link">Calculus</a>
                    <a href="../../physics/index.html" class="sidebar-link">Physics</a>
                </nav>
            </div>
        </aside>

        <!-- Main Article -->
        <main class="tutorial-main">

            <article class="article-content" id="theory">

                <h1>14. Autoencoders</h1>
                <p class="lead" style="color: var(--color-text-secondary); font-size: 1.125rem; margin-bottom: 2rem;">
                    Autoencoders learn to compress data into a lower-dimensional representation and reconstruct it.
                    We derive the architecture, understand the information bottleneck, and explore variants:
                    sparse, denoising, and contractive autoencoders.
                </p>

                <!-- Section 1 -->
                <h2 id="why-autoencoders">Why Autoencoders?</h2>

                <p>
                    Raw data lives in very high-dimensional spaces, but the <strong>intrinsic dimensionality</strong>
                    is often much lower. A 64×64 grayscale image has 4,096 dimensions, but the space of "meaningful"
                    images is a tiny manifold within $\mathbb{R}^{4096}$.
                </p>

                <div class="definition-box">
                    <div class="box-title">Autoencoder Architecture</div>
                    <p style="margin-bottom: 0;">
                        <strong>Encoder $f_\theta$:</strong> Maps input $x \in \mathbb{R}^n$ to latent code $z \in \mathbb{R}^d$ where $d \ll n$<br>
                        <strong>Decoder $g_\phi$:</strong> Maps latent code back to reconstruction $\hat{x} \in \mathbb{R}^n$<br>
                        <strong>Objective:</strong> Minimize $\|x - \hat{x}\|^2$ — the bottleneck forces learning what matters
                    </p>
                </div>

                <p>
                    A single-layer linear autoencoder learns the same subspace as PCA.
                    The power of autoencoders comes from <strong>nonlinear</strong> encodings via deep networks.
                </p>

                <!-- Section 2 -->
                <h2 id="mathematics">The Mathematics</h2>

                <p>
                    The encoder and decoder are parameterized as neural networks:
                </p>

                <div class="math-block">
                    $$z = f_\theta(x) = \sigma(Wx + b)$$
                </div>

                <div class="math-block">
                    $$\hat{x} = g_\phi(z) = \sigma'(W'z + b')$$
                </div>

                <p>For continuous data, we use <strong>MSE loss</strong>:</p>

                <div class="math-block">
                    $$\mathcal{L}_{MSE} = \|x - \hat{x}\|^2 = \sum_{j=1}^n (x_j - \hat{x}_j)^2$$
                </div>

                <p>For binary data, we use <strong>binary cross-entropy</strong> (maximum likelihood):</p>

                <div class="math-block">
                    $$\mathcal{L}_{BCE} = -\sum_{j=1}^n \left[x_j \log \hat{x}_j + (1 - x_j) \log(1 - \hat{x}_j)\right]$$
                </div>

                <div class="note-box">
                    <div class="box-title">Tied Weights</div>
                    <p style="margin-bottom: 0;">
                        Setting decoder weights $W' = W^T$ halves parameters and acts as regularization.
                        The decoder is forced to use the transpose of the encoder's learned features.
                    </p>
                </div>

                <!-- Section 3 -->
                <h2 id="information-bottleneck">The Information Bottleneck</h2>

                <p>
                    The encoder performs <strong>lossy compression</strong>. With $d &lt; n$, information must be lost.
                    The autoencoder learns <em>which</em> information to keep. To understand this precisely,
                    we need three information-theoretic quantities.
                </p>

                <h3>Entropy $H(X)$</h3>

                <div class="math-block">
                    $$H(X) = -\sum_x p(x) \log p(x)$$
                </div>

                <p>
                    <strong>Intuition:</strong> $H(X)$ is the total uncertainty or average surprise in $X$. For our data,
                    $H(X)$ is fixed — it's how much information the images inherently contain.
                    (We covered this in detail in the <a href="../01-entropy/index.html">Entropy Fundamentals</a> tutorial.)
                </p>

                <h3>Conditional Entropy $H(X|Z)$</h3>

                <div class="math-block">
                    $$H(X|Z) = -\sum_{x,z} p(x,z) \log p(x|z)$$
                </div>

                <p>
                    <strong>Intuition:</strong> $H(X|Z)$ is the uncertainty remaining in $X$ after observing the latent code $Z$.
                    If the autoencoder is perfect, $H(X|Z) = 0$ — knowing $Z$ tells us everything about $X$.
                    If the encoding is useless, $H(X|Z) = H(X)$ — $Z$ tells us nothing.
                </p>

                <h3>Mutual Information $I(X; Z)$</h3>

                <div class="definition-box">
                    <div class="box-title">Mutual Information</div>
                    <p>
                        $$I(X; Z) = H(X) - H(X|Z)$$
                    </p>
                    <p style="margin-bottom: 0;">
                        $I(X; Z)$ measures how many bits of information about $X$ are preserved in $Z$.
                        The autoencoder's reconstruction loss is a proxy for maximizing $I(X; Z)$ — the better the
                        reconstruction, the more information $Z$ retained.
                    </p>
                </div>

                <p>
                    <strong>Key insight:</strong> Since $H(X)$ is fixed for our dataset, minimizing $H(X|Z)$
                    (which corresponds to minimizing reconstruction error) is equivalent to maximizing $I(X; Z)$.
                </p>

                <h3>Shannon's Rate-Distortion Theory</h3>

                <div class="math-derivation">
                    <div class="math-derivation-title">Shannon's Rate-Distortion Theory</div>

                    <div class="math-step">
                        <div class="math-step-number">1</div>
                        <div class="math-step-content">
                            <strong>The question:</strong> What is the minimum number of bits (rate $R$) needed to describe
                            a source $X$ such that the expected distortion $\mathbb{E}[d(X, \hat{X})]$ is at most $D$?
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">2</div>
                        <div class="math-step-content">
                            <strong>Rate-distortion function:</strong>
                            $$R(D) = \min_{p(z|x): \mathbb{E}[d(x,\hat{x})] \leq D} I(X; Z)$$
                            This is the theoretical minimum — no encoder-decoder pair can achieve distortion $\leq D$
                            while transmitting fewer than $R(D)$ bits.
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">3</div>
                        <div class="math-step-content">
                            <strong>The autoencoder connection:</strong> The bottleneck dimension $d$ implicitly controls
                            the rate. A $d$-dimensional continuous bottleneck can transmit at most a certain number of bits
                            per sample. The training process finds the best encoding within this budget.
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">4</div>
                        <div class="math-step-content">
                            <strong>The trade-off curve:</strong> $R(D)$ is a monotonically decreasing, convex curve.
                            At one extreme ($R = 0$), the best reconstruction is just $\hat{x} = \mathbb{E}[X]$ (the mean).
                            At the other extreme ($R \to \infty$), distortion drops to zero (perfect reconstruction).
                        </div>
                    </div>
                </div>

                <div class="note-box">
                    <div class="box-title">Example: Gaussian Rate-Distortion</div>
                    <p style="margin-bottom: 0;">
                        <strong>Example:</strong> For a Gaussian source $X \sim \mathcal{N}(0, \sigma^2)$ with MSE distortion,
                        Shannon proved $R(D) = \frac{1}{2}\log_2\frac{\sigma^2}{D}$ for $D \leq \sigma^2$.
                        A source with variance $\sigma^2 = 4$ requires at least $R(1) = 1$ bit to achieve distortion $D = 1$.
                        Halving the distortion to $D = 0.5$ costs an additional half bit: $R(0.5) = 1.5$ bits.
                    </p>
                </div>

                <div class="definition-box">
                    <div class="box-title">Rate-Distortion Trade-off</div>
                    <p>
                        <strong>Rate:</strong> Bits needed to represent $z$ (limited by bottleneck dimension $d$)<br>
                        <strong>Distortion:</strong> Reconstruction error $\|x - \hat{x}\|^2$<br>
                        <strong>Trade-off:</strong> Smaller bottleneck → more compression → higher distortion
                    </p>
                    <p style="margin-bottom: 0;">
                        An autoencoder with bottleneck dimension $d=2$ on MNIST (784-dim) achieves roughly 99.7% compression
                        — the network must learn which 0.3% of information matters most.
                    </p>
                </div>

                <!-- Section 4 -->
                <h2 id="undercomplete-overcomplete">Undercomplete vs Overcomplete</h2>

                <p>
                    <strong>Undercomplete ($d &lt; n$):</strong> The bottleneck alone forces compression. Simple and effective,
                    but may be too restrictive for complex data.
                </p>

                <p>
                    <strong>Overcomplete ($d \geq n$):</strong> The network can learn the identity function with zero loss!
                    We must add explicit regularization to prevent trivial solutions.
                </p>

                <!-- Section 5 -->
                <h2 id="sparse-autoencoders">Sparse Autoencoders</h2>

                <p>
                    Add a sparsity penalty so most latent units are inactive for any given input:
                </p>

                <div class="math-block">
                    $$\mathcal{L} = \|x - \hat{x}\|^2 + \lambda \sum_{j=1}^d |z_j|$$
                </div>

                <p>
                    For a more principled approach, use the <strong>KL divergence sparsity penalty</strong>.
                    Define the average activation $\hat{\rho}_j = \frac{1}{N}\sum_{i=1}^N z_j^{(i)}$ and
                    penalize deviation from target sparsity $\rho$:
                </p>

                <div class="math-block">
                    $$\Omega_{sparse} = \sum_{j=1}^d D_{KL}(\rho \| \hat{\rho}_j) = \sum_{j=1}^d \left[\rho \log \frac{\rho}{\hat{\rho}_j} + (1-\rho)\log\frac{1-\rho}{1-\hat{\rho}_j}\right]$$
                </div>

                <div class="note-box">
                    <div class="box-title">Why Sparsity?</div>
                    <p style="margin-bottom: 0;">
                        Sparse codes enable overcomplete representations, produce interpretable features
                        (each unit learns a distinct pattern), and connect to biological neural coding where
                        cortical neurons have sparse firing patterns.
                    </p>
                </div>

                <!-- Section 6 -->
                <h2 id="denoising-autoencoders">Denoising Autoencoders</h2>

                <p>
                    Train the autoencoder to reconstruct <strong>clean</strong> input from <strong>corrupted</strong> input:
                </p>

                <div class="math-derivation">
                    <div class="math-derivation-title">Denoising Training</div>

                    <div class="math-step">
                        <div class="math-step-number">1</div>
                        <div class="math-step-content">
                            <strong>Corrupt:</strong> $\tilde{x} = \text{corrupt}(x)$ (masking, Gaussian noise, or salt-and-pepper)
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">2</div>
                        <div class="math-step-content">
                            <strong>Encode:</strong> $z = f_\theta(\tilde{x})$
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">3</div>
                        <div class="math-step-content">
                            <strong>Decode:</strong> $\hat{x} = g_\phi(z)$
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">4</div>
                        <div class="math-step-content">
                            <strong>Loss:</strong> $\|x - \hat{x}\|^2$ — compare to the <em>original</em> $x$, not corrupted $\tilde{x}$
                        </div>
                    </div>
                </div>

                <p>
                    The optimal denoising function satisfies $g^*(\tilde{x}) = \mathbb{E}[X | \tilde{X} = \tilde{x}]$.
                    For Gaussian noise, this implicitly learns the <strong>score function</strong>:
                </p>

                <div class="math-block">
                    $$\nabla_{\tilde{x}} \log p(\tilde{x}) \approx \frac{g^*(\tilde{x}) - \tilde{x}}{\sigma^2}$$
                </div>

                <div class="warning-box">
                    <div class="box-title">Connection to Diffusion Models</div>
                    <p style="margin-bottom: 0;">
                        Denoising autoencoders implicitly learn the score function (gradient of log-density).
                        This is the same principle behind modern diffusion models like DDPM and score-based generative models.
                    </p>
                </div>

                <!-- Section 7 -->
                <h2 id="contractive-autoencoders">Contractive Autoencoders</h2>

                <p>
                    Penalize the encoder's Jacobian to encourage locally invariant representations:
                </p>

                <div class="math-block">
                    $$\mathcal{L} = \|x - \hat{x}\|^2 + \lambda \left\|\frac{\partial f_\theta(x)}{\partial x}\right\|_F^2$$
                </div>

                <p>
                    The Frobenius norm of the Jacobian measures how sensitive the encoding is to input perturbations.
                    Minimizing it makes $z$ locally invariant, pulling the representation toward a lower-dimensional manifold.
                </p>

                <h3>Comparison of Regularized Autoencoders</h3>

                <table style="width: 100%; border-collapse: collapse; margin: 1rem 0;">
                    <thead>
                        <tr style="border-bottom: 2px solid var(--color-border);">
                            <th style="padding: 0.75rem; text-align: left;">Variant</th>
                            <th style="padding: 0.75rem; text-align: left;">Regularizer</th>
                            <th style="padding: 0.75rem; text-align: left;">Effect</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.75rem;">Sparse</td>
                            <td style="padding: 0.75rem;">$\lambda \sum|z_j|$</td>
                            <td style="padding: 0.75rem;">Few active units per input</td>
                        </tr>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.75rem;">Denoising</td>
                            <td style="padding: 0.75rem;">Corrupt input</td>
                            <td style="padding: 0.75rem;">Learn to denoise; score matching</td>
                        </tr>
                        <tr>
                            <td style="padding: 0.75rem;">Contractive</td>
                            <td style="padding: 0.75rem;">$\lambda \|J\|_F^2$</td>
                            <td style="padding: 0.75rem;">Locally invariant encodings</td>
                        </tr>
                    </tbody>
                </table>

                <!-- Section 8 -->
                <h2 id="applications">Applications</h2>

                <h3>Dimensionality Reduction</h3>
                <p>Use encoder output $z$ for visualization (2D/3D), clustering, or classification.</p>

                <h3>Anomaly Detection</h3>
                <p>
                    Train on "normal" data. Anomalies produce high reconstruction error:
                </p>
                <div class="math-block">
                    $$\text{anomaly score}(x) = \|x - g_\phi(f_\theta(x))\|^2$$
                </div>

                <h3>Pretraining</h3>
                <p>
                    Layer-wise autoencoder pretraining was historically crucial for deep networks.
                    The principle of learning useful representations remains central to modern self-supervised learning.
                </p>

                <!-- Section 9 -->
                <h2 id="from-ae-to-vae">From Autoencoders to VAEs</h2>

                <p>
                    Standard autoencoders learn <strong>deterministic</strong> encodings: $z = f_\theta(x)$.
                    This means the latent space may have "holes" and we can't meaningfully sample new data.
                </p>

                <div class="definition-box">
                    <div class="box-title">The VAE Solution (Next Tutorial)</div>
                    <p style="margin-bottom: 0;">
                        VAEs make encoding <strong>probabilistic</strong>: $q_\phi(z|x) = \mathcal{N}(\mu_\phi(x), \sigma_\phi^2(x))$<br>
                        Adding KL regularization gives a smooth, continuous latent space that enables generation.
                    </p>
                </div>

                <table style="width: 100%; border-collapse: collapse; margin: 1rem 0;">
                    <thead>
                        <tr style="border-bottom: 2px solid var(--color-border);">
                            <th style="padding: 0.75rem; text-align: left;">Autoencoder</th>
                            <th style="padding: 0.75rem; text-align: left;">VAE</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.75rem;">$z = f(x)$ (point)</td>
                            <td style="padding: 0.75rem;">$z \sim q(z|x)$ (distribution)</td>
                        </tr>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.75rem;">Bottleneck regularizes</td>
                            <td style="padding: 0.75rem;">KL divergence regularizes</td>
                        </tr>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.75rem;">Can't generate</td>
                            <td style="padding: 0.75rem;">Can generate</td>
                        </tr>
                        <tr>
                            <td style="padding: 0.75rem;">Reconstruction only</td>
                            <td style="padding: 0.75rem;">Reconstruction + KL</td>
                        </tr>
                    </tbody>
                </table>

                <!-- Navigation -->
                <div class="tutorial-nav">
                    <a href="../11-rnn/index.html" class="tutorial-nav-link prev">
                        <span class="nav-label">Previous</span>
                        <span class="nav-title">← RNNs</span>
                    </a>
                    <a href="../13-variational-inference/index.html" class="tutorial-nav-link next">
                        <span class="nav-label">Next</span>
                        <span class="nav-title">Variational Inference →</span>
                    </a>
                </div>

            </article>

            <!-- ==================== CODE TAB ==================== -->
            <article class="article-content" id="code" style="display: none;">
                <h2>Python Code Examples</h2>
                <p>Three complete, runnable examples demonstrating autoencoder variants on MNIST using PyTorch: a basic autoencoder, a sparse autoencoder with KL sparsity penalty, and a denoising autoencoder.</p>

                <!-- Code Example 1 -->
                <h3>1. Basic Autoencoder on MNIST</h3>
                <p>A fully-connected autoencoder that compresses 784-dimensional MNIST images to a 32-dimensional latent space and reconstructs them. Includes the encoder, decoder, training loop, and visualization of reconstructions.</p>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np

# ---- Model ----
class Autoencoder(nn.Module):
    def __init__(self, input_dim=784, latent_dim=32):
        super().__init__()
        # Encoder: 784 -> 256 -> 128 -> 32
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, latent_dim),
            nn.ReLU()
        )
        # Decoder: 32 -> 128 -> 256 -> 784
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Linear(256, input_dim),
            nn.Sigmoid()  # output in [0, 1] for pixel values
        )

    def forward(self, x):
        z = self.encoder(x)
        x_hat = self.decoder(z)
        return x_hat, z

# ---- Data ----
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Lambda(lambda x: x.view(-1))  # flatten 28x28 -> 784
])

train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)

# ---- Training ----
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = Autoencoder(input_dim=784, latent_dim=32).to(device)
optimizer = optim.Adam(model.parameters(), lr=1e-3)
criterion = nn.MSELoss()

num_epochs = 20
for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    for batch_x, _ in train_loader:
        batch_x = batch_x.to(device)

        x_hat, z = model(batch_x)
        loss = criterion(x_hat, batch_x)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item() * batch_x.size(0)

    avg_loss = total_loss / len(train_dataset)
    if (epoch + 1) % 5 == 0:
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.6f}")

# ---- Evaluate and Visualize ----
model.eval()
with torch.no_grad():
    test_x, _ = next(iter(test_loader))
    test_x = test_x.to(device)
    recon_x, latent_z = model(test_x)

n = 10  # number of images to display
fig, axes = plt.subplots(2, n, figsize=(15, 3))
for i in range(n):
    # Original
    axes[0, i].imshow(test_x[i].cpu().view(28, 28), cmap='gray')
    axes[0, i].axis('off')
    if i == 0:
        axes[0, i].set_title('Original', fontsize=10)

    # Reconstruction
    axes[1, i].imshow(recon_x[i].cpu().view(28, 28), cmap='gray')
    axes[1, i].axis('off')
    if i == 0:
        axes[1, i].set_title('Reconstructed', fontsize=10)

plt.suptitle(f'Basic Autoencoder (latent_dim=32, MSE={avg_loss:.4f})')
plt.tight_layout()
plt.savefig('basic_autoencoder_results.png', dpi=150)
plt.show()

print(f"\nCompression ratio: {784}/{32} = {784/32:.1f}x")
print(f"Information retained: {32}/{784} = {32/784*100:.1f}%")</code></pre>

                <!-- Code Example 2 -->
                <h3>2. Sparse Autoencoder with KL Sparsity Penalty</h3>
                <p>This extends the basic autoencoder with a KL divergence sparsity penalty that encourages most hidden units to be inactive for any given input. The target sparsity $\rho$ controls how sparse the latent activations should be.</p>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

# ---- KL Sparsity Penalty ----
def kl_divergence_sparsity(rho, rho_hat):
    """
    KL divergence between Bernoulli(rho) and Bernoulli(rho_hat).
    rho: target sparsity (scalar, e.g. 0.05)
    rho_hat: average activation per unit (tensor of shape [latent_dim])
    Returns: sum of KL divergences across all latent units
    """
    # Clamp to avoid log(0)
    rho_hat = torch.clamp(rho_hat, 1e-6, 1 - 1e-6)
    kl = rho * torch.log(rho / rho_hat) + (1 - rho) * torch.log((1 - rho) / (1 - rho_hat))
    return kl.sum()

# ---- Model ----
class SparseAutoencoder(nn.Module):
    def __init__(self, input_dim=784, latent_dim=128):
        super().__init__()
        # Overcomplete: latent_dim > input bottleneck (but we add sparsity)
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Linear(256, latent_dim),
            nn.Sigmoid()  # activations in [0,1] for KL sparsity
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 256),
            nn.ReLU(),
            nn.Linear(256, input_dim),
            nn.Sigmoid()
        )

    def forward(self, x):
        z = self.encoder(x)
        x_hat = self.decoder(z)
        return x_hat, z

# ---- Data ----
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Lambda(lambda x: x.view(-1))
])
train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)

# ---- Training ----
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = SparseAutoencoder(input_dim=784, latent_dim=128).to(device)
optimizer = optim.Adam(model.parameters(), lr=1e-3)

rho = 0.05          # target sparsity: 5% average activation
beta = 3.0          # sparsity penalty weight
num_epochs = 20

for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    total_recon = 0
    total_sparse = 0

    for batch_x, _ in train_loader:
        batch_x = batch_x.to(device)

        x_hat, z = model(batch_x)

        # Reconstruction loss
        recon_loss = nn.functional.mse_loss(x_hat, batch_x)

        # Sparsity penalty: average activation across the batch
        rho_hat = z.mean(dim=0)  # shape: [latent_dim]
        sparse_loss = kl_divergence_sparsity(rho, rho_hat)

        # Total loss
        loss = recon_loss + beta * sparse_loss

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item() * batch_x.size(0)
        total_recon += recon_loss.item() * batch_x.size(0)
        total_sparse += sparse_loss.item() * batch_x.size(0)

    n = len(train_dataset)
    if (epoch + 1) % 5 == 0:
        print(f"Epoch [{epoch+1}/{num_epochs}] "
              f"Total: {total_loss/n:.4f}  "
              f"Recon: {total_recon/n:.6f}  "
              f"Sparsity: {total_sparse/n:.4f}")

# ---- Analyze Sparsity ----
model.eval()
with torch.no_grad():
    sample_x, _ = next(iter(train_loader))
    sample_x = sample_x.to(device)
    _, z = model(sample_x)

    avg_activation = z.mean(dim=0)  # per-unit average
    print(f"\nTarget sparsity (rho): {rho}")
    print(f"Achieved avg activation: {avg_activation.mean().item():.4f}")
    print(f"Units with avg activation < 0.1: {(avg_activation < 0.1).sum().item()}/{z.shape[1]}")
    print(f"Units with avg activation < 0.01: {(avg_activation < 0.01).sum().item()}/{z.shape[1]}")

    # Show activation histogram
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))

    axes[0].hist(avg_activation.cpu().numpy(), bins=50, edgecolor='black')
    axes[0].axvline(x=rho, color='red', linestyle='--', label=f'target rho={rho}')
    axes[0].set_xlabel('Average Activation')
    axes[0].set_ylabel('Number of Units')
    axes[0].set_title('Per-Unit Average Activation Distribution')
    axes[0].legend()

    # Show a single sample's activations (should be sparse)
    single_z = z[0].cpu().numpy()
    axes[1].bar(range(len(single_z)), single_z, width=1.0)
    axes[1].set_xlabel('Latent Unit Index')
    axes[1].set_ylabel('Activation')
    axes[1].set_title(f'Single Sample Activations ({(single_z > 0.1).sum()} active units)')

    plt.tight_layout()
    plt.savefig('sparse_autoencoder_analysis.png', dpi=150)
    plt.show()</code></pre>

                <!-- Code Example 3 -->
                <h3>3. Denoising Autoencoder</h3>
                <p>A denoising autoencoder trained to reconstruct clean MNIST images from noisy inputs. Gaussian noise is added during training, and the model learns to remove it. This implicitly learns the data manifold structure.</p>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np

# ---- Model ----
class DenoisingAutoencoder(nn.Module):
    def __init__(self, input_dim=784, latent_dim=64):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, latent_dim),
            nn.ReLU()
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Linear(256, input_dim),
            nn.Sigmoid()
        )

    def forward(self, x):
        z = self.encoder(x)
        x_hat = self.decoder(z)
        return x_hat, z

def add_gaussian_noise(x, noise_factor=0.5):
    """Add Gaussian noise and clamp to [0, 1]."""
    noisy = x + noise_factor * torch.randn_like(x)
    return torch.clamp(noisy, 0.0, 1.0)

# ---- Data ----
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Lambda(lambda x: x.view(-1))
])
train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)

# ---- Training ----
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = DenoisingAutoencoder(input_dim=784, latent_dim=64).to(device)
optimizer = optim.Adam(model.parameters(), lr=1e-3)
criterion = nn.MSELoss()

noise_factor = 0.5
num_epochs = 20

for epoch in range(num_epochs):
    model.train()
    total_loss = 0

    for batch_x, _ in train_loader:
        batch_x = batch_x.to(device)

        # Step 1: Corrupt the input
        noisy_x = add_gaussian_noise(batch_x, noise_factor)

        # Step 2-3: Encode noisy input, decode
        x_hat, z = model(noisy_x)

        # Step 4: Loss compares to CLEAN input (not noisy)
        loss = criterion(x_hat, batch_x)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item() * batch_x.size(0)

    avg_loss = total_loss / len(train_dataset)
    if (epoch + 1) % 5 == 0:
        print(f"Epoch [{epoch+1}/{num_epochs}], Denoising Loss: {avg_loss:.6f}")

# ---- Evaluate: Denoise Test Images ----
model.eval()
with torch.no_grad():
    test_x, _ = next(iter(test_loader))
    test_x = test_x.to(device)
    noisy_test = add_gaussian_noise(test_x, noise_factor)
    denoised, _ = model(noisy_test)

n = 10
fig, axes = plt.subplots(3, n, figsize=(15, 4.5))
for i in range(n):
    # Original
    axes[0, i].imshow(test_x[i].cpu().view(28, 28), cmap='gray')
    axes[0, i].axis('off')
    if i == 0:
        axes[0, i].set_title('Original', fontsize=10)

    # Noisy
    axes[1, i].imshow(noisy_test[i].cpu().view(28, 28), cmap='gray')
    axes[1, i].axis('off')
    if i == 0:
        axes[1, i].set_title('Noisy', fontsize=10)

    # Denoised
    axes[2, i].imshow(denoised[i].cpu().view(28, 28), cmap='gray')
    axes[2, i].axis('off')
    if i == 0:
        axes[2, i].set_title('Denoised', fontsize=10)

plt.suptitle(f'Denoising Autoencoder (noise_factor={noise_factor}, loss={avg_loss:.4f})')
plt.tight_layout()
plt.savefig('denoising_autoencoder_results.png', dpi=150)
plt.show()

# ---- Compare: noise levels ----
print("\nDenoising performance at different noise levels:")
for nf in [0.1, 0.3, 0.5, 0.7, 1.0]:
    with torch.no_grad():
        noisy = add_gaussian_noise(test_x, nf)
        recon, _ = model(noisy)
        mse = nn.functional.mse_loss(recon, test_x).item()
        print(f"  noise_factor={nf:.1f}: MSE={mse:.6f}")</code></pre>

            </article>

            <!-- ==================== EXERCISES TAB ==================== -->
            <article class="article-content" id="exercises" style="display: none;">
                <h2>Exercises</h2>
                <p>Test your understanding of autoencoder theory, from linear autoencoders and PCA equivalence to information-theoretic foundations and advanced regularization techniques. Solutions are provided for self-study.</p>

                <div class="exercise-group">

                    <h3 style="margin-top: 1rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">Easy</h3>

                    <!-- Exercise 1 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">1. Linear Autoencoder = PCA</span>
                        </div>
                        <div class="exercise-body">
                            <p>Consider a linear autoencoder with encoder $z = Wx$ and decoder $\hat{x} = W^T z$ (no activation functions, tied weights), trained with MSE loss $\mathcal{L} = \frac{1}{N}\sum_{i=1}^N \|x^{(i)} - W^T W x^{(i)}\|^2$. Show that the optimal $W \in \mathbb{R}^{d \times n}$ has rows equal to the top-$d$ eigenvectors of the data covariance matrix $\Sigma = \frac{1}{N}\sum_{i=1}^N x^{(i)} (x^{(i)})^T$.</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p>We want to minimize $\mathcal{L} = \frac{1}{N}\sum_i \|x^{(i)} - W^T W x^{(i)}\|^2 = \mathbb{E}[\|x - W^T Wx\|^2]$.</p>
                                <p>Let $P = W^T W$. This is a projection matrix (onto the column space of $W^T$). The loss becomes $\mathbb{E}[\|x - Px\|^2] = \mathbb{E}[\|(I - P)x\|^2]$.</p>
                                <p>Expanding: $\mathbb{E}[\|(I-P)x\|^2] = \text{tr}((I-P)\Sigma(I-P)^T) = \text{tr}(\Sigma) - 2\text{tr}(P\Sigma) + \text{tr}(P\Sigma P^T)$.</p>
                                <p>Since $P$ is an orthogonal projection ($P^2 = P$, $P = P^T$), we have $P\Sigma P^T = P\Sigma P$, and the loss simplifies to $\text{tr}(\Sigma) - \text{tr}(P\Sigma)$.</p>
                                <p>Minimizing the loss is equivalent to maximizing $\text{tr}(P\Sigma) = \text{tr}(W^T W \Sigma)$. Writing $W$ with rows $w_1, \ldots, w_d$ (orthonormal), this equals $\sum_{k=1}^d w_k^T \Sigma w_k$.</p>
                                <p>By the Rayleigh quotient, each $w_k^T \Sigma w_k$ is maximized when $w_k$ is an eigenvector of $\Sigma$, and the sum is maximized by choosing the top-$d$ eigenvectors. Therefore $\Sigma w_k = \lambda_k w_k$ for the $d$ largest eigenvalues $\lambda_1 \geq \cdots \geq \lambda_d$.</p>
                                <p>This is exactly PCA: the rows of the optimal $W$ are the principal components, and $z = Wx$ gives the PCA projection.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 2 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">2. Reconstruction Error Calculation</span>
                        </div>
                        <div class="exercise-body">
                            <p>Given a 1-layer autoencoder with $W = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \end{pmatrix}$, $b = 0$, $W' = W^T$, $b' = 0$ (no activation function), compute the latent code and reconstruction for $x = (3, 4, 5)^T$. What information is lost?</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>Encoding:</strong> $z = Wx = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \end{pmatrix} \begin{pmatrix} 3 \\ 4 \\ 5 \end{pmatrix} = \begin{pmatrix} 3 \\ 4 \end{pmatrix}$</p>
                                <p><strong>Decoding:</strong> $\hat{x} = W^T z = \begin{pmatrix} 1 & 0 \\ 0 & 1 \\ 0 & 0 \end{pmatrix} \begin{pmatrix} 3 \\ 4 \end{pmatrix} = \begin{pmatrix} 3 \\ 4 \\ 0 \end{pmatrix}$</p>
                                <p><strong>What is lost:</strong> The third component ($x_3 = 5$) is completely lost. The encoder projects onto the first two coordinates, discarding the third dimension entirely.</p>
                                <p><strong>Reconstruction error:</strong> $\|x - \hat{x}\|^2 = (3-3)^2 + (4-4)^2 + (5-0)^2 = 0 + 0 + 25 = 25$</p>
                                <p>This illustrates how the bottleneck ($d=2 < n=3$) forces information loss. A smarter autoencoder might learn a different projection that distributes the error more evenly, but with this fixed $W$, all error concentrates on the discarded dimension.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 3 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">3. Undercomplete vs Overcomplete</span>
                        </div>
                        <div class="exercise-body">
                            <p>An autoencoder has input dimension $n = 100$ and latent dimension $d = 150$. (a) Is this undercomplete or overcomplete? (b) What pathological solution can it learn? (c) Name two regularization strategies to prevent this.</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> This is <strong>overcomplete</strong> because $d = 150 > n = 100$. The latent space is higher-dimensional than the input.</p>
                                <p><strong>(b)</strong> The network can learn the <strong>identity mapping</strong>: the encoder simply copies the input (plus padding with zeros), and the decoder copies it back. This achieves zero reconstruction loss without learning any useful features. Formally, the encoder can set $z = (x, 0_{50})$ and the decoder can read out the first 100 components.</p>
                                <p><strong>(c)</strong> Two regularization strategies:</p>
                                <ul>
                                    <li><strong>Sparse autoencoder:</strong> Add a sparsity penalty ($L_1$ or KL divergence) on the latent activations. This forces most latent units to be inactive for any given input, preventing the identity solution even though $d > n$.</li>
                                    <li><strong>Denoising autoencoder:</strong> Corrupt the input before encoding. The identity mapping on corrupted inputs would produce corrupted outputs, so the network must learn the data structure to denoise.</li>
                                </ul>
                                <p>A third option is the <strong>contractive autoencoder</strong>, which penalizes the Frobenius norm of the encoder's Jacobian, discouraging sensitivity to input perturbations.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 4 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">4. Activation Function Effects</span>
                        </div>
                        <div class="exercise-body">
                            <p>Given encoder weights $W = \begin{pmatrix} 0.5 & -0.3 \\ 0.2 & 0.8 \end{pmatrix}$, bias $b = (0.1, -0.1)^T$, and input $x = (1, 2)^T$:
                            (a) Compute $z$ with sigmoid activation $\sigma(a) = 1/(1+e^{-a})$.
                            (b) Compute $z$ with ReLU activation $\text{ReLU}(a) = \max(0, a)$.
                            (c) Which activation allows negative latent values? Why does this matter?</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p>Pre-activation: $a = Wx + b = (0.5 \cdot 1 + (-0.3) \cdot 2 + 0.1,\; 0.2 \cdot 1 + 0.8 \cdot 2 + (-0.1))^T = (0.5 - 0.6 + 0.1,\; 0.2 + 1.6 - 0.1)^T = (0.0, 1.7)^T$.</p>
                                <p><strong>(a) Sigmoid:</strong> $z = (\sigma(0.0), \sigma(1.7))^T = (0.5, 0.846)^T$. Both values lie in $(0,1)$.</p>
                                <p><strong>(b) ReLU:</strong> $z = (\max(0, 0.0), \max(0, 1.7))^T = (0.0, 1.7)^T$. One unit is dead (exactly zero), the other is unbounded.</p>
                                <p><strong>(c)</strong> Neither allows negative values. But if pre-activations were negative, ReLU would output exactly 0 (dead neuron), while sigmoid would output values in $(0, 0.5)$. This matters because dead ReLU neurons lose all gradient flow, while sigmoid always passes some gradient. For latent spaces, ReLU creates exact sparsity (zeros) while sigmoid creates soft activations.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 5 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">5. Compression Ratio Analysis</span>
                        </div>
                        <div class="exercise-body">
                            <p>An autoencoder maps $28 \times 28$ grayscale images (8 bits/pixel) to a 16-dimensional latent vector stored as 32-bit floats. (a) Compute the raw compression ratio. (b) A JPEG at quality 10 compresses the same image to ~800 bytes. Which is better? (c) At what bottleneck dimension does the autoencoder match JPEG's ratio?</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> Input: $28 \times 28 \times 8 = 6272$ bits $= 784$ bytes. Latent: $16 \times 32 = 512$ bits $= 64$ bytes. Ratio $= 784/64 = 12.25:1$.</p>
                                <p><strong>(b)</strong> JPEG: $784/800 \approx 0.98:1$ (barely compresses; JPEG overhead dominates for tiny images). The autoencoder achieves much better compression. BUT this comparison is misleading: the AE can only reconstruct images from its training distribution (e.g., MNIST digits), while JPEG works on any image.</p>
                                <p><strong>(c)</strong> To match JPEG's ~800 bytes: $800 = d \times 4$ bytes, so $d = 200$. But this exceeds the input dimension (784 pixels), showing that for tiny images JPEG's fixed overhead makes it inefficient.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 6 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">6. Binary Cross-Entropy as Log-Likelihood</span>
                        </div>
                        <div class="exercise-body">
                            <p>For normalized pixel values $x_i \in [0,1]$ modeled as independent Bernoulli random variables with parameter $\hat{x}_i$ (decoder output), derive the negative log-likelihood and show it equals the BCE loss: $-\sum_i [x_i \log \hat{x}_i + (1-x_i)\log(1-\hat{x}_i)]$.</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p>For a Bernoulli distribution: $p(x_i | \hat{x}_i) = \hat{x}_i^{x_i} (1-\hat{x}_i)^{1-x_i}$.</p>
                                <p>For independent pixels: $p(x|\hat{x}) = \prod_i p(x_i|\hat{x}_i) = \prod_i \hat{x}_i^{x_i}(1-\hat{x}_i)^{1-x_i}$.</p>
                                <p>Taking the negative log:</p>
                                $$-\log p(x|\hat{x}) = -\sum_i [x_i \log \hat{x}_i + (1-x_i)\log(1-\hat{x}_i)]$$
                                <p>This is exactly BCE! So minimizing BCE = maximizing the likelihood of the decoder model. This is why BCE is preferred over MSE when pixels are bounded in $[0,1]$: it correctly models the data-generating distribution. MSE implicitly assumes Gaussian noise, which can assign non-zero probability to values outside $[0,1]$.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 7 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">7. Latent Space Interpolation</span>
                        </div>
                        <div class="exercise-body">
                            <p>Given two images $x_1$ (digit "3") and $x_2$ (digit "7") with latent codes $z_1$ and $z_2$:
                            (a) Write the formula for linear interpolation $z(\alpha) = (1-\alpha)z_1 + \alpha z_2$ for $\alpha \in [0,1]$.
                            (b) What should $\text{decode}(z(0.5))$ look like?
                            (c) Name one problem with linear interpolation in high-dimensional latent spaces and one alternative.</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> $z(\alpha) = (1-\alpha)z_1 + \alpha z_2$. At $\alpha = 0$: $z(0) = z_1$. At $\alpha = 1$: $z(1) = z_2$.</p>
                                <p><strong>(b)</strong> $z(0.5) = \frac{z_1 + z_2}{2}$. The decoded image should look like a smooth blend between "3" and "7" — possibly a character with features of both digits. In a well-trained AE with a smooth latent space, this might look like an "8" or an ambiguous character.</p>
                                <p><strong>(c)</strong> Problem: In high dimensions, the midpoint $(z_1 + z_2)/2$ may lie in a low-density region of the latent space (the "soap bubble" effect: most mass of a high-dimensional Gaussian lies on a shell, not at the center). The decoder hasn't seen latent codes like this during training, producing blurry or nonsensical output.</p>
                                <p>Alternative: spherical linear interpolation (slerp):</p>
                                $$z(\alpha) = \frac{\sin((1-\alpha)\theta)}{\sin\theta} z_1 + \frac{\sin(\alpha\theta)}{\sin\theta} z_2$$
                                <p>where $\cos\theta = \frac{z_1 \cdot z_2}{\|z_1\|\|z_2\|}$. This keeps $\|z(\alpha)\|$ roughly constant, staying on the shell where the data lives.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 8 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">8. Depth vs Width Trade-off</span>
                        </div>
                        <div class="exercise-body">
                            <p>Compare two autoencoders, both with bottleneck $d=32$:
                            Shallow: $784 \to 32 \to 784$ (one encoder layer, one decoder layer).
                            Deep: $784 \to 256 \to 64 \to 32 \to 64 \to 256 \to 784$ (three encoder layers, three decoder layers).
                            (a) Count parameters for each (ignore biases). (b) Which learns better representations for MNIST? Why?</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a) Shallow:</strong> Encoder: $784 \times 32 = 25{,}088$. Decoder: $32 \times 784 = 25{,}088$. Total: $50{,}176$.</p>
                                <p><strong>Deep:</strong> Encoder: $784 \times 256 + 256 \times 64 + 64 \times 32 = 200{,}704 + 16{,}384 + 2{,}048 = 219{,}136$. Decoder (symmetric): also $219{,}136$. Total: $438{,}272$.</p>
                                <p><strong>(b)</strong> The deep AE learns better representations despite being ~8.7x more parameters, because: (1) the shallow AE performs a single linear+nonlinear transformation, similar to PCA with a nonlinearity — it can only capture simple patterns; (2) the deep AE performs hierarchical feature extraction: early layers detect edges, middle layers detect parts (strokes), and the bottleneck captures digit identity; (3) the gradual dimension reduction ($784 \to 256 \to 64 \to 32$) is gentler than the abrupt $784 \to 32$, making optimization easier. This is why practical autoencoders are always deep.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 9 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">9. Training Diagnostics</span>
                        </div>
                        <div class="exercise-body">
                            <p>Diagnose each scenario:
                            (a) Training loss = 0.001, test loss = 0.15, latent dim = 128, dataset = 1000 MNIST images.
                            (b) Training loss = test loss = 0.08, latent dim = 2, dataset = 60000 MNIST images.
                            (c) Training loss drops quickly to 0, latent dim = 800, input dim = 784.
                            For each: identify the problem and propose a fix.</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a) Overfitting.</strong> The model memorized the training data but doesn't generalize. With only 1000 samples and $d = 128$, the AE has far too much capacity. Fixes: reduce latent dimension (try $d = 16$ or $d = 32$), add regularization (dropout, weight decay, or use a sparse/contractive AE), or collect more data.</p>
                                <p><strong>(b) Underfitting.</strong> Both losses are similar (no overfitting), but 0.08 MSE on MNIST means blurry reconstructions. The bottleneck $d = 2$ is too small to capture the complexity of 10 digit classes with their variations. Fix: increase latent dimension to $d = 16$ or $d = 32$.</p>
                                <p><strong>(c) The identity function problem.</strong> With $d = 800 > n = 784$, the AE can learn the identity mapping without learning any useful features. Training loss = 0 is suspicious, not good. Fix: either reduce $d < 784$ (undercomplete) or add regularization: sparsity penalty, noise injection, or contractive penalty.</p>
                            </div>
                        </div>
                    </div>

                    <h3 style="margin-top: 2rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">Medium</h3>

                    <!-- Exercise 4 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">10. KL Sparsity Gradient</span>
                        </div>
                        <div class="exercise-body">
                            <p>Derive $\frac{\partial}{\partial \hat{\rho}_j} D_{KL}(\rho \| \hat{\rho}_j)$ where $D_{KL}(\rho \| \hat{\rho}_j) = \rho \log\frac{\rho}{\hat{\rho}_j} + (1-\rho)\log\frac{1-\rho}{1-\hat{\rho}_j}$, and show it equals $-\frac{\rho}{\hat{\rho}_j} + \frac{1-\rho}{1-\hat{\rho}_j}$. Discuss: what happens when $\hat{\rho}_j > \rho$? When $\hat{\rho}_j < \rho$?</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p>Starting from $D_{KL}(\rho \| \hat{\rho}_j) = \rho \log\rho - \rho\log\hat{\rho}_j + (1-\rho)\log(1-\rho) - (1-\rho)\log(1-\hat{\rho}_j)$:</p>
                                <p>Taking the derivative with respect to $\hat{\rho}_j$:</p>
                                $$\frac{\partial}{\partial \hat{\rho}_j} D_{KL} = -\frac{\rho}{\hat{\rho}_j} - (1-\rho) \cdot \frac{-1}{1-\hat{\rho}_j} = -\frac{\rho}{\hat{\rho}_j} + \frac{1-\rho}{1-\hat{\rho}_j}$$
                                <p><strong>When $\hat{\rho}_j > \rho$</strong> (unit is too active): The gradient is positive. Since we are minimizing the KL divergence, gradient descent will decrease $\hat{\rho}_j$, pushing it back toward $\rho$. The positive gradient means "the average activation is too high; reduce it."</p>
                                <p><strong>When $\hat{\rho}_j < \rho$</strong> (unit is too inactive): The gradient is negative. Gradient descent will increase $\hat{\rho}_j$, pushing it toward $\rho$. The negative gradient means "the average activation is too low; increase it."</p>
                                <p><strong>At equilibrium</strong> $\hat{\rho}_j = \rho$: The gradient is $-\frac{\rho}{\rho} + \frac{1-\rho}{1-\rho} = -1 + 1 = 0$, confirming that $\hat{\rho}_j = \rho$ is the stationary point. This is also the global minimum of $D_{KL}(\rho \| \hat{\rho}_j)$ since KL divergence is non-negative and equals zero iff the distributions match.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 5 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">11. Tied Weights Constraint</span>
                        </div>
                        <div class="exercise-body">
                            <p>If $W' = W^T$ (tied weights) and $W \in \mathbb{R}^{d \times n}$, how many free parameters does the autoencoder have compared to untied weights? For $n = 784, d = 32$, compute both counts. (Include bias terms: encoder bias $b \in \mathbb{R}^d$, decoder bias $b' \in \mathbb{R}^n$.)</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>Tied weights:</strong></p>
                                <ul>
                                    <li>Encoder weight $W \in \mathbb{R}^{d \times n}$: $d \times n$ parameters</li>
                                    <li>Decoder weight $W' = W^T$: 0 additional parameters (shared)</li>
                                    <li>Encoder bias $b \in \mathbb{R}^d$: $d$ parameters</li>
                                    <li>Decoder bias $b' \in \mathbb{R}^n$: $n$ parameters</li>
                                    <li><strong>Total:</strong> $dn + d + n$</li>
                                </ul>
                                <p><strong>Untied weights:</strong></p>
                                <ul>
                                    <li>Encoder weight $W \in \mathbb{R}^{d \times n}$: $d \times n$ parameters</li>
                                    <li>Decoder weight $W' \in \mathbb{R}^{n \times d}$: $n \times d$ parameters</li>
                                    <li>Encoder bias $b \in \mathbb{R}^d$: $d$ parameters</li>
                                    <li>Decoder bias $b' \in \mathbb{R}^n$: $n$ parameters</li>
                                    <li><strong>Total:</strong> $2dn + d + n$</li>
                                </ul>
                                <p><strong>For $n = 784, d = 32$:</strong></p>
                                <ul>
                                    <li>Tied: $784 \times 32 + 32 + 784 = 25{,}088 + 32 + 784 = 25{,}904$</li>
                                    <li>Untied: $2 \times 784 \times 32 + 32 + 784 = 50{,}176 + 32 + 784 = 50{,}992$</li>
                                </ul>
                                <p>Tied weights use roughly half the parameters ($25{,}904$ vs $50{,}992$), which acts as a form of regularization and can help prevent overfitting, especially with limited training data.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 6 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">12. Denoising Optimality</span>
                        </div>
                        <div class="exercise-body">
                            <p>Show that for the MSE loss with noise $\tilde{x} = x + \epsilon$ where $\epsilon \sim \mathcal{N}(0, \sigma^2 I)$, the optimal denoising function is $g^*(\tilde{x}) = \mathbb{E}[X | \tilde{X} = \tilde{x}]$.</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p>We want to find $g^*$ that minimizes the expected MSE:</p>
                                $$\mathcal{L}(g) = \mathbb{E}_{X, \tilde{X}}[\|X - g(\tilde{X})\|^2]$$
                                <p>We can decompose this using the law of iterated expectations, conditioning on $\tilde{X}$:</p>
                                $$\mathcal{L}(g) = \mathbb{E}_{\tilde{X}}\left[\mathbb{E}_{X|\tilde{X}}[\|X - g(\tilde{X})\|^2 \mid \tilde{X}]\right]$$
                                <p>For any fixed $\tilde{x}$, the inner expectation $\mathbb{E}[\|X - c\|^2 | \tilde{X} = \tilde{x}]$ over a constant $c = g(\tilde{x})$ is minimized when $c = \mathbb{E}[X | \tilde{X} = \tilde{x}]$. This is a standard result: the conditional mean minimizes the conditional MSE.</p>
                                <p><strong>Proof of the inner claim:</strong> Let $\mu = \mathbb{E}[X | \tilde{X} = \tilde{x}]$. Then:</p>
                                $$\mathbb{E}[\|X - c\|^2 | \tilde{X}] = \mathbb{E}[\|X - \mu + \mu - c\|^2 | \tilde{X}]$$
                                $$= \mathbb{E}[\|X - \mu\|^2 | \tilde{X}] + 2\mathbb{E}[(X - \mu)^T(\mu - c) | \tilde{X}] + \|\mu - c\|^2$$
                                <p>The cross term vanishes because $\mathbb{E}[X - \mu | \tilde{X}] = 0$ by definition of conditional expectation. The first term is the irreducible conditional variance (does not depend on $c$). The third term $\|\mu - c\|^2 \geq 0$ is minimized when $c = \mu$.</p>
                                <p>Since this holds for each $\tilde{x}$, the optimal $g^*(\tilde{x}) = \mathbb{E}[X | \tilde{X} = \tilde{x}]$ minimizes the overall loss.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 7 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">13. Information Content of Bottleneck</span>
                        </div>
                        <div class="exercise-body">
                            <p>A 2-dimensional bottleneck uses 32-bit floats per dimension. What is the maximum number of bits per sample? Why is the effective rate typically much lower?</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>Maximum bits:</strong> $2 \times 32 = 64$ bits per sample. Each of the 2 dimensions is stored as a 32-bit float, so the raw storage capacity is 64 bits.</p>
                                <p><strong>Why the effective rate is much lower:</strong></p>
                                <p>The autoencoder maps inputs to a low-dimensional manifold within $\mathbb{R}^2$. The effective information content is determined by the <em>entropy</em> of the latent distribution $p(z)$, not the bit-width of the representation.</p>
                                <ul>
                                    <li><strong>Concentrated distribution:</strong> If the encoder maps all MNIST digits to a small region of $\mathbb{R}^2$ (say, 10 clusters for 10 digit classes), the effective entropy is $H(Z) \approx \log_2 10 \approx 3.3$ bits — far less than 64.</li>
                                    <li><strong>Smooth manifold:</strong> The encoder learns a continuous mapping, so $z$ values are not spread uniformly across the full 32-bit float range. Most of the $2^{64}$ possible bit patterns are never produced.</li>
                                    <li><strong>Correlation:</strong> The two latent dimensions may be correlated, further reducing the effective information.</li>
                                    <li><strong>Precision vs information:</strong> Using 32-bit floats provides numerical precision for gradient-based optimization, but the meaningful information content is determined by the signal-to-noise ratio of the latent activations, which is much less than 32 bits per dimension.</li>
                                </ul>
                                <p>A rough estimate: if each latent dimension effectively uses about 4-8 bits of meaningful precision (based on the spread of activations relative to reconstruction sensitivity), the effective rate is roughly 8-16 bits per sample, not 64.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 14 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">14. L1 vs KL Sparsity</span>
                        </div>
                        <div class="exercise-body">
                            <p>Compare two sparsity penalties for an overcomplete AE ($d=256$, $n=784$): (a) $\Omega_{L_1} = \lambda \sum_j |z_j|$ and (b) $\Omega_{KL} = \beta \sum_j D_{KL}(\rho \| \hat{\rho}_j)$ with target $\rho = 0.05$. Show that $L_1$ pushes activations to exactly zero while KL pushes them toward $\rho$. Which produces sparser representations? Which is more biologically plausible?</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>For $L_1$:</strong> The subgradient $\frac{\partial |z_j|}{\partial z_j} = \text{sign}(z_j)$ for $z_j \neq 0$. At $z_j = 0$, the subgradient includes 0, so zero is a stable point. The penalty creates a "dead zone" around zero — any activation with gradient magnitude less than $\lambda$ gets pushed to exactly zero. This is the proximal operator effect: the optimal solution has $z_j = 0$ for many units.</p>
                                <p><strong>For KL:</strong> $\frac{\partial D_{KL}}{\partial \hat{\rho}_j} = 0$ when $\hat{\rho}_j = \rho = 0.05$. The equilibrium is not zero but the target sparsity $\rho$. Each unit is encouraged to be active $5\%$ of the time across the dataset.</p>
                                <p>$L_1$ produces sparser representations (exact zeros vs. small-but-nonzero values). However, KL sparsity is more biologically plausible: real neurons have low but nonzero baseline firing rates, and the "5% active" interpretation matches sparse coding theories in neuroscience (Olshausen &amp; Field, 1996).</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 15 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">15. Convolutional Autoencoder Dimensions</span>
                        </div>
                        <div class="exercise-body">
                            <p>An encoder has: Conv2d(1, 16, 3, stride=2, padding=1) $\to$ ReLU $\to$ Conv2d(16, 32, 3, stride=2, padding=1) $\to$ ReLU $\to$ Flatten $\to$ Linear(?, 64). For a $28 \times 28$ input: (a) Compute the spatial dimensions after each conv layer using $\lfloor(n + 2p - k)/s\rfloor + 1$. (b) What is the "?" in the Linear layer? (c) Total encoder parameters (with biases).</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> After Conv1: $\lfloor(28 + 2 - 3)/2\rfloor + 1 = \lfloor 27/2 \rfloor + 1 = 13 + 1 = 14$. Output: 16 channels $\times$ 14 $\times$ 14.</p>
                                <p>After Conv2: $\lfloor(14 + 2 - 3)/2\rfloor + 1 = \lfloor 13/2 \rfloor + 1 = 6 + 1 = 7$. Output: 32 channels $\times$ 7 $\times$ 7.</p>
                                <p><strong>(b)</strong> After flatten: $32 \times 7 \times 7 = 1568$. So the Linear layer is Linear(1568, 64).</p>
                                <p><strong>(c)</strong> Conv1: $1 \times 16 \times 3 \times 3 + 16 = 144 + 16 = 160$. Conv2: $16 \times 32 \times 3 \times 3 + 32 = 4608 + 32 = 4640$. Linear: $1568 \times 64 + 64 = 100{,}352 + 64 = 100{,}416$. Total: $160 + 4640 + 100{,}416 = 105{,}216$.</p>
                                <p>Note: the linear layer dominates (~95% of parameters). This motivates fully convolutional autoencoders that replace it with global average pooling.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 16 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">16. Symmetric Decoder Design</span>
                        </div>
                        <div class="exercise-body">
                            <p>For the encoder in Exercise 15, design a symmetric decoder. (a) Write the decoder architecture using ConvTranspose2d to go from 64-dim latent back to $28 \times 28$. (b) What is the "checkerboard artifact" problem with transposed convolutions? (c) Propose an alternative.</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> Decoder: Linear(64, 1568) $\to$ Reshape to $32 \times 7 \times 7$ $\to$ ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1) $\to$ ReLU $\to$ ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1) $\to$ Sigmoid.</p>
                                <p>Verification: ConvT1: $(7-1)\times 2 - 2\times 1 + 3 + 1 = 14$. Output: $16 \times 14 \times 14$. ConvT2: $(14-1)\times 2 - 2\times 1 + 3 + 1 = 28$. Output: $1 \times 28 \times 28$. &#10003;</p>
                                <p><strong>(b)</strong> Checkerboard artifacts: ConvTranspose2d with stride $> 1$ produces overlapping patterns in the output. When stride doesn't evenly divide the kernel size, some output pixels receive contributions from more kernel elements than others, creating a grid-like pattern. For kernel=3, stride=2, every other pixel gets 2 contributions while adjacent ones get 1.</p>
                                <p><strong>(c)</strong> Alternative: Upsample + Conv2d. Replace each ConvTranspose2d with: nn.Upsample(scale_factor=2, mode='bilinear') followed by nn.Conv2d with stride=1. This separates the upsampling (bilinear interpolation) from the learned filtering (convolution), eliminating checkerboard artifacts. This is the approach used in many modern architectures (e.g., U-Net decoders).</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 17 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">17. Manifold Dimension Theorem</span>
                        </div>
                        <div class="exercise-body">
                            <p>Suppose data lies exactly on a $k$-dimensional linear subspace $V \subset \mathbb{R}^n$ (so $x = U\alpha$ where $U \in \mathbb{R}^{n \times k}$ has orthonormal columns and $\alpha \in \mathbb{R}^k$). Prove: (a) A linear AE with $d \geq k$ can achieve zero reconstruction error. (b) A linear AE with $d < k$ cannot achieve zero error. (c) What changes for a nonlinear manifold?</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> Since $x = U\alpha \in V$, the data covariance $\Sigma = U \text{diag}(\sigma_1^2, \ldots, \sigma_k^2) U^T$ has rank $k$, with $k$ positive eigenvalues and $n-k$ zero eigenvalues. A linear AE with $d \geq k$ can set $W = U^T$ (the top-$d$ eigenvectors of $\Sigma$, which include all $k$ non-zero directions). Then $P = W^T W = UU^T$ is the projection onto $V$, and since $x \in V$: $\hat{x} = Px = x$. Error = 0.</p>
                                <p><strong>(b)</strong> If $d < k$, the projection $P = W^T W$ has rank $d < k$, so its column space is a strict subspace of $V$. There exist $x \in V$ not in this subspace, giving $Px \neq x$. Formally, the error is $\sum_{i=d+1}^k \sigma_i^2 > 0$ (the sum of discarded eigenvalues).</p>
                                <p><strong>(c)</strong> For nonlinear manifolds, a linear AE with $d = k$ may fail because it can only capture linear subspaces. But a nonlinear AE (with sufficient depth and width) can parameterize a nonlinear encoder $f: \mathbb{R}^n \to \mathbb{R}^k$ that maps the manifold to $\mathbb{R}^k$ bijectively, and a decoder $g: \mathbb{R}^k \to \mathbb{R}^n$ that inverts it. Universal approximation theorems guarantee such $f, g$ exist. This is why nonlinear AEs outperform PCA on real data.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 18 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">18. Dropout as Implicit Ensemble</span>
                        </div>
                        <div class="exercise-body">
                            <p>Applying dropout with rate $p$ to the $d$-dimensional latent layer: $\tilde{z}_i = z_i \cdot m_i / (1-p)$ where $m_i \sim \text{Bernoulli}(1-p)$. (a) How many distinct "sub-autoencoders" does this create? (b) Show the scaling factor $1/(1-p)$ ensures $\mathbb{E}[\tilde{z}_i] = z_i$. (c) How is this different from a denoising autoencoder?</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> Each of the $d$ latent units is either on or off, giving $2^d$ possible binary masks. Each mask defines a different sub-autoencoder that uses a subset of the latent dimensions. For $d = 32$, that's $2^{32} \approx 4.3 \times 10^9$ sub-networks.</p>
                                <p><strong>(b)</strong> $\mathbb{E}[\tilde{z}_i] = \mathbb{E}[z_i \cdot m_i / (1-p)] = z_i \cdot \mathbb{E}[m_i] / (1-p) = z_i \cdot (1-p)/(1-p) = z_i$. The scaling ensures that at test time (no dropout), the expected latent values match training, so no rescaling is needed.</p>
                                <p><strong>(c)</strong> Key differences: (1) Dropout corrupts the latent code $z$ (internal representation), while denoising AEs corrupt the input $x$. (2) Dropout applies multiplicative Bernoulli noise (binary masking), while denoising AEs typically apply additive Gaussian noise. (3) Dropout trains the decoder to be robust to missing features, while denoising trains the encoder to be robust to noisy inputs. (4) Dropout can be seen as Bayesian approximate inference (Gal &amp; Ghahramani, 2016), while denoising connects to score matching.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 19 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">19. Backpropagation Through the Decoder</span>
                        </div>
                        <div class="exercise-body">
                            <p>For a single-layer decoder $\hat{x} = \sigma(W_2 z + b_2)$ with sigmoid activation, MSE loss $\mathcal{L} = \|x - \hat{x}\|^2$: (a) Derive $\frac{\partial \mathcal{L}}{\partial z}$ (the gradient that flows back to the encoder). (b) When does this gradient vanish? What does this mean for training?</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> Let $a = W_2 z + b_2$ (pre-activation), $\hat{x} = \sigma(a)$.</p>
                                <p>$\frac{\partial \mathcal{L}}{\partial \hat{x}_i} = -2(x_i - \hat{x}_i)$</p>
                                <p>$\frac{\partial \hat{x}_i}{\partial a_i} = \hat{x}_i(1 - \hat{x}_i)$</p>
                                <p>$\frac{\partial a_i}{\partial z_j} = (W_2)_{ij}$</p>
                                <p>By chain rule:</p>
                                $$\frac{\partial \mathcal{L}}{\partial z_j} = \sum_i \frac{\partial \mathcal{L}}{\partial \hat{x}_i} \cdot \frac{\partial \hat{x}_i}{\partial a_i} \cdot \frac{\partial a_i}{\partial z_j} = -2 \sum_i (x_i - \hat{x}_i) \hat{x}_i(1-\hat{x}_i) (W_2)_{ij}$$
                                <p>In matrix form: $\frac{\partial \mathcal{L}}{\partial z} = -2 W_2^T [\delta \odot \hat{x} \odot (1-\hat{x})]$ where $\delta = x - \hat{x}$.</p>
                                <p><strong>(b)</strong> The gradient vanishes when: (1) $\hat{x}_i \approx x_i$ (reconstruction is already good — this is the desired equilibrium); (2) $\hat{x}_i \approx 0$ or $\hat{x}_i \approx 1$ (sigmoid saturation): the factor $\hat{x}_i(1-\hat{x}_i) \approx 0$ kills the gradient regardless of reconstruction quality. This is the vanishing gradient problem with sigmoid decoders. Fix: use ReLU intermediate layers or skip connections.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 20 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">20. Wasserstein Autoencoder Motivation</span>
                        </div>
                        <div class="exercise-body">
                            <p>In a VAE, we minimize $D_{KL}(q(z|x) \| p(z))$ for each input $x$, matching each individual posterior to the prior. In a Wasserstein AE (WAE), we instead match the aggregated posterior $q(z) = \mathbb{E}_{p(x)}[q(z|x)]$ to $p(z)$. (a) Why is matching the aggregated posterior weaker than matching individual posteriors? (b) Show with an example: let $p(z) = \mathcal{N}(0, 1)$ and $q(z|x)$ be point masses at $z = f(x)$. When does $q(z)$ match $p(z)$? (c) What is the practical benefit?</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> Matching individual posteriors: $D_{KL}(q(z|x) \| p(z)) = 0$ for all $x$ means $q(z|x) = p(z)$ for every $x$. This forces the encoder to ignore the input entirely! In practice, the VAE trades off between this term and reconstruction, but the KL penalty still pushes toward ignoring the input. Matching the aggregate: $q(z) = p(z)$ only requires that the overall distribution of latent codes matches the prior, not that each individual input maps to the prior. This is a much weaker constraint.</p>
                                <p><strong>(b)</strong> If $q(z|x) = \delta(z - f(x))$ (deterministic encoder), then $q(z) = p_{f(X)}(z)$, the push-forward of $p(x)$ through $f$. This matches $p(z) = \mathcal{N}(0,1)$ when $f$ is a measure-preserving map: e.g., $f = \Phi^{-1} \circ F_X$ where $F_X$ is the data CDF and $\Phi^{-1}$ is the standard normal quantile function. The encoder just needs to rearrange the data distribution into a Gaussian — without adding noise!</p>
                                <p><strong>(c)</strong> Practical benefit: WAE allows deterministic encoders (no sampling noise, no reparameterization trick needed), producing sharper reconstructions than VAE. The latent space is still regularized (covers the prior), but individual encodings are precise, not blurred.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 21 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">21. Memorization vs Generalization</span>
                        </div>
                        <div class="exercise-body">
                            <p>An autoencoder has a single hidden layer with $d$ units and is trained on $N$ samples in $\mathbb{R}^n$ with ReLU activations. (a) Derive a condition on $d$ such that the AE can perfectly memorize all $N$ training samples (zero training loss) without learning any generalizable features. (b) Why is this bad? Give a concrete example.</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> A ReLU network with $d$ hidden units can represent a piecewise-linear function with at most $d$ linear regions. For the encoder $f: \mathbb{R}^n \to \mathbb{R}^d$, each of the $N$ training points $x^{(i)}$ can be mapped to a unique latent code $z^{(i)} = f(x^{(i)})$ as long as the encoder has enough capacity.</p>
                                <p>The decoder $g: \mathbb{R}^d \to \mathbb{R}^n$ needs to map each $z^{(i)}$ back to $x^{(i)}$. This is an interpolation problem: fitting $N$ points in $\mathbb{R}^d$ to $\mathbb{R}^n$. A single-layer ReLU network can memorize $N$ arbitrary input-output pairs if $d \geq N$ (each hidden unit can "specialize" to one training sample using indicator-like activation patterns).</p>
                                <p>So the condition is $d \geq N$: with at least as many hidden units as training samples, perfect memorization is possible.</p>
                                <p><strong>(b)</strong> This is bad because the AE acts as a lookup table: for training point $x^{(7)}$, the encoder stores a unique code, and the decoder retrieves the original. For any input not seen during training, the output is meaningless. Example: train an AE with $d = 1000$ on $N = 500$ MNIST digits. It achieves zero training loss but produces garbage for any new digit, even from the same distribution. The latent space has no structure — nearby latent codes don't correspond to similar images. The model has learned nothing about "digit-ness," only about these specific 500 images.</p>
                            </div>
                        </div>
                    </div>

                    <h3 style="margin-top: 2rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">Hard</h3>

                    <!-- Exercise 8 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">22. Contractive Jacobian Computation</span>
                        </div>
                        <div class="exercise-body">
                            <p>For encoder $f(x) = \sigma(Wx + b)$ with sigmoid activation $\sigma$, derive the Jacobian $J_{ij} = \frac{\partial z_i}{\partial x_j}$ and show that $\|J\|_F^2 = \sum_i z_i^2(1-z_i)^2 \sum_j W_{ij}^2$.</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p>The encoder computes $z_i = \sigma(a_i)$ where $a_i = \sum_j W_{ij} x_j + b_i$ is the pre-activation.</p>
                                <p><strong>Step 1: Jacobian entry.</strong> By the chain rule:</p>
                                $$J_{ij} = \frac{\partial z_i}{\partial x_j} = \frac{\partial \sigma(a_i)}{\partial a_i} \cdot \frac{\partial a_i}{\partial x_j} = \sigma'(a_i) \cdot W_{ij}$$
                                <p><strong>Step 2: Sigmoid derivative.</strong> For the sigmoid function, $\sigma'(a) = \sigma(a)(1 - \sigma(a)) = z_i(1 - z_i)$. Therefore:</p>
                                $$J_{ij} = z_i(1 - z_i) W_{ij}$$
                                <p><strong>Step 3: Frobenius norm.</strong></p>
                                $$\|J\|_F^2 = \sum_{i} \sum_{j} J_{ij}^2 = \sum_{i} \sum_{j} [z_i(1-z_i)]^2 W_{ij}^2$$
                                $$= \sum_{i} z_i^2(1-z_i)^2 \sum_{j} W_{ij}^2$$
                                <p>The last step factors because $z_i(1-z_i)$ does not depend on $j$.</p>
                                <p><strong>Interpretation:</strong> The contractive penalty $\|J\|_F^2$ has two factors for each hidden unit $i$:</p>
                                <ul>
                                    <li>$z_i^2(1-z_i)^2$: the squared sigmoid derivative, which is largest when $z_i \approx 0.5$ (on the linear part of the sigmoid) and smallest when $z_i \approx 0$ or $z_i \approx 1$ (saturated). Units in saturation are already locally invariant.</li>
                                    <li>$\sum_j W_{ij}^2$: the squared norm of the $i$-th row of $W$. Large weights amplify input perturbations.</li>
                                </ul>
                                <p>The penalty thus encourages either small weights (like $L_2$ regularization) or saturated activations (which create flat regions in the encoding function).</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 9 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">23. Rate-Distortion for Gaussian Source</span>
                        </div>
                        <div class="exercise-body">
                            <p>For a Gaussian source $X \sim \mathcal{N}(0, \sigma^2)$ with MSE distortion, verify that $R(D) = \frac{1}{2}\log_2 \frac{\sigma^2}{D}$ by computing $I(X; Z)$ for the "test channel" $Z = X + N$ where $N \sim \mathcal{N}(0, \sigma_N^2)$ is independent noise, with $\hat{X} = \frac{\sigma^2}{\sigma^2 + \sigma_N^2} Z$ being the MMSE estimator. Set the distortion equal to $D$ and solve for $I(X;Z)$.</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>Step 1: Compute the distortion.</strong> The MMSE estimator of $X$ given $Z = X + N$ is:</p>
                                $$\hat{X} = \mathbb{E}[X|Z] = \frac{\sigma^2}{\sigma^2 + \sigma_N^2} Z$$
                                <p>The MSE distortion is:</p>
                                $$D = \mathbb{E}[(X - \hat{X})^2] = \text{Var}(X|Z) = \sigma^2 - \frac{\sigma^4}{\sigma^2 + \sigma_N^2} = \frac{\sigma^2 \sigma_N^2}{\sigma^2 + \sigma_N^2}$$
                                <p><strong>Step 2: Solve for $\sigma_N^2$ in terms of $D$.</strong></p>
                                $$D(\sigma^2 + \sigma_N^2) = \sigma^2 \sigma_N^2 \implies D\sigma^2 + D\sigma_N^2 = \sigma^2 \sigma_N^2 \implies \sigma_N^2 = \frac{D\sigma^2}{\sigma^2 - D}$$
                                <p><strong>Step 3: Compute $I(X; Z)$.</strong> Since $X$ and $N$ are independent Gaussians, $Z \sim \mathcal{N}(0, \sigma^2 + \sigma_N^2)$. Using $I(X;Z) = h(Z) - h(Z|X) = h(Z) - h(N)$:</p>
                                $$h(Z) = \frac{1}{2}\log_2(2\pi e(\sigma^2 + \sigma_N^2))$$
                                $$h(N) = \frac{1}{2}\log_2(2\pi e \sigma_N^2)$$
                                $$I(X;Z) = \frac{1}{2}\log_2\frac{\sigma^2 + \sigma_N^2}{\sigma_N^2}$$
                                <p><strong>Step 4: Substitute $\sigma_N^2$.</strong></p>
                                $$\frac{\sigma^2 + \sigma_N^2}{\sigma_N^2} = 1 + \frac{\sigma^2}{\sigma_N^2} = 1 + \frac{\sigma^2(\sigma^2 - D)}{D\sigma^2} = 1 + \frac{\sigma^2 - D}{D} = \frac{\sigma^2}{D}$$
                                <p>Therefore:</p>
                                $$I(X; Z) = \frac{1}{2}\log_2\frac{\sigma^2}{D} = R(D)$$
                                <p>This confirms the rate-distortion function. The Gaussian test channel achieves the theoretical minimum: no encoding scheme can do better than $R(D) = \frac{1}{2}\log_2\frac{\sigma^2}{D}$ bits for distortion $D$.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 10 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">24. Score Matching Connection</span>
                        </div>
                        <div class="exercise-body">
                            <p>Show that the denoising autoencoder loss with Gaussian noise $\epsilon \sim \mathcal{N}(0, \sigma^2 I)$ is equivalent to learning the score function $\nabla_x \log p(x)$ up to a constant. Start from $\mathcal{L} = \mathbb{E}_{x, \epsilon}[\|g(\tilde{x}) - x\|^2]$ and derive the connection $\nabla_{\tilde{x}} \log p(\tilde{x}) \approx \frac{g^*(\tilde{x}) - \tilde{x}}{\sigma^2}$.</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>Step 1: Optimal denoiser via Tweedie's formula.</strong> From Exercise 6, we know $g^*(\tilde{x}) = \mathbb{E}[X | \tilde{X} = \tilde{x}]$. For the Gaussian noise model $\tilde{X} = X + \epsilon$ with $\epsilon \sim \mathcal{N}(0, \sigma^2 I)$:</p>
                                <p>By Bayes' rule: $p(x|\tilde{x}) = \frac{p(\tilde{x}|x) p(x)}{p(\tilde{x})}$ where $p(\tilde{x}|x) = \mathcal{N}(\tilde{x}; x, \sigma^2 I)$.</p>
                                <p><strong>Step 2: Compute the score of the noisy distribution.</strong></p>
                                $$\nabla_{\tilde{x}} \log p(\tilde{x}) = \nabla_{\tilde{x}} \log \int p(\tilde{x}|x) p(x) dx = \frac{\int \nabla_{\tilde{x}} p(\tilde{x}|x) p(x) dx}{p(\tilde{x})}$$
                                <p>Since $p(\tilde{x}|x) = \mathcal{N}(\tilde{x}; x, \sigma^2 I)$, we have $\nabla_{\tilde{x}} \log p(\tilde{x}|x) = \frac{x - \tilde{x}}{\sigma^2}$, so $\nabla_{\tilde{x}} p(\tilde{x}|x) = p(\tilde{x}|x) \cdot \frac{x - \tilde{x}}{\sigma^2}$.</p>
                                <p>Substituting:</p>
                                $$\nabla_{\tilde{x}} \log p(\tilde{x}) = \frac{1}{p(\tilde{x})} \int \frac{x - \tilde{x}}{\sigma^2} p(\tilde{x}|x) p(x) dx = \frac{1}{\sigma^2}\left(\int x \cdot p(x|\tilde{x}) dx - \tilde{x}\right)$$
                                $$= \frac{\mathbb{E}[X|\tilde{X} = \tilde{x}] - \tilde{x}}{\sigma^2} = \frac{g^*(\tilde{x}) - \tilde{x}}{\sigma^2}$$
                                <p>This is <strong>Tweedie's formula</strong>: the score of the noisy distribution equals the optimal denoising direction scaled by $1/\sigma^2$.</p>
                                <p><strong>Step 3: The connection.</strong> A trained denoising autoencoder with output $g(\tilde{x}) \approx g^*(\tilde{x})$ implicitly provides an estimate of the score:</p>
                                $$\nabla_{\tilde{x}} \log p_\sigma(\tilde{x}) \approx \frac{g(\tilde{x}) - \tilde{x}}{\sigma^2}$$
                                <p>As $\sigma \to 0$, $p_\sigma(\tilde{x}) \to p(x)$ and we recover the score of the clean data distribution. This is the foundation of <strong>score-based generative models</strong> and <strong>diffusion models</strong> (DDPM, Score SDE), which train denoisers at multiple noise levels $\sigma$ to estimate the score function across scales.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 25 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">25. VQ-VAE: Vector Quantized Latent Codes</span>
                        </div>
                        <div class="exercise-body">
                            <p>In VQ-VAE, the encoder outputs $z_e \in \mathbb{R}^d$, which is quantized to the nearest codebook vector: $z_q = e_k$ where $k = \arg\min_j \|z_e - e_j\|^2$, with codebook $\{e_j\}_{j=1}^K$. The loss is:</p>
                            $$\mathcal{L} = \|x - \hat{x}\|^2 + \|\text{sg}[z_e] - z_q\|^2 + \beta\|z_e - \text{sg}[z_q]\|^2$$
                            <p>where $\text{sg}[\cdot]$ is stop-gradient. (a) Explain what each of the three loss terms does. (b) Since $\arg\min$ is not differentiable, how does the gradient flow from $\hat{x}$ to the encoder? Derive the "straight-through estimator." (c) Compute the information-theoretic capacity: with $K$ codebook vectors and a sequence of $L$ latent positions, how many bits can be stored?</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> Term 1: $\|x - \hat{x}\|^2$ — reconstruction loss. Trains the decoder to reconstruct $x$ from $z_q$, and (via straight-through) the encoder to produce useful $z_e$.</p>
                                <p>Term 2: $\|\text{sg}[z_e] - z_q\|^2$ — codebook loss. Moves codebook vectors $e_k$ toward the encoder outputs $z_e$. The stop-gradient on $z_e$ means this only updates the codebook, not the encoder.</p>
                                <p>Term 3: $\beta\|z_e - \text{sg}[z_q]\|^2$ — commitment loss. Encourages the encoder output $z_e$ to stay close to the chosen codebook vector. Prevents $z_e$ from fluctuating wildly and "jumping" between codebook entries. $\beta$ is typically 0.25.</p>
                                <p><strong>(b)</strong> The quantization $z_q = e_k$ is non-differentiable. The straight-through estimator simply copies the gradient from $z_q$ to $z_e$:</p>
                                $$\frac{\partial \mathcal{L}}{\partial z_e} \approx \frac{\partial \mathcal{L}}{\partial z_q}$$
                                <p>In code: $z_q = z_e + \text{sg}[z_q - z_e]$. During forward pass, this equals $z_q$. During backward pass, $\text{sg}[\cdot]$ has zero gradient, so $\frac{\partial z_q}{\partial z_e} = 1$. The decoder's gradient flows unchanged to the encoder.</p>
                                <p><strong>(c)</strong> Each latent position selects one of $K$ codebook vectors, encoding $\log_2 K$ bits. With $L$ positions: total capacity $= L \log_2 K$ bits. For $K = 512, L = 32$: $32 \times \log_2 512 = 32 \times 9 = 288$ bits. This is an exact, discrete capacity — unlike continuous latent spaces where the effective bit rate depends on precision.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 26 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">26. Information Bottleneck Lagrangian</span>
                        </div>
                        <div class="exercise-body">
                            <p>The Information Bottleneck (IB) objective minimizes $I(X;Z)$ while preserving $I(Z;Y)$: $\mathcal{L}_{IB} = I(X;Z) - \beta I(Z;Y)$ where $Y$ is a target variable. (a) Using calculus of variations, derive the self-consistent equation for the optimal encoder $p(z|x)$. (b) Show that the optimal $p(z|x) \propto p(z) \exp(-\beta D_{KL}(p(y|x) \| p(y|z)))$. (c) Explain how $\beta$ controls the rate-relevance trade-off.</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> We minimize $\mathcal{L} = I(X;Z) - \beta I(Z;Y)$ over $p(z|x)$ subject to normalization $\sum_z p(z|x) = 1$.</p>
                                $$I(X;Z) = \sum_{x,z} p(x) p(z|x) \log \frac{p(z|x)}{p(z)}$$
                                $$I(Z;Y) = \sum_{z,y} p(z,y) \log \frac{p(y|z)}{p(y)}$$
                                <p>where $p(z) = \sum_x p(x) p(z|x)$ and $p(y|z) = \sum_x p(y|x) p(x|z)$. Using a Lagrange multiplier $\lambda(x)$ for normalization:</p>
                                $$\frac{\delta}{\delta p(z|x)} \left[\mathcal{L} - \sum_x \lambda(x) \sum_z p(z|x)\right] = 0$$
                                <p><strong>(b)</strong> Taking the functional derivative and solving (noting that $p(z)$ and $p(y|z)$ depend on $p(z|x)$, requiring self-consistency):</p>
                                $$p(z|x) = \frac{p(z)}{Z(\beta, x)} \exp\left(-\beta D_{KL}(p(y|x) \| p(y|z))\right)$$
                                <p>where $Z(\beta, x) = \sum_z p(z) \exp(-\beta D_{KL}(p(y|x) \| p(y|z)))$ is the normalization constant.</p>
                                <p>Intuition: The encoder $p(z|x)$ assigns $x$ to cluster $z$ proportionally to: (1) the cluster's size $p(z)$ (prior); (2) how similar $x$'s label distribution $p(y|x)$ is to cluster $z$'s label distribution $p(y|z)$. The KL divergence penalizes assigning $x$ to a cluster whose predictions differ from the true labels.</p>
                                <p><strong>(c)</strong> $\beta \to 0$: $p(z|x) \propto p(z)$ — the encoder ignores the input entirely. $I(X;Z) = 0$ (perfect compression), $I(Z;Y) = 0$ (no information preserved). $\beta \to \infty$: the encoder becomes deterministic, mapping each $x$ to the $z$ with the best-matching $p(y|z)$. $I(X;Z)$ is maximized, $I(Z;Y) \approx I(X;Y)$ (all relevant information preserved). Intermediate $\beta$: trades off compression against predictive power. This traces out the information curve, analogous to the rate-distortion curve.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 27 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">27. Contractive AE with ReLU</span>
                        </div>
                        <div class="exercise-body">
                            <p>Repeat the Jacobian analysis of Exercise 22 (Contractive Jacobian) for ReLU activation $\sigma(a) = \max(0, a)$. (a) Derive $J_{ij}$ and show $\|J\|_F^2 = \sum_i \mathbb{1}[a_i > 0] \sum_j W_{ij}^2$. (b) Why is this harder to optimize than the sigmoid case? (c) Is the penalty still useful? What does it actually penalize?</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> With ReLU, $\sigma'(a_i) = \mathbb{1}[a_i > 0]$ (1 if pre-activation is positive, 0 otherwise).</p>
                                $$J_{ij} = \sigma'(a_i) \cdot W_{ij} = \mathbb{1}[a_i > 0] \cdot W_{ij}$$
                                $$\|J\|_F^2 = \sum_i \sum_j [\mathbb{1}[a_i > 0]]^2 W_{ij}^2 = \sum_i \mathbb{1}[a_i > 0] \sum_j W_{ij}^2$$
                                <p>Since $\mathbb{1}[a_i > 0]^2 = \mathbb{1}[a_i > 0]$ (the indicator is 0 or 1).</p>
                                <p><strong>(b)</strong> Two problems: (1) The gradient of the indicator $\mathbb{1}[a_i > 0]$ is zero almost everywhere (and undefined at $a_i = 0$). The penalty can only reduce $\|J\|_F^2$ by shrinking $W_{ij}$ (like weight decay) for active units, not by moving units toward saturation. (2) The penalty is piecewise constant in $a_i$: it doesn't smoothly encourage any particular activation level. With sigmoid, the $z_i^2(1-z_i)^2$ factor smoothly encourages saturation ($z_i \to 0$ or $1$), providing a richer optimization landscape.</p>
                                <p><strong>(c)</strong> Yes, still useful but acts differently. For active units ($a_i > 0$), the penalty is $\sum_j W_{ij}^2$ — pure weight decay on the $i$-th row of $W$. For dead units ($a_i \leq 0$), the penalty is 0, so they contribute nothing. The penalty thus: (1) shrinks weights of active units, reducing input sensitivity; (2) doesn't affect dead units at all; (3) indirectly encourages sparsity by making it "free" (zero penalty) to kill a unit. With ReLU, the contractive AE behaves more like sparse AE + weight decay than the sigmoid version's contraction toward saturated states.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 28 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">28. Optimal Transport View of Autoencoders</span>
                        </div>
                        <div class="exercise-body">
                            <p>The autoencoder objective $\min_{f,g} \mathbb{E}_{p(x)}[\|x - g(f(x))\|^2]$ can be viewed through optimal transport. (a) In the Monge formulation, optimal transport finds a map $T: \mathbb{R}^n \to \mathbb{R}^n$ minimizing $\mathbb{E}[\|x - T(x)\|^2]$ subject to $T_{\#}p = q$ (pushforward constraint). Show that an autoencoder computes a map $T = g \circ f$ with the implicit constraint $T_{\#}p \approx p$. (b) The key difference: autoencoder maps factor through a bottleneck. Show that if $f: \mathbb{R}^n \to \mathbb{R}^d$ with $d < n$, then $T = g \circ f$ can only produce outputs on a $d$-dimensional manifold. (c) Interpret: what does the autoencoder "transport" and where?</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> Define $T(x) = g(f(x)) = \hat{x}$. The AE loss $\mathbb{E}[\|x - T(x)\|^2]$ is the quadratic transport cost from $p(x)$ to $T_{\#}p$. If the AE achieves zero loss, $T(x) = x$ for $p$-a.e. $x$, meaning $T_{\#}p = p$ exactly. In practice, $T_{\#}p \approx p$ — the reconstruction distribution approximates the data distribution. But unlike standard OT, we don't specify the target measure; the objective itself drives $T_{\#}p$ toward $p$.</p>
                                <p><strong>(b)</strong> Since $f$ maps to $\mathbb{R}^d$ and $g$ maps back to $\mathbb{R}^n$, the image of $T = g \circ f$ is $\text{Im}(g) \subseteq \mathbb{R}^n$. By the rank theorem, if $g$ is smooth and $d < n$, then $\text{Im}(g)$ has measure zero in $\mathbb{R}^n$ — it's a $d$-dimensional manifold (assuming $g$ is an immersion). Therefore $T_{\#}p$ is a distribution supported on a $d$-dimensional manifold, even if $p$ has full $n$-dimensional support.</p>
                                <p><strong>(c)</strong> The autoencoder "transports" each data point $x$ to its nearest point $\hat{x}$ on the learned manifold $\mathcal{M} = g(\mathbb{R}^d)$. The manifold is optimized so that the total transport cost (sum of squared distances from data to manifold) is minimized. This is a nonlinear generalization of PCA, where PCA finds the best $d$-dimensional hyperplane and the AE finds the best $d$-dimensional manifold. The "transport plan" is deterministic: each $x$ maps to exactly one $\hat{x} = g(f(x))$, which is the autoencoder's version of projection onto the manifold.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 29 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">29. Posterior Collapse in VAEs</span>
                        </div>
                        <div class="exercise-body">
                            <p>In a VAE, the ELBO is $\mathcal{L} = \mathbb{E}_{q(z|x)}[\log p(x|z)] - D_{KL}(q(z|x) \| p(z))$. (a) Show that if the decoder $p(x|z)$ is a powerful autoregressive model (e.g., PixelCNN) that ignores $z$, then the optimal encoder is $q(z|x) = p(z)$. This is "posterior collapse." (b) Compute the ELBO when posterior collapse occurs. (c) Name three remedies and explain the mechanism of each.</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> If the decoder ignores $z$: $p(x|z) = p(x)$ for all $z$. Then the reconstruction term becomes $\mathbb{E}_{q(z|x)}[\log p(x|z)] = \mathbb{E}_{q(z|x)}[\log p(x)] = \log p(x)$, which doesn't depend on $q(z|x)$. The only term involving $q$ is $-D_{KL}(q(z|x) \| p(z))$, which is maximized (least negative) when $q(z|x) = p(z)$, giving $D_{KL} = 0$. So the optimal encoder outputs the prior regardless of $x$.</p>
                                <p><strong>(b)</strong> With $q(z|x) = p(z)$: $\mathcal{L} = \log p(x) - 0 = \log p(x)$. The ELBO equals the log-evidence. This is tight, but trivial: the latent variable is unused. The model degenerates to $p(x)$ alone.</p>
                                <p><strong>(c)</strong> Three remedies:</p>
                                <p>(1) <strong>KL annealing (warm-up):</strong> start training with $\beta = 0$ in $\beta \cdot D_{KL}$ and slowly increase to 1. This lets the encoder learn useful representations before the KL penalty pushes toward collapse. Mechanism: the reconstruction term dominates early, establishing encoder-decoder communication.</p>
                                <p>(2) <strong>Free bits (Kingma et al.):</strong> use $\max(\delta, D_{KL})$ instead of $D_{KL}$, ensuring each latent dimension uses at least $\delta$ nats. Mechanism: directly prevents the KL from reaching zero.</p>
                                <p>(3) <strong>Weaken the decoder:</strong> use a simpler decoder (MLP instead of PixelCNN) that cannot model $p(x)$ without $z$. Mechanism: forces $p(x|z) \neq p(x)$, making the reconstruction term depend on $z$, so the encoder must transmit information.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 30 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">30. Denoising Score Matching to Diffusion Models</span>
                        </div>
                        <div class="exercise-body">
                            <p>Extend Exercise 24 (Score Matching) to multiple noise levels. Consider $L$ noise levels $\sigma_1 > \sigma_2 > \cdots > \sigma_L > 0$ and the multi-scale denoising score matching objective:</p>
                            $$\mathcal{L} = \sum_{i=1}^L \lambda(\sigma_i) \mathbb{E}_{x \sim p(x)} \mathbb{E}_{\tilde{x} \sim \mathcal{N}(x, \sigma_i^2 I)} \left[\left\|s_\theta(\tilde{x}, \sigma_i) - \nabla_{\tilde{x}} \log p_{\sigma_i}(\tilde{x}|x)\right\|^2\right]$$
                            <p>(a) Show that $\nabla_{\tilde{x}} \log p_{\sigma_i}(\tilde{x}|x) = \frac{x - \tilde{x}}{\sigma_i^2}$ and simplify the objective. (b) With $\lambda(\sigma_i) = \sigma_i^2$, show this is equivalent to training a denoiser: $\epsilon_\theta(\tilde{x}, \sigma_i) \approx \epsilon$ where $\tilde{x} = x + \sigma_i \epsilon$. (c) Explain how this connects to DDPM: as $L \to \infty$ and we take $\sigma_i^2 = \bar{\sigma}^2(t_i)$ for a continuous noise schedule $t \in [0, T]$, the discrete sum becomes an integral and the iterative denoising process becomes the reverse of a stochastic differential equation.</p>
                            <button class="solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> Since $p(\tilde{x}|x) = \mathcal{N}(\tilde{x}; x, \sigma_i^2 I)$:</p>
                                $$\log p(\tilde{x}|x) = -\frac{\|\tilde{x} - x\|^2}{2\sigma_i^2} - \frac{n}{2}\log(2\pi\sigma_i^2)$$
                                $$\nabla_{\tilde{x}} \log p(\tilde{x}|x) = -\frac{\tilde{x} - x}{\sigma_i^2} = \frac{x - \tilde{x}}{\sigma_i^2}$$
                                <p>Substituting into the objective:</p>
                                $$\mathcal{L} = \sum_{i=1}^L \lambda(\sigma_i) \mathbb{E}\left[\left\|s_\theta(\tilde{x}, \sigma_i) - \frac{x - \tilde{x}}{\sigma_i^2}\right\|^2\right]$$
                                <p><strong>(b)</strong> Let $\epsilon = \frac{\tilde{x} - x}{\sigma_i} \sim \mathcal{N}(0, I)$, so $x - \tilde{x} = -\sigma_i \epsilon$ and $\frac{x - \tilde{x}}{\sigma_i^2} = \frac{-\epsilon}{\sigma_i}$.</p>
                                <p>Reparametrize: $s_\theta(\tilde{x}, \sigma_i) = \frac{-\epsilon_\theta(\tilde{x}, \sigma_i)}{\sigma_i}$.</p>
                                <p>With $\lambda(\sigma_i) = \sigma_i^2$:</p>
                                $$\mathcal{L} = \sum_{i=1}^L \sigma_i^2 \mathbb{E}\left[\left\|\frac{-\epsilon_\theta}{\sigma_i} - \frac{-\epsilon}{\sigma_i}\right\|^2\right] = \sum_{i=1}^L \mathbb{E}\left[\|\epsilon_\theta(\tilde{x}, \sigma_i) - \epsilon\|^2\right]$$
                                <p>This is exactly the DDPM loss: predict the noise $\epsilon$ that was added.</p>
                                <p><strong>(c)</strong> As $L \to \infty$ with a continuous schedule $\sigma^2(t)$: the forward process $x_t = x_0 + \sigma(t)\epsilon$ becomes an Ito SDE: $dx_t = f(x_t, t)dt + g(t)dw_t$ where $g(t) = \left(\frac{d\sigma^2(t)}{dt}\right)^{1/2}$ and $f$ depends on the schedule parameterization. The discrete denoising iterations converge to the reverse-time SDE:</p>
                                $$dx = [f(x,t) - g^2(t)\nabla_x \log p_t(x)]dt + g(t)d\bar{w}$$
                                <p>where $\bar{w}$ is a reverse-time Brownian motion. The trained score network $s_\theta \approx \nabla_x \log p_t$ provides the drift correction. This is Anderson's (1982) reverse-time SDE theorem applied to generative modeling — the foundation of Score SDE (Song et al., 2021).</p>
                            </div>
                        </div>
                    </div>

                </div>
            </article>

        </main>

        <!-- TOC (Right Side) -->
        <aside class="toc-container">
        <h4 class="toc-title">Contents</h4>
        <nav class="toc-list">
            <a href="#why-autoencoders" class="toc-link">Why Autoencoders?</a>
            <a href="#mathematics" class="toc-link">The Mathematics</a>
            <a href="#information-bottleneck" class="toc-link">Information Bottleneck</a>
            <a href="#undercomplete-overcomplete" class="toc-link">Undercomplete vs Overcomplete</a>
            <a href="#sparse-autoencoders" class="toc-link">Sparse Autoencoders</a>
            <a href="#denoising-autoencoders" class="toc-link">Denoising Autoencoders</a>
            <a href="#contractive-autoencoders" class="toc-link">Contractive Autoencoders</a>
            <a href="#applications" class="toc-link">Applications</a>
            <a href="#from-ae-to-vae" class="toc-link">From AE to VAE</a>
        </nav>
    </aside>
    </div>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-brand">
                    <span class="logo-symbol">∇</span>
                    <span>ML Fundamentals</span>
                </div>
                <p class="footer-tagline">Deep understanding through first principles.</p>
            </div>
            <div class="footer-links">
                <a href="../../../index.html">Home</a>
                <a href="https://github.com/ml-entropy/ml-entropy.github.io" target="_blank">GitHub</a>
            </div>
        </div>
    </footer>

    <!-- Scripts -->
    <script src="../../../js/main.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            if (typeof renderMathInElement === 'function') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }

            const tabs = document.querySelectorAll('.tutorial-tab');
            const articles = document.querySelectorAll('.article-content');

            function switchTab(targetId) {
                if (!targetId || targetId === '#') targetId = '#theory';
                tabs.forEach(tab => {
                    if (tab.getAttribute('href') === targetId) {
                        tab.classList.add('active');
                    } else {
                        tab.classList.remove('active');
                    }
                });
                articles.forEach(article => {
                    const articleId = '#' + article.id;
                    if (articleId === targetId) {
                        article.style.display = 'block';
                    } else {
                        article.style.display = 'none';
                    }
                });
                if (typeof renderMathInElement === 'function') {
                    renderMathInElement(document.body, {
                        delimiters: [
                            {left: '$$', right: '$$', display: true},
                            {left: '$', right: '$', display: false},
                            {left: '\\[', right: '\\]', display: true},
                            {left: '\\(', right: '\\)', display: false}
                        ],
                        throwOnError: false
                    });
                }
                const toc = document.querySelector('.toc-container');
                if (toc) {
                    if (targetId === '#theory') {
                        toc.style.display = 'block';
                        setTimeout(() => toc.classList.add('visible'), 100);
                    } else {
                        toc.classList.remove('visible');
                        setTimeout(() => toc.style.display = 'none', 300);
                    }
                }
            }

            tabs.forEach(tab => {
                tab.addEventListener('click', (e) => {
                    e.preventDefault();
                    const targetId = tab.getAttribute('href');
                    history.pushState(null, null, targetId);
                    switchTab(targetId);
                });
            });

            window.addEventListener('popstate', () => {
                switchTab(window.location.hash);
            });

            switchTab(window.location.hash);
        });
    </script>
</body>
</html>
