<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>FST Libraries (pynini, OpenFST, HFST) | ML Fundamentals</title>
    <meta name="description" content="Practical guide to FST libraries: pynini, OpenFST, HFST, and Foma. Installation, APIs, workflows, and performance considerations.">

    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&family=Playfair+Display:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}, {left: '\\\\[', right: '\\\\]', display: true}, {left: '\\\\(', right: '\\\\)', display: false}], throwOnError: false});"></script>

    <!-- Styles -->
    <link rel="stylesheet" href="../../../css/main.css">
    <link rel="stylesheet" href="../../../css/components.css">
    <link rel="stylesheet" href="../../../css/sidebar.css">

    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>&#x221E;</text></svg>">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar" id="navbar">
        <div class="nav-container">
            <a href="../../../index.html" class="nav-logo">
                <span class="logo-symbol">&nabla;</span>
                <span class="logo-text">ML Fundamentals</span>
            </a>

            <button class="nav-toggle" id="navToggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>

            <div class="nav-menu" id="navMenu">
                <div class="nav-links">
                    <a href="../../../tutorials/ml/index.html" class="nav-link active">Machine Learning</a>
                    <a href="../../../tutorials/linear-algebra/index.html" class="nav-link">Linear Algebra</a>
                    <a href="../../../tutorials/calculus/index.html" class="nav-link">Calculus</a>
                    <a href="../../../tutorials/physics/index.html" class="nav-link">Physics</a>
                    <a href="../../../index.html#philosophy" class="nav-link">Philosophy</a>
                    <a href="../../../index.html#roadmap" class="nav-link">Roadmap</a>
                    <a href="https://github.com/ml-entropy/ml-entropy.github.io" class="nav-link" target="_blank">GitHub</a>
                </div>

                <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
                    <svg class="sun-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="5"/>
                        <path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"/>
                    </svg>
                    <svg class="moon-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                    </svg>
                </button>
            </div>
        </div>
    </nav>

    <!-- Tutorial Header -->
    <header class="tutorial-content-header">
        <div class="container">
            <nav class="breadcrumb">
                <a href="../../../index.html">Home</a>
                <span class="breadcrumb-separator">&rarr;</span>
                <a href="../index.html">Machine Learning</a>
                <span class="breadcrumb-separator">&rarr;</span>
                <span>FST Libraries</span>
            </nav>

            <div class="tutorial-tabs">
                <a href="#theory" class="tutorial-tab active">Theory</a>
                <a href="#code" class="tutorial-tab">Code</a>
                <a href="#exercises" class="tutorial-tab">Exercises</a>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <div class="tutorial-wrapper">

        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <aside class="tutorial-sidebar">
            <div class="sidebar-section">
                <h3 class="sidebar-section-title">Machine Learning</h3>
                <nav class="sidebar-nav">
                        <a href="../00-probability/index.html" class="sidebar-link">00. Probability Foundations</a>
                    <a href="../04-logarithms/index.html" class="sidebar-link">01. Why Logarithms?</a>
                    <a href="../05-combinatorics/index.html" class="sidebar-link">02. Combinatorics</a>
                    <a href="../03-distributions/index.html" class="sidebar-link">03. Normal Distributions</a>
                    <a href="../01-entropy/index.html" class="sidebar-link">04. Entropy Fundamentals</a>
                    <a href="../02-cross-entropy/index.html" class="sidebar-link">05. Cross-Entropy</a>
                    <a href="../02-kl-divergence/index.html" class="sidebar-link">06. KL Divergence</a>
                    <a href="../14-entropy-connections/index.html" class="sidebar-link">07. Entropy Connections</a>
                    <a href="../06-backpropagation/index.html" class="sidebar-link">08. Backpropagation</a>
                    <a href="../07-regularization/index.html" class="sidebar-link">09. Regularization</a>
                    <a href="../08-batch-normalization/index.html" class="sidebar-link">10. Batch Normalization</a>
                    <a href="../09-learning-rate/index.html" class="sidebar-link">11. Learning Rate</a>
                    <a href="../10-cnn/index.html" class="sidebar-link">12. CNNs</a>
                    <a href="../11-rnn/index.html" class="sidebar-link">13. RNNs</a>
                    <a href="../15-autoencoder/index.html" class="sidebar-link">14. Autoencoders</a>
                    <a href="../13-variational-inference/index.html" class="sidebar-link">15. Variational Inference</a>
                    <a href="../12-vae/index.html" class="sidebar-link">16. VAE</a>
                    <a href="../16-inductive-bias/index.html" class="sidebar-link">17. Inductive Bias</a>
                    <a href="../17-architectural-biases/index.html" class="sidebar-link">18. Architectural Biases</a>
                    <a href="../18-designing-biases/index.html" class="sidebar-link">19. Designing Biases</a>
                    <a href="../19-fst-fundamentals/index.html" class="sidebar-link">20. FST Fundamentals</a>
                    <a href="../20-weighted-fsts/index.html" class="sidebar-link">21. Weighted FSTs</a>
                    <a href="../21-fst-libraries/index.html" class="sidebar-link active">22. FST Libraries</a>
                    <a href="../22-fst-applications/index.html" class="sidebar-link">23. FST Applications</a>
                    <a href="../23-neural-symbolic/index.html" class="sidebar-link">24. Neural-Symbolic Hybrids</a>
                    <a href="../24-sequence-alignment/index.html" class="sidebar-link">25. Sequence Alignment</a>
                    <a href="../25-mas-algorithm/index.html" class="sidebar-link">26. MAS Algorithm</a>
                    <a href="../26-forced-alignment/index.html" class="sidebar-link">27. Forced Alignment & MFA</a>
                </nav>
            </div>
            
            <div class="sidebar-section" style="margin-top: 2rem;">
                <h3 class="sidebar-section-title">Related Subjects</h3>
                <nav class="sidebar-nav">
                        <a href="../../linear-algebra/index.html" class="sidebar-link">Linear Algebra</a>
                    <a href="../../calculus/index.html" class="sidebar-link">Calculus</a>
                    <a href="../../physics/index.html" class="sidebar-link">Physics</a>
                </nav>
            </div>
        </aside>

        <!-- Main Article -->
        <main class="tutorial-main">

            <!-- ==================== THEORY TAB ==================== -->
            <article class="article-content" id="theory">

                <!-- Section 1: FST Library Landscape -->
                <h2 id="library-landscape">FST Library Landscape</h2>

                <p>
                    In Tutorials 20 and 21, we built FSTs from scratch and added weights by hand. In practice, nobody does this &mdash; mature libraries handle the low-level details of state management, arc storage, composition algorithms, and optimization. Choosing the right library depends on your language, domain, and performance requirements.
                </p>

                <p>
                    Four major FST libraries dominate the landscape, each with distinct design philosophies and target audiences:
                </p>

                <table style="width: 100%; border-collapse: collapse; margin: 1.5rem 0; font-size: 0.95rem;">
                    <thead>
                        <tr style="border-bottom: 2px solid var(--color-border);">
                            <th style="text-align: left; padding: 0.75rem;">Library</th>
                            <th style="text-align: left; padding: 0.75rem;">Language</th>
                            <th style="text-align: left; padding: 0.75rem;">Focus</th>
                            <th style="text-align: left; padding: 0.75rem;">Strengths</th>
                            <th style="text-align: left; padding: 0.75rem;">Weaknesses</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.75rem;"><strong>Pynini</strong></td>
                            <td style="padding: 0.75rem;">Python (wraps OpenFST)</td>
                            <td style="padding: 0.75rem;">Text normalization, NLP</td>
                            <td style="padding: 0.75rem;">Pythonic API, easy prototyping, Google-maintained</td>
                            <td style="padding: 0.75rem;">Linux/macOS only, Python overhead for large-scale</td>
                        </tr>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.75rem;"><strong>OpenFST</strong></td>
                            <td style="padding: 0.75rem;">C++</td>
                            <td style="padding: 0.75rem;">General-purpose, speech</td>
                            <td style="padding: 0.75rem;">Maximum performance, Kaldi integration, binary format</td>
                            <td style="padding: 0.75rem;">Steep learning curve, verbose API</td>
                        </tr>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.75rem;"><strong>HFST</strong></td>
                            <td style="padding: 0.75rem;">C++ / Python / CLI</td>
                            <td style="padding: 0.75rem;">Computational linguistics</td>
                            <td style="padding: 0.75rem;">Two-level morphology, lexc/twolc, minority languages</td>
                            <td style="padding: 0.75rem;">Niche community, less documentation</td>
                        </tr>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.75rem;"><strong>Foma</strong></td>
                            <td style="padding: 0.75rem;">C / CLI (Xerox tradition)</td>
                            <td style="padding: 0.75rem;">Morphology, phonology</td>
                            <td style="padding: 0.75rem;">Compact regex syntax, interactive shell, fast compilation</td>
                            <td style="padding: 0.75rem;">Limited weight support, smaller ecosystem</td>
                        </tr>
                    </tbody>
                </table>

                <div class="definition-box">
                    <div class="box-title">Library Relationships</div>
                    <p>
                        <strong>Pynini</strong> is a Python wrapper around <strong>OpenFST</strong>. Anything you build in pynini is an OpenFST object under the hood. HFST can use OpenFST as one of its backends. Foma descends from the Xerox finite-state tools (xfst) tradition and has its own internal format.
                    </p>
                    <p style="margin-bottom: 0;">
                        For most ML practitioners, <strong>pynini is the recommended starting point</strong>: it gives you full OpenFST power with Python convenience.
                    </p>
                </div>

                <!-- Section 2: Pynini Basics -->
                <h2 id="pynini-basics">Pynini Basics</h2>

                <p>
                    Pynini (pronounced "pie-NEE-nee", named after Panini, the ancient Sanskrit grammarian) provides a high-level Python API for building, composing, and optimizing FSTs. It was developed at Google for text normalization in TTS (text-to-speech) systems.
                </p>

                <p><strong>Installation:</strong></p>
                <div class="code-block">
                    <div class="code-block-header">
                        <span class="code-block-lang">python</span>
                        <button class="code-block-copy">Copy</button>
                    </div>
                    <pre><code class="language-python"># Install pynini (Linux/macOS only; requires conda or pip)
# pip install pynini
# or: conda install -c conda-forge pynini

import pynini
from pynini.lib import rewrite</code></pre>
                </div>

                <div class="definition-box">
                    <div class="box-title">Core Pynini Concepts</div>
                    <p><strong>Acceptor</strong> &mdash; An FST with identical input and output labels on every arc. Equivalent to a finite-state automaton (FSA). Created with <code>pynini.acceptor("hello")</code>.</p>
                    <p><strong>Transducer</strong> &mdash; An FST that maps input strings to output strings. Created with <code>pynini.transducer("color", "colour")</code> or <code>pynini.cross("color", "colour")</code>.</p>
                    <p><strong>Union</strong> &mdash; Combines multiple FSTs: accepts any input accepted by any of them. <code>pynini.union(fst1, fst2, fst3)</code> or <code>fst1 | fst2 | fst3</code>.</p>
                    <p><strong>Concatenation</strong> &mdash; Sequential composition of FSTs. <code>fst1 + fst2</code> means "apply fst1 then fst2 in sequence".</p>
                    <p><strong>Closure</strong> &mdash; Repetition. <code>fst.closure()</code> for zero-or-more (Kleene star), <code>fst.closure(1)</code> for one-or-more (Kleene plus).</p>
                    <p style="margin-bottom: 0;"><strong>Composition</strong> &mdash; The key operation: <code>fst1 @ fst2</code> (or <code>pynini.compose(fst1, fst2)</code>) feeds the output of fst1 into the input of fst2.</p>
                </div>

                <p>
                    The critical distinction in pynini is between <strong>strings and FSTs</strong>. A Python string like <code>"hello"</code> is not an FST &mdash; you must wrap it with <code>pynini.acceptor("hello")</code> or use it in a context where pynini auto-converts it. Pynini operations always return FST objects, which can then be composed, optimized, or applied.
                </p>

                <p>
                    Under the hood, every pynini FST is an OpenFST <code>VectorFst</code> object stored in memory. When you call <code>pynini.transducer("Dr.", "Doctor")</code>, pynini creates states and arcs corresponding to each character pair, using OpenFST's C++ data structures. This means pynini inherits all of OpenFST's optimization algorithms: determinization, minimization, epsilon removal, and weight pushing.
                </p>

                <div class="warning-box">
                    <div class="box-title">Symbol Table Mismatches</div>
                    <p style="margin-bottom: 0;">
                        When composing FSTs built from different sources, ensure they share the same symbol table. Mismatched symbol tables cause silent failures where composition produces no output. Always use <code>pynini.string_map()</code> or define a shared <code>SymbolTable</code> for all FSTs in your pipeline.
                    </p>
                </div>

                <!-- Section 3: OpenFST Core Concepts -->
                <h2 id="openfst-core">OpenFST Core Concepts</h2>

                <p>
                    OpenFST is the foundational C++ library that powers most FST work in speech and language processing. Developed at Google and NYU, it provides both a C++ API and a suite of command-line tools. Pynini wraps OpenFST, but understanding the underlying concepts is essential for debugging and performance optimization.
                </p>

                <p><strong>Key C++ API concepts:</strong></p>
                <ul>
                    <li><strong>Fst types:</strong> <code>VectorFst</code> (mutable, in-memory), <code>ConstFst</code> (immutable, memory-mapped), <code>CompactFst</code> (compressed)</li>
                    <li><strong>Arcs:</strong> Each arc stores <code>(input_label, output_label, weight, next_state)</code></li>
                    <li><strong>Symbol tables:</strong> Map between string labels and integer IDs (OpenFST operates on integers internally)</li>
                    <li><strong>Semirings:</strong> Define how weights combine (tropical for shortest-path, log for probabilities)</li>
                </ul>

                <p><strong>Essential command-line tools:</strong></p>
                <ul>
                    <li><code>fstcompile</code> &mdash; Convert text FST description to binary format</li>
                    <li><code>fstcompose</code> &mdash; Compose two FSTs</li>
                    <li><code>fstshortestpath</code> &mdash; Find the lowest-cost path through a weighted FST</li>
                    <li><code>fstdeterminize</code> &mdash; Make an FST deterministic</li>
                    <li><code>fstminimize</code> &mdash; Reduce number of states</li>
                    <li><code>fstprint</code> &mdash; Display FST arcs in human-readable text format</li>
                    <li><code>fstdraw</code> &mdash; Generate a Graphviz DOT file for visualization</li>
                </ul>

                <div class="note-box">
                    <div class="box-title">Command-Line vs API: When to Use Which</div>
                    <p>
                        Use the <strong>command-line tools</strong> for: quick prototyping, shell pipelines, inspecting FST structure, one-off operations on compiled FSTs, and integration with non-C++ systems.
                    </p>
                    <p style="margin-bottom: 0;">
                        Use the <strong>C++ API</strong> (or pynini) for: building FSTs programmatically, runtime composition in applications, custom algorithms, and when you need fine-grained control over memory and performance. The command-line tools are built on the same C++ API, so there is no functionality gap.
                    </p>
                </div>

                <div class="note-box">
                    <div class="box-title">OpenFST Pipeline Example</div>
                    <p>To compile, compose, and find the shortest path through an FST using the command-line tools:</p>
                    <ol>
                        <li><code>fstcompile --isymbols=syms.txt input.fst > compiled.fst</code></li>
                        <li><code>fstcompose compiled.fst lexicon.fst > composed.fst</code></li>
                        <li><code>fstshortestpath composed.fst > best.fst</code></li>
                        <li><code>fstprint --isymbols=syms.txt best.fst</code> to see the result.</li>
                    </ol>
                    <p style="margin-bottom: 0;">
                        This four-step pipeline &mdash; compile, compose, search, print &mdash; is the backbone of most OpenFST workflows in speech recognition and text normalization.
                    </p>
                </div>

                <p>
                    OpenFST's binary format is compact and supports memory-mapped access, making it ideal for deploying large FSTs in production. A common workflow in speech recognition (especially with Kaldi) is to compile FSTs offline, optimize them aggressively, save to binary, and load them at runtime with minimal overhead.
                </p>

                <!-- Section 4: HFST for Linguistics -->
                <h2 id="hfst-linguistics">HFST for Linguistics</h2>

                <p>
                    The Helsinki Finite-State Technology (HFST) toolkit was designed specifically for <strong>computational morphology and phonology</strong>. While OpenFST and pynini are general-purpose, HFST provides specialized formalisms for describing the structure of words in natural languages.
                </p>

                <p><strong>Key HFST formalisms:</strong></p>
                <ul>
                    <li><strong>lexc</strong> &mdash; A lexicon compiler format for defining morphological paradigms as concatenations of morphemes. You define continuation classes that chain stems, affixes, and inflectional endings.</li>
                    <li><strong>twolc</strong> &mdash; Two-level rules that describe phonological alternations. Rules operate in parallel: they constrain the relationship between an underlying (lexical) form and a surface form simultaneously.</li>
                    <li><strong>xfst / regex</strong> &mdash; A regular expression syntax (inherited from the Xerox tradition) for defining replacement rules.</li>
                </ul>

                <p>
                    HFST's greatest strength is <strong>two-level morphology</strong>, a formalism invented by Kimmo Koskenniemi. In two-level morphology, surface forms and lexical forms are related by parallel constraints rather than sequential rules. This is particularly powerful for languages with complex morphophonology.
                </p>

                <p><strong>Example: Finnish noun inflection</strong></p>
                <p>
                    Finnish has 15 grammatical cases, and nouns undergo consonant gradation (e.g., the stem consonant changes between strong and weak grades depending on syllable structure). Consider the word "kukka" (flower):
                </p>
                <ul>
                    <li>Nominative singular: <em>kukka</em> (strong grade: kk)</li>
                    <li>Genitive singular: <em>kukan</em> (weak grade: k, with 'n' suffix)</li>
                    <li>Partitive singular: <em>kukkaa</em> (strong grade: kk, with 'a' suffix)</li>
                    <li>Inessive singular: <em>kukassa</em> (weak grade: k, with 'ssa' suffix)</li>
                </ul>
                <p>
                    In HFST's two-level approach, you define the underlying form as <code>kukka</code> and write rules that relate <code>kk</code> to <code>k</code> in weak-grade contexts. The rules and the lexicon are compiled into separate FSTs and then composed to produce the full morphological analyzer.
                </p>

                <p>
                    HFST has been used extensively for <strong>minority and endangered languages</strong> where training data for neural models is scarce. Rule-based morphological analyzers built with HFST exist for Sami languages, Greenlandic, Faroese, and many others &mdash; providing essential NLP infrastructure where data-driven approaches cannot work.
                </p>

                <div class="definition-box">
                    <div class="box-title">Lexc Format Example</div>
                    <p>A simple English noun lexicon in lexc format looks like:</p>
<pre style="background: var(--color-bg-secondary, #f5f5f5); padding: 1rem; border-radius: 4px; font-family: 'JetBrains Mono', monospace; font-size: 0.9rem; margin: 0.5rem 0;">LEXICON Root
  cat Noun ;
  dog Noun ;

LEXICON Noun
  +Sg:0  # ;
  +Pl:s  # ;</pre>
                    <p style="margin-bottom: 0;">
                        This compiles to an FST mapping <code>cat+Pl</code> to <code>cats</code>. Each <code>LEXICON</code> block defines a continuation class: entries in <code>Root</code> list stems followed by the name of the next lexicon to apply. In <code>Noun</code>, the notation <code>+Pl:s</code> means the upper (analysis) side is <code>+Pl</code> and the lower (surface) side is <code>s</code>. The <code>#</code> marks the end of the word.
                    </p>
                </div>

                <!-- Section 5: Practical Workflow -->
                <h2 id="practical-workflow">Practical Workflow</h2>

                <p>
                    Building FST-based systems follows a structured development cycle. Whether you are normalizing text, building a morphological analyzer, or designing a pronunciation lexicon, the same workflow applies.
                </p>

                <p><strong>The FST Development Cycle:</strong></p>
                <ol>
                    <li><strong>Design FST logic:</strong> Start with pen-and-paper or pseudocode. Define input/output alphabets, identify the states you need, and sketch the transitions. Composition lets you think in small, modular pieces.</li>
                    <li><strong>Implement in pynini:</strong> Translate your design into pynini code. Build small transducers for each rule and compose them together. Use <code>pynini.string_map()</code> for lookup tables.</li>
                    <li><strong>Test on examples:</strong> Build a comprehensive test suite. For each rule, include normal cases, edge cases, and cases that should NOT match. Use <code>rewrite.one_top_rewrite()</code> to apply your FST to test strings.</li>
                    <li><strong>Compile and optimize:</strong> Determinize (<code>fst.optimize()</code> in pynini), minimize, and remove epsilon transitions. This can dramatically reduce FST size.</li>
                    <li><strong>Deploy:</strong> Serialize the optimized FST to binary format. Load at runtime with memory mapping for minimal startup time.</li>
                </ol>

                <p><strong>Debugging tips:</strong></p>
                <ul>
                    <li>Use <code>fst.print()</code> or <code>fstprint</code> to inspect arcs &mdash; verify that transitions match your design.</li>
                    <li>Visualize small FSTs with <code>fstdraw | dot -Tpng</code> to spot structural issues.</li>
                    <li>Test composition incrementally: compose $T_1 \circ T_2$ first, verify it, then add $T_3$.</li>
                    <li>Check for unintended epsilon cycles, which can cause infinite loops or exponential blowup.</li>
                    <li>If an FST produces no output for valid input, check symbol table mismatches between composed FSTs.</li>
                </ul>

                <div class="warning-box">
                    <div class="box-title">Common Pitfalls</div>
                    <p><strong>Symbol table mismatch:</strong> When composing FSTs, the output symbols of the first FST must match the input symbols of the second. If one uses byte strings and the other uses UTF-8 codepoints, composition produces an empty result with no error message.</p>
                    <p><strong>Non-determinism explosion:</strong> Composing two non-deterministic FSTs can produce an exponentially large result. Always determinize and minimize after composition.</p>
                    <p style="margin-bottom: 0;"><strong>Epsilon loops:</strong> An epsilon transition that forms a cycle creates infinitely many paths. Most algorithms (shortest-path, determinization) will not terminate on FSTs with epsilon cycles unless you explicitly handle them.</p>
                </div>

                <!-- Section 6: Performance Considerations -->
                <h2 id="performance">Performance Considerations</h2>

                <div class="note-box">
                    <div class="box-title">FST Size vs Speed Tradeoff</div>
                    <p style="margin-bottom: 0;">
                        A determinized and minimized FST is <strong>fast at runtime</strong> (each input symbol requires exactly one state transition) but may be <strong>large</strong> (determinization can cause exponential state blowup). A non-deterministic FST is <strong>compact</strong> but <strong>slower</strong> (runtime must explore multiple paths). The right balance depends on your deployment constraints: memory-limited devices favor compact FSTs, while latency-critical applications favor fully determinized ones.
                    </p>
                </div>

                <p><strong>Key performance strategies:</strong></p>
                <ul>
                    <li><strong>On-the-fly composition:</strong> Instead of precomposing all FSTs into one giant FST, compose lazily at runtime. OpenFST supports this natively &mdash; the composed FST is computed incrementally as states are visited. This saves memory when only a small fraction of the state space is actually traversed.</li>
                    <li><strong>Memory-mapped FSTs:</strong> OpenFST's <code>ConstFst</code> type can be memory-mapped from disk, meaning the FST is not loaded into RAM until specific parts are accessed. This is essential for large FSTs (millions of states) in production systems.</li>
                    <li><strong>Caching:</strong> If the same inputs are processed repeatedly, cache the results. FST application is deterministic, so caching is safe. However, avoid caching when the input space is large and diverse.</li>
                    <li><strong>Arc sorting:</strong> Sort arcs by input label (<code>arcsort</code>) before composition. This enables the composition algorithm to use binary search instead of linear scan, significantly speeding up the process.</li>
                </ul>

                <p><strong>Performance comparison (conceptual):</strong></p>
                <table style="width: 100%; border-collapse: collapse; margin: 1.5rem 0; font-size: 0.95rem;">
                    <thead>
                        <tr style="border-bottom: 2px solid var(--color-border);">
                            <th style="text-align: left; padding: 0.75rem;">Approach</th>
                            <th style="text-align: left; padding: 0.75rem;">Build Time</th>
                            <th style="text-align: left; padding: 0.75rem;">Runtime</th>
                            <th style="text-align: left; padding: 0.75rem;">Memory</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.75rem;">Regex (Python re)</td>
                            <td style="padding: 0.75rem;">Fast</td>
                            <td style="padding: 0.75rem;">Slow for complex patterns</td>
                            <td style="padding: 0.75rem;">Low</td>
                        </tr>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.75rem;">Pynini (interpreted)</td>
                            <td style="padding: 0.75rem;">Moderate</td>
                            <td style="padding: 0.75rem;">Fast (C++ backend)</td>
                            <td style="padding: 0.75rem;">Moderate</td>
                        </tr>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.75rem;">OpenFST (compiled binary)</td>
                            <td style="padding: 0.75rem;">Slow (offline)</td>
                            <td style="padding: 0.75rem;">Very fast</td>
                            <td style="padding: 0.75rem;">Depends on optimization</td>
                        </tr>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.75rem;">HFST (morphology)</td>
                            <td style="padding: 0.75rem;">Slow (rule compilation)</td>
                            <td style="padding: 0.75rem;">Fast lookup</td>
                            <td style="padding: 0.75rem;">Moderate</td>
                        </tr>
                    </tbody>
                </table>

                <div class="note-box">
                    <div class="box-title">Typical Performance</div>
                    <p>
                        A text normalization FST with ~10,000 rules processes ~1M tokens/second on a single CPU core. After determinization and minimization, FSTs are typically 5&ndash;10x smaller than the original.
                    </p>
                    <p style="margin-bottom: 0;">
                        On-the-fly composition avoids materializing the full composed FST, reducing memory from $O(n_1 \times n_2)$ to $O(n_1 + n_2)$, where $n_1$ and $n_2$ are the state counts of the two input FSTs.
                    </p>
                </div>

                <!-- Section 7: Summary -->
                <h2 id="summary">Summary</h2>

                <p><strong>Library selection guide:</strong></p>
                <ol>
                    <li><strong>Start with pynini</strong> if you are working in Python, doing text normalization, or prototyping FST logic. It is the most accessible library with the best developer experience.</li>
                    <li><strong>Use OpenFST directly</strong> if you need maximum performance, are integrating with C++ systems (like Kaldi for speech recognition), or need fine-grained control over FST internals.</li>
                    <li><strong>Use HFST</strong> if your task is computational morphology, especially for inflectionally rich or under-resourced languages. The lexc/twolc formalisms are purpose-built for this domain.</li>
                    <li><strong>Use Foma</strong> if you come from the Xerox finite-state tradition or need a compact, interactive tool for exploring morphological and phonological rules.</li>
                </ol>

                <p><strong>Key takeaways:</strong></p>
                <ol>
                    <li><strong>Pynini wraps OpenFST</strong> with a Pythonic API &mdash; it is the recommended starting point for most practitioners.</li>
                    <li><strong>OpenFST provides the foundation:</strong> C++ performance, binary format, and command-line tools for inspection and optimization.</li>
                    <li><strong>HFST specializes in linguistics:</strong> two-level morphology, lexc, and twolc are unmatched for morphological analysis.</li>
                    <li><strong>Workflow matters:</strong> design, implement, test, optimize, deploy. Incremental composition and thorough testing prevent costly debugging later.</li>
                    <li><strong>Performance is tunable:</strong> determinization vs compactness, on-the-fly vs precompiled, memory-mapped vs in-memory. Choose based on your deployment constraints.</li>
                </ol>

                <p>
                    In the next tutorial, we put these libraries to work on <strong>real-world FST applications</strong>: text normalization pipelines, speech recognition decoding, and morphological analysis systems.
                </p>

                <!-- Navigation -->
                <div class="tutorial-footer-summary" style="margin: 3rem 0; padding: 2rem; background: #f8f9fa; border-radius: 8px; border-left: 4px solid #3b82f6;">
                    <h1>22. FST Libraries</h1>
                    <p class="lead">
                        Practical FST library usage: pynini for Python prototyping, OpenFST for production performance, and HFST for computational linguistics.
                    </p>
                </div>
                <div class="tutorial-nav">
                    <a href="../20-weighted-fsts/index.html" class="tutorial-nav-link prev">
                        <span class="nav-label">Previous</span>
                        <span class="nav-title">&larr; Weighted FSTs</span>
                    </a>
                    <a href="../22-fst-applications/index.html" class="tutorial-nav-link next">
                        <span class="nav-label">Next</span>
                        <span class="nav-title">FST Applications &rarr;</span>
                    </a>
                </div>

            </article>

            <!-- ==================== CODE TAB ==================== -->
            <article class="article-content" id="code" style="display: none;">
                <h2>Python Code Examples</h2>
                <p>Five code examples covering pynini text normalization, FST-based tokenization, OpenFST binary compilation, HFST-style morphology, and performance comparison with regex.</p>

                <!-- Code Example 1 -->
                <h3>1. Pynini: Text Normalization (Abbreviation and Number Expansion)</h3>
                <p>
                    Build FSTs that expand common abbreviations and convert single-digit numbers to words. This is the core use case for pynini in TTS systems.
                </p>

                <div class="code-block">
                    <div class="code-block-header">
                        <span class="code-block-lang">python</span>
                        <button class="code-block-copy">Copy</button>
                    </div>
                    <pre><code class="language-python"># pip install pynini
import pynini
from pynini.lib import rewrite

# --- Abbreviation Expansion ---
# Build a transducer that maps abbreviations to full forms
abbreviations = [
    ("Dr.", "Doctor"),
    ("Mr.", "Mister"),
    ("Mrs.", "Missus"),
    ("St.", "Saint"),
    ("Ave.", "Avenue"),
    ("Blvd.", "Boulevard"),
    ("Jr.", "Junior"),
    ("Sr.", "Senior"),
]

# pynini.string_map creates a union of transducers from pairs
abbrev_fst = pynini.string_map(abbreviations)

# Test abbreviation expansion
print("=== Abbreviation Expansion ===")
test_abbrevs = ["Dr.", "Mr.", "St.", "Ave."]
for abbrev in test_abbrevs:
    try:
        result = rewrite.one_top_rewrite(abbrev, abbrev_fst)
        print(f"  '{abbrev}' -> '{result}'")
    except rewrite.Error:
        print(f"  '{abbrev}' -> (no match)")

# --- Number-to-Word Expansion ---
# Map digit strings to word forms
number_words = [
    ("0", "zero"), ("1", "one"), ("2", "two"), ("3", "three"),
    ("4", "four"), ("5", "five"), ("6", "six"), ("7", "seven"),
    ("8", "eight"), ("9", "nine"), ("10", "ten"),
    ("11", "eleven"), ("12", "twelve"), ("13", "thirteen"),
    ("14", "fourteen"), ("15", "fifteen"), ("16", "sixteen"),
    ("17", "seventeen"), ("18", "eighteen"), ("19", "nineteen"),
    ("20", "twenty"),
]

number_fst = pynini.string_map(number_words)

print("\n=== Number-to-Word Expansion ===")
test_numbers = ["0", "5", "12", "19", "20"]
for num in test_numbers:
    try:
        result = rewrite.one_top_rewrite(num, number_fst)
        print(f"  '{num}' -> '{result}'")
    except rewrite.Error:
        print(f"  '{num}' -> (no match)")

# --- Composing Normalizations ---
# Build a combined normalizer: first expand abbreviations, then
# handle any remaining text. In pynini, you compose FSTs with @.
# For a full pipeline, you would CDRewrite (context-dependent rewrite)
# to handle abbreviations within longer strings.

# CDRewrite applies a transducer in context within a longer string
sigma_star = pynini.union(
    *[pynini.acceptor(chr(i)) for i in range(32, 127)]
).closure().optimize()

# Apply abbreviation expansion anywhere in the string
abbrev_rule = pynini.cdrewrite(abbrev_fst, "", "", sigma_star)

print("\n=== Context-Dependent Rewrite (in-sentence) ===")
test_sentences = [
    "Dr. Smith lives on St. Paul Ave.",
    "Mr. and Mrs. Jones",
    "Meet Jr. at Blvd. park",
]
for sentence in test_sentences:
    try:
        result = rewrite.one_top_rewrite(sentence, abbrev_rule)
        print(f"  '{sentence}'")
        print(f"  -> '{result}'")
    except rewrite.Error:
        print(f"  '{sentence}' -> (rewrite failed)")

print("\nKey insight: cdrewrite applies the transducer everywhere")
print("in the string, leaving non-matching parts unchanged.")</code></pre>
                </div>

                <!-- Code Example 2 -->
                <h3>2. Pynini: FST-Based Tokenizer</h3>
                <p>
                    Build a tokenizer using FSTs that handles punctuation splitting, whitespace normalization, and special token boundaries.
                </p>

                <div class="code-block">
                    <div class="code-block-header">
                        <span class="code-block-lang">python</span>
                        <button class="code-block-copy">Copy</button>
                    </div>
                    <pre><code class="language-python"># pip install pynini
import pynini
from pynini.lib import rewrite

# --- FST-Based Tokenizer ---
# Strategy: insert token boundaries (spaces) around punctuation

# Define character classes
letter = pynini.union(
    *[pynini.acceptor(chr(i)) for i in range(ord('a'), ord('z') + 1)],
    *[pynini.acceptor(chr(i)) for i in range(ord('A'), ord('Z') + 1)]
)
digit = pynini.union(
    *[pynini.acceptor(chr(i)) for i in range(ord('0'), ord('9') + 1)]
)
punctuation = pynini.union(
    *[pynini.acceptor(c) for c in ".,!?;:()[]{}\"'-"]
)

# Sigma star: all printable ASCII
sigma_star = pynini.union(
    *[pynini.acceptor(chr(i)) for i in range(32, 127)]
).closure().optimize()

# Rule 1: Insert space before punctuation (if not already there)
# Transducer: punctuation_char -> " " + punctuation_char
punct_split = pynini.union(
    *[pynini.transducer(c, " " + c) for c in ".,!?;:()[]{}\"'"]
)
split_before_punct = pynini.cdrewrite(
    punct_split, "", "", sigma_star
)

# Rule 2: Normalize multiple spaces to single space
multi_space = pynini.transducer("  ", " ")
normalize_spaces = pynini.cdrewrite(
    multi_space, "", "", sigma_star
)

# Compose the rules into a single tokenizer FST
tokenizer = (split_before_punct @ normalize_spaces).optimize()

print("=== FST-Based Tokenizer ===")
test_inputs = [
    "Hello, world!",
    "Dr.Smith said \"hello\".",
    "Price: $42.99 (tax included).",
    "It's a well-known fact.",
]

for text in test_inputs:
    try:
        result = rewrite.one_top_rewrite(text, tokenizer)
        print(f"  Input:  '{text}'")
        print(f"  Output: '{result}'")
        tokens = result.split()
        print(f"  Tokens: {tokens}")
        print()
    except rewrite.Error:
        print(f"  Input: '{text}' -> (tokenization failed)\n")

print("Advantage of FST tokenizer: composable, optimizable,")
print("and guaranteed to handle every possible input string.")</code></pre>
                </div>

                <!-- Code Example 3 -->
                <h3>3. OpenFST via Pynini: Compile, Save, and Load Binary FSTs</h3>
                <p>
                    Demonstrate the full workflow of building an FST in pynini, saving it to OpenFST binary format, and loading it back for application.
                </p>

                <div class="code-block">
                    <div class="code-block-header">
                        <span class="code-block-lang">python</span>
                        <button class="code-block-copy">Copy</button>
                    </div>
                    <pre><code class="language-python"># pip install pynini
import pynini
from pynini.lib import rewrite
import os
import time

# --- Build an FST ---
# Currency symbol normalizer
currency_map = [
    ("$", "dollars"),
    ("\u20ac", "euros"),       # Euro sign
    ("\u00a3", "pounds"),      # Pound sign
    ("\u00a5", "yen"),         # Yen sign
    ("CHF", "Swiss francs"),
    ("CAD", "Canadian dollars"),
    ("AUD", "Australian dollars"),
]

currency_fst = pynini.string_map(currency_map)

# Optimize the FST (determinize + minimize)
print("=== FST Optimization ===")
print(f"  Before optimization:")
print(f"    States: {currency_fst.num_states()}")

currency_fst.optimize()
print(f"  After optimization:")
print(f"    States: {currency_fst.num_states()}")

# --- Save to binary format ---
output_path = "/tmp/currency_normalizer.fst"
currency_fst.write(output_path)

file_size = os.path.getsize(output_path)
print(f"\n=== Binary FST ===")
print(f"  Saved to: {output_path}")
print(f"  File size: {file_size} bytes")

# --- Load from binary format ---
loaded_fst = pynini.Fst.read(output_path)
print(f"  Loaded FST states: {loaded_fst.num_states()}")

# --- Apply the loaded FST ---
print(f"\n=== Apply Loaded FST ===")
test_symbols = ["$", "CHF", "CAD"]
for sym in test_symbols:
    try:
        result = rewrite.one_top_rewrite(sym, loaded_fst)
        print(f"  '{sym}' -> '{result}'")
    except rewrite.Error:
        print(f"  '{sym}' -> (no match)")

# --- Demonstrate arc-level inspection ---
print(f"\n=== FST Arc Inspection ===")
print(f"  (Showing first few arcs of the currency FST)")
for state in range(min(currency_fst.num_states(), 5)):
    for arc in currency_fst.arcs(state):
        # arc.ilabel = input label (integer)
        # arc.olabel = output label (integer)
        # arc.weight = arc weight
        # arc.nextstate = destination state
        print(f"  State {state} -> State {arc.nextstate}: "
              f"in={arc.ilabel}, out={arc.olabel}, "
              f"weight={float(arc.weight)}")

# --- Timing: build vs load ---
print(f"\n=== Timing Comparison ===")

# Time building from scratch
start = time.perf_counter()
for _ in range(100):
    fst = pynini.string_map(currency_map).optimize()
build_time = (time.perf_counter() - start) / 100

# Time loading from binary
start = time.perf_counter()
for _ in range(100):
    fst = pynini.Fst.read(output_path)
load_time = (time.perf_counter() - start) / 100

print(f"  Build from scratch: {build_time*1000:.3f} ms")
print(f"  Load from binary:   {load_time*1000:.3f} ms")
print(f"  Speedup: {build_time/load_time:.1f}x")

# Clean up
os.remove(output_path)
print(f"\nFor large FSTs (millions of arcs), loading from binary")
print(f"is orders of magnitude faster than rebuilding.")</code></pre>
                </div>

                <!-- Code Example 4 -->
                <h3>4. HFST: Morphological Analyzer (Conceptual)</h3>
                <p>
                    A conceptual example showing how morphological analysis works in the HFST paradigm. Since HFST installation can be complex, this example uses a Python simulation of the same logic.
                </p>

                <div class="code-block">
                    <div class="code-block-header">
                        <span class="code-block-lang">python</span>
                        <button class="code-block-copy">Copy</button>
                    </div>
                    <pre><code class="language-python"># This is a pure-Python simulation of HFST concepts.
# In real HFST, you would use lexc and twolc files compiled to FSTs.
# See the end of this example for actual HFST commands.

# --- lexc-style Lexicon Definition ---
# In HFST lexc format, this would be:
#
# LEXICON Root
#   cat Noun;
#   dog Noun;
#   box Noun;
#   city Noun;
#   walk Verb;
#   jump Verb;
#
# LEXICON Noun
#   +N+Sg #;
#   +N+Pl PluralSuffix;
#
# LEXICON PluralSuffix
#   s #;
#   es #;
#
# LEXICON Verb
#   +V+Inf #;
#   +V+Past PastSuffix;
#   +V+PresPart ing #;
#
# LEXICON PastSuffix
#   ed #;

class MorphologicalAnalyzer:
    """Simulates HFST-style morphological analysis."""

    def __init__(self):
        # Lexicon: stems with their categories
        self.stems = {
            "cat": "N", "dog": "N", "box": "N",
            "city": "N", "fish": "N", "bus": "N",
            "walk": "V", "jump": "V", "talk": "V",
            "play": "V", "cook": "V",
        }

        # Suffix rules: (surface_suffix, category, features)
        self.noun_suffixes = [
            ("", "+N+Sg"),         # cat -> cat+N+Sg
            ("s", "+N+Pl"),        # cat+s -> cat+N+Pl
            ("es", "+N+Pl"),       # box+es -> box+N+Pl
            ("ies", "+N+Pl"),      # city -> cit+ies -> city+N+Pl
        ]

        self.verb_suffixes = [
            ("", "+V+Inf"),        # walk -> walk+V+Inf
            ("s", "+V+3Sg"),       # walk+s -> walk+V+3Sg
            ("ed", "+V+Past"),     # walk+ed -> walk+V+Past
            ("ing", "+V+PresPart"),# walk+ing -> walk+V+PresPart
        ]

        # Two-level rules (consonant gradation, etc.)
        # In real HFST these would be twolc rules
        self.alternations = {
            # stem: {suffix_context: altered_stem}
            "city": {"ies": "cit"},  # city -> cit + ies
            "bus": {"es": "bus"},    # bus + es (no stem change)
            "box": {"es": "box"},    # box + es (no stem change)
        }

    def analyze(self, surface_form):
        """Return all possible analyses of a surface form."""
        analyses = []

        for stem, category in self.stems.items():
            suffixes = (self.noun_suffixes if category == "N"
                       else self.verb_suffixes)

            for suffix, features in suffixes:
                # Check if stem has alternation for this suffix
                if stem in self.alternations and suffix in self.alternations[stem]:
                    actual_stem = self.alternations[stem][suffix]
                else:
                    actual_stem = stem

                # Check if surface form matches stem + suffix
                candidate = actual_stem + suffix
                if candidate == surface_form:
                    analyses.append(f"{stem}{features}")

        return analyses

    def generate(self, lemma, features):
        """Generate surface form from lemma + features."""
        if lemma not in self.stems:
            return None

        category = self.stems[lemma]
        suffixes = (self.noun_suffixes if category == "N"
                   else self.verb_suffixes)

        for suffix, feat in suffixes:
            if feat == features:
                if lemma in self.alternations and suffix in self.alternations[lemma]:
                    return self.alternations[lemma][suffix] + suffix
                return lemma + suffix

        return None


# Test the analyzer
analyzer = MorphologicalAnalyzer()

print("=== Morphological Analysis (HFST-style) ===")
print("  (surface form -> possible analyses)\n")

test_words = ["cat", "cats", "boxes", "cities", "walking",
              "walked", "jumps", "dogs", "played", "cooking"]

for word in test_words:
    analyses = analyzer.analyze(word)
    if analyses:
        for a in analyses:
            print(f"  {word:12s} -> {a}")
    else:
        print(f"  {word:12s} -> (unknown)")

print("\n=== Morphological Generation ===")
print("  (lemma + features -> surface form)\n")

generations = [
    ("cat", "+N+Sg"), ("cat", "+N+Pl"),
    ("box", "+N+Pl"), ("city", "+N+Pl"),
    ("walk", "+V+Past"), ("walk", "+V+PresPart"),
    ("jump", "+V+3Sg"),
]

for lemma, features in generations:
    surface = analyzer.generate(lemma, features)
    print(f"  {lemma}{features:16s} -> {surface}")

print("\nIn real HFST, the lexicon and rules compile to FSTs.")
print("Composition of lexicon FST and rule FSTs produces the")
print("full analyzer, handling thousands of stems and rules.")

# ---------------------------------------------------------------
# What actual HFST commands would look like:
# ---------------------------------------------------------------
#
# 1. Write the lexicon file (english.lexc):
#    LEXICON Root
#      cat Noun ;
#      dog Noun ;
#      box Noun ;
#      walk Verb ;
#      jump Verb ;
#
#    LEXICON Noun
#      +N+Sg:0  # ;
#      +N+Pl:s  # ;
#
#    LEXICON Verb
#      +V+Inf:0    # ;
#      +V+Past:ed  # ;
#      +V+PresPart:ing # ;
#
# 2. Compile the lexicon to an FST:
#    $ hfst-lexc english.lexc -o english.lexc.hfst
#
# 3. (Optional) Write two-level rules (english.twolc) and compile:
#    $ hfst-twolc english.twolc -o english.twolc.hfst
#
# 4. Compose lexicon with rules:
#    $ hfst-compose-intersect english.lexc.hfst english.twolc.hfst \
#        -o english.hfst
#
# 5. Analyze a surface form:
#    $ echo "cats" | hfst-lookup english.hfst
#    cats	cat+N+Pl	0.000000
#
# 6. Generate a surface form from analysis:
#    $ echo "cat+N+Pl" | hfst-lookup -I english.hfst
#    cat+N+Pl	cats	0.000000</code></pre>
                </div>

                <!-- Code Example 5 -->
                <h3>5. Performance Comparison: FST vs Regex for Pattern Matching</h3>
                <p>
                    Compare the performance of FST-based and regex-based approaches for complex pattern matching with many alternatives.
                </p>

                <div class="code-block">
                    <div class="code-block-header">
                        <span class="code-block-lang">python</span>
                        <button class="code-block-copy">Copy</button>
                    </div>
                    <pre><code class="language-python">import re
import time
import random
import string

# --- Build a large set of replacement rules ---
# Scenario: normalize many abbreviations/misspellings to canonical forms

def generate_rules(n):
    """Generate n random replacement rules."""
    rules = []
    for i in range(n):
        # Random "misspelling" (3-8 chars)
        src_len = random.randint(3, 8)
        src = ''.join(random.choices(string.ascii_lowercase, k=src_len))
        # Random "correction" (3-8 chars)
        tgt_len = random.randint(3, 8)
        tgt = ''.join(random.choices(string.ascii_lowercase, k=tgt_len))
        rules.append((src, tgt))
    return rules

random.seed(42)

print("=== FST vs Regex Performance Comparison ===\n")

for num_rules in [10, 100, 1000]:
    rules = generate_rules(num_rules)

    # --- Build regex replacer ---
    # Combine all rules into one big alternation regex
    start = time.perf_counter()
    pattern = '|'.join(re.escape(src) for src, _ in rules)
    replacement_dict = {src: tgt for src, tgt in rules}
    compiled_regex = re.compile(pattern)

    def regex_replace(text):
        return compiled_regex.sub(
            lambda m: replacement_dict[m.group(0)], text
        )

    regex_build_time = time.perf_counter() - start

    # --- Build FST replacer (simulated without pynini) ---
    # We simulate the FST approach using a trie-based matcher
    # In real pynini: fst = pynini.string_map(rules); cdrewrite(...)
    start = time.perf_counter()

    class TrieNode:
        def __init__(self):
            self.children = {}
            self.output = None

    class TrieFST:
        """Trie-based FST simulator for string replacement."""
        def __init__(self, rules):
            self.root = TrieNode()
            for src, tgt in rules:
                node = self.root
                for ch in src:
                    if ch not in node.children:
                        node.children[ch] = TrieNode()
                    node = node.children[ch]
                node.output = tgt

        def apply(self, text):
            result = []
            i = 0
            while i < len(text):
                node = self.root
                longest_match = None
                longest_end = i
                j = i
                while j < len(text) and text[j] in node.children:
                    node = node.children[text[j]]
                    j += 1
                    if node.output is not None:
                        longest_match = node.output
                        longest_end = j
                if longest_match is not None:
                    result.append(longest_match)
                    i = longest_end
                else:
                    result.append(text[i])
                    i += 1
            return ''.join(result)

    trie_fst = TrieFST(rules)
    fst_build_time = time.perf_counter() - start

    # --- Generate test data ---
    # Create texts that contain some of the source strings
    test_texts = []
    for _ in range(1000):
        # Mix random text with some rule sources
        parts = []
        for _ in range(10):
            if random.random() < 0.3 and rules:
                parts.append(random.choice(rules)[0])
            else:
                parts.append(''.join(
                    random.choices(string.ascii_lowercase, k=5)
                ))
        test_texts.append(' '.join(parts))

    # --- Benchmark regex ---
    start = time.perf_counter()
    for text in test_texts:
        regex_replace(text)
    regex_time = time.perf_counter() - start

    # --- Benchmark FST (trie) ---
    start = time.perf_counter()
    for text in test_texts:
        trie_fst.apply(text)
    fst_time = time.perf_counter() - start

    # --- Results ---
    print(f"  Rules: {num_rules}")
    print(f"    Build time  - Regex: {regex_build_time*1000:8.3f} ms, "
          f"FST: {fst_build_time*1000:8.3f} ms")
    print(f"    Apply time  - Regex: {regex_time*1000:8.1f} ms, "
          f"FST: {fst_time*1000:8.1f} ms "
          f"(1000 texts)")
    speedup = regex_time / fst_time if fst_time > 0 else float('inf')
    print(f"    FST speedup: {speedup:.2f}x")
    print()

    # Verify both produce the same output
    mismatches = 0
    for text in test_texts[:100]:
        r1 = regex_replace(text)
        r2 = trie_fst.apply(text)
        if r1 != r2:
            mismatches += 1
    if mismatches > 0:
        print(f"    WARNING: {mismatches}/100 mismatches "
              f"(expected due to overlapping patterns)")
    print()

print("Note: Real pynini FSTs use compiled C++ and are even faster")
print("than this Python trie simulation. The advantage grows with")
print("more rules and with composition of multiple rule sets.")</code></pre>
                </div>

            </article>

            <!-- ==================== EXERCISES TAB ==================== -->
            <article class="article-content" id="exercises" style="display: none;">
                <h2>Exercises</h2>
                <p>Test your understanding of FST libraries. Exercises cover pynini basics, OpenFST workflows, optimization, debugging, and advanced morphological analysis.</p>

                <div class="exercise-list">

                    <!-- Easy -->
                    <h3 style="margin-top: 1rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">Easy</h3>

                    <!-- Exercise 1 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">1. Install Pynini and Create a Simple FST</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Install pynini and create an FST that maps country codes to country names: "US" &rarr; "United States", "UK" &rarr; "United Kingdom", "DE" &rarr; "Germany", "FR" &rarr; "France", "JP" &rarr; "Japan". Test it on all five inputs and verify the outputs.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <div class="code-block">
                                    <div class="code-block-header">
                                        <span class="code-block-lang">python</span>
                                        <button class="code-block-copy">Copy</button>
                                    </div>
                                    <pre><code class="language-python"># pip install pynini
import pynini
from pynini.lib import rewrite

# Define the mapping as a list of (input, output) pairs
country_codes = [
    ("US", "United States"),
    ("UK", "United Kingdom"),
    ("DE", "Germany"),
    ("FR", "France"),
    ("JP", "Japan"),
]

# Create the FST using string_map
country_fst = pynini.string_map(country_codes)

# Optimize (determinize + minimize)
country_fst.optimize()

# Test on all inputs
print("Country Code FST")
print("=" * 40)
for code, expected in country_codes:
    result = rewrite.one_top_rewrite(code, country_fst)
    status = "OK" if result == expected else "FAIL"
    print(f"  [{status}] '{code}' -> '{result}'")

# Test on invalid input
try:
    result = rewrite.one_top_rewrite("XX", country_fst)
    print(f"  'XX' -> '{result}'")
except rewrite.Error:
    print(f"  'XX' -> (no match, as expected)")

# Print FST statistics
print(f"\nFST has {country_fst.num_states()} states")</code></pre>
                                </div>
                                <p>The <code>string_map</code> function creates a union of simple transducers, one for each pair. After optimization, pynini merges shared prefixes (e.g., "U" is shared between "US" and "UK") to minimize the number of states.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 2 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">2. Write an FST for Currency Symbols</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Create an FST that normalizes currency symbols to their full names: "$" &rarr; "dollar", "EUR" &rarr; "euro", "GBP" &rarr; "pound", "JPY" &rarr; "yen", "CNY" &rarr; "yuan". Then wrap it with <code>cdrewrite</code> so it works inside sentences like "The price is $50" &rarr; "The price is dollar50".</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <div class="code-block">
                                    <div class="code-block-header">
                                        <span class="code-block-lang">python</span>
                                        <button class="code-block-copy">Copy</button>
                                    </div>
                                    <pre><code class="language-python">import pynini
from pynini.lib import rewrite

# Currency symbol mapping
currencies = [
    ("$", "dollar"),
    ("EUR", "euro"),
    ("GBP", "pound"),
    ("JPY", "yen"),
    ("CNY", "yuan"),
]

currency_fst = pynini.string_map(currencies)

# Build sigma_star (closure over all printable ASCII)
sigma_star = pynini.union(
    *[pynini.acceptor(chr(i)) for i in range(32, 127)]
).closure().optimize()

# Context-dependent rewrite: apply currency_fst
# anywhere in a string, leaving the rest unchanged
currency_rule = pynini.cdrewrite(
    currency_fst, "", "", sigma_star
).optimize()

# Test standalone
print("=== Standalone Currency FST ===")
for sym, expected in currencies:
    result = rewrite.one_top_rewrite(sym, currency_fst)
    print(f"  '{sym}' -> '{result}'")

# Test in-context
print("\n=== In-Context Currency Normalization ===")
test_sentences = [
    "The price is $50",
    "Convert EUR to GBP",
    "JPY is weakening against CNY",
]
for sentence in test_sentences:
    result = rewrite.one_top_rewrite(sentence, currency_rule)
    print(f"  '{sentence}'")
    print(f"  -> '{result}'")</code></pre>
                                </div>
                                <p>The <code>cdrewrite</code> (context-dependent rewrite) function creates an FST that applies the transduction rule wherever it matches within a longer string, passing all other characters through unchanged. The empty strings <code>""</code> for left and right context mean "apply in any context".</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 3 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">3. Test an FST on Edge Cases</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Given the abbreviation expansion FST from Code Example 1, write a test suite that checks at least 10 edge cases. Include: empty string, partial matches ("Dr" without period), overlapping patterns, repeated abbreviations, mixed case, and unknown abbreviations. Document which cases pass and which fail.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <div class="code-block">
                                    <div class="code-block-header">
                                        <span class="code-block-lang">python</span>
                                        <button class="code-block-copy">Copy</button>
                                    </div>
                                    <pre><code class="language-python">import pynini
from pynini.lib import rewrite

# Build the abbreviation FST (same as Code Example 1)
abbreviations = [
    ("Dr.", "Doctor"),
    ("Mr.", "Mister"),
    ("Mrs.", "Missus"),
    ("St.", "Saint"),
    ("Ave.", "Avenue"),
]
abbrev_fst = pynini.string_map(abbreviations)

sigma_star = pynini.union(
    *[pynini.acceptor(chr(i)) for i in range(32, 127)]
).closure().optimize()
abbrev_rule = pynini.cdrewrite(
    abbrev_fst, "", "", sigma_star
).optimize()

# Edge case test suite
test_cases = [
    # (input, expected_behavior, description)
    ("", "", "Empty string"),
    ("Dr.", "Doctor", "Standalone abbreviation"),
    ("Dr", None, "Missing period - should NOT match"),
    ("dr.", None, "Lowercase - should NOT match"),
    ("Dr. Dr.", "Doctor Doctor",
     "Repeated abbreviation"),
    ("Dr.Smith", "DoctorSmith",
     "No space after abbreviation"),
    ("XDr.X", "XDoctorX",
     "Abbreviation embedded in text"),
    ("Hello world", "Hello world",
     "No abbreviations present"),
    ("Dr. and Mrs. Smith on St. Ave.",
     "Doctor and Missus Smith on Saint Avenue",
     "Multiple different abbreviations"),
    ("PhD.", "PhD.",
     "Unknown abbreviation - pass through"),
    ("   Dr.   ", "   Doctor   ",
     "Extra whitespace"),
    ("Dr.Mr.", "DoctorMister",
     "Adjacent abbreviations, no space"),
]

print("=== Edge Case Test Suite ===\n")
passed = 0
failed = 0
for inp, expected, desc in test_cases:
    try:
        result = rewrite.one_top_rewrite(inp, abbrev_rule)
    except rewrite.Error:
        result = "(rewrite error)"

    # For cases where we expect no match, the result
    # should be the input unchanged
    if expected is None:
        expected = inp

    status = "PASS" if result == expected else "FAIL"
    if status == "PASS":
        passed += 1
    else:
        failed += 1

    print(f"  [{status}] {desc}")
    print(f"         Input:    '{inp}'")
    print(f"         Expected: '{expected}'")
    if result != expected:
        print(f"         Got:      '{result}'")
    print()

print(f"Results: {passed} passed, {failed} failed")</code></pre>
                                </div>
                                <p>Key insight: case sensitivity, missing punctuation, and adjacent patterns are common sources of bugs. The <code>cdrewrite</code> approach handles most of these correctly, but case-insensitive matching requires additional rules (e.g., a separate lowercase-to-uppercase normalizer composed before the abbreviation FST).</p>
                            </div>
                        </div>
                    </div>

                    <!-- Medium -->
                    <h3 style="margin-top: 2rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">Medium</h3>

                    <!-- Exercise 4 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">4. Build a Text Normalizer for Dates, Times, and Numbers</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Using pynini, build a composed text normalizer that handles three types of normalization:</p>
                            <ul>
                                <li>Dates: "1/15" &rarr; "January fifteenth"</li>
                                <li>Times: "3:00" &rarr; "three o'clock", "3:30" &rarr; "three thirty"</li>
                                <li>Small numbers: "1" through "12" &rarr; words</li>
                            </ul>
                            <p>Compose them into a single normalizer FST using the <code>@</code> operator.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <div class="code-block">
                                    <div class="code-block-header">
                                        <span class="code-block-lang">python</span>
                                        <button class="code-block-copy">Copy</button>
                                    </div>
                                    <pre><code class="language-python">import pynini
from pynini.lib import rewrite

# Sigma star for context-dependent rewrite
sigma_star = pynini.union(
    *[pynini.acceptor(chr(i)) for i in range(32, 127)]
).closure().optimize()

# --- Number words (1-12) ---
number_words = {
    "1": "one", "2": "two", "3": "three",
    "4": "four", "5": "five", "6": "six",
    "7": "seven", "8": "eight", "9": "nine",
    "10": "ten", "11": "eleven", "12": "twelve",
}

# --- Month names ---
months = {
    "1": "January", "2": "February", "3": "March",
    "4": "April", "5": "May", "6": "June",
    "7": "July", "8": "August", "9": "September",
    "10": "October", "11": "November", "12": "December",
}

# --- Ordinals for days ---
ordinals = {
    "1": "first", "2": "second", "3": "third",
    "4": "fourth", "5": "fifth", "6": "sixth",
    "7": "seventh", "8": "eighth", "9": "ninth",
    "10": "tenth", "11": "eleventh", "12": "twelfth",
    "13": "thirteenth", "14": "fourteenth",
    "15": "fifteenth", "16": "sixteenth",
    "17": "seventeenth", "18": "eighteenth",
    "19": "nineteenth", "20": "twentieth",
    "21": "twenty-first", "22": "twenty-second",
    "23": "twenty-third", "24": "twenty-fourth",
    "25": "twenty-fifth", "26": "twenty-sixth",
    "27": "twenty-seventh", "28": "twenty-eighth",
    "29": "twenty-ninth", "30": "thirtieth",
    "31": "thirty-first",
}

# --- Date FST: M/D -> Month Dth ---
date_pairs = []
for m_num, m_name in months.items():
    for d_num, d_ord in ordinals.items():
        date_pairs.append(
            (f"{m_num}/{d_num}", f"{m_name} {d_ord}")
        )
date_fst = pynini.string_map(date_pairs).optimize()

# --- Time FST: H:MM -> word form ---
time_pairs = []
for h in range(1, 13):
    h_word = number_words[str(h)]
    time_pairs.append((f"{h}:00", f"{h_word} o'clock"))
    # Add :30 for half past
    time_pairs.append((f"{h}:30", f"{h_word} thirty"))
time_fst = pynini.string_map(time_pairs).optimize()

# --- Number FST: standalone digits -> words ---
number_fst = pynini.string_map(
    list(number_words.items())
).optimize()

# Wrap each in cdrewrite for in-context application
date_rule = pynini.cdrewrite(
    date_fst, "", "", sigma_star
).optimize()
time_rule = pynini.cdrewrite(
    time_fst, "", "", sigma_star
).optimize()

# Compose: apply date rules first, then time rules
normalizer = (date_rule @ time_rule).optimize()

# Test
print("=== Composed Text Normalizer ===")
tests = [
    "Meeting on 1/15 at 3:00",
    "Birthday: 12/25",
    "Alarm set for 7:30",
    "Event on 6/4 from 9:00 to 11:30",
]
for text in tests:
    result = rewrite.one_top_rewrite(text, normalizer)
    print(f"  '{text}'")
    print(f"  -> '{result}'\n")</code></pre>
                                </div>
                                <p>The key insight is that <strong>composition order matters</strong>. We apply date normalization first (which consumes "/" characters that might otherwise confuse later rules), then time normalization. Each rule is a <code>cdrewrite</code> that operates in context, and the <code>@</code> operator composes them into a single FST.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 5 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">5. Optimize an FST and Measure State Reduction</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Build a large FST (union of at least 100 string transductions), then measure the number of states before and after each optimization step: (a) epsilon removal, (b) determinization, (c) minimization. Report the state count at each stage.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <div class="code-block">
                                    <div class="code-block-header">
                                        <span class="code-block-lang">python</span>
                                        <button class="code-block-copy">Copy</button>
                                    </div>
                                    <pre><code class="language-python">import pynini
import random
import string
import time

random.seed(42)

# Generate 200 random string-to-string rules
rules = []
for i in range(200):
    src_len = random.randint(3, 6)
    tgt_len = random.randint(3, 8)
    src = ''.join(random.choices(string.ascii_lowercase, k=src_len))
    tgt = ''.join(random.choices(string.ascii_lowercase, k=tgt_len))
    rules.append((src, tgt))

# Build the FST as a union of transducers
fst = pynini.string_map(rules)

print("=== FST Optimization: State Count at Each Stage ===\n")
print(f"  Original (after string_map union):")
print(f"    States: {fst.num_states()}")
print(f"    Arcs:   {sum(fst.num_arcs(s) for s in range(fst.num_states()))}")

# Step 1: Epsilon removal
start = time.perf_counter()
fst_no_eps = pynini.epsnormalize(fst)
eps_time = time.perf_counter() - start
print(f"\n  After epsilon removal ({eps_time*1000:.1f} ms):")
print(f"    States: {fst_no_eps.num_states()}")
print(f"    Arcs:   {sum(fst_no_eps.num_arcs(s) for s in range(fst_no_eps.num_states()))}")

# Step 2: Determinization
# Note: determinization of transducers requires encoding
# input:output pairs as single labels first
start = time.perf_counter()
# Use pynini's encode/decode for transducer determinization
encoder = pynini.EncodeMapper("standard", encode_labels=True)
fst_encoded = pynini.Fst(fst_no_eps)
fst_encoded.encode(encoder)
fst_det = pynini.determinize(fst_encoded)
fst_det.decode(encoder)
det_time = time.perf_counter() - start
print(f"\n  After determinization ({det_time*1000:.1f} ms):")
print(f"    States: {fst_det.num_states()}")
print(f"    Arcs:   {sum(fst_det.num_arcs(s) for s in range(fst_det.num_states()))}")

# Step 3: Minimization
start = time.perf_counter()
fst_min = pynini.Fst(fst_det)
encoder2 = pynini.EncodeMapper("standard", encode_labels=True)
fst_min.encode(encoder2)
fst_min = pynini.minimize(fst_min)
fst_min.decode(encoder2)
min_time = time.perf_counter() - start
print(f"\n  After minimization ({min_time*1000:.1f} ms):")
print(f"    States: {fst_min.num_states()}")
print(f"    Arcs:   {sum(fst_min.num_arcs(s) for s in range(fst_min.num_states()))}")

# Compare with .optimize() (does all steps at once)
fst_opt = pynini.string_map(rules)
start = time.perf_counter()
fst_opt.optimize()
opt_time = time.perf_counter() - start
print(f"\n  Using .optimize() directly ({opt_time*1000:.1f} ms):")
print(f"    States: {fst_opt.num_states()}")
print(f"    Arcs:   {sum(fst_opt.num_arcs(s) for s in range(fst_opt.num_states()))}")

# Summary
orig_states = fst.num_states()
final_states = fst_opt.num_states()
reduction = (1 - final_states / orig_states) * 100
print(f"\n  Total state reduction: {orig_states} -> "
      f"{final_states} ({reduction:.1f}% reduction)")</code></pre>
                                </div>
                                <p>Typical results: the initial union FST has many states (roughly the sum of all string lengths). Epsilon removal does not reduce states much but simplifies the structure. Determinization can increase or decrease states depending on shared prefixes. Minimization then merges equivalent states, often achieving 50-80% total reduction. The <code>.optimize()</code> method does all three steps plus arc sorting.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 6 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">6. Debug Non-Determinism in an FST</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Build a pynini FST with intentional non-determinism: create overlapping rules like "Dr." &rarr; "Doctor" and "Dr." &rarr; "Drive". Verify that the FST is non-deterministic (produces multiple outputs). Then determinize it and observe what happens. Which output wins and why?</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <div class="code-block">
                                    <div class="code-block-header">
                                        <span class="code-block-lang">python</span>
                                        <button class="code-block-copy">Copy</button>
                                    </div>
                                    <pre><code class="language-python">import pynini
from pynini.lib import rewrite

# Build an intentionally non-deterministic FST
# Two rules with the same input but different outputs
rule1 = pynini.transducer("Dr.", "Doctor")
rule2 = pynini.transducer("Dr.", "Drive")

# Union creates non-determinism: both paths exist
ambiguous_fst = pynini.union(rule1, rule2)

print("=== Debugging Non-Determinism ===\n")
print(f"  Ambiguous FST states: {ambiguous_fst.num_states()}")

# Show all arcs to see the non-determinism
print(f"\n  Arc structure (showing non-determinism):")
for state in range(ambiguous_fst.num_states()):
    for arc in ambiguous_fst.arcs(state):
        print(f"    State {state} -> State {arc.nextstate}: "
              f"in={arc.ilabel}, out={arc.olabel}")

# Try to get all outputs
# pynini.shortestpath with nshortest > 1 returns multiple
paths = pynini.shortestpath(ambiguous_fst, nshortest=5)
print(f"\n  All paths through the FST:")
for i, path_str in enumerate(paths.paths().ostrings()):
    print(f"    Path {i+1}: '{path_str}'")

# Now determinize
# For a transducer, we need to encode first
print(f"\n  After determinization:")
det_fst = ambiguous_fst.copy()
det_fst.optimize()

# With optimization, one path is chosen
try:
    result = rewrite.one_top_rewrite("Dr.", det_fst)
    print(f"    'Dr.' -> '{result}'")
except rewrite.Error as e:
    print(f"    Rewrite error: {e}")

# The "winner" depends on path weights and ordering.
# In tropical semiring, both paths have weight 0,
# so the first one found (by arc order) wins.
# To control priority, add weights:
print(f"\n  With explicit weights:")
rule1_weighted = pynini.transducer("Dr.", "Doctor")
# Add weight to make rule2 less preferred
rule2_weighted = pynini.transducer("Dr.", "Drive")
# In tropical semiring, LOWER weight = HIGHER priority
# We can't easily add weights to string transducers,
# but we can construct arcs manually or use string_map
# with a column for weights.

# Practical solution: use ordering in string_map
# The first matching rule takes priority after optimization
ordered_fst = pynini.string_map([
    ("Dr.", "Doctor"),   # Priority 1 (lower weight)
    ("Dr.", "Drive"),    # Priority 2 (higher weight)
])
ordered_fst.optimize()

result = rewrite.one_top_rewrite("Dr.", ordered_fst)
print(f"    'Dr.' -> '{result}' (first rule wins)")

print(f"\nLesson: Always check for unintended non-determinism.")
print(f"Use .optimize() to resolve it, or add weights to")
print(f"control which output is preferred.")</code></pre>
                                </div>
                                <p>Non-determinism in FSTs means multiple valid outputs for the same input. After determinization (which pynini's <code>.optimize()</code> performs), one output is selected. In the tropical semiring, the path with the lowest total weight wins. When weights are equal, the result depends on implementation-specific ordering. To guarantee a specific preference, assign explicit weights to rules.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 7 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">7. Port a Pynini FST to OpenFST CLI</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Take the abbreviation expansion FST from Code Example 1 and write the equivalent OpenFST text format. Then write the shell commands to compile it, compose it with an input acceptor, and find the shortest path. You do not need to actually run the commands &mdash; just write the text FST file and the shell pipeline.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <div class="code-block">
                                    <div class="code-block-header">
                                        <span class="code-block-lang">python</span>
                                        <button class="code-block-copy">Copy</button>
                                    </div>
                                    <pre><code class="language-python"># This script generates all the files needed for an OpenFST
# command-line workflow. Run it to create symbols.txt and
# abbrev.txt, then use the shell commands at the end.

import os

output_dir = "/tmp/openfst_exercise"
os.makedirs(output_dir, exist_ok=True)

# --- Step 1: Generate the symbol table ---
# OpenFST requires a symbol table mapping characters to integer IDs.
# Both input and output sides use the same table here.

abbreviations = [
    ("Dr.", "Doctor"),
    ("Mr.", "Mister"),
    ("Mrs.", "Missus"),
    ("St.", "Saint"),
]

# Collect all unique characters from both sides
all_chars = set()
for src, tgt in abbreviations:
    for c in src:
        all_chars.add(c)
    for c in tgt:
        all_chars.add(c)

all_chars = sorted(all_chars)

# Write symbols.txt
# Format: symbol integer_id (one per line)
# <eps> must always be ID 0
sym_path = os.path.join(output_dir, "symbols.txt")
sym_lines = ["<eps> 0"]
char_to_id = {"<eps>": 0}
for i, ch in enumerate(all_chars, start=1):
    # OpenFST symbol names cannot contain whitespace;
    # use readable names for special characters
    if ch == '.':
        label = "<dot>"
    elif ch == ' ':
        label = "<space>"
    else:
        label = ch
    sym_lines.append(f"{label} {i}")
    char_to_id[ch] = i

with open(sym_path, "w") as f:
    f.write("\n".join(sym_lines) + "\n")
print(f"=== {sym_path} ===")
print("\n".join(sym_lines))

# --- Step 2: Generate the FST text format ---
# Format per arc: src_state dst_state input_label output_label [weight]
# Final states: state_id [final_weight]

fst_path = os.path.join(output_dir, "abbrev.txt")
fst_lines = []
state_id = 1  # 0 is the start state

for src, tgt in abbreviations:
    max_len = max(len(src), len(tgt))
    current_state = 0

    for i in range(max_len):
        next_state = state_id
        # Use <eps> for positions beyond the end of the shorter string
        in_ch = src[i] if i < len(src) else None
        out_ch = tgt[i] if i < len(tgt) else None
        in_sym = "<eps>" if in_ch is None else ("<dot>" if in_ch == '.' else in_ch)
        out_sym = "<eps>" if out_ch is None else ("<dot>" if out_ch == '.' else out_ch)
        fst_lines.append(f"{current_state} {next_state} {in_sym} {out_sym}")
        current_state = next_state
        state_id += 1

    # Mark as final state (weight 0 = tropical semiring default)
    fst_lines.append(f"{current_state}")

with open(fst_path, "w") as f:
    f.write("\n".join(fst_lines) + "\n")
print(f"\n=== {fst_path} ===")
print("\n".join(fst_lines))

# --- Step 3: Generate an input acceptor for "Dr." ---
input_path = os.path.join(output_dir, "input_dr.txt")
input_lines = [
    "0 1 D D",
    "1 2 r r",
    "2 3 <dot> <dot>",
    "3",
]
with open(input_path, "w") as f:
    f.write("\n".join(input_lines) + "\n")
print(f"\n=== {input_path} ===")
print("\n".join(input_lines))

# --- Step 4: Print the executable shell pipeline ---
print(f"\n=== Shell Commands (run from {output_dir}) ===")
commands = f"""
cd {output_dir}

# 1. Compile the transducer FST from text to binary
fstcompile --isymbols=symbols.txt --osymbols=symbols.txt \\
    abbrev.txt > abbrev.fst

# 2. Compile the input acceptor for "Dr."
fstcompile --isymbols=symbols.txt --osymbols=symbols.txt \\
    input_dr.txt > input_dr.fst

# 3. Compose the input with the transducer
fstcompose input_dr.fst abbrev.fst > composed.fst

# 4. Find the shortest (best) path
fstshortestpath composed.fst > best.fst

# 5. Print the result in human-readable form
fstprint --isymbols=symbols.txt --osymbols=symbols.txt best.fst
# Expected output shows arcs: D:D r:o <dot>:c <eps>:t <eps>:o <eps>:r

# 6. (Optional) Visualize the full transducer (requires Graphviz)
fstdraw --isymbols=symbols.txt --osymbols=symbols.txt \\
    abbrev.fst | dot -Tpng -o abbrev.png
""".strip()
print(commands)

print(f"\nAll files written to {output_dir}/")
print("Install OpenFST to run: apt-get install libfst-tools")</code></pre>
                                </div>
                                <p>The OpenFST text format represents each arc as a line: <code>src_state dst_state input_label output_label [weight]</code>. Final states appear as lines with just the state number. The shell pipeline compiles text to binary, composes with input, finds the shortest path, and prints the result. This is the standard OpenFST workflow used in Kaldi and other speech systems.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Hard -->
                    <h3 style="margin-top: 2rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">Hard</h3>

                    <!-- Exercise 8 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">8. Design a Phone Number Parser FST</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Build a pynini FST that parses US phone numbers in multiple formats and normalizes them:</p>
                            <ul>
                                <li>"(555) 123-4567" &rarr; "555-123-4567"</li>
                                <li>"555.123.4567" &rarr; "555-123-4567"</li>
                                <li>"5551234567" &rarr; "555-123-4567"</li>
                                <li>"1-555-123-4567" &rarr; "555-123-4567"</li>
                            </ul>
                            <p>Handle the digit groups (3-3-4 pattern) and strip the leading "1-" country code.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <div class="code-block">
                                    <div class="code-block-header">
                                        <span class="code-block-lang">python</span>
                                        <button class="code-block-copy">Copy</button>
                                    </div>
                                    <pre><code class="language-python">import pynini
from pynini.lib import rewrite

# Define digit acceptor
digit = pynini.union(*[pynini.acceptor(str(d)) for d in range(10)])

# 3-digit group and 4-digit group (pass through digits)
d3 = digit + digit + digit  # Matches exactly 3 digits
d4 = digit + digit + digit + digit  # Matches exactly 4 digits

# --- Format 1: (XXX) XXX-XXXX ---
fmt1 = (
    pynini.transducer("(", "")   # consume "("
    + d3                          # 3 digits (area code)
    + pynini.transducer(") ", "-")# ") " -> "-"
    + d3                          # 3 digits
    + pynini.acceptor("-")        # pass through "-"
    + d4                          # 4 digits
)

# --- Format 2: XXX.XXX.XXXX ---
fmt2 = (
    d3
    + pynini.transducer(".", "-")  # "." -> "-"
    + d3
    + pynini.transducer(".", "-")  # "." -> "-"
    + d4
)

# --- Format 3: XXXXXXXXXX (10 digits, no separators) ---
# Need to insert dashes after positions 3 and 6
# This requires buffering, which we handle by
# explicitly reading 3 + 3 + 4 digits and inserting dashes
fmt3_parts = []
for i in range(3):
    fmt3_parts.append(digit)
fmt3_parts.append(pynini.transducer("", "-"))  # Insert dash
for i in range(3):
    fmt3_parts.append(digit)
fmt3_parts.append(pynini.transducer("", "-"))  # Insert dash
for i in range(4):
    fmt3_parts.append(digit)

fmt3 = fmt3_parts[0]
for part in fmt3_parts[1:]:
    fmt3 = fmt3 + part

# --- Format 4: 1-XXX-XXX-XXXX (with country code) ---
fmt4 = (
    pynini.transducer("1-", "")  # Strip country code
    + d3
    + pynini.acceptor("-")       # pass through "-"
    + d3
    + pynini.acceptor("-")       # pass through "-"
    + d4
)

# Union of all formats
phone_parser = pynini.union(fmt1, fmt2, fmt3, fmt4).optimize()

# Test
print("=== Phone Number Parser FST ===\n")
test_numbers = [
    "(555) 123-4567",
    "555.123.4567",
    "5551234567",
    "1-555-123-4567",
    "(800) 555-0100",
    "800.555.0100",
    "1-800-555-0100",
]

for phone in test_numbers:
    try:
        result = rewrite.one_top_rewrite(phone, phone_parser)
        print(f"  '{phone}'")
        print(f"  -> '{result}'\n")
    except rewrite.Error:
        print(f"  '{phone}'")
        print(f"  -> (no match)\n")

# Statistics
print(f"FST states: {phone_parser.num_states()}")
print(f"FST arcs: {sum(phone_parser.num_arcs(s) for s in range(phone_parser.num_states()))}")</code></pre>
                                </div>
                                <p>The key challenge is handling the no-separator format (Format 3), where we must insert dashes at specific positions. In an FST, this is done by using epsilon output (no output character) for the input digits, then emitting the dash as epsilon input (no input character). The union of all four format FSTs creates a single parser that handles any format. After optimization, shared digit-reading states are merged.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 9 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">9. Build an Email Address Validator FST</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Build an FST that validates and normalizes email addresses. The FST should:</p>
                            <ul>
                                <li>Accept: letters, digits, dots, hyphens, underscores in local part</li>
                                <li>Require exactly one "@" symbol</li>
                                <li>Accept: letters, digits, dots, hyphens in domain part</li>
                                <li>Require at least one dot in the domain (for TLD)</li>
                                <li>Normalize: convert the entire email to lowercase</li>
                            </ul>
                            <p>Output the validated, lowercased email or reject invalid inputs.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <div class="code-block">
                                    <div class="code-block-header">
                                        <span class="code-block-lang">python</span>
                                        <button class="code-block-copy">Copy</button>
                                    </div>
                                    <pre><code class="language-python">import pynini

# --- Character class definitions ---
lower = pynini.union(
    *[pynini.acceptor(chr(c)) for c in range(ord('a'), ord('z')+1)]
)
upper_to_lower = pynini.union(
    *[pynini.transducer(chr(c), chr(c+32))
      for c in range(ord('A'), ord('Z')+1)]
)
letter = pynini.union(lower, upper_to_lower)

digit = pynini.union(
    *[pynini.acceptor(str(d)) for d in range(10)]
)

# Local part characters: letter, digit, '.', '-', '_'
local_char = pynini.union(
    letter, digit,
    pynini.acceptor("."),
    pynini.acceptor("-"),
    pynini.acceptor("_"),
)

# Domain characters: letter, digit, '-'
domain_char = pynini.union(letter, digit, pynini.acceptor("-"))

# Domain label: one or more domain characters
domain_label = domain_char.closure(1)

# --- Email structure ---
# local_part @ domain_label . domain_label (. domain_label)*
# At minimum: x@y.z

local_part = local_char.closure(1)  # 1+ local chars
at_sign = pynini.acceptor("@")
dot = pynini.acceptor(".")

# Domain: at least two labels separated by dots
# e.g., "gmail.com" or "mail.co.uk"
domain = (
    domain_label + dot + domain_label
    + (dot + domain_label).closure(0)  # optional extra dots
)

# Full email FST
email_fst = (local_part + at_sign + domain).optimize()

# Test
print("=== Email Validator FST ===\n")
test_emails = [
    "user@gmail.com",
    "John.Doe@Company.ORG",
    "test_123@sub.domain.co.uk",
    "admin@localhost",        # No TLD dot -> should fail
    "user@.com",             # Empty domain label
    "@gmail.com",            # Empty local part
    "user@gmail.com.",       # Trailing dot
    "simple-name@example.com",
    "UPPER@CASE.COM",
]

for email in test_emails:
    try:
        # Create an acceptor for the input
        inp = pynini.acceptor(email)
        # Compose with the email FST
        result_fst = pynini.compose(inp, email_fst)
        # Get the output string
        result_fst.optimize()
        if result_fst.num_states() == 0:
            print(f"  REJECT: '{email}'")
        else:
            # Extract output string
            output = result_fst.string()
            print(f"  ACCEPT: '{email}' -> '{output}'")
    except Exception as e:
        print(f"  REJECT: '{email}' ({type(e).__name__})")

print("\nNote: This FST simultaneously validates structure")
print("and normalizes case in a single pass.")</code></pre>
                                </div>
                                <p>The email validator FST combines validation and normalization. The <code>upper_to_lower</code> transducer maps each uppercase letter to its lowercase equivalent, so any uppercase input is simultaneously validated and lowercased. Invalid emails (missing @, no TLD dot, empty parts) are rejected because no valid path exists through the FST. This is more robust than regex because the FST can be composed with other normalizers.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 10 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">10. Implement Two-Level Morphology Rules</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Implement a simplified two-level morphology system for English past tense formation using pynini. Handle these rules:</p>
                            <ul>
                                <li>Default: add "-ed" (walk &rarr; walked)</li>
                                <li>Silent-e: just add "-d" (bake &rarr; baked, not bakeed)</li>
                                <li>Consonant doubling: double final consonant before "-ed" (stop &rarr; stopped)</li>
                                <li>y-to-i: change "y" to "ied" (carry &rarr; carried)</li>
                            </ul>
                            <p>Build separate rule FSTs and compose them with a lexicon FST. Test on at least 12 verbs.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <div class="code-block">
                                    <div class="code-block-header">
                                        <span class="code-block-lang">python</span>
                                        <button class="code-block-copy">Copy</button>
                                    </div>
                                    <pre><code class="language-python">import pynini
from pynini.lib import rewrite

# --- Approach: explicit lexicon with rule categories ---
# In true two-level morphology, rules would be parallel
# constraints. Here we approximate with composed FSTs.

# Lexicon: (verb, rule_category)
lexicon = {
    # Default -ed
    "walk": "default", "jump": "default",
    "talk": "default", "cook": "default",
    "play": "default", "rain": "default",
    # Silent-e: just add -d
    "bake": "silent_e", "love": "silent_e",
    "hope": "silent_e", "dance": "silent_e",
    # Consonant doubling: CVC pattern
    "stop": "double", "plan": "double",
    "drop": "double", "rob": "double",
    # Y-to-I: consonant + y -> consonant + ied
    "carry": "y_to_i", "worry": "y_to_i",
    "hurry": "y_to_i", "study": "y_to_i",
}

# Build past tense forms using rules
past_tense_pairs = []

for verb, rule in lexicon.items():
    if rule == "default":
        past = verb + "ed"
    elif rule == "silent_e":
        # Remove final 'e', add 'ed'
        past = verb[:-1] + "ed"  # bake -> bak + ed = baked
    elif rule == "double":
        # Double final consonant, add 'ed'
        past = verb + verb[-1] + "ed"  # stop -> stopp + ed
    elif rule == "y_to_i":
        # Replace final 'y' with 'ied'
        past = verb[:-1] + "ied"  # carry -> carr + ied
    else:
        past = verb + "ed"

    past_tense_pairs.append((verb, past))

# Build the FST from the lexicon
past_tense_fst = pynini.string_map(past_tense_pairs).optimize()

print("=== Two-Level Morphology: English Past Tense ===\n")
print("  Verb          -> Past Tense     Rule")
print("  " + "-" * 50)

for verb, rule in sorted(lexicon.items()):
    try:
        result = rewrite.one_top_rewrite(verb, past_tense_fst)
        print(f"  {verb:14s} -> {result:14s} ({rule})")
    except rewrite.Error:
        print(f"  {verb:14s} -> (error)")

# --- Now build it as composed rule FSTs ---
# This is closer to the true two-level approach
print("\n=== Composed Rule FST Approach ===\n")

# Step 1: Lexicon FST maps verb to verb+PAST tag
lexicon_pairs = [(v, v + "+PAST") for v in lexicon.keys()]
lexicon_fst = pynini.string_map(lexicon_pairs).optimize()

# Step 2: Rule FSTs transform verb+PAST to surface form
# We build specific rules for each pattern

# For the composed approach, we map verb+PAST -> past form
# This requires knowing the verb stem and applying the rule
composed_pairs = []
for verb, rule in lexicon.items():
    if rule == "default":
        composed_pairs.append((verb + "+PAST", verb + "ed"))
    elif rule == "silent_e":
        composed_pairs.append((verb + "+PAST", verb[:-1] + "ed"))
    elif rule == "double":
        composed_pairs.append(
            (verb + "+PAST", verb + verb[-1] + "ed")
        )
    elif rule == "y_to_i":
        composed_pairs.append(
            (verb + "+PAST", verb[:-1] + "ied")
        )

rule_fst = pynini.string_map(composed_pairs).optimize()

# Compose: lexicon @ rules
full_pipeline = (lexicon_fst @ rule_fst).optimize()

print("  Testing composed pipeline (lexicon @ rules):")
for verb in sorted(lexicon.keys()):
    try:
        result = rewrite.one_top_rewrite(verb, full_pipeline)
        print(f"    {verb:14s} -> {result}")
    except rewrite.Error:
        print(f"    {verb:14s} -> (error)")

# Statistics
print(f"\n  Lexicon FST: {lexicon_fst.num_states()} states")
print(f"  Rule FST:    {rule_fst.num_states()} states")
print(f"  Composed:    {full_pipeline.num_states()} states")

print("\nIn real HFST two-level morphology, the rules would be")
print("general patterns (e.g., 'double final C in CVC context')")
print("rather than verb-specific entries. The rules are compiled")
print("to FSTs that apply to ANY matching stem, not just the ones")
print("in the lexicon.")</code></pre>
                                </div>
                                <p>This exercise demonstrates the two key approaches to FST morphology: (1) a flat lookup table mapping verbs directly to past tenses, and (2) a composed pipeline where the lexicon FST adds morphological tags and the rule FST transforms tagged forms to surface forms. The composed approach is more modular and closer to HFST's architecture. In a real system, the rules would be general phonological constraints (e.g., "double the final consonant of a CVC stem before a vowel-initial suffix") rather than verb-specific entries.</p>
                            </div>
                        </div>
                    </div>

                </div>
            </article>

        </main>

        <!-- TOC (Right Side) -->
        <aside class="toc-container">
            <h4 class="toc-title">Contents</h4>
            <nav class="toc-list">
                <a href="#library-landscape" class="toc-link">FST Library Landscape</a>
                <a href="#pynini-basics" class="toc-link">Pynini Basics</a>
                <a href="#openfst-core" class="toc-link">OpenFST Core Concepts</a>
                <a href="#hfst-linguistics" class="toc-link">HFST for Linguistics</a>
                <a href="#practical-workflow" class="toc-link">Practical Workflow</a>
                <a href="#performance" class="toc-link">Performance Considerations</a>
                <a href="#summary" class="toc-link">Summary</a>
            </nav>
        </aside>
    </div>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-brand">
                    <span class="logo-symbol">&nabla;</span>
                    <span>ML Fundamentals</span>
                </div>
                <p class="footer-tagline">Deep understanding through first principles.</p>
            </div>
            <div class="footer-links">
                <a href="../../../index.html">Home</a>
                <a href="https://github.com/ml-entropy/ml-entropy.github.io" target="_blank">GitHub</a>
            </div>
        </div>
    </footer>

    <!-- Scripts -->
    <script src="../../../js/main.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // KaTeX Rendering
            if (typeof renderMathInElement === 'function') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }

            // Tab Switching Logic
            const tabs = document.querySelectorAll('.tutorial-tab');
            const articles = document.querySelectorAll('.article-content');

            function switchTab(targetId) {
                if (!targetId || targetId === '#') targetId = '#theory';

                tabs.forEach(tab => {
                    if (tab.getAttribute('href') === targetId) {
                        tab.classList.add('active');
                    } else {
                        tab.classList.remove('active');
                    }
                });

                articles.forEach(article => {
                    const articleId = '#' + article.id;
                    if (articleId === targetId) {
                        article.style.display = 'block';
                    } else {
                        article.style.display = 'none';
                    }
                });

                if (typeof renderMathInElement === 'function') {
                    renderMathInElement(document.body, {
                        delimiters: [
                            {left: '$$', right: '$$', display: true},
                            {left: '$', right: '$', display: false},
                            {left: '\\[', right: '\\]', display: true},
                            {left: '\\(', right: '\\)', display: false}
                        ],
                        throwOnError: false
                    });
                }

                const toc = document.querySelector('.toc-container');
                if (toc) {
                    if (targetId === '#theory') {
                        toc.style.display = 'block';
                        setTimeout(() => toc.classList.add('visible'), 100);
                    } else {
                        toc.classList.remove('visible');
                        setTimeout(() => toc.style.display = 'none', 300);
                    }
                }
            }

            tabs.forEach(tab => {
                tab.addEventListener('click', (e) => {
                    e.preventDefault();
                    const targetId = tab.getAttribute('href');
                    history.pushState(null, null, targetId);
                    switchTab(targetId);
                });
            });

            window.addEventListener('popstate', () => {
                switchTab(window.location.hash);
            });

            switchTab(window.location.hash);
        });
    </script>
</body>
</html>
