<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Logarithms in ML | ML Fundamentals</title>
    <meta name="description" content="Why logarithms appear everywhere in machine learning. From numerical stability to information theory, master the math behind log-transforms.">
    
    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&family=Playfair+Display:wght@400;500;600;700&display=swap" rel="stylesheet">
    
    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}, {left: '\\\\[', right: '\\\\]', display: true}, {left: '\\\\(', right: '\\\\)', display: false}], throwOnError: false});"></script>
    
    <!-- Styles -->
    <link rel="stylesheet" href="../../../css/main.css">
    <link rel="stylesheet" href="../../../css/components.css">
    
    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>∞</text></svg>">
<link rel="stylesheet" href="../../../css/sidebar.css">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar" id="navbar">
        <div class="nav-container">
            <a href="../../../index.html" class="nav-logo">
                <span class="logo-symbol">∇</span>
                <span class="logo-text">ML Fundamentals</span>
            </a>
            
            <button class="nav-toggle" id="navToggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            
            <div class="nav-menu" id="navMenu">
                <div class="nav-links">
                    <a href="../../../tutorials/ml/index.html" class="nav-link active">Machine Learning</a>
                    <a href="../../../tutorials/linear-algebra/index.html" class="nav-link">Linear Algebra</a>
                    <a href="../../../tutorials/calculus/index.html" class="nav-link">Calculus</a>
                    <a href="../../../tutorials/physics/index.html" class="nav-link">Physics</a>
                    <a href="../../../index.html#philosophy" class="nav-link">Philosophy</a>
                    <a href="../../../index.html#roadmap" class="nav-link">Roadmap</a>
                    <a href="https://github.com/ml-entropy/ml-entropy.github.io" class="nav-link" target="_blank">GitHub</a>
                </div>
                
                <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
                    <svg class="sun-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="5"/>
                        <path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"/>
                    </svg>
                    <svg class="moon-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                    </svg>
                </button>
            </div>
        </div>
    </nav>

    <!-- Tutorial Header -->
    <header class="tutorial-content-header">
        <div class="container">
            <nav class="breadcrumb">
                <a href="../../../index.html">Home</a>
                <span class="breadcrumb-separator">→</span>
                <a href="../index.html">Machine Learning</a>
                <span class="breadcrumb-separator">→</span>
                <span>Logarithms in ML</span>
            </nav>
            
            
            
            
            <div class="tutorial-tabs">
                <a href="#theory" class="tutorial-tab active">Theory</a>
                <a href="#code" class="tutorial-tab">Code</a>
                <a href="#exercises" class="tutorial-tab">Exercises</a>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    
    <!-- Main Content -->
    <div class="tutorial-wrapper">
        
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <aside class="tutorial-sidebar">
            <div class="sidebar-section">
                <h3 class="sidebar-section-title">Machine Learning</h3>
                <nav class="sidebar-nav">
                        <a href="../00-probability/index.html" class="sidebar-link">00. Probability Foundations</a>
                    <a href="../04-logarithms/index.html" class="sidebar-link active">01. Why Logarithms?</a>
                    <a href="../05-combinatorics/index.html" class="sidebar-link">02. Combinatorics</a>
                    <a href="../03-distributions/index.html" class="sidebar-link">03. Normal Distributions</a>
                    <a href="../01-entropy/index.html" class="sidebar-link">04. Entropy Fundamentals</a>
                    <a href="../02-cross-entropy/index.html" class="sidebar-link">05. Cross-Entropy</a>
                    <a href="../02-kl-divergence/index.html" class="sidebar-link">06. KL Divergence</a>
                    <a href="../14-entropy-connections/index.html" class="sidebar-link">07. Entropy Connections</a>
                    <a href="../06-backpropagation/index.html" class="sidebar-link">08. Backpropagation</a>
                    <a href="../07-regularization/index.html" class="sidebar-link">09. Regularization</a>
                    <a href="../08-batch-normalization/index.html" class="sidebar-link">10. Batch Normalization</a>
                    <a href="../09-learning-rate/index.html" class="sidebar-link">11. Learning Rate</a>
                    <a href="../10-cnn/index.html" class="sidebar-link">12. CNNs</a>
                    <a href="../11-rnn/index.html" class="sidebar-link">13. RNNs</a>
                    <a href="../34-rate-distortion/index.html" class="sidebar-link">14. Rate-Distortion Theory</a>
                    <a href="../15-autoencoder/index.html" class="sidebar-link">15. Autoencoders</a>
                    <a href="../13-variational-inference/index.html" class="sidebar-link">16. Variational Inference</a>
                    <a href="../12-vae/index.html" class="sidebar-link">17. VAE</a>
                    <a href="../16-inductive-bias/index.html" class="sidebar-link">18. Inductive Bias</a>
                    <a href="../17-architectural-biases/index.html" class="sidebar-link">19. Architectural Biases</a>
                    <a href="../18-designing-biases/index.html" class="sidebar-link">20. Designing Biases</a>
                    <a href="../19-fst-fundamentals/index.html" class="sidebar-link">21. FST Fundamentals</a>
                    <a href="../20-weighted-fsts/index.html" class="sidebar-link">22. Weighted FSTs</a>
                    <a href="../21-fst-libraries/index.html" class="sidebar-link">23. FST Libraries</a>
                    <a href="../22-fst-applications/index.html" class="sidebar-link">24. FST Applications</a>
                    <a href="../23-neural-symbolic/index.html" class="sidebar-link">25. Neural-Symbolic Hybrids</a>
                    <a href="../24-sequence-alignment/index.html" class="sidebar-link">26. Sequence Alignment</a>
                    <a href="../25-mas-algorithm/index.html" class="sidebar-link">27. MAS Algorithm</a>
                    <a href="../26-forced-alignment/index.html" class="sidebar-link">28. Forced Alignment & MFA</a>
                    <a href="../27-tts-fundamentals/index.html" class="sidebar-link">29. TTS Fundamentals</a>
                    <a href="../28-neural-vocoders/index.html" class="sidebar-link">30. Neural Vocoders</a>
                    <a href="../29-tacotron/index.html" class="sidebar-link">31. Tacotron & Attention TTS</a>
                    <a href="../30-fastspeech/index.html" class="sidebar-link">32. FastSpeech & Non-AR TTS</a>
                    <a href="../31-glow-tts/index.html" class="sidebar-link">33. Glow-TTS & Flows</a>
                    <a href="../32-vits/index.html" class="sidebar-link">34. VITS: End-to-End TTS</a>
                    <a href="../33-bilingual-tts/index.html" class="sidebar-link">35. Bilingual TTS: RU+KY</a>
                </nav>
            </div>
            
            <div class="sidebar-section" style="margin-top: 2rem;">
                <h3 class="sidebar-section-title">Related Subjects</h3>
                <nav class="sidebar-nav">
                        <a href="../../linear-algebra/index.html" class="sidebar-link">Linear Algebra</a>
                    <a href="../../calculus/index.html" class="sidebar-link">Calculus</a>
                    <a href="../../physics/index.html" class="sidebar-link">Physics</a>
                </nav>
            </div>
        </aside>

        <!-- Main Article -->
        <main class="tutorial-main">
            
            <article class="article-content" id="theory">
                
                <!-- Section 1 -->
                <h2 id="fundamentals">Logarithm Fundamentals</h2>
                
                <p>
                    The logarithm is the <strong>inverse of exponentiation</strong>:
                </p>
                
                <div class="math-block">
                    $$y = \log_b(x) \quad \Leftrightarrow \quad b^y = x$$
                </div>
                
                <div class="definition-box">
                    <div class="box-title">Key Properties</div>
                    <p style="margin-bottom: 0;">
                        <strong>Product → Sum:</strong> $\log(ab) = \log(a) + \log(b)$<br>
                        <strong>Power → Multiply:</strong> $\log(a^n) = n\log(a)$<br>
                        <strong>Base change:</strong> $\log_b(x) = \frac{\ln(x)}{\ln(b)}$
                    </p>
                </div>
                
                <p>
                    In ML, we typically use natural log ($\ln$, base $e$) or log base 2 (for bits).
                </p>
                
                <!-- Section 2 -->
                <h2 id="numerical-stability">Numerical Stability</h2>
                
                <p>
                    Probabilities are often very small numbers. Multiplying many small probabilities 
                    causes <strong>underflow</strong>—the result becomes indistinguishable from zero.
                </p>
                
                <div class="math-derivation">
                    <div class="math-derivation-title">The Underflow Problem</div>
                    
                    <div class="math-step">
                        <div class="math-step-number">1</div>
                        <div class="math-step-content">
                            Likelihood of 1000 i.i.d. samples: $L = \prod_{i=1}^{1000} p(x_i)$
                        </div>
                    </div>
                    
                    <div class="math-step">
                        <div class="math-step-number">2</div>
                        <div class="math-step-content">
                            If $p(x_i) \approx 0.01$, then $L \approx (0.01)^{1000} = 10^{-2000}$ — underflow!
                        </div>
                    </div>
                    
                    <div class="math-step">
                        <div class="math-step-number">3</div>
                        <div class="math-step-content">
                            Solution: Work with log-likelihood $\ell = \sum_{i=1}^{1000} \log p(x_i)$
                        </div>
                    </div>
                    
                    <div class="math-step">
                        <div class="math-step-number">4</div>
                        <div class="math-step-content">
                            Now $\ell \approx 1000 \times (-4.6) = -4600$ — perfectly manageable!
                        </div>
                    </div>
                </div>
                
                <div class="note-box">
                    <div class="box-title">Log-Sum-Exp Trick</div>
                    <p style="margin-bottom: 0;">
                        To compute $\log\sum_i e^{x_i}$ stably: 
                        $$\log\sum_i e^{x_i} = m + \log\sum_i e^{x_i - m}$$
                        where $m = \max_i x_i$. This prevents overflow in the exponential.
                    </p>
                </div>
                
                <!-- Section 3 -->
                <h2 id="optimization">Optimization Benefits</h2>
                
                <p>
                    Log-transforms improve optimization in multiple ways:
                </p>
                
                <h3>Products Become Sums</h3>
                
                <p>
                    Gradients of sums are easier to compute than gradients of products:
                </p>
                
                <div class="math-block">
                    $$\frac{\partial}{\partial \theta} \log \prod_i p_i(\theta) = \sum_i \frac{\partial \log p_i(\theta)}{\partial \theta} = \sum_i \frac{1}{p_i} \frac{\partial p_i}{\partial \theta}$$
                </div>
                
                <h3>Better Conditioning</h3>
                
                <p>
                    The log-likelihood surface is often more convex than the likelihood surface, 
                    leading to better-behaved optimization.
                </p>
                
                <h3>Derivative of Log-Softmax</h3>
                
                <div class="math-derivation">
                    <div class="math-derivation-title">Why Log-Softmax?</div>
                    
                    <div class="math-step">
                        <div class="math-step-number">1</div>
                        <div class="math-step-content">
                            Softmax: $\sigma(z)_i = \frac{e^{z_i}}{\sum_j e^{z_j}}$
                        </div>
                    </div>
                    
                    <div class="math-step">
                        <div class="math-step-number">2</div>
                        <div class="math-step-content">
                            Log-softmax: $\log\sigma(z)_i = z_i - \log\sum_j e^{z_j}$
                        </div>
                    </div>
                    
                    <div class="math-step">
                        <div class="math-step-number">3</div>
                        <div class="math-step-content">
                            Gradient: $\frac{\partial}{\partial z_k}\log\sigma(z)_i = \delta_{ik} - \sigma(z)_k$
                        </div>
                    </div>
                    
                    <div class="math-step">
                        <div class="math-step-number">4</div>
                        <div class="math-step-content">
                            For cross-entropy loss: $\nabla_{z} L = \hat{y} - y$ — beautifully simple!
                        </div>
                    </div>
                </div>
                
                <!-- Section 4 -->
                <h2 id="information-theory">Information-Theoretic Meaning</h2>
                
                <p>
                    The logarithm has deep connections to information theory:
                </p>
                
                <ul>
                    <li><strong>Surprise:</strong> $-\log p(x)$ = bits of surprise when observing $x$</li>
                    <li><strong>Entropy:</strong> $H = \mathbb{E}[-\log p(X)]$ = average surprise</li>
                    <li><strong>Code length:</strong> Optimal code for $x$ has length $\approx -\log_2 p(x)$ bits</li>
                </ul>
                
                <div class="definition-box">
                    <div class="box-title">Why Logarithm for Surprise?</div>
                    <p style="margin-bottom: 0;">
                        Logarithm is the <em>unique function</em> (up to scaling) satisfying:<br>
                        1. Surprise of independent events adds: $I(p_1 p_2) = I(p_1) + I(p_2)$<br>
                        2. Surprise is continuous in probability<br>
                        3. Surprise of certain event is zero: $I(1) = 0$
                    </p>
                </div>
                
                <!-- Section 5 -->
                <h2 id="log-scale">Log-Scale Thinking</h2>
                
                <p>
                    Many quantities in ML span orders of magnitude. Log-scale helps us reason about them:
                </p>
                
                <ul>
                    <li><strong>Learning rates:</strong> Search $10^{-1}$ to $10^{-5}$, not 0.1 to 0.00001</li>
                    <li><strong>Regularization:</strong> Weight decay $\lambda$ varies logarithmically</li>
                    <li><strong>Loss curves:</strong> Plot log-loss to see exponential decay linearly</li>
                </ul>
                
                <div class="warning-box">
                    <div class="box-title">Practical Tip</div>
                    <p style="margin-bottom: 0;">
                        When hyperparameter tuning, use logarithmic grids: 
                        <code>[1e-4, 1e-3, 1e-2, 1e-1]</code> not <code>[0.001, 0.002, 0.003, ...]</code>
                    </p>
                </div>
                
                <!-- Navigation -->
                
            <div class="tutorial-footer-summary" style="margin: 3rem 0; padding: 2rem; background: #f8f9fa; border-radius: 8px; border-left: 4px solid #3b82f6;">
                <h1>05. Logarithms in ML</h1>
                <p class="lead">
                Why does every ML paper use log-probabilities? Why log-likelihood, not likelihood? 
                The logarithm isn't arbitrary—it's essential for computation, theory, and intuition.
            </p>
            </div>
                <div class="tutorial-nav">
                    <a href="../00-probability/index.html" class="tutorial-nav-link prev">
                        <span class="nav-label">Previous</span>
                        <span class="nav-title">← Probability Concepts</span>
                    </a>
                    <a href="../05-combinatorics/index.html" class="tutorial-nav-link next">
                        <span class="nav-label">Next</span>
                        <span class="nav-title">Combinatorics →</span>
                    </a>
                </div>
                
            </article>
        
        </main>

        <!-- TOC (Right Side) -->
        <aside class="toc-container">
        <h4 class="toc-title">Contents</h4>
        <nav class="toc-list">
            <a href="#fundamentals" class="toc-link">Fundamentals</a>
            <a href="#numerical-stability" class="toc-link">Numerical Stability</a>
            <a href="#optimization" class="toc-link">Optimization Benefits</a>
            <a href="#information-theory" class="toc-link">Information Theory</a>
            <a href="#log-scale" class="toc-link">Log-Scale Thinking</a>
        </nav>
    </aside>
    </div>
    

    <!-- Table of Contents (floating) -->
    

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-brand">
                    <span class="logo-symbol">∇</span>
                    <span>ML Fundamentals</span>
                </div>
                <p class="footer-tagline">Deep understanding through first principles.</p>
            </div>
            <div class="footer-links">
                <a href="../../../index.html">Home</a>
                <a href="https://github.com/ml-entropy/ml-entropy.github.io" target="_blank">GitHub</a>
            </div>
        </div>
    </footer>

    <!-- Scripts -->
    <script src="../../../js/main.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            if (typeof renderMathInElement === 'function') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }
        });
    </script>
</body>
</html>
