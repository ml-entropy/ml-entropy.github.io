<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Exercises: Entropy Connections - Opus Tutorials</title>
    <meta name="description" content="Practice exercises for Shannon entropy, Huffman coding, and information theory concepts.">
    
    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&family=Playfair+Display:wght@400;500;600;700&display=swap" rel="stylesheet">
    
    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}, {left: '\[', right: '\]', display: true}, {left: '\(', right: '\)', display: false}], throwOnError: false});"></script>
    
    <!-- Styles -->
    <link rel="stylesheet" href="../../../css/main.css">
    <link rel="stylesheet" href="../../../css/components.css">
    <link rel="stylesheet" href="../../../css/sidebar.css">
    
    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>∞</text></svg>">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar" id="navbar">
        <div class="nav-container">
            <a href="../../../index.html" class="nav-logo">
                <span class="logo-symbol">∇</span>
                <span class="logo-text">ML Fundamentals</span>
            </a>
            
            <button class="nav-toggle" id="navToggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            
            <div class="nav-menu" id="navMenu">
                <div class="nav-links">
                    <a href="../../../tutorials/ml/index.html" class="nav-link active">Machine Learning</a>
                    <a href="../../../tutorials/linear-algebra/index.html" class="nav-link">Linear Algebra</a>
                    <a href="../../../tutorials/calculus/index.html" class="nav-link">Calculus</a>
                    <a href="../../../tutorials/physics/index.html" class="nav-link">Physics</a>
                    <a href="../../../index.html#philosophy" class="nav-link">Philosophy</a>
                    <a href="../../../index.html#roadmap" class="nav-link">Roadmap</a>
                    <a href="https://github.com/ml-entropy/ml-entropy.github.io" class="nav-link" target="_blank">GitHub</a>
                </div>
                
                <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
                    <svg class="sun-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="5"/>
                        <path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"/>
                    </svg>
                    <svg class="moon-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                    </svg>
                </button>
            </div>
        </div>
    </nav>

    <!-- Tutorial Header -->
    <header class="tutorial-content-header">
        <div class="container">
            <nav class="breadcrumb">
                <a href="../../../index.html">Home</a>
                <span class="breadcrumb-separator">→</span>
                <a href="../index.html">Machine Learning</a>
                <span class="breadcrumb-separator">→</span>
                <span>Entropy Exercises</span>
            </nav>
            
            <h1>Entropy Connections</h1>
            <p class="lead">
                Practice exercises to master entropy, surprise, and coding theory.
            </p>
            
            <div class="tutorial-tabs">
                <a href="index.html" class="tutorial-tab">Theory</a>
                <a href="index.html#code" class="tutorial-tab">Code</a>
                <a href="#" class="tutorial-tab active">Exercises</a>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <div class="tutorial-wrapper">
        <!-- Sidebar Navigation -->
        <aside class="tutorial-sidebar">
            <div class="sidebar-section">
                <h3 class="sidebar-section-title">Machine Learning</h3>
                <nav class="sidebar-nav">
                        <a href="../00-probability/index.html" class="sidebar-link">00. Probability Foundations</a>
                    <a href="../01-entropy/index.html" class="sidebar-link active">01. Entropy Fundamentals</a>
                    <a href="../02-kl-divergence/index.html" class="sidebar-link">02. KL Divergence</a>
                    <a href="../03-distributions/index.html" class="sidebar-link">03. Normal Distributions</a>
                    <a href="../04-logarithms/index.html" class="sidebar-link">04. Why Logarithms?</a>
                    <a href="../05-combinatorics/index.html" class="sidebar-link">05. Combinatorics</a>
                    <a href="../06-backpropagation/index.html" class="sidebar-link">06. Backpropagation</a>
                    <a href="../07-regularization/index.html" class="sidebar-link">07. Regularization</a>
                    <a href="../08-batch-normalization/index.html" class="sidebar-link">08. Batch Normalization</a>
                    <a href="../09-learning-rate/index.html" class="sidebar-link">09. Learning Rate</a>
                    <a href="../10-cnn/index.html" class="sidebar-link">10. CNNs</a>
                    <a href="../11-rnn/index.html" class="sidebar-link">11. RNNs</a>
                    <a href="../12-vae/index.html" class="sidebar-link">12. VAE</a>
                    <a href="../13-variational-inference/index.html" class="sidebar-link">13. Variational Inference</a>
                </nav>
            </div>
            
            <div class="sidebar-section" style="margin-top: 2rem;">
                <h3 class="sidebar-section-title">Related Subjects</h3>
                <nav class="sidebar-nav">
                        <a href="../../linear-algebra/index.html" class="sidebar-link">Linear Algebra</a>
                    <a href="../../calculus/index.html" class="sidebar-link">Calculus</a>
                    <a href="../../physics/index.html" class="sidebar-link">Physics</a>
                </nav>
            </div>
        </aside>

        <!-- Main Article -->
        <main class="tutorial-main">

            <section id="exercises">
                <h2 style="margin-bottom: 1.5rem;">Practice Exercises</h2>
                
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>A1. — Definitions</h3>
                        <span class="difficulty-badge diff-medium">Practice</span>
                    </div>
                    <p>In your own words, define the following and explain their relationship:
a) Entropy $H(P)$
b) Cross-Entropy $H(P, Q)$
c) KL Divergence $D_{KL}(P || Q)$</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>a) <strong>Entropy $H(P)$</strong>: The average amount of information or "surprise" from a distribution $P$. It's the theoretical minimum average number of bits required to encode a message from $P$.
b) <strong>Cross-Entropy $H(P, Q)$</strong>: The average number of bits required to encode messages from a true distribution $P$ when using a code optimized for a different distribution $Q$.
c) <strong>KL Divergence $D_{KL}(P || Q)$</strong>: The "penalty" or extra bits wasted by using a code from $Q$ instead of the optimal code from $P$. It measures the inefficiency of the approximation $Q$.</p>

<p><strong>Relationship</strong>: KL Divergence is the difference between Cross-Entropy and Entropy. $D_{KL}(P || Q) = H(P, Q) - H(P)$.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>A2. — The Grand Equation</h3>
                        <span class="difficulty-badge diff-medium">Practice</span>
                    </div>
                    <p>Write down the equation that connects the three concepts and explain why, in machine learning, minimizing cross-entropy is equivalent to minimizing KL divergence.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>The equation is:
<div class="math-block">$$ H(P, Q) = H(P) + D_{KL}(P || Q) $$</div></p>

<p>In machine learning, we want to make our model's distribution $Q$ as close as possible to the true data distribution $P$. This is equivalent to minimizing the "distance" between them, which is the KL Divergence $D_{KL}(P || Q)$.</p>

<p>When we train a model, the true data distribution $P$ is fixed, which means its entropy $H(P)$ is a constant. Therefore, minimizing the cross-entropy $H(P, Q)$ has the exact same effect as minimizing the KL divergence, because the constant $H(P)$ does not affect the location of the minimum.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>A3. — Weather Prediction</h3>
                        <span class="difficulty-badge diff-medium">Practice</span>
                    </div>
                    <p>Let the true probability of weather be $P = \{\text{sun: } 0.5, \text{rain: } 0.5\}$.
Your weather model predicts $Q = \{\text{sun: } 0.8, \text{rain: } 0.2\}$.
Calculate (using log base 2):
a) The entropy of the true distribution, $H(P)$.
b) The cross-entropy between your model and the true distribution, $H(P, Q)$.
c) The KL divergence, $D_{KL}(P || Q)$.
d) Verify that $H(P, Q) = H(P) + D_{KL}(P || Q)$.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>Given $P = [0.5, 0.5]$ and $Q = [0.8, 0.2]$.</p>

<p>a) <strong>Entropy</strong>:
<div class="math-block">$$ H(P) = - (0.5 \log_2 0.5 + 0.5 \log_2 0.5) = - (0.5 \times -1 + 0.5 \times -1) = \boxed{1 \text{ bit}} $$</div></p>

<p>b) <strong>Cross-Entropy</strong>:
<div class="math-block">$$ H(P, Q) = - (0.5 \log_2 0.8 + 0.5 \log_2 0.2) = - (0.5 \times -0.322 + 0.5 \times -2.322) = -(-0.161 - 1.161) = \boxed{1.322 \text{ bits}} $$</div></p>

<p>c) <strong>KL Divergence</strong>:
<div class="math-block">$$ D_{KL}(P || Q) = \sum P(x) \log_2 \frac{P(x)}{Q(x)} = 0.5 \log_2 \frac{0.5}{0.8} + 0.5 \log_2 \frac{0.5}{0.2} $$</div>
<div class="math-block">$$ = 0.5 \log_2(0.625) + 0.5 \log_2(2.5) = 0.5 \times (-0.678) + 0.5 \times (1.322) = -0.339 + 0.661 = \boxed{0.322 \text{ bits}} $$</div></p>

<p>d) <strong>Verification</strong>:
<div class="math-block">$$ H(P) + D_{KL}(P || Q) = 1 + 0.322 = 1.322 = H(P, Q) $$</div>
The equation holds.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>A4. — Perfect vs. Terrible Model</h3>
                        <span class="difficulty-badge diff-medium">Practice</span>
                    </div>
                    <p>Consider a 3-class classification problem where the true label is class A.
<ul>
<li>  True distribution $P = \{A: 1, B: 0, C: 0\}$.</li>
</ul>
a) What is the entropy $H(P)$ of this true distribution?
b) A "perfect" model predicts $Q_{perfect} = \{A: 0.99, B: 0.005, C: 0.005\}$. Calculate $H(P, Q_{perfect})$.
c) A "terrible" model predicts $Q_{terrible} = \{A: 0.1, B: 0.5, C: 0.4\}$. Calculate $H(P, Q_{terrible})$.
d) Compare the results. What does this tell you about cross-entropy as a loss function?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>Given $P = [1, 0, 0]$.</p>

<p>a) <strong>Entropy</strong>:
<div class="math-block">$$ H(P) = - (1 \log_2 1 + 0 \log_2 0 + 0 \log_2 0) = 0 $$</div>
(Note: $0 \log 0$ is defined as 0). There is no uncertainty, so entropy is zero.</p>

<p>b) <strong>Perfect Model</strong>: $Q_{perfect} = [0.99, 0.005, 0.005]$
<div class="math-block">$$ H(P, Q_{perfect}) = - (1 \log_2 0.99 + 0 \log_2 0.005 + 0 \log_2 0.005) = - \log_2 0.99 \approx \boxed{0.014 \text{ bits}} $$</div>
This is a very low loss.</p>

<p>c) <strong>Terrible Model</strong>: $Q_{terrible} = [0.1, 0.5, 0.4]$
<div class="math-block">$$ H(P, Q_{terrible}) = - (1 \log_2 0.1 + 0 \log_2 0.5 + 0 \log_2 0.4) = - \log_2 0.1 \approx \boxed{3.32 \text{ bits}} $$</div>
This is a much higher loss.</p>

<p>d) <strong>Comparison</strong>: Cross-entropy heavily penalizes models that are "confidently wrong." The terrible model assigned a very low probability (0.1) to the true class, resulting in a high loss. The perfect model was very confident about the correct class, resulting in a loss close to zero.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>A5. — Asymmetry of KL Divergence</h3>
                        <span class="difficulty-badge diff-medium">Practice</span>
                    </div>
                    <p>Using the weather example from A3 ($P = \{0.5, 0.5\}$ and $Q = \{0.8, 0.2\}$), calculate $D_{KL}(Q || P)$.
Is it the same as $D_{KL}(P || Q)$? Why is this property important?</p>

<p>---</p>

<p>## Part B: Coding</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <div class="math-block">$$ D_{KL}(Q || P) = \sum Q(x) \log_2 \frac{Q(x)}{P(x)} = 0.8 \log_2 \frac{0.8}{0.5} + 0.2 \log_2 \frac{0.2}{0.5} $$</div>
<div class="math-block">$$ = 0.8 \log_2(1.6) + 0.2 \log_2(0.4) = 0.8 \times (0.678) + 0.2 \times (-1.322) = 0.542 - 0.264 = \boxed{0.278 \text{ bits}} $$</div>
This is not equal to $D_{KL}(P || Q) = 0.322$. KL Divergence is asymmetric.

<p><strong>Importance</strong>: This reflects that the "cost" of approximating P with Q is different from the cost of approximating Q with P. For example, if P has zero probability for an event but Q gives it a non-zero probability, $D_{KL}(P || Q)$ is fine, but $D_{KL}(Q || P)$ would be infinite. This is crucial in generative models where we want our model Q to avoid generating samples that are impossible under P.</p>

<p>---</p>

<p>## Part B: Coding Solutions</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>B1. — Basic Calculator</h3>
                        <span class="difficulty-badge diff-medium">Practice</span>
                    </div>
                    <p>Write Python functions to calculate <code>entropy(P)</code>, <code>cross_entropy(P, Q)</code>, and <code>kl_divergence(P, Q)</code>.
<ul>
<li>  The inputs should be NumPy arrays.</li>
<li>  Use <code>np.log2</code>.</li>
<li>  Handle the case where a probability in Q is 0 (add a small epsilon).</li>
</ul></p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <pre><code>python
import numpy as np

<p>def entropy(P):
    """Calculate Shannon Entropy."""
    # Add a small epsilon to prevent log(0)
    P = P + 1e-9
    return -np.sum(P * np.log2(P))</p>

<p>def cross_entropy(P, Q):
    """Calculate Cross-Entropy."""
    P = P + 1e-9
    Q = Q + 1e-9
    return -np.sum(P * np.log2(Q))</p>

<p>def kl_divergence(P, Q):
    """Calculate KL Divergence."""
    P = P + 1e-9
    Q = Q + 1e-9
    return np.sum(P * np.log2(P / Q))</p>

<p># Example from A3
P = np.array([0.5, 0.5])
Q = np.array([0.8, 0.2])</p>

<p>H_P = entropy(P)
H_PQ = cross_entropy(P, Q)
D_KL_PQ = kl_divergence(P, Q)</p>

<p>print(f"H(P) = {H_P:.3f}")
print(f"H(P, Q) = {H_PQ:.3f}")
print(f"D_KL(P || Q) = {D_KL_PQ:.3f}")
print(f"H(P) + D_KL(P || Q) = {H_P + D_KL_PQ:.3f}")
</code></pre></p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>B2. — Visualizing the Loss</h3>
                        <span class="difficulty-badge diff-medium">Practice</span>
                    </div>
                    <p>Create a plot that shows how the cross-entropy loss changes as a model's prediction for the correct class moves from 0 to 1.
<ul>
<li>  Assume a 2-class problem where the true label is <code>[1, 0]</code>.</li>
<li>  The model's prediction is <code>[p, 1-p]</code>.</li>
<li>  Plot the cross-entropy for <code>p</code> ranging from 0.01 to 0.99.</li>
<li>  What does the shape of the curve tell you about the penalty for confident wrong predictions?</li>
</ul></p>

<p>---</p>

<p>## Part C: Conceptual Deep Dive</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <pre><code>python
import matplotlib.pyplot as plt

<p># True label is class 0
P = np.array([1.0, 0.0])</p>

<p># Model's predicted probability for the correct class
p_correct = np.linspace(0.01, 0.99, 100)</p>

<p># Calculate cross-entropy for each prediction
losses = []
for p in p_correct:
    Q = np.array([p, 1-p])
    loss = cross_entropy(P, Q)
    losses.append(loss)</p>

<p>plt.figure(figsize=(10, 6))
plt.plot(p_correct, losses, lw=2)
plt.xlabel("Predicted Probability for Correct Class")
plt.ylabel("Cross-Entropy Loss")
plt.title("Cross-Entropy Loss vs. Model Confidence")
plt.grid(True, linestyle='--', alpha=0.6)
plt.show()
</code></pre>
<strong>Interpretation</strong>: The curve shows that the loss is very close to zero when the model correctly predicts a high probability for the true class. However, as the predicted probability for the true class approaches zero, the loss skyrockets towards infinity. This demonstrates that cross-entropy severely punishes predictions that are confidently wrong.</p>

<p>---</p>

<p>## Part C: Conceptual Solutions</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>C1. — Why Not MSE?</h3>
                        <span class="difficulty-badge diff-medium">Practice</span>
                    </div>
                    <p>For classification problems, why is cross-entropy a better loss function than Mean Squared Error (MSE)?
<em>Hint: Think about the loss surface and the magnitude of gradients for predictions that are far from the true label.</em></p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>1.  <strong>Non-convex Loss Surface</strong>: For models that use a sigmoid or softmax output, using MSE can create a non-convex loss surface with multiple local minima, making optimization difficult. Cross-entropy provides a convex loss surface, which is much easier to optimize.
2.  <strong>Gradient Saturation</strong>: When a prediction is very wrong (e.g., sigmoid output is 0 when it should be 1), the gradient of the sigmoid function becomes very small. With MSE, this small gradient leads to very slow learning. The derivative of cross-entropy with a sigmoid/softmax output cancels out the sigmoid's derivative term, resulting in a strong gradient (<code>p - y</code>) that learns quickly from large errors.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>C2. — VAEs and KL Divergence</h3>
                        <span class="difficulty-badge diff-medium">Practice</span>
                    </div>
                    <p>In a Variational Autoencoder (VAE), the loss function has two parts: a reconstruction loss and a KL divergence term. This KL term often measures the divergence between the learned latent distribution $q(z|x)$ and a standard normal prior $p(z) = \mathcal{N}(0, 1)$.
Explain the role of this KL divergence term. What is it "enforcing" on the latent space? What would happen if this term were removed?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>The KL divergence term in a VAE loss function, $D_{KL}(q(z|x) || p(z))$, acts as a <strong>regularizer</strong> on the latent space.</p>

<ul>
<li>  <strong>What it enforces</strong>: It forces the distribution of the learned latent vectors (the encodings), $q(z|x)$, to be close to a prior distribution, typically a standard normal distribution $\mathcal{N}(0, 1)$. This means the model is penalized if it creates encodings that are "far" from a simple, continuous, and well-defined distribution.</li>
</ul>

<ul>
<li>  <strong>What would happen without it</strong>: Without the KL term, the VAE would behave like a standard autoencoder. It would learn to perfectly reconstruct the input by creating highly specific, disjointed encodings for each data point. The latent space would have no regular structure, and you wouldn't be able to sample from it to generate new, plausible data. The KL term ensures the latent space is <strong>continuous and dense</strong>, which is the key property that allows for meaningful generation.</li>
</ul>
                        </div>
                    </details>
                </div>
            </section>
    
</main>
    </div>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-brand">
                    <span class="logo-symbol">∇</span>
                    <span>ML Fundamentals</span>
                </div>
                <p class="footer-tagline">Deep understanding through first principles.</p>
            </div>
            <div class="footer-links">
                <a href="../../../index.html">Home</a>
                <a href="https://github.com/ml-entropy/ml-entropy.github.io" target="_blank">GitHub</a>
            </div>
        </div>
    </footer>

    <!-- Scripts -->
    <script src="../../../js/main.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            if (typeof renderMathInElement === 'function') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\[', right: '\]', display: true},
                        {left: '\(', right: '\)', display: false}
                    ],
                    throwOnError: false
                });
            }
        });
    </script>
</body>
</html>
