{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 14: Entropy, Cross-Entropy, and KL Divergence - Code\n",
    "\n",
    "This notebook provides interactive code examples to accompany the theory and exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implementing the Core Functions\n",
    "\n",
    "First, let's implement the three core concepts from scratch. We'll use these to verify the examples in the exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(P):\n",
    "    \"\"\"Calculate Shannon Entropy.\"\"\"\n",
    "    # Add a small epsilon to prevent log(0)\n",
    "    P = P + 1e-9\n",
    "    return -np.sum(P * np.log2(P))\n",
    "\n",
    "def cross_entropy(P, Q):\n",
    "    \"\"\"Calculate Cross-Entropy.\"\"\"\n",
    "    P = P + 1e-9\n",
    "    Q = Q + 1e-9\n",
    "    return -np.sum(P * np.log2(Q))\n",
    "\n",
    "def kl_divergence(P, Q):\n",
    "    \"\"\"Calculate KL Divergence.\"\"\"\n",
    "    P = P + 1e-9\n",
    "    Q = Q + 1e-9\n",
    "    return np.sum(P * np.log2(P / Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification using the Weather Example\n",
    "\n",
    "Let's test our functions with the weather example from Exercise A3:\n",
    "-   True Distribution $P = [0.5, 0.5]$\n",
    "-   Model Distribution $Q = [0.8, 0.2]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_weather = np.array([0.5, 0.5])\n",
    "Q_weather = np.array([0.8, 0.2])\n",
    "\n",
    "H_P = entropy(P_weather)\n",
    "H_PQ = cross_entropy(P_weather, Q_weather)\n",
    "D_KL_PQ = kl_divergence(P_weather, Q_weather)\n",
    "\n",
    "print(f\"Entropy H(P) = {H_P:.3f} bits\")\n",
    "print(f\"Cross-Entropy H(P, Q) = {H_PQ:.3f} bits\")\n",
    "print(f\"KL Divergence D_KL(P || Q) = {D_KL_PQ:.3f} bits\")\n",
    "print(\"-\"*30)\n",
    "print(f\"H(P) + D_KL(P || Q) = {H_P + D_KL_PQ:.3f}\")\n",
    "print(f\"Does H(P,Q) == H(P) + D_KL(P||Q)? {'Yes' if np.isclose(H_PQ, H_P + D_KL_PQ) else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualizing Cross-Entropy as a Loss Function\n",
    "\n",
    "This visualization corresponds to Exercise B2. We can see how the loss behaves as our model's prediction for the correct class varies. The true label is `1` (or `[1, 0]` in one-hot form)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The true distribution for a single data point where the correct class is the first one.\n",
    "P_true = np.array([1.0, 0.0])\n",
    "\n",
    "# A range of model predictions for the correct class.\n",
    "pred_correct_prob = np.linspace(0.01, 0.99, 200)\n",
    "\n",
    "# Calculate the cross-entropy loss for each prediction.\n",
    "losses = [cross_entropy(P_true, np.array([p, 1-p])) for p in pred_correct_prob]\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(pred_correct_prob, losses, label='Cross-Entropy Loss', color='darkred', lw=2.5)\n",
    "plt.xlabel(\"Model's Predicted Probability for the Correct Class\", fontsize=12)\n",
    "plt.ylabel(\"Loss\", fontsize=12)\n",
    "plt.title(\"Behavior of Cross-Entropy Loss\", fontsize=14)\n",
    "plt.axvline(0.5, color='gray', linestyle='--', label='Uncertain Prediction (0.5)')\n",
    "plt.ylim(0, 7) # Limit y-axis to see the curve shape better\n",
    "plt.legend()\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight from the plot:**\n",
    "\n",
    "The loss is minimized as the predicted probability for the correct class approaches 1. Conversely, the loss grows exponentially as the prediction approaches 0. This steep penalty for being confidently wrong is what makes cross-entropy such an effective loss function for classification tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}