<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TTS Fundamentals | ML Fundamentals</title>
    <meta name="description" content="Text-to-speech pipeline fundamentals: text analysis, phonemes, mel spectrograms, concatenative and parametric synthesis, neural TTS revolution, and evaluation with MOS.">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&family=Playfair+Display:wght@400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}, {left: '\\\\[', right: '\\\\]', display: true}, {left: '\\\\(', right: '\\\\)', display: false}], throwOnError: false});"></script>
    <link rel="stylesheet" href="../../../css/main.css">
    <link rel="stylesheet" href="../../../css/components.css">
    <link rel="stylesheet" href="../../../css/sidebar.css">
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>&#x221E;</text></svg>">
</head>
<body>
    <nav class="navbar" id="navbar">
        <div class="nav-container">
            <a href="../../../index.html" class="nav-logo">
                <span class="logo-symbol">&nabla;</span>
                <span class="logo-text">ML Fundamentals</span>
            </a>
            <button class="nav-toggle" id="navToggle" aria-label="Toggle navigation">
                <span></span><span></span><span></span>
            </button>
            <div class="nav-menu" id="navMenu">
                <div class="nav-links">
                    <a href="../../../tutorials/ml/index.html" class="nav-link active">Machine Learning</a>
                    <a href="../../../tutorials/linear-algebra/index.html" class="nav-link">Linear Algebra</a>
                    <a href="../../../tutorials/calculus/index.html" class="nav-link">Calculus</a>
                    <a href="../../../tutorials/physics/index.html" class="nav-link">Physics</a>
                    <a href="../../../index.html#philosophy" class="nav-link">Philosophy</a>
                    <a href="../../../index.html#roadmap" class="nav-link">Roadmap</a>
                    <a href="https://github.com/ml-entropy/ml-entropy.github.io" class="nav-link" target="_blank">GitHub</a>
                </div>
                <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
                    <svg class="sun-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="5"/><path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"/></svg>
                    <svg class="moon-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/></svg>
                </button>
            </div>
        </div>
    </nav>

    <header class="tutorial-content-header">
        <div class="container">
            <nav class="breadcrumb">
                <a href="../../../index.html">Home</a>
                <span class="breadcrumb-separator">&rarr;</span>
                <a href="../index.html">Machine Learning</a>
                <span class="breadcrumb-separator">&rarr;</span>
                <span>TTS Fundamentals</span>
            </nav>
            <div class="tutorial-tabs">
                <a href="#theory" class="tutorial-tab active">Theory</a>
                <a href="#code" class="tutorial-tab">Code</a>
                <a href="#exercises" class="tutorial-tab">Exercises</a>
            </div>
        </div>
    </header>

    <div class="tutorial-wrapper">

        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <aside class="tutorial-sidebar">
            <div class="sidebar-section">
                <h3 class="sidebar-section-title">Machine Learning</h3>
                <nav class="sidebar-nav">
                        <a href="../00-probability/index.html" class="sidebar-link">00. Probability Foundations</a>
                    <a href="../01-logarithms/index.html" class="sidebar-link">01. Why Logarithms?</a>
                    <a href="../02-combinatorics/index.html" class="sidebar-link">02. Combinatorics</a>
                    <a href="../03-distributions/index.html" class="sidebar-link">03. Normal Distributions</a>
                    <a href="../04-entropy/index.html" class="sidebar-link">04. Entropy Fundamentals</a>
                    <a href="../05-cross-entropy/index.html" class="sidebar-link">05. Cross-Entropy</a>
                    <a href="../06-kl-divergence/index.html" class="sidebar-link">06. KL Divergence</a>
                    <a href="../07-entropy-connections/index.html" class="sidebar-link">07. Entropy Connections</a>
                    <a href="../08-backpropagation/index.html" class="sidebar-link">08. Backpropagation</a>
                    <a href="../09-regularization/index.html" class="sidebar-link">09. Regularization</a>
                    <a href="../10-batch-normalization/index.html" class="sidebar-link">10. Batch Normalization</a>
                    <a href="../11-learning-rate/index.html" class="sidebar-link">11. Learning Rate</a>
                    <a href="../12-cnn/index.html" class="sidebar-link">12. CNNs</a>
                    <a href="../13-rnn/index.html" class="sidebar-link">13. RNNs</a>
                    <a href="../14-rate-distortion/index.html" class="sidebar-link">14. Rate-Distortion Theory</a>
                    <a href="../15-autoencoder/index.html" class="sidebar-link">15. Autoencoders</a>
                    <a href="../16-variational-inference/index.html" class="sidebar-link">16. Variational Inference</a>
                    <a href="../17-vae/index.html" class="sidebar-link">17. VAE</a>
                    <a href="../18-inductive-bias/index.html" class="sidebar-link">18. Inductive Bias</a>
                    <a href="../19-architectural-biases/index.html" class="sidebar-link">19. Architectural Biases</a>
                    <a href="../20-designing-biases/index.html" class="sidebar-link">20. Designing Biases</a>
                    <a href="../21-fst-fundamentals/index.html" class="sidebar-link">21. FST Fundamentals</a>
                    <a href="../22-weighted-fsts/index.html" class="sidebar-link">22. Weighted FSTs</a>
                    <a href="../23-fst-libraries/index.html" class="sidebar-link">23. FST Libraries</a>
                    <a href="../24-fst-applications/index.html" class="sidebar-link">24. FST Applications</a>
                    <a href="../25-neural-symbolic/index.html" class="sidebar-link">25. Neural-Symbolic Hybrids</a>
                    <a href="../26-sequence-alignment/index.html" class="sidebar-link">26. Sequence Alignment</a>
                    <a href="../27-mas-algorithm/index.html" class="sidebar-link">27. MAS Algorithm</a>
                    <a href="../28-forced-alignment/index.html" class="sidebar-link">28. Forced Alignment & MFA</a>
                    <a href="../29-tts-fundamentals/index.html" class="sidebar-link active">29. TTS Fundamentals</a>
                    <a href="../30-neural-vocoders/index.html" class="sidebar-link">30. Neural Vocoders</a>
                    <a href="../31-tacotron/index.html" class="sidebar-link">31. Tacotron & Attention TTS</a>
                    <a href="../32-fastspeech/index.html" class="sidebar-link">32. FastSpeech & Non-AR TTS</a>
                    <a href="../33-glow-tts/index.html" class="sidebar-link">33. Glow-TTS & Flows</a>
                    <a href="../34-vits/index.html" class="sidebar-link">34. VITS: End-to-End TTS</a>
                    <a href="../35-bilingual-tts/index.html" class="sidebar-link">35. Bilingual TTS: RU+KY</a>
                </nav>
            </div>
            
            <div class="sidebar-section" style="margin-top: 2rem;">
                <h3 class="sidebar-section-title">Related Subjects</h3>
                <nav class="sidebar-nav">
                        <a href="../../linear-algebra/index.html" class="sidebar-link">Linear Algebra</a>
                    <a href="../../calculus/index.html" class="sidebar-link">Calculus</a>
                    <a href="../../physics/index.html" class="sidebar-link">Physics</a>
                </nav>
            </div>
        </aside>

        <main class="tutorial-main">

            <!-- ==================== THEORY TAB ==================== -->
            <article class="article-content" id="theory">

                <!-- Section 1: What Is Text-to-Speech? -->
                <h2 id="what-is-tts">What Is Text-to-Speech?</h2>

                <p>
                    Text-to-speech (TTS) is the task of generating natural-sounding human speech from written text. At first glance this seems straightforward &mdash; humans read text aloud effortlessly. But the conversion from orthography to acoustic waveform involves a deep chain of transformations: understanding the text, deciding how it should be pronounced, generating an acoustic representation, and synthesizing an audible waveform. Each step has its own challenges, and the history of TTS is a story of progressively better solutions to each one.
                </p>

                <p>
                    The modern TTS pipeline, regardless of whether the system is classical or neural, follows a common conceptual flow:
                </p>

                <ol>
                    <li><strong>Text analysis (linguistic frontend):</strong> Normalize raw text (expand numbers, abbreviations, dates), perform grapheme-to-phoneme (G2P) conversion, and produce a sequence of linguistic features (phonemes, prosody markers, etc.).</li>
                    <li><strong>Acoustic model:</strong> Convert linguistic features into an intermediate acoustic representation, typically a mel spectrogram &mdash; a compact, perceptually-motivated representation of the speech signal's frequency content over time.</li>
                    <li><strong>Vocoder:</strong> Convert the mel spectrogram (or other acoustic features) into a raw audio waveform that can be played through a speaker.</li>
                </ol>

<pre><code>Text &rarr; [Text Analysis] &rarr; Phonemes &rarr; [Acoustic Model] &rarr; Mel Spectrogram &rarr; [Vocoder] &rarr; Waveform

"Hello world"  &rarr;  /HH AH L OW/ /W ER L D/  &rarr;  80-bin mel frames  &rarr;  16-bit PCM audio</code></pre>

                <p>
                    <strong>Historical perspective.</strong> TTS has evolved through several distinct eras. The earliest systems (1960s&ndash;1980s) were <em>rule-based</em>: engineers manually wrote rules to map text to formant parameters, producing speech that was intelligible but unmistakably robotic. The 1990s saw the rise of <em>concatenative synthesis</em>, which stitched together pre-recorded speech segments to produce more natural output. The 2000s brought <em>statistical parametric synthesis</em> using Hidden Markov Models (HMMs), which offered flexibility at the cost of a characteristic "buzzy" quality. Finally, starting in 2016 with WaveNet, <em>neural TTS</em> systems achieved near-human quality, triggering an explosion of research that continues today.
                </p>

                <p>
                    This tutorial &mdash; the first in Part VIII (Neural Text-to-Speech) &mdash; establishes the foundations. We will cover the representations (phonemes, mel spectrograms, waveforms), the classical approaches (concatenative and parametric), the neural revolution, and how TTS quality is evaluated. The subsequent tutorials will dive deep into each neural component: vocoders, Tacotron, FastSpeech, Glow-TTS, VITS, and bilingual TTS.
                </p>

                <div class="definition-box">
                    <div class="box-title">Definition: Text-to-Speech (TTS)</div>
                    <p>
                        Given an input text string $\mathbf{x} = (x_1, x_2, \ldots, x_L)$ of $L$ characters (or, after processing, a phoneme sequence $\mathbf{p} = (p_1, p_2, \ldots, p_K)$), a <strong>text-to-speech system</strong> generates a speech waveform $\mathbf{y} = (y_1, y_2, \ldots, y_T) \in \mathbb{R}^T$ where $T$ is the number of audio samples. The system approximates the conditional distribution:
                    </p>
                    $$P(\mathbf{y} \mid \mathbf{x}) = P(\mathbf{y} \mid \text{vocoder}(\text{acoustic\_model}(\text{frontend}(\mathbf{x}))))$$
                    <p>
                        For speech sampled at 22050 Hz, a 5-second utterance requires generating $T = 110{,}250$ sample values &mdash; a high-dimensional generation problem that must also satisfy the strict temporal coherence constraints of human auditory perception.
                    </p>
                </div>

                <p>
                    The challenge of TTS is not merely generating <em>any</em> waveform that contains the right words. The generated speech must sound natural, with appropriate prosody (rhythm, stress, intonation), speaker identity, and emotional tone. Humans are extraordinarily sensitive to subtle deviations from natural speech &mdash; even a few milliseconds of unnatural timing or a slight spectral artifact can make synthesized speech feel "off." This is what makes TTS one of the most demanding generative modeling tasks in machine learning.
                </p>

                <p>
                    To appreciate the scale of the problem, consider the dimensionality mismatch. A typical sentence like "Hello, how are you?" contains about 15 phonemes. The corresponding mel spectrogram at standard TTS settings (22050 Hz, hop 256, 80 mel bins) has roughly 300 frames of 80 dimensions each &mdash; a total of 24,000 values. The final waveform has roughly 75,000 samples. The TTS system must expand 15 discrete symbols into 75,000 continuous values, maintaining temporal coherence, spectral naturalness, and prosodic appropriateness throughout.
                </p>

                <!-- Section 2: Text Analysis & Linguistic Frontend -->
                <h2 id="text-analysis">Text Analysis &amp; Linguistic Frontend</h2>

                <p>
                    Before any acoustic modeling can happen, raw text must be transformed into a linguistic representation suitable for speech generation. This stage is called the <strong>linguistic frontend</strong> or <strong>text analysis</strong> module. It solves two main problems: text normalization and grapheme-to-phoneme conversion.
                </p>

                <p>
                    <strong>Text normalization</strong> converts written text into its spoken form. Written language is full of conventions that do not directly correspond to speech:
                </p>
                <ul>
                    <li><strong>Numbers:</strong> "123" should be spoken as "one hundred twenty three" (or "one two three" in a phone number context).</li>
                    <li><strong>Abbreviations:</strong> "Dr." &rarr; "doctor," "St." &rarr; "street" or "saint" (context-dependent!).</li>
                    <li><strong>Dates:</strong> "02/19/2026" &rarr; "February nineteenth, twenty twenty-six."</li>
                    <li><strong>Currency:</strong> "$3.50" &rarr; "three dollars and fifty cents."</li>
                    <li><strong>URLs and emails:</strong> "user@example.com" &rarr; "user at example dot com."</li>
                    <li><strong>Punctuation as prosody cues:</strong> Periods, commas, question marks, and exclamation marks signal pauses and intonation patterns.</li>
                </ul>

                <p>
                    Text normalization is deceptively hard. The same written form can have different spoken forms depending on context. Consider "Dr.": it is "doctor" before a name ("Dr. Smith") but "drive" in an address ("100 Oak Dr."). Similarly, "read" is /riyd/ (present tense) or /rehd/ (past tense) depending on the sentence. Classical TTS systems used finite-state transducers (FSTs) for text normalization &mdash; see <a href="../24-fst-applications/index.html">Tutorial 23: FST Applications</a> for details on this approach.
                </p>

                <p>
                    <strong>Grapheme-to-phoneme (G2P) conversion</strong> maps written words to their phonemic representations. A <em>grapheme</em> is a written unit (letter or character), while a <em>phoneme</em> is a unit of sound that distinguishes meaning. English is notoriously irregular: "through," "though," "thought," "tough," and "thorough" all have the letter sequence "ough" but pronounce it differently. G2P must resolve these irregularities.
                </p>

                <p>
                    The two main approaches to G2P are:
                </p>
                <ul>
                    <li><strong>Dictionary lookup:</strong> Maintain a pronunciation dictionary (lexicon) that maps each word to its phoneme sequence. The CMU Pronouncing Dictionary (CMUdict) contains ~134,000 English words in ARPAbet notation. For example: <code>HELLO &rarr; HH AH L OW</code>, <code>WORLD &rarr; W ER L D</code>.</li>
                    <li><strong>Learned G2P models:</strong> For out-of-vocabulary (OOV) words, a trained sequence-to-sequence model predicts the phoneme sequence from the character sequence. Modern G2P models use transformer architectures and achieve &gt;95% word-level accuracy on English.</li>
                </ul>

                <p>
                    <strong>Phoneme inventories.</strong> Two standard phoneme notation systems dominate TTS:
                </p>
                <ul>
                    <li><strong>ARPAbet:</strong> An ASCII-based phoneme set designed for American English, using 1&ndash;2 letter codes. It has 39 phonemes (15 vowels + 24 consonants). Vowels carry stress markers: <code>AH0</code> (unstressed), <code>AH1</code> (primary stress), <code>AH2</code> (secondary stress). ARPAbet is the standard for most English TTS systems.</li>
                    <li><strong>International Phonetic Alphabet (IPA):</strong> A universal phoneme set that covers all human languages using Unicode symbols. For example, "hello" in IPA is /h&#601;&#712;lo&#650;/. IPA is essential for multilingual TTS but is more complex to process computationally.</li>
                </ul>

                <p>
                    The output of the linguistic frontend is a sequence of phonemes (or phoneme-like tokens) along with prosodic markers. This phoneme sequence becomes the input to the acoustic model. The quality of the linguistic frontend directly impacts the final speech &mdash; if a word is mapped to the wrong phonemes, no amount of acoustic modeling can fix the error.
                </p>

                <p>
                    <strong>Characters vs. phonemes as model input.</strong> An important design decision in modern TTS is whether the acoustic model takes characters or phonemes as input. Character-level models (like the original Tacotron) avoid the G2P step entirely, letting the neural network learn pronunciation implicitly. This simplifies the pipeline but requires more training data (the model must learn English spelling irregularities from examples) and can fail on rare words. Phoneme-level models (like FastSpeech 2, Glow-TTS, VITS) use the G2P output, giving the model a cleaner, more regular input but depending on the G2P's accuracy. For languages with phonemic orthographies (Turkish, Finnish, Kyrgyz), the distinction is less important because characters and phonemes are nearly identical.
                </p>

                <div class="note-box">
                    <div class="box-title">Cross-Reference: FST-based Text Normalization</div>
                    <p style="margin-bottom: 0;">
                        Classical TTS systems implement text normalization using weighted finite-state transducers (WFSTs). A WFST maps input strings (raw text) to output strings (normalized text) through a series of transduction rules. For example, a number normalization WFST maps the digit string "123" to the word string "one hundred twenty three" through composed transducers for digit-to-word mapping, place-value expansion, and conjunction insertion. See <a href="../24-fst-applications/index.html">Tutorial 23: FST Applications</a> for implementation details.
                    </p>
                </div>

                <!-- Section 3: Key Representations -->
                <h2 id="key-representations">Key Representations</h2>

                <p>
                    TTS involves three fundamental representations, each operating at a different level of abstraction. Understanding these representations and the mathematical relationships between them is essential for working with any TTS system.
                </p>

                <p>
                    <strong>Phonemes as input.</strong> The phoneme sequence is a discrete, symbolic representation of "what to say." A typical English utterance of 10 words might contain 30&ndash;50 phonemes. Each phoneme is represented as an integer index into the phoneme vocabulary (e.g., ARPAbet has ~39 phonemes plus special tokens for silence, word boundaries, etc.). In a neural TTS model, each phoneme index is mapped to a learned embedding vector (typically 256 or 512 dimensions) through an embedding table, similar to word embeddings in NLP. The acoustic model's job is to expand this sparse sequence into a dense acoustic representation &mdash; a many-to-one mapping since each phoneme corresponds to multiple acoustic frames.
                </p>

                <p>
                    The relationship between phonemes and acoustic frames is not fixed: the same phoneme can have very different durations depending on context. A stressed vowel in a content word might last 120 ms (about 10 mel frames), while the same vowel unstressed in a function word might last only 40 ms (about 3 mel frames). This variability is precisely what makes the alignment problem &mdash; determining which phoneme corresponds to which frames &mdash; so central to TTS. Different architectures handle this differently: Tacotron learns alignment via attention, FastSpeech uses an explicit duration predictor, and Glow-TTS/VITS use Monotonic Alignment Search (MAS).
                </p>

                <p>
                    <strong>Mel spectrograms as the intermediate representation.</strong> The mel spectrogram is the bridge between the symbolic world of phonemes and the physical world of sound waves. It represents the frequency content of a signal over time, with frequencies warped according to the <strong>mel scale</strong> &mdash; a perceptual scale that approximates how humans perceive pitch.
                </p>

                <p>
                    The mel scale was proposed by Stevens, Volkmann, and Newman (1937) based on psychoacoustic experiments. Humans perceive equal ratios of frequency as equal differences in pitch at low frequencies, but this relationship becomes logarithmic at higher frequencies. The standard formula for converting from Hertz to mel is:
                </p>
                $$m = 2595 \log_{10}\left(1 + \frac{f}{700}\right)$$

                <p>
                    And the inverse:
                </p>
                $$f = 700 \left(10^{m/2595} - 1\right)$$

                <div class="note-box">
                    <div class="box-title">Derivation: The Mel Scale</div>
                    <p>
                        The mel scale is designed so that equal perceptual pitch intervals correspond to equal distances on the scale. Consider what happens to different frequency intervals:
                    </p>
                    <ul>
                        <li>$f = 1000$ Hz $\Rightarrow$ $m = 2595 \log_{10}(1 + 1000/700) = 2595 \log_{10}(2.4286) \approx 999.9$ mel</li>
                        <li>$f = 2000$ Hz $\Rightarrow$ $m = 2595 \log_{10}(1 + 2000/700) = 2595 \log_{10}(3.8571) \approx 1521.2$ mel</li>
                        <li>$f = 4000$ Hz $\Rightarrow$ $m = 2595 \log_{10}(1 + 4000/700) = 2595 \log_{10}(6.7143) \approx 2146.1$ mel</li>
                    </ul>
                    <p>
                        The interval from 1000 to 2000 Hz (1000 Hz in linear) maps to $1521.2 - 999.9 = 521.3$ mel. The interval from 2000 to 4000 Hz (2000 Hz in linear, double the previous gap) maps to $2146.1 - 1521.2 = 624.9$ mel. Despite the frequency gap doubling, the mel gap increased by only 20%. This compression of higher frequencies reflects human perception: we are much more sensitive to frequency differences at lower frequencies than at higher ones.
                    </p>
                    <p>
                        A mel spectrogram with $n_{\text{mels}} = 80$ bins places these bins equally spaced on the mel scale between $f_{\min}$ and $f_{\max}$, then maps them back to Hz to create triangular filterbank weights. The result is high frequency resolution at low frequencies (where speech formants are most informative) and lower resolution at high frequencies.
                    </p>
                </div>

                <p>
                    The design choice of 80 mel bins is not arbitrary. Early TTS systems used 13 mel-frequency cepstral coefficients (MFCCs), which are a further compression of the mel spectrogram via the discrete cosine transform (DCT). However, MFCCs discard fine spectral structure that neural vocoders can exploit. Modern systems use 80 mel bins as a compromise: enough resolution to preserve the perceptually important spectral details, but compact enough for efficient processing. Some systems use 128 bins for higher quality at the cost of larger intermediate representations.
                </p>

                <p>
                    <strong>Computing a mel spectrogram.</strong> The process involves three steps:
                </p>
                <ol>
                    <li><strong>Short-time Fourier transform (STFT):</strong> Divide the waveform into overlapping frames of length <code>n_fft</code> (typically 1024 samples), shifted by <code>hop_length</code> (typically 256 samples). Apply the FFT to each frame to get the magnitude spectrum.</li>
                    <li><strong>Mel filterbank:</strong> Multiply the magnitude spectrum by a bank of $n_{\text{mels}}$ triangular filters spaced equally on the mel scale. This compresses the frequency axis according to human perception.</li>
                    <li><strong>Log compression:</strong> Take the logarithm of the mel-filtered energies. This further compresses the dynamic range, mirroring how human loudness perception is roughly logarithmic.</li>
                </ol>

                <p>
                    <strong>Hop-size arithmetic.</strong> The hop length determines the frame rate of the mel spectrogram. With a sample rate of $sr = 22050$ Hz and hop length $h = 256$:
                </p>
                <ul>
                    <li>Frame rate: $sr / h = 22050 / 256 \approx 86.1$ frames per second.</li>
                    <li>Frame duration: $h / sr = 256 / 22050 \approx 11.6$ ms per frame.</li>
                    <li>A 3-second audio clip produces $\lfloor 3 \times 22050 / 256 \rfloor = 258$ mel frames.</li>
                    <li>Each frame is an 80-dimensional vector (for $n_{\text{mels}} = 80$), so the full mel spectrogram is a matrix of shape $(80 \times 258)$.</li>
                </ul>

                <p>
                    <strong>Waveforms as the final output.</strong> The waveform is the raw audio signal &mdash; a one-dimensional sequence of amplitude values sampled at a fixed rate (typically 22050 Hz for TTS, or 16000 Hz for some systems). At 22050 Hz, one second of audio is 22050 floating-point numbers. For 16-bit PCM encoding, each sample is an integer in the range $[-32768, 32767]$, but during processing, waveforms are typically represented as floating-point values in $[-1, 1]$.
                </p>

                <p>
                    The vocoder's job is to convert the mel spectrogram back to this waveform. This is an ill-posed problem because the mel spectrogram transformation is lossy in multiple ways: (1) phase information is discarded (only magnitude is kept), (2) the mel filterbank merges fine spectral details, and (3) the log compression reduces dynamic range. The vocoder must "hallucinate" the missing information in a perceptually plausible way. This is why vocoding was historically the weakest link in the TTS chain, and why the development of high-quality neural vocoders (WaveNet, HiFi-GAN) was so transformative.
                </p>

                <!-- Section 4: Concatenative Synthesis -->
                <h2 id="concatenative-synthesis">Concatenative Synthesis</h2>

                <p>
                    Concatenative synthesis was the dominant approach to TTS from the mid-1990s until around 2016. The core idea is beautifully simple: instead of generating speech from scratch, record a large database of speech and stitch together the right pieces. Since the building blocks are real human speech, the output naturally sounds human &mdash; provided the stitching is done well.
                </p>

                <p>
                    <strong>Unit selection.</strong> The most successful form of concatenative synthesis is <em>unit selection</em>. The speech database contains many hours (typically 10&ndash;50+) of recorded speech from a single speaker, read from carefully designed scripts that maximize phonetic coverage. The recordings are segmented into units at various levels: phones, diphones (phone-to-phone transitions), syllables, words, and even phrases. Each unit is indexed with its linguistic context (surrounding phones, position in word, stress, etc.) and acoustic properties (pitch, energy, duration, spectral characteristics). A typical unit selection database for high-quality English TTS might contain 500,000+ unit candidates.
                </p>

                <p>
                    At synthesis time, the system selects the best sequence of units from the database. This is formulated as an optimization problem over two costs:
                </p>
                <ul>
                    <li><strong>Target cost</strong> $C_t(u_i, t_i)$: How well does unit $u_i$ match the linguistic specification $t_i$ (desired phoneme, pitch, stress, duration, etc.)?</li>
                    <li><strong>Join cost</strong> $C_j(u_i, u_{i+1})$: How smooth is the acoustic transition from unit $u_i$ to unit $u_{i+1}$? This measures spectral discontinuity, pitch discontinuity, and energy discontinuity at the concatenation point.</li>
                </ul>

                <p>
                    The optimal unit sequence minimizes the total cost:
                </p>
                $$\hat{\mathbf{u}} = \arg\min_{\mathbf{u}} \sum_{i=1}^{N} \left[ w_t \cdot C_t(u_i, t_i) + w_j \cdot C_j(u_{i-1}, u_i) \right]$$

                <p>
                    where $w_t$ and $w_j$ are weights balancing the two costs. This is solved efficiently using a Viterbi-style dynamic programming algorithm over the unit lattice, where each node represents a candidate unit and edges represent concatenation costs.
                </p>

                <p>
                    <strong>Strengths:</strong> When concatenative synthesis finds a good sequence of units, the output sounds remarkably natural because it is literally made of real speech. Longer units (phrases, sentences) that happen to exist in the database can be used directly, sounding perfect.
                </p>

                <p>
                    <strong>Weaknesses:</strong> The approach has fundamental limitations:
                </p>
                <ul>
                    <li><strong>Inflexibility:</strong> The system can only produce speech that its database can cover. Novel word combinations, unusual prosodic patterns, or emotions not present in the database result in poor output.</li>
                    <li><strong>Database size:</strong> High quality requires enormous databases (10&ndash;50+ hours), all from a single speaker in consistent recording conditions. Adding a new voice means recording an entirely new database.</li>
                    <li><strong>Concatenation artifacts:</strong> Despite sophisticated join cost functions, audible discontinuities (clicks, pitch jumps, spectral glitches) at concatenation points remain a persistent problem.</li>
                    <li><strong>No controllability:</strong> It is very difficult to control speaking rate, emphasis, or emotion beyond what was recorded in the database.</li>
                </ul>

                <p>
                    Commercial systems like AT&amp;T Natural Voices, Nuance Vocalizer, and early versions of Apple Siri used concatenative synthesis. These systems worked well for constrained domains (navigation, weather) but struggled with open-domain text.
                </p>

                <p>
                    <strong>Diphone synthesis</strong> is a simpler variant that uses only diphone units (the transition from the center of one phone to the center of the next). A diphone inventory for English requires only about $40^2 / 2 \approx 800$ units, making it compact. However, because diphones are short and numerous joins are needed, the output tends to sound less natural than full unit-selection synthesis. Festival and MBROLA were popular open-source diphone systems.
                </p>

                <p>
                    The fundamental insight from the concatenative era is that <strong>real speech sounds real</strong>. Any synthesis method that directly uses (or closely imitates) recorded speech segments will achieve higher perceived naturalness than methods that generate speech from abstract parameters. This insight carries forward to the neural era: the best neural vocoders are those that produce waveforms with the same statistical properties as natural speech, including the micro-variations that give speech its lifelike quality.
                </p>

                <!-- Section 5: Statistical Parametric Synthesis -->
                <h2 id="parametric-synthesis">Statistical Parametric Synthesis</h2>

                <p>
                    Statistical parametric synthesis, dominant from the mid-2000s to 2016, took a fundamentally different approach: instead of storing and selecting real speech segments, it trained statistical models to <em>generate</em> acoustic features from linguistic features. The generated features were then converted to a waveform by a vocoder.
                </p>

                <p>
                    The most successful framework was <strong>HMM-based TTS (HTS &mdash; HMM-based Speech Synthesis System)</strong>. HTS used the same HMM machinery developed for speech recognition, but in reverse: instead of recognizing speech from audio, it generated audio parameters from text.
                </p>

                <p>
                    <strong>The HTS pipeline:</strong>
                </p>
                <ol>
                    <li><strong>Linguistic features:</strong> Extract a rich feature vector for each phoneme, including phoneme identity, position in syllable/word/phrase, stress, neighboring phonemes (quinphone context), etc. A typical feature vector has 500+ dimensions.</li>
                    <li><strong>Duration model:</strong> An HMM-based model (or decision tree) predicts the duration (in frames) of each phoneme state. This determines the rhythm of the output speech.</li>
                    <li><strong>Acoustic model:</strong> Context-dependent HMMs with decision-tree clustered states generate frame-level acoustic features: typically mel-cepstral coefficients (MCCs), fundamental frequency (F0), and band aperiodicity. The decision trees cluster similar contexts together, enabling the model to generalize to unseen phoneme contexts. See <a href="../28-forced-alignment/index.html">Tutorial 27</a> for background on HMM-based speech models.</li>
                    <li><strong>Vocoder:</strong> A signal-processing vocoder (e.g., STRAIGHT, WORLD) converts the generated MCCs, F0, and aperiodicity back into a waveform.</li>
                </ol>

                <p>
                    <strong>The over-smoothing problem.</strong> HTS-generated speech has a characteristic "buzzy" or "muffled" quality that sounds distinctly synthetic. The root cause is <em>over-smoothing</em>: the statistical model, when predicting acoustic features, produces the mean of the distribution for each frame. This averaging removes the natural micro-variations (jitter, shimmer, spectral detail) that make human speech sound alive. Imagine averaging together 100 photographs of different cats &mdash; you get a blurry blob that is statistically representative but perceptually unnatural. The same thing happens to the generated speech spectra.
                </p>

                <p>
                    Formally, if the model learns a distribution $P(\mathbf{o}_t | \text{context})$ over acoustic features $\mathbf{o}_t$, the MMSE (minimum mean-squared error) prediction is $\hat{\mathbf{o}}_t = \mathbb{E}[\mathbf{o}_t | \text{context}]$. This expectation is smoother than any actual realization from the distribution. The variance of the generated features is systematically lower than that of real speech, and this perceptual gap is very noticeable.
                </p>

                <p>
                    <strong>Advantages over concatenative synthesis:</strong> Despite its sound quality limitations, parametric synthesis offered significant benefits:
                </p>
                <ul>
                    <li><strong>Small footprint:</strong> The entire model fits in a few megabytes (vs. gigabytes for concatenative databases).</li>
                    <li><strong>Controllability:</strong> Speaking rate, pitch, and voice quality can be modified by adjusting model parameters at synthesis time.</li>
                    <li><strong>New voices:</strong> Speaker adaptation techniques (e.g., MLLR, CMLLR) can create new voices from as little as a few minutes of data.</li>
                    <li><strong>Consistency:</strong> No concatenation artifacts &mdash; the output is always smooth (too smooth, in fact).</li>
                    <li><strong>Multilingual support:</strong> The same framework works for any language, given appropriate phone sets and training data.</li>
                </ul>

                <p>
                    <strong>DNN-based parametric synthesis (2013&ndash;2016).</strong> Before the fully neural systems, researchers replaced HMM emission distributions with deep neural networks (DNNs), while keeping the rest of the HTS pipeline. DNNs could model more complex mappings from linguistic features to acoustic features, reducing (but not eliminating) the over-smoothing problem. This represented an incremental improvement but still relied on signal-processing vocoders, which were the bottleneck for quality. The real breakthrough came when both the acoustic model and the vocoder became fully neural.
                </p>

                <!-- Section 6: The Neural Revolution -->
                <h2 id="neural-revolution">The Neural Revolution</h2>

                <p>
                    The landscape of TTS changed dramatically starting in 2016, when DeepMind published WaveNet. Within just five years, neural TTS systems achieved quality indistinguishable from human speech in controlled evaluations. This section traces the key milestones.
                </p>

                <p>
                    <strong>WaveNet (2016).</strong> WaveNet, published by DeepMind, was a deep autoregressive neural network that generated audio samples one at a time, conditioned on all previous samples. Each sample was predicted as a categorical distribution over 256 possible quantized values (using mu-law companding), using a stack of dilated causal convolutions. The receptive field grew exponentially with depth, allowing the model to capture both local (individual waveform cycles) and global (prosodic) patterns. The key insight was that by modeling the raw waveform directly (instead of vocoder parameters), the model could capture the fine acoustic details that parametric synthesis smoothed away. WaveNet achieved a MOS of 4.21 (vs. 3.47 for the best parametric system), an unprecedented quality leap. However, it was extraordinarily slow: generating one second of audio at 16 kHz required 16,000 sequential neural network forward passes, taking minutes on a GPU.
                </p>

                <p>
                    <strong>Tacotron (2017).</strong> While WaveNet solved the vocoder problem, the acoustic model still needed improvement. Google's Tacotron was an end-to-end sequence-to-sequence model that directly mapped character sequences to mel spectrograms, eliminating the need for the complex linguistic frontend of traditional systems. It used an encoder-decoder architecture with content-based attention (Bahdanau attention), learning the alignment between text and mel frames implicitly during training. The decoder was autoregressive, generating one mel frame at a time (or a few frames per step using a "reduction factor"). Tacotron's output was then passed to a vocoder (initially Griffin-Lim, later WaveNet) to produce the final waveform. Despite its simplicity, Tacotron produced remarkably natural-sounding speech and demonstrated that end-to-end learning could replace the hand-engineered linguistic frontend.
                </p>

                <p>
                    <strong>Tacotron 2 (2017).</strong> An improved version that combined a refined Tacotron-like acoustic model (with location-sensitive attention and a more powerful decoder) with a modified WaveNet vocoder conditioned on mel spectrograms. The architecture used a convolutional encoder followed by a bidirectional LSTM, and the decoder used two LSTM layers with a pre-net bottleneck that helped regularize the autoregressive generation. Tacotron 2 achieved MOS scores of 4.53 &mdash; statistically indistinguishable from human speech (4.58) in the tested conditions. This was the first system to truly close the gap with human speech quality.
                </p>

                <p>
                    <strong>FastSpeech (2019).</strong> Tacotron's autoregressive decoder generated mel frames one at a time, making it slow and prone to attention failures (repeated words, skipped words, garbled output). FastSpeech, from Microsoft, replaced the autoregressive decoder with a feed-forward transformer that generated all mel frames in parallel. The key innovation was the <strong>length regulator</strong>: a duration predictor trained on alignments extracted from a pre-trained Tacotron 2 teacher model determined how many mel frames each phoneme should produce. Each phoneme's hidden representation was then duplicated that many times to create the expanded sequence, which was processed by the decoder in parallel. FastSpeech was ~300x faster than real-time and eliminated robustness issues entirely (no attention means no attention failures), at a small cost in naturalness. The follow-up <strong>FastSpeech 2</strong> (2021) improved quality by adding explicit pitch and energy predictors and using ground-truth durations from MFA instead of teacher distillation.
                </p>

                <p>
                    <strong>Glow-TTS (2020).</strong> Glow-TTS, from KAIST, combined a flow-based generative model with Monotonic Alignment Search (MAS) to learn the alignment between text and mel spectrogram without any external aligner. The core idea is elegant: model the mel spectrogram as a learned invertible transformation (normalizing flow) of a simple isotropic Gaussian prior. The mean and variance of this prior are predicted from the text encoder output. During training, MAS finds the optimal monotonic alignment between text tokens and mel frames by maximizing the likelihood of the data under the flow model. At inference, the model samples from the prior and applies the inverse flow to generate the mel spectrogram, all in parallel.
                </p>

                <p>
                    Glow-TTS achieved both parallel generation and diverse outputs (different prosodic variations for the same input text, controlled by the temperature of the prior sampling). Crucially, it eliminated the need for an external forced aligner like MFA, making the training pipeline self-contained. See <a href="../27-mas-algorithm/index.html">Tutorial 26: MAS Algorithm</a> for the alignment mechanism.
                </p>

                <p>
                    <strong>VITS (2021).</strong> VITS (Variational Inference with adversarial learning for end-to-end Text-to-Speech) unified the acoustic model and vocoder into a single end-to-end model. It combined three powerful generative modeling techniques: (1) a conditional VAE that models the latent space of speech, (2) normalizing flows that transform a simple prior into the complex distribution of speech features, and (3) adversarial training (using a HiFi-GAN-style discriminator) that ensures the generated waveforms have realistic spectral structure. The text encoder feeds into a stochastic duration predictor and the flow-based posterior encoder, while MAS learns the alignment during training. At inference, VITS generates waveforms directly from text in a single forward pass. VITS achieved quality comparable to Tacotron 2 + HiFi-GAN while being a single model, and it supports both deterministic and stochastic synthesis (generating different prosodic variations for the same input text).
                </p>

                <p>
                    <strong>The quality trajectory.</strong> To put the neural revolution in perspective, here are approximate MOS scores across eras (from comparable evaluations on English):
                </p>
                <ul>
                    <li>Concatenative synthesis (2000s): MOS ~3.5&ndash;4.0</li>
                    <li>HMM-based parametric (HTS, 2010s): MOS ~2.5&ndash;3.5</li>
                    <li>DNN-based parametric (2013&ndash;2016): MOS ~3.0&ndash;3.8</li>
                    <li>WaveNet vocoder (2016): MOS ~4.2</li>
                    <li>Tacotron 2 + WaveNet (2017): MOS ~4.5</li>
                    <li>VITS / FastSpeech 2 + HiFi-GAN (2021): MOS ~4.3&ndash;4.5</li>
                    <li>Human speech: MOS ~4.5&ndash;4.7</li>
                </ul>

                <div class="note-box">
                    <div class="box-title">Part VIII Roadmap</div>
                    <p style="margin-bottom: 0;">
                        The remaining tutorials in this series dive deep into each of these systems. <strong>Tutorial 29</strong> covers neural vocoders (WaveNet, WaveRNN, HiFi-GAN). <strong>Tutorial 30</strong> covers Tacotron and Tacotron 2. <strong>Tutorial 31</strong> covers FastSpeech and FastSpeech 2. <strong>Tutorial 32</strong> covers Glow-TTS. <strong>Tutorial 33</strong> covers VITS. <strong>Tutorial 34</strong> covers bilingual and multilingual TTS. Each tutorial assumes the foundations established here.
                    </p>
                </div>

                <!-- Section 7: Evaluating TTS: MOS and Beyond -->
                <h2 id="evaluation-metrics">Evaluating TTS: MOS and Beyond</h2>

                <p>
                    Evaluating TTS quality is surprisingly difficult. Unlike classification tasks (where accuracy is a clear metric) or language modeling (where perplexity gives a principled measure), there is no single number that reliably captures "how good does this speech sound?" The gold standard remains human judgment, but human evaluations are expensive, slow, and fraught with methodological pitfalls.
                </p>

                <p>
                    <strong>Mean Opinion Score (MOS).</strong> MOS is the most widely used TTS evaluation metric. In a MOS test, human listeners rate the naturalness of speech samples on a 5-point scale:
                </p>
                <ul>
                    <li><strong>5 &mdash; Excellent:</strong> Completely natural, indistinguishable from human speech.</li>
                    <li><strong>4 &mdash; Good:</strong> Natural-sounding with minor imperfections.</li>
                    <li><strong>3 &mdash; Fair:</strong> Somewhat natural, noticeable artifacts.</li>
                    <li><strong>2 &mdash; Poor:</strong> Unnatural, significant degradation.</li>
                    <li><strong>1 &mdash; Bad:</strong> Very unnatural, difficult to understand.</li>
                </ul>

                <p>
                    The MOS for a system is the arithmetic mean of all ratings from all listeners across all test sentences. A MOS difference of 0.1 or more is generally considered perceptually meaningful. Human speech typically receives a MOS of 4.5&ndash;4.7 (not 5.0, because even real recordings have some imperfections).
                </p>

                <p>
                    <strong>MOS test protocol.</strong> A rigorous MOS test requires:
                </p>
                <ul>
                    <li>At least 20&ndash;30 listeners (preferably native speakers of the language).</li>
                    <li>At least 20&ndash;30 test sentences per system, covering diverse phonetic content.</li>
                    <li>Randomized presentation order to avoid ordering effects.</li>
                    <li>Anchor samples (very good and very bad) to calibrate the rating scale.</li>
                    <li>Headphones in a quiet environment (or a crowdsourced setup with attention checks).</li>
                </ul>

                <p>
                    <strong>MUSHRA (MUltiple Stimuli with Hidden Reference and Anchor).</strong> MUSHRA is a more discriminative evaluation method. Listeners rate all systems simultaneously on a 0&ndash;100 scale, with a hidden reference (real speech) and a hidden anchor (low quality). Because listeners can directly compare systems side by side, MUSHRA reveals differences that MOS might miss. It is particularly useful for comparing systems that are close in quality.
                </p>

                <p>
                    <strong>Objective metrics.</strong> Researchers have developed automated metrics that correlate with human perception:
                </p>
                <ul>
                    <li><strong>MCD (Mel Cepstral Distortion):</strong> The Euclidean distance between the mel-cepstral coefficients of synthesized and reference speech, measured in dB. Lower is better. MCD $\leq 5$ dB generally indicates good quality. Formally: $\text{MCD} = \frac{10}{\ln 10} \sqrt{2 \sum_{d=1}^{D} (c_d^{\text{synth}} - c_d^{\text{ref}})^2}$ where $c_d$ are mel-cepstral coefficients.</li>
                    <li><strong>PESQ (Perceptual Evaluation of Speech Quality):</strong> An ITU-T standard (P.862) that predicts the subjective MOS of degraded speech. Originally designed for telephone quality assessment, it has been adapted for TTS evaluation. Scores range from -0.5 to 4.5.</li>
                    <li><strong>F0 RMSE:</strong> Root mean squared error of fundamental frequency between synthesized and reference speech, measuring pitch accuracy.</li>
                    <li><strong>V/UV error rate:</strong> The percentage of frames where the voiced/unvoiced decision disagrees between synthesized and reference speech.</li>
                </ul>

                <p>
                    <strong>Why objective metrics are insufficient.</strong> The gap between objective metrics and subjective quality is fundamental. MCD computes a point-by-point spectral distance, but human perception is not point-by-point: we process speech through a cascade of auditory, phonetic, and linguistic systems that are selectively sensitive to certain types of distortion and tolerant of others. A subtle pitch discontinuity at a syllable boundary might be imperceptible, while the same magnitude of distortion during a sustained vowel is jarring. No single scalar metric captures this context-dependent sensitivity, which is why human evaluation remains essential.
                </p>

                <div class="warning-box">
                    <div class="box-title">Warning: MOS Limitations</div>
                    <p>
                        MOS scores are <strong>not comparable across studies</strong>. A MOS of 4.2 in one paper does not mean the same quality as 4.2 in another paper. The scores depend on: listener pool (experts vs. crowdworkers), test sentences, rating interface, anchor quality, language, and even the other systems being compared (context effects). The only valid comparison is between systems evaluated in the <em>same</em> listening test.
                    </p>
                    <p style="margin-bottom: 0;">
                        Objective metrics have a similar problem: MCD correlates with perceived quality within a system family but can be misleading across architectures. A flow-based model might have higher MCD than an autoregressive model yet sound better to listeners, because MCD penalizes spectral differences uniformly while human perception weights some differences more than others. Recent work on neural MOS prediction (UTMOS, DNSMOS) attempts to bridge this gap by training neural networks to predict human MOS ratings from audio, achieving moderate success but still falling short of actual human evaluation for fine-grained comparisons.
                    </p>
                </div>

                <!-- Section 8: The Modern TTS Landscape -->
                <h2 id="tts-landscape">The Modern TTS Landscape</h2>

                <p>
                    Modern neural TTS systems can be organized along several axes. Understanding this taxonomy helps navigate the design space and appreciate the trade-offs each system makes.
                </p>

                <p>
                    <strong>Autoregressive (AR) vs. non-autoregressive (non-AR).</strong> AR models generate one output frame at a time, conditioned on all previously generated frames. This gives them excellent quality (each prediction uses maximum context) but makes them slow and susceptible to error accumulation (a bad frame can derail all subsequent frames). In Tacotron 2, the decoder generates one mel frame per step using an LSTM that takes the previous frame as input. A "stop token" predictor decides when to stop generating. Non-AR models generate all frames in parallel, offering fast inference and robustness but requiring explicit duration information and sometimes sacrificing fine-grained temporal details. The speed advantage is dramatic: a non-AR model can generate a full utterance in a single forward pass (~10ms), while an AR model requires hundreds of sequential steps.
                </p>

                <p>
                    <strong>Two-stage vs. end-to-end.</strong> Two-stage systems separate the problem into acoustic model (text &rarr; mel spectrogram) and vocoder (mel spectrogram &rarr; waveform). This modularity allows mixing and matching components (e.g., Tacotron 2 + HiFi-GAN) but introduces a potential mismatch between training and inference. End-to-end systems (like VITS) generate waveforms directly from text in a single model, eliminating the mismatch at the cost of more complex training.
                </p>

                <p>
                    <strong>Alignment strategy.</strong> How does the model learn the mapping between input tokens and output frames? This is arguably the most important design decision in a TTS system, as it determines both robustness and the flexibility of the pipeline.
                </p>
                <ul>
                    <li><strong>Attention-based:</strong> The model learns a soft alignment (attention weights) between encoder and decoder states. The attention mechanism computes a weighted sum over all encoder states at each decoder step, effectively learning which parts of the input to focus on. Flexible but fragile &mdash; attention can fail, causing word repetitions, skipping, or garbled output, especially on long or unusual sentences. (Tacotron, Tacotron 2)</li>
                    <li><strong>External alignment:</strong> Phone durations are provided by an external tool (MFA for ground-truth durations, or MAS from a teacher model via knowledge distillation). A duration predictor is trained to predict these durations from text, and at inference the predicted durations determine the expansion factor for each phoneme. Robust and reliable, but requires a separate alignment step and the quality depends on the external aligner. (FastSpeech, FastSpeech 2)</li>
                    <li><strong>Learned alignment (non-attention):</strong> The model learns a monotonic alignment during training without soft attention, using algorithms like Monotonic Alignment Search (MAS). MAS finds the hard monotonic alignment that maximizes the data likelihood, running as part of the forward pass during training. This combines the robustness of external alignment with the end-to-end nature of attention-based methods. (Glow-TTS, VITS)</li>
                </ul>

                <p>
                    The trend in the field has been away from attention-based alignment and toward MAS-based alignment, because attention failures are the most common cause of synthesis errors in production systems. The tutorials that follow will explore each alignment strategy in detail.
                </p>

                <table style="width: 100%; border-collapse: collapse; margin: 1rem 0;">
                    <thead>
                        <tr style="border-bottom: 2px solid var(--color-border);">
                            <th style="text-align: left; padding: 0.5rem;">System</th>
                            <th style="text-align: left; padding: 0.5rem;">Type</th>
                            <th style="text-align: left; padding: 0.5rem;">Alignment</th>
                            <th style="text-align: left; padding: 0.5rem;">Vocoder</th>
                            <th style="text-align: left; padding: 0.5rem;">Speed</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.5rem;">Tacotron 2</td>
                            <td style="padding: 0.5rem;">AR, 2-stage</td>
                            <td style="padding: 0.5rem;">Attention</td>
                            <td style="padding: 0.5rem;">WaveNet / HiFi-GAN</td>
                            <td style="padding: 0.5rem;">Slow</td>
                        </tr>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.5rem;">FastSpeech 2</td>
                            <td style="padding: 0.5rem;">Non-AR, 2-stage</td>
                            <td style="padding: 0.5rem;">External (MFA)</td>
                            <td style="padding: 0.5rem;">HiFi-GAN</td>
                            <td style="padding: 0.5rem;">Fast</td>
                        </tr>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.5rem;">Glow-TTS</td>
                            <td style="padding: 0.5rem;">Non-AR, 2-stage</td>
                            <td style="padding: 0.5rem;">MAS (learned)</td>
                            <td style="padding: 0.5rem;">HiFi-GAN</td>
                            <td style="padding: 0.5rem;">Fast</td>
                        </tr>
                        <tr>
                            <td style="padding: 0.5rem;">VITS</td>
                            <td style="padding: 0.5rem;">Non-AR, end-to-end</td>
                            <td style="padding: 0.5rem;">MAS (learned)</td>
                            <td style="padding: 0.5rem;">Built-in (HiFi-GAN decoder)</td>
                            <td style="padding: 0.5rem;">Fast</td>
                        </tr>
                    </tbody>
                </table>

                <p>
                    <strong>Training data requirements.</strong> The amount and quality of training data varies significantly across architectures:
                </p>
                <ul>
                    <li><strong>Single-speaker high-quality:</strong> Most academic systems are trained on LJSpeech (24 hours, single female English speaker) or similar corpora. 10&ndash;20 hours of clean, consistently recorded speech from one speaker is the sweet spot for most architectures.</li>
                    <li><strong>Multi-speaker:</strong> Systems like multi-speaker VITS or YourTTS require hundreds of hours from many speakers to learn speaker-independent representations. LibriTTS (585 hours, 2,456 speakers) is a common choice.</li>
                    <li><strong>Low-resource:</strong> For under-resourced languages, transfer learning from a related language can reduce the data requirement to as little as 1&ndash;2 hours, though quality degrades. Fine-tuning a multilingual model is the most data-efficient approach.</li>
                </ul>

                <p>
                    <strong>Beyond single-sentence synthesis.</strong> The systems described above generate speech one sentence at a time. Active research areas include: long-form synthesis (maintaining consistent prosody across paragraphs), expressive synthesis (controlling emotion, speaking style, emphasis), zero-shot voice cloning (synthesizing in a new voice from a few seconds of reference audio), and conversational TTS (generating natural turn-taking behavior in dialogue). These advances are gradually making TTS indistinguishable from human speech not just in controlled evaluations but in real-world applications.
                </p>

                <p>
                    <strong>Where we are going.</strong> This tutorial has laid the groundwork: you now understand the TTS pipeline, the key representations, the historical approaches, and how quality is measured. The next tutorials will build on this foundation:
                </p>
                <ul>
                    <li><strong>Tutorial 29 (Neural Vocoders):</strong> How to convert mel spectrograms to waveforms using WaveNet, WaveRNN, and HiFi-GAN.</li>
                    <li><strong>Tutorial 30 (Tacotron):</strong> The attention-based acoustic model that launched end-to-end TTS.</li>
                    <li><strong>Tutorial 31 (FastSpeech):</strong> Parallel generation with explicit duration modeling.</li>
                    <li><strong>Tutorial 32 (Glow-TTS):</strong> Flow-based TTS with learned alignment.</li>
                    <li><strong>Tutorial 33 (VITS):</strong> The fully end-to-end system combining VAE, flows, and adversarial training.</li>
                    <li><strong>Tutorial 34 (Bilingual TTS):</strong> Extending TTS to multiple languages and code-switching.</li>
                </ul>

                <!-- Tutorial Navigation -->
                <div class="tutorial-nav">
                    <a href="../28-forced-alignment/index.html" class="tutorial-nav-link prev">
                        <span class="nav-label">Previous</span>
                        <span class="nav-title">&larr; Forced Alignment &amp; MFA</span>
                    </a>
                    <a href="../30-neural-vocoders/index.html" class="tutorial-nav-link next">
                        <span class="nav-label">Next</span>
                        <span class="nav-title">Neural Vocoders &rarr;</span>
                    </a>
                </div>

            </article>

            <!-- ==================== CODE TAB ==================== -->
            <article class="article-content" id="code" style="display: none;">
                <h2>Code Examples</h2>
                <p>Three implementations covering the core TTS representations: mel spectrogram extraction and visualization, a phoneme conversion pipeline with text normalization, and MOS score statistical analysis.</p>

                <!-- Code Example 1: Mel Spectrogram Extraction -->
                <h3>1. Mel Spectrogram Extraction</h3>
                <p>Extract a mel spectrogram from audio using torchaudio, demonstrating the standard TTS preprocessing pipeline. The parameters used here (<code>n_fft=1024</code>, <code>hop_length=256</code>, <code>n_mels=80</code>, <code>sample_rate=22050</code>) are the de facto standard for most TTS systems (Tacotron 2, FastSpeech 2, VITS all use these or very similar settings). The code also demonstrates the mel scale conversion formulas and shows how frequency resolution is compressed at higher frequencies.</p>

<pre><code>import torch
import torchaudio
import matplotlib.pyplot as plt
import numpy as np

# Load audio
waveform, sr = torchaudio.load("sample.wav")
# Resample to 22050 Hz (standard for TTS)
if sr != 22050:
    resampler = torchaudio.transforms.Resample(sr, 22050)
    waveform = resampler(waveform)
    sr = 22050

# Mel spectrogram parameters
n_fft = 1024
hop_length = 256
n_mels = 80
# Create mel spectrogram transform
mel_transform = torchaudio.transforms.MelSpectrogram(
    sample_rate=sr, n_fft=n_fft, hop_length=hop_length,
    n_mels=n_mels, f_min=0, f_max=8000
)
mel_spec = mel_transform(waveform)
# Convert to log scale (dB)
log_mel = torch.log(torch.clamp(mel_spec, min=1e-5))

print(f"Audio: {waveform.shape[1]} samples at {sr} Hz = {waveform.shape[1]/sr:.2f}s")
print(f"Mel spectrogram: {log_mel.shape}")
print(f"Frames: {log_mel.shape[2]}, Mel bins: {log_mel.shape[1]}")
print(f"Frame rate: {sr/hop_length:.1f} Hz ({hop_length/sr*1000:.1f} ms per frame)")

# Mel scale visualization
def hz_to_mel(hz):
    return 2595 * np.log10(1 + hz / 700)

def mel_to_hz(mel):
    return 700 * (10**(mel / 2595) - 1)

freqs_hz = np.linspace(0, 8000, 1000)
freqs_mel = hz_to_mel(freqs_hz)
print(f"\nMel scale examples:")
print(f"  1000 Hz = {hz_to_mel(1000):.1f} mel")
print(f"  2000 Hz = {hz_to_mel(2000):.1f} mel")
print(f"  4000 Hz = {hz_to_mel(4000):.1f} mel")</code></pre>

                <!-- Code Example 2: Phoneme Conversion Pipeline -->
                <h3>2. Phoneme Conversion Pipeline</h3>
                <p>A complete text normalization and grapheme-to-phoneme conversion pipeline for TTS preprocessing. This implements a simplified version of the linguistic frontend: text normalization (expanding abbreviations and numbers), followed by dictionary-based G2P conversion using the ARPAbet phoneme set. In production systems, the <code>phonemizer</code> library (wrapping eSpeak-NG) or CMUdict handles G2P, but this implementation shows the mechanics from scratch. The code also demonstrates how phonemes are classified into vowels and consonants &mdash; a distinction important for duration modeling, since vowels are typically longer than consonants.</p>

<pre><code># G2P and phoneme processing for TTS
# Using phonemizer library (wrapper around espeak-ng)
# Install: pip install phonemizer

from dataclasses import dataclass
from typing import List, Dict, Optional
import re

# --- Text Normalization ---
class TextNormalizer:
    """Simple text normalizer for TTS preprocessing."""
    def __init__(self):
        self.number_words = {
            '0': 'zero', '1': 'one', '2': 'two', '3': 'three',
            '4': 'four', '5': 'five', '6': 'six', '7': 'seven',
            '8': 'eight', '9': 'nine', '10': 'ten'
        }

    def normalize(self, text: str) -&gt; str:
        text = text.lower().strip()
        text = re.sub(r'[&ldquo;&rdquo;&quot;]', '', text)
        # Expand common abbreviations
        text = text.replace('mr.', 'mister')
        text = text.replace('mrs.', 'missus')
        text = text.replace('dr.', 'doctor')
        text = text.replace("'s", ' is')
        # Simple number expansion (single digits)
        for digit, word in self.number_words.items():
            text = re.sub(r'\b' + digit + r'\b', word, text)
        return text

# --- Phoneme Set ---
ARPABET_VOWELS = [
    'AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'EH', 'ER',
    'EY', 'IH', 'IY', 'OW', 'OY', 'UH', 'UW'
]
ARPABET_CONSONANTS = [
    'B', 'CH', 'D', 'DH', 'F', 'G', 'HH', 'JH', 'K',
    'L', 'M', 'N', 'NG', 'P', 'R', 'S', 'SH', 'T',
    'TH', 'V', 'W', 'Y', 'Z', 'ZH'
]

# Simple word-to-phoneme dictionary
LEXICON = {
    'hello': ['HH', 'AH', 'L', 'OW'],
    'world': ['W', 'ER', 'L', 'D'],
    'the': ['DH', 'AH'],
    'cat': ['K', 'AE', 'T'],
    'sat': ['S', 'AE', 'T'],
    'on': ['AA', 'N'],
    'mat': ['M', 'AE', 'T'],
}

def text_to_phonemes(text: str, lexicon: dict) -&gt; List[str]:
    normalizer = TextNormalizer()
    text = normalizer.normalize(text)
    words = text.split()
    phonemes = []
    for i, word in enumerate(words):
        if word in lexicon:
            phonemes.extend(lexicon[word])
        else:
            phonemes.extend(list(word.upper()))  # fallback: characters
        if i &lt; len(words) - 1:
            phonemes.append('SIL')  # inter-word silence
    return phonemes

# Demo
text = "The cat sat on the mat"
phones = text_to_phonemes(text, LEXICON)
print(f"Text: {text}")
print(f"Phonemes: {' '.join(phones)}")
print(f"Total phonemes: {len(phones)}")
print(f"Vowels: {sum(1 for p in phones if p in ARPABET_VOWELS)}")
print(f"Consonants: {sum(1 for p in phones if p in ARPABET_CONSONANTS)}")</code></pre>

                <!-- Code Example 3: MOS Score Analysis -->
                <h3>3. MOS Score Analysis</h3>
                <p>Statistical analysis of Mean Opinion Score (MOS) results from a TTS evaluation. This code computes 95% confidence intervals using the $t$-distribution (appropriate for small sample sizes), which is the standard statistical methodology for TTS evaluation papers. The simulated scores illustrate a typical pattern: ground truth speech scores highest, followed by modern neural systems (VITS, Tacotron 2), then older parametric systems (HTS). The confidence intervals show how much uncertainty there is in the MOS estimates given 30 listeners.</p>

<pre><code>import numpy as np
from scipy import stats
from typing import List, Tuple

def analyze_mos_scores(
    system_scores: dict,
    confidence_level: float = 0.95
) -&gt; dict:
    """Analyze MOS scores from a TTS evaluation."""
    results = {}
    for system_name, scores in system_scores.items():
        scores = np.array(scores, dtype=float)
        n = len(scores)
        mean = np.mean(scores)
        std = np.std(scores, ddof=1)
        se = std / np.sqrt(n)
        t_crit = stats.t.ppf((1 + confidence_level) / 2, df=n-1)
        ci_low = mean - t_crit * se
        ci_high = mean + t_crit * se
        results[system_name] = {
            'mean': mean, 'std': std, 'n': n,
            'ci_95': (ci_low, ci_high), 'se': se
        }
    return results

# Simulated MOS evaluation
np.random.seed(42)
scores = {
    'Ground Truth': np.clip(np.random.normal(4.5, 0.5, 30), 1, 5),
    'VITS': np.clip(np.random.normal(4.3, 0.6, 30), 1, 5),
    'Tacotron 2 + HiFi-GAN': np.clip(np.random.normal(4.1, 0.7, 30), 1, 5),
    'FastSpeech 2 + HiFi-GAN': np.clip(np.random.normal(3.9, 0.7, 30), 1, 5),
    'Parametric (HTS)': np.clip(np.random.normal(2.8, 0.8, 30), 1, 5),
}

results = analyze_mos_scores(scores)
print("MOS Evaluation Results")
print("=" * 70)
print(f"{'System':&lt;28} {'MOS':&gt;6} {'95% CI':&gt;16} {'n':&gt;5}")
print("-" * 70)
for system, r in sorted(results.items(), key=lambda x: -x[1]['mean']):
    ci = r['ci_95']
    print(f"{system:&lt;28} {r['mean']:&gt;6.2f}  [{ci[0]:.2f}, {ci[1]:.2f}]  {r['n']:&gt;5}")</code></pre>

            </article>

            <!-- ==================== EXERCISES TAB ==================== -->
            <article class="article-content" id="exercises" style="display: none;">
                <h2>Exercises</h2>
                <p>Test your understanding of TTS fundamentals, from mel scale arithmetic to system design. The easy exercises cover the core calculations every TTS practitioner should be able to do quickly. The medium exercises require deeper reasoning about system design trade-offs. The hard exercises involve real-world challenges like building TTS for under-resourced languages and analyzing the information-theoretic limitations of mel spectrograms. Solutions are provided for self-study.</p>

                <div class="exercise-list">

                    <h3 style="margin-top: 1rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">Easy</h3>

                    <!-- Exercise 1 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">1. Mel Scale Conversion</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Using the mel scale formula $m = 2595 \log_{10}(1 + f/700)$, convert the following frequencies to mel: 500 Hz, 1000 Hz, 2000 Hz, 4000 Hz. Then compute the ratio of each consecutive mel interval to each consecutive Hz interval. What does this tell you about the mel scale's compression at higher frequencies?</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p>Apply the formula to each frequency:</p>
                                <ul>
                                    <li>$f = 500$ Hz: $m = 2595 \log_{10}(1 + 500/700) = 2595 \log_{10}(1.7143) = 2595 \times 0.2341 = 607.4$ mel</li>
                                    <li>$f = 1000$ Hz: $m = 2595 \log_{10}(1 + 1000/700) = 2595 \log_{10}(2.4286) = 2595 \times 0.3855 = 999.9$ mel</li>
                                    <li>$f = 2000$ Hz: $m = 2595 \log_{10}(1 + 2000/700) = 2595 \log_{10}(3.8571) = 2595 \times 0.5863 = 1521.2$ mel</li>
                                    <li>$f = 4000$ Hz: $m = 2595 \log_{10}(1 + 4000/700) = 2595 \log_{10}(6.7143) = 2595 \times 0.8270 = 2146.1$ mel</li>
                                </ul>
                                <p><strong>Consecutive intervals:</strong></p>
                                <table style="width: 100%; border-collapse: collapse; margin: 0.5rem 0;">
                                    <thead><tr style="border-bottom: 2px solid var(--color-border);"><th style="padding: 0.3rem; text-align: left;">Interval</th><th style="padding: 0.3rem; text-align: right;">Hz gap</th><th style="padding: 0.3rem; text-align: right;">Mel gap</th><th style="padding: 0.3rem; text-align: right;">Ratio (mel/Hz)</th></tr></thead>
                                    <tbody>
                                        <tr style="border-bottom: 1px solid var(--color-border);"><td style="padding: 0.3rem;">500 &rarr; 1000</td><td style="padding: 0.3rem; text-align: right;">500</td><td style="padding: 0.3rem; text-align: right;">392.5</td><td style="padding: 0.3rem; text-align: right;">0.785</td></tr>
                                        <tr style="border-bottom: 1px solid var(--color-border);"><td style="padding: 0.3rem;">1000 &rarr; 2000</td><td style="padding: 0.3rem; text-align: right;">1000</td><td style="padding: 0.3rem; text-align: right;">521.3</td><td style="padding: 0.3rem; text-align: right;">0.521</td></tr>
                                        <tr><td style="padding: 0.3rem;">2000 &rarr; 4000</td><td style="padding: 0.3rem; text-align: right;">2000</td><td style="padding: 0.3rem; text-align: right;">624.9</td><td style="padding: 0.3rem; text-align: right;">0.312</td></tr>
                                    </tbody>
                                </table>
                                <p>The ratio drops from 0.785 to 0.521 to 0.312 as frequency increases. This means the mel scale <strong>compresses higher frequencies more aggressively</strong>. Doubling the Hz gap from 1000 to 2000 only increased the mel gap by a factor of $624.9/521.3 = 1.20$. This reflects the fact that humans have much finer frequency resolution at low frequencies (where speech formants carry the most information) than at high frequencies.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 2 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">2. Phoneme Counting</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Given the sentence "The quick brown fox jumps," use the following CMUdict-style pronunciations to count phonemes and classify them:</p>
                            <ul>
                                <li>THE &rarr; DH AH</li>
                                <li>QUICK &rarr; K W IH K</li>
                                <li>BROWN &rarr; B R AW N</li>
                                <li>FOX &rarr; F AA K S</li>
                                <li>JUMPS &rarr; JH AH M P S</li>
                            </ul>
                            <p>How many total phonemes? How many vowels vs. consonants? What is the average number of phonemes per word?</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p>List all phonemes per word:</p>
                                <ul>
                                    <li>THE: DH, AH (2 phonemes: 1C + 1V)</li>
                                    <li>QUICK: K, W, IH, K (4 phonemes: 3C + 1V)</li>
                                    <li>BROWN: B, R, AW, N (4 phonemes: 3C + 1V)</li>
                                    <li>FOX: F, AA, K, S (4 phonemes: 3C + 1V)</li>
                                    <li>JUMPS: JH, AH, M, P, S (5 phonemes: 3C + 2V)</li>
                                </ul>
                                <p><strong>Total phonemes:</strong> $2 + 4 + 4 + 4 + 5 = 19$</p>
                                <p><strong>Vowels</strong> (from ARPAbet vowel set: AH, IH, AW, AA): $1 + 1 + 1 + 1 + 1 = 6$ (note: AH appears twice but we count each occurrence)</p>
                                <p>Wait, let us recount: AH (THE) + IH (QUICK) + AW (BROWN) + AA (FOX) + AH (JUMPS) = <strong>6 vowels</strong>. But JUMPS has JH, AH, M, P, S &mdash; that is 2 vowels? No: AH is the only vowel. So vowels = $1 + 1 + 1 + 1 + 1 = 5$.</p>
                                <p>Correction: Let me recount carefully:</p>
                                <ul>
                                    <li>THE: DH (C), AH (V) &rarr; 1V, 1C</li>
                                    <li>QUICK: K (C), W (C), IH (V), K (C) &rarr; 1V, 3C</li>
                                    <li>BROWN: B (C), R (C), AW (V), N (C) &rarr; 1V, 3C</li>
                                    <li>FOX: F (C), AA (V), K (C), S (C) &rarr; 1V, 3C</li>
                                    <li>JUMPS: JH (C), AH (V), M (C), P (C), S (C) &rarr; 1V, 4C</li>
                                </ul>
                                <p><strong>Total vowels: 5.</strong> Total consonants: $1 + 3 + 3 + 3 + 4 = 14$. Check: $5 + 14 = 19$.</p>
                                <p><strong>Average phonemes per word:</strong> $19 / 5 = 3.8$ phonemes/word.</p>
                                <p>This is close to the English average of about 3&ndash;4 phonemes per word. Note that English spelling is a poor predictor of phoneme count: "QUICK" has 5 letters but only 4 phonemes, while "FOX" has 3 letters but 4 phonemes (the "x" maps to /K S/).</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 3 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">3. Hop Size Calculation</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>A TTS system uses sample rate $sr = 22050$ Hz and hop length $h = 256$. Calculate: (a) the frame rate in frames per second, (b) the duration of each frame in milliseconds, (c) how many mel frames a 3-second audio clip produces, and (d) if the mel spectrogram has 80 bins, what is the total size (in floating-point numbers) of the mel spectrogram for 3 seconds of audio?</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a) Frame rate:</strong></p>
                                $$\text{frame rate} = \frac{sr}{h} = \frac{22050}{256} \approx 86.13 \text{ frames/second}$$
                                <p><strong>(b) Frame duration:</strong></p>
                                $$\text{frame duration} = \frac{h}{sr} = \frac{256}{22050} \approx 0.01161 \text{ s} = 11.61 \text{ ms}$$
                                <p><strong>(c) Mel frames for 3 seconds:</strong></p>
                                <p>Total audio samples: $3 \times 22050 = 66150$. Number of frames: $\lfloor 66150 / 256 \rfloor = 258$ (we use floor because the last partial frame is typically dropped or padded). More precisely: $66150 / 256 = 258.4$, so we get 258 complete frames.</p>
                                <p><strong>(d) Total size:</strong></p>
                                $$\text{size} = n_{\text{mels}} \times n_{\text{frames}} = 80 \times 258 = 20{,}640 \text{ floating-point numbers}$$
                                <p>At 4 bytes per float (float32), this is $20640 \times 4 = 82{,}560$ bytes $\approx 80.6$ KB. Compare this to the raw audio: $66150 \times 2 = 132{,}300$ bytes $\approx 129.2$ KB (16-bit PCM). The mel spectrogram is actually a compact representation &mdash; about 62% of the raw audio size, while capturing the perceptually relevant information.</p>
                                <p>Note the <strong>dimensionality expansion</strong> from text to mel: the sentence "the cat sat on the mat" has ~19 phonemes but produces ~258 mel frames. The acoustic model must expand the sequence by a factor of roughly $258/19 \approx 13.6$. This is why the alignment problem (mapping each phoneme to the right number of frames) is so central to TTS.</p>
                            </div>
                        </div>
                    </div>

                    <h3 style="margin-top: 2rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">Medium</h3>

                    <!-- Exercise 4 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">4. G2P Ambiguity</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>English contains many <em>heteronyms</em> &mdash; words spelled the same but pronounced differently depending on meaning. Consider: "read" (/riyd/ vs. /rehd/), "lead" (/liyd/ vs. /lehd/), "bass" (/beys/ vs. /baes/), "wind" (/wihnd/ vs. /waynd/). How does a TTS system determine the correct pronunciation? Discuss at least three approaches, from simple to sophisticated.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>Approach 1: Part-of-speech (POS) tagging.</strong> Many heteronyms can be disambiguated by their part of speech. "Read" as a verb in present tense is /riyd/; as a past participle it is /rehd/. A POS tagger (e.g., a simple rule-based tagger or a neural tagger like those in spaCy) can distinguish these cases. For example:</p>
                                <ul>
                                    <li>"I will <em>read</em> the book" &rarr; verb/present &rarr; /riyd/</li>
                                    <li>"I have <em>read</em> the book" &rarr; verb/past participle &rarr; /rehd/</li>
                                    <li>"The <em>lead</em> singer" &rarr; adjective &rarr; /lehd/ (as in "lead role")</li>
                                    <li>"The pipe is made of <em>lead</em>" &rarr; noun &rarr; /lehd/</li>
                                    <li>"She will <em>lead</em> the team" &rarr; verb &rarr; /liyd/</li>
                                </ul>
                                <p><strong>Limitation:</strong> POS alone does not resolve all cases. Both "lead" (metal) and "lead" (verb) can be nouns ("a lead in the play" &rarr; /liyd/).</p>
                                <p><strong>Approach 2: Context window / n-gram lookup.</strong> Use surrounding words to disambiguate. Maintain a table of heteronym-context patterns: if "bass" is preceded by "sea" or "striped," it is /baes/ (fish); if preceded by "electric" or "upright," it is /beys/ (musical instrument). This can be implemented as a set of hand-crafted rules or a small decision tree.</p>
                                <p><strong>Limitation:</strong> Requires manual curation for each heteronym. Cannot handle novel contexts.</p>
                                <p><strong>Approach 3: Neural G2P with sentence context.</strong> Modern TTS systems use a neural model (typically a transformer) that takes the entire sentence as input and predicts the pronunciation of each word in context. The model is trained on a large corpus of sentence-phoneme pairs and learns to resolve heteronyms implicitly from context. For example, Google's TTS system uses a BERT-like model for homograph disambiguation, achieving &gt;97% accuracy on English heteronyms.</p>
                                <p><strong>Approach 4: End-to-end character-level TTS.</strong> Systems like Tacotron bypass the G2P problem entirely by operating on characters. The encoder-decoder architecture learns to map characters to mel spectrograms, implicitly resolving pronunciation from the character context. However, this requires much more training data and can still fail on rare heteronyms.</p>
                                <p>In practice, production TTS systems combine approaches 1 and 3: a POS tagger handles the easy cases, and a neural disambiguation model handles the rest.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 5 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">5. Concatenative Cost Function</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>In concatenative TTS, the unit selection optimization minimizes $C = \sum_i [w_t \cdot C_t(u_i, t_i) + w_j \cdot C_j(u_{i-1}, u_i)]$. (a) Define specific sub-costs for $C_t$ (target cost) covering at least three linguistic features. (b) Define specific sub-costs for $C_j$ (join cost) covering at least two acoustic features. (c) If $w_t$ is much larger than $w_j$, what happens to the output? What if $w_j \gg w_t$?</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a) Target cost sub-components:</strong></p>
                                $$C_t(u_i, t_i) = \alpha_1 \cdot \delta_{\text{phone}}(u_i, t_i) + \alpha_2 \cdot |F_{0}^{u_i} - F_{0}^{t_i}|_{\text{norm}} + \alpha_3 \cdot |\text{dur}^{u_i} - \text{dur}^{t_i}|_{\text{norm}} + \alpha_4 \cdot \delta_{\text{stress}}(u_i, t_i) + \alpha_5 \cdot \delta_{\text{position}}(u_i, t_i)$$
                                <ul>
                                    <li>$\delta_{\text{phone}}$: Binary cost &mdash; 0 if the unit's phoneme matches the target, $\infty$ otherwise (hard constraint).</li>
                                    <li>$F_0$ difference: Normalized difference between the unit's pitch and the target pitch (predicted by a prosody model).</li>
                                    <li>Duration difference: Normalized difference between the unit's duration and the target duration.</li>
                                    <li>$\delta_{\text{stress}}$: Binary cost for stress mismatch (stressed vs. unstressed vowel).</li>
                                    <li>$\delta_{\text{position}}$: Cost for positional mismatch (word-initial vs. word-medial vs. word-final).</li>
                                </ul>
                                <p><strong>(b) Join cost sub-components:</strong></p>
                                $$C_j(u_{i-1}, u_i) = \beta_1 \cdot \|\mathbf{c}_{u_{i-1}}^{\text{end}} - \mathbf{c}_{u_i}^{\text{start}}\|_2 + \beta_2 \cdot |F_{0, u_{i-1}}^{\text{end}} - F_{0, u_i}^{\text{start}}| + \beta_3 \cdot |E_{u_{i-1}}^{\text{end}} - E_{u_i}^{\text{start}}|$$
                                <ul>
                                    <li>Spectral discontinuity ($\beta_1$): Euclidean distance between the MFCCs at the end of unit $u_{i-1}$ and the start of unit $u_i$. Large spectral jumps produce audible clicks.</li>
                                    <li>Pitch discontinuity ($\beta_2$): Absolute difference in F0 at the boundary. Pitch jumps sound unnatural.</li>
                                    <li>Energy discontinuity ($\beta_3$): Absolute difference in energy (loudness) at the boundary.</li>
                                </ul>
                                <p><strong>(c) Weight trade-offs:</strong></p>
                                <p><strong>If $w_t \gg w_j$:</strong> The system prioritizes finding units that exactly match the target linguistic specification, even if the joins between consecutive units are rough. Result: correct pronunciation and prosody, but audible concatenation artifacts (clicks, spectral jumps, pitch discontinuities). The speech sounds choppy.</p>
                                <p><strong>If $w_j \gg w_t$:</strong> The system prioritizes smooth transitions between units, even if the selected units are a poor match for the target. Result: smooth, flowing audio, but potentially wrong phonemes, wrong pitch contour, or wrong speaking rate. The speech sounds smooth but may be incorrect.</p>
                                <p>In practice, $w_t$ is set high enough to enforce hard constraints (correct phoneme) while allowing soft trade-offs on prosodic features, and $w_j$ is tuned to minimize perceptible artifacts. The optimal balance is typically found through perceptual experiments.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 6 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">6. End-to-End Trade-Offs</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Compare the two-stage TTS approach (separate acoustic model + vocoder, e.g., FastSpeech 2 + HiFi-GAN) with the end-to-end approach (single model, e.g., VITS). For each of the following criteria, explain which approach has the advantage and why: (a) training data efficiency, (b) inference speed, (c) component reusability, (d) training-inference mismatch, (e) debugging and interpretability.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a) Training data efficiency:</strong> <em>Two-stage wins.</em> Each component can be trained on different datasets. The vocoder (HiFi-GAN) can be trained on any speech data (no text labels needed), while the acoustic model needs paired text-audio data. An end-to-end model requires all training data to have both text and audio, and the model must learn both the text-to-acoustic mapping and the acoustic-to-waveform mapping simultaneously, which typically requires more data.</p>
                                <p><strong>(b) Inference speed:</strong> <em>Roughly tied, slight edge to end-to-end.</em> A two-stage system runs two models sequentially. VITS runs a single model but must perform similar computations (encoder + flow + decoder). In practice, VITS is slightly faster because there is no overhead of serializing the intermediate mel spectrogram, but the difference is small. Both approaches can achieve real-time or faster synthesis.</p>
                                <p><strong>(c) Component reusability:</strong> <em>Two-stage wins clearly.</em> The vocoder trained for one acoustic model can be reused with another. If you switch from FastSpeech 2 to Glow-TTS, you keep the same HiFi-GAN vocoder. With end-to-end models, changing any part of the architecture requires retraining the entire system.</p>
                                <p><strong>(d) Training-inference mismatch:</strong> <em>End-to-end wins clearly.</em> In a two-stage system, the vocoder is trained on ground-truth mel spectrograms but at inference time receives predicted (slightly imperfect) mel spectrograms. This mismatch degrades quality. Mitigations include fine-tuning the vocoder on predicted mels or using mel prediction error augmentation, but the mismatch never fully disappears. End-to-end models have no such mismatch because there is no intermediate representation passed between separately trained components.</p>
                                <p><strong>(e) Debugging and interpretability:</strong> <em>Two-stage wins clearly.</em> When something sounds wrong, you can inspect the mel spectrogram to determine whether the problem is in the acoustic model or the vocoder. You can even listen to the mel spectrogram using Griffin-Lim for a rough check. With an end-to-end model, the internal representations are less interpretable, and isolating the source of quality issues is harder.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 7 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">7. MOS Experimental Design</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>You have built three TTS systems for Kyrgyz and want to compare them against each other and against ground truth recordings. Design a rigorous MOS evaluation. Specify: (a) how many listeners, (b) how many test sentences, (c) how to select the test sentences, (d) the rating protocol (instructions to listeners), (e) how to handle listener quality control, and (f) how to determine if the MOS difference between two systems is statistically significant.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a) Listeners:</strong> At least 30 native Kyrgyz speakers. For a minority language, this may require targeted recruitment. Use at least 20 for statistical power; 30 provides a safety margin for excluding unreliable listeners. Listeners should have no hearing impairments and should use headphones.</p>
                                <p><strong>(b) Test sentences:</strong> 25 sentences per system. With 4 systems (3 TTS + 1 ground truth), each listener rates $25 \times 4 = 100$ samples. This takes about 20&ndash;30 minutes, which is near the upper limit of listener attention span.</p>
                                <p><strong>(c) Sentence selection:</strong> Select sentences that cover: (1) phonetically balanced content (all Kyrgyz phonemes represented), (2) varying lengths (5&ndash;20 words), (3) both declarative and interrogative sentences, (4) common and uncommon words, (5) words with vowel harmony patterns typical of Kyrgyz. Avoid sentences from the training set. Preferably use sentences from a held-out test corpus or from newspaper text.</p>
                                <p><strong>(d) Rating protocol:</strong> Present samples in randomized order (different for each listener). Do not reveal which system produced which sample. Instructions: "You will hear speech samples. Rate each sample on a scale from 1 to 5 for naturalness: 5 = completely natural, 4 = mostly natural with minor imperfections, 3 = somewhat natural, 2 = unnatural, 1 = very unnatural. Focus on how natural the speech sounds, not on the content of the sentence." Include 5 practice samples (not counted) at the beginning to calibrate the listener.</p>
                                <p><strong>(e) Listener quality control:</strong> (1) Include 3&ndash;5 "trap" samples: obviously degraded audio (e.g., heavily distorted) that should receive a score of 1&ndash;2, and ground truth recordings that should receive 4&ndash;5. (2) Exclude listeners who rate trap samples incorrectly (e.g., giving distorted audio a 4+). (3) Exclude listeners with zero variance in ratings (giving the same score to everything). (4) Exclude listeners who complete the test in less than 50% of the expected time.</p>
                                <p><strong>(f) Statistical significance:</strong> For each pair of systems, perform a paired $t$-test or (better) a Wilcoxon signed-rank test on the per-sentence mean scores. Apply Bonferroni correction for multiple comparisons: with $\binom{4}{2} = 6$ pairwise tests, use significance level $\alpha = 0.05 / 6 = 0.0083$. Report 95% confidence intervals for each system's MOS and for the MOS difference between each pair.</p>
                            </div>
                        </div>
                    </div>

                    <h3 style="margin-top: 2rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">Hard</h3>

                    <!-- Exercise 8 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">8. TTS for Agglutinative Languages</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Agglutinative languages like Kyrgyz and Turkish form words by chaining morphemes, producing very long words (e.g., Kyrgyz "&#1199;&#1081;&#1083;&#1257;&#1088;&#1199;&#1084;&#1076;&#1257;&#1075;&#1199;&#1083;&#1257;&#1088;&#1199;&#1084;&#1076;&#1257;&#1085;&#1199;&#1187;&#1076;&#1257;&#1088;&#1076;&#1199;&#1082;" = "of those among my family members"). These languages also have vowel harmony and are typically under-resourced. Discuss at least four specific challenges these properties create for TTS, and propose a solution for each.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>Challenge 1: OOV words and G2P.</strong> Because words are formed by productive morpheme concatenation, the vocabulary is essentially infinite. A word like "&#1082;&#1257;&#1088;&#1199;&#1096;&#1082;&#1257;&#1085;&#1076;&#1257;&#1088;&#1199;&#1187;&#1199;&#1079;&#1076;&#1257;&#1088;" may never appear in a pronunciation dictionary.</p>
                                <p><strong>Solution:</strong> Kyrgyz and Turkish have highly phonemic orthographies &mdash; the spelling directly reflects the pronunciation. This means G2P is almost a trivial character-by-character mapping (with a few rules for soft/hard consonants and vowel harmony effects on suffixes). A rule-based G2P with 20&ndash;30 rules achieves near-perfect accuracy, unlike English where learned G2P is necessary.</p>
                                <p><strong>Challenge 2: Vowel harmony and prosody.</strong> Vowel harmony means that suffixes change their vowels to harmonize with the root (e.g., Kyrgyz back-vowel roots take back-vowel suffixes: "&#1073;&#1072;&#1083;&#1072;-&#1083;&#1072;&#1088;" but "&#1199;&#1081;-&#1083;&#1257;&#1088;"). This affects the spectral characteristics of vowels throughout the word and creates long-distance phonological dependencies.</p>
                                <p><strong>Solution:</strong> Use phoneme-level or morpheme-level input representations that make harmony explicit. If using character input, the model should learn harmony patterns from data. Add morpheme boundary markers (e.g., "|") to the input: "&#1199;&#1081;|&#1083;&#1257;&#1088;|&#1199;&#1084;|&#1076;&#1257;" helps the model learn morpheme-level prosodic patterns.</p>
                                <p><strong>Challenge 3: Long words and duration modeling.</strong> Agglutinative words can be 15+ syllables long. For duration-based TTS (FastSpeech), the duration predictor must handle words much longer than those seen in English training. Attention-based models (Tacotron) may struggle with very long encoder sequences where a single word occupies many positions.</p>
                                <p><strong>Solution:</strong> Operate at the phoneme level (not word level) to normalize the input length. A 15-syllable word becomes ~30 phonemes &mdash; long but manageable. For attention-based models, use monotonic attention or location-sensitive attention to prevent attention from getting lost in long sequences. Alternatively, use non-AR models with external alignment (FastSpeech 2 + MFA) which handle long inputs robustly.</p>
                                <p><strong>Challenge 4: Data scarcity.</strong> Kyrgyz has perhaps 5&ndash;10 hours of publicly available TTS-quality data, compared to 100+ hours for English. Training a high-quality neural TTS model on limited data leads to poor generalization, muffled output, and frequent mispronunciations of rare morpheme combinations.</p>
                                <p><strong>Solution:</strong> (1) Use transfer learning from a related, better-resourced language (Turkish for Kyrgyz, as they are both Turkic languages with similar phoneme inventories and vowel harmony). Pretrain on Turkish data, then fine-tune on Kyrgyz. (2) Use multilingual TTS with shared parameters across languages (see Tutorial 34). (3) Apply data augmentation: speed perturbation, pitch shifting, noise addition to increase effective training set size. (4) Use a smaller model architecture appropriate for the data size (e.g., a lightweight VITS variant with fewer parameters).</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 9 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">9. Mel Spectrogram Information Loss</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>The mel spectrogram is a lossy representation of the original waveform. Identify and explain at least three types of information that are lost in the mel spectrogram transformation. For each type, explain: (a) what exactly is lost, (b) why the mel spectrogram does not preserve it, and (c) what consequences this has for TTS vocoding.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>Information loss 1: Phase.</strong></p>
                                <p>(a) The STFT produces complex-valued coefficients with both magnitude and phase: $X[k] = |X[k]| \cdot e^{j\phi[k]}$. The mel spectrogram uses only the magnitude $|X[k]|^2$ and discards the phase $\phi[k]$ entirely.</p>
                                <p>(b) The mel spectrogram applies the mel filterbank to the <em>magnitude spectrum</em> (or power spectrum). Phase is never computed or stored.</p>
                                <p>(c) Phase encodes the temporal fine structure of the signal. Without phase, the vocoder must estimate or generate plausible phase. This is why naive methods like Griffin-Lim (iterative phase reconstruction) produce "metallic" or "phasey" artifacts. Neural vocoders (WaveNet, HiFi-GAN) bypass this problem by generating the waveform directly, implicitly producing appropriate phase. Phase reconstruction is the single largest challenge in mel-spectrogram-based vocoding.</p>
                                <p><strong>Information loss 2: Fine spectral resolution at high frequencies.</strong></p>
                                <p>(a) The mel filterbank has broad, overlapping filters at high frequencies. A mel spectrogram with 80 bins covering 0&ndash;8000 Hz has roughly 50 bins for 0&ndash;4000 Hz and only 30 bins for 4000&ndash;8000 Hz. Individual harmonics above ~3000 Hz are merged together and cannot be distinguished.</p>
                                <p>(b) This is by design: the mel scale compresses higher frequencies because human pitch perception has lower resolution at high frequencies. However, the spectral fine structure at high frequencies does carry perceptually relevant information (e.g., the difference between /s/ and /sh/ involves spectral shape in the 3&ndash;8 kHz range).</p>
                                <p>(c) The vocoder must reconstruct high-frequency detail from the coarse mel representation. Neural vocoders learn to "fill in" plausible high-frequency detail from context, but errors in this region can produce "lispy" sibilants or muffled fricatives.</p>
                                <p><strong>Information loss 3: Temporal fine structure within each frame.</strong></p>
                                <p>(a) Each mel frame represents the average spectral content over <code>n_fft</code> = 1024 samples (~46 ms at 22050 Hz), with frames shifted by <code>hop_length</code> = 256 samples (~11.6 ms). Any acoustic events shorter than one frame (e.g., plosive bursts, which can be 5&ndash;10 ms) are smeared across the frame.</p>
                                <p>(b) The windowing and averaging inherent in the STFT smooth out sub-frame temporal detail. The trade-off between temporal and spectral resolution is fundamental (the uncertainty principle of signal processing).</p>
                                <p>(c) The vocoder must reconstruct sharp transients (plosive releases, click sounds) from a smoothed representation. This is particularly challenging for stop consonants (/p/, /t/, /k/) where the burst is very brief. Neural vocoders handle this surprisingly well by learning to generate appropriate transients from contextual cues in the mel spectrogram.</p>
                                <p><strong>Bonus: Dynamic range compression.</strong> The log compression applied after mel filtering reduces the dynamic range from ~60 dB to a much smaller range. While this aids training stability, very quiet sounds (whispers, unvoiced fricatives) and very loud sounds (shouts) are compressed toward the same range, making it harder for the vocoder to reproduce the correct absolute loudness.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 10 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">10. Building a TTS Pipeline from Scratch</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>You are tasked with building a complete TTS system for a new language (e.g., Uzbek). Outline every component you would need, from raw text input to audio output. For each component, specify: what it does, what options exist for building it, what data it needs, and what the key design decisions are. Your answer should cover the full pipeline.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>Component 1: Text normalization.</strong></p>
                                <ul>
                                    <li><strong>What:</strong> Convert raw text to spoken form (expand numbers, abbreviations, dates, symbols).</li>
                                    <li><strong>Options:</strong> (a) Hand-crafted rules / FSTs (see Tutorial 23), (b) Seq2seq neural normalizer.</li>
                                    <li><strong>Data needed:</strong> List of normalization rules for Uzbek. Numbers follow regular patterns. Abbreviations require a curated list.</li>
                                    <li><strong>Key decisions:</strong> Uzbek uses both Latin and Cyrillic scripts &mdash; decide which to support (or both). Handle mixed-script input. Define rules for Uzbek-specific number formatting.</li>
                                </ul>
                                <p><strong>Component 2: Grapheme-to-phoneme (G2P) conversion.</strong></p>
                                <ul>
                                    <li><strong>What:</strong> Convert normalized text to phoneme sequences.</li>
                                    <li><strong>Options:</strong> (a) Rule-based G2P (Uzbek has a largely phonemic orthography, so rules work well), (b) Pronunciation dictionary + neural G2P for OOV words.</li>
                                    <li><strong>Data needed:</strong> Phoneme inventory for Uzbek (~30 phonemes), mapping rules from graphemes to phonemes, handling of loanwords.</li>
                                    <li><strong>Key decisions:</strong> Choose phoneme set (IPA or custom). Handle vowel harmony (Uzbek is Turkic). Decide on stress marking (Uzbek typically has final-syllable stress with exceptions for loanwords).</li>
                                </ul>
                                <p><strong>Component 3: Prosody model (optional, depending on architecture).</strong></p>
                                <ul>
                                    <li><strong>What:</strong> Predict sentence-level intonation, stress, and phrasing from text.</li>
                                    <li><strong>Options:</strong> (a) Rule-based (punctuation-driven), (b) Learned as part of the acoustic model (Tacotron-style), (c) Explicit prosody predictor (FastSpeech 2 style with pitch and energy predictors).</li>
                                    <li><strong>Data needed:</strong> If explicit: annotated prosodic contours. If implicit: the acoustic model learns prosody from text-audio pairs.</li>
                                    <li><strong>Key decisions:</strong> Whether to model prosody explicitly (more controllable) or implicitly (simpler pipeline).</li>
                                </ul>
                                <p><strong>Component 4: Data collection and preparation.</strong></p>
                                <ul>
                                    <li><strong>What:</strong> Record or acquire a speech corpus for training.</li>
                                    <li><strong>Requirements:</strong> Single speaker (for single-speaker TTS), professional recording quality (quiet room, consistent microphone), at least 5&ndash;10 hours (ideally 20+), phonetically balanced script covering all Uzbek phonemes and common phoneme combinations.</li>
                                    <li><strong>Processing:</strong> Segment into utterances, normalize audio (consistent volume), resample to 22050 Hz, verify transcriptions.</li>
                                    <li><strong>Key decisions:</strong> Speaker selection (gender, age, dialect), recording conditions, script design.</li>
                                </ul>
                                <p><strong>Component 5: Forced alignment.</strong></p>
                                <ul>
                                    <li><strong>What:</strong> Obtain phone-level time alignments for training data.</li>
                                    <li><strong>Options:</strong> (a) Train MFA on Uzbek data with a custom dictionary, (b) Use MAS inside Glow-TTS (no external aligner needed), (c) CTC alignment with multilingual wav2vec2.</li>
                                    <li><strong>Data needed:</strong> Pronunciation dictionary (for MFA), the speech corpus.</li>
                                    <li><strong>Key decisions:</strong> If using FastSpeech, MFA alignment is necessary. If using Glow-TTS/VITS, alignment is learned during training.</li>
                                </ul>
                                <p><strong>Component 6: Acoustic model.</strong></p>
                                <ul>
                                    <li><strong>What:</strong> Map phoneme sequences to mel spectrograms.</li>
                                    <li><strong>Options:</strong> (a) Tacotron 2 (AR, attention-based), (b) FastSpeech 2 (non-AR, requires external durations), (c) Glow-TTS (non-AR, flow-based, learns alignment), (d) VITS (end-to-end, includes vocoder).</li>
                                    <li><strong>Data needed:</strong> Paired (phoneme sequence, mel spectrogram) for each utterance. Plus phone durations for FastSpeech 2.</li>
                                    <li><strong>Key decisions:</strong> For a new language with limited data, start with a simpler model (FastSpeech 2 or Glow-TTS). Consider transfer learning from Turkish (closely related Turkic language). If data is very limited (&lt;5 hours), use VITS with transfer learning.</li>
                                </ul>
                                <p><strong>Component 7: Vocoder (unless using end-to-end model).</strong></p>
                                <ul>
                                    <li><strong>What:</strong> Convert mel spectrograms to waveforms.</li>
                                    <li><strong>Options:</strong> (a) HiFi-GAN (fast, high quality, standard choice), (b) WaveRNN (good quality, moderate speed), (c) Griffin-Lim (fast, low quality, good for debugging).</li>
                                    <li><strong>Data needed:</strong> Any speech audio (no text labels needed). Can pretrain on Turkish/English data and fine-tune on Uzbek.</li>
                                    <li><strong>Key decisions:</strong> HiFi-GAN is the standard choice. Fine-tune on predicted mel spectrograms to reduce training-inference mismatch.</li>
                                </ul>
                                <p><strong>Component 8: Evaluation.</strong></p>
                                <ul>
                                    <li><strong>What:</strong> Measure synthesis quality.</li>
                                    <li><strong>Objective:</strong> MCD against ground truth, F0 RMSE, V/UV error rate.</li>
                                    <li><strong>Subjective:</strong> MOS test with native Uzbek speakers (at least 20 listeners, 25+ sentences).</li>
                                    <li><strong>Key decisions:</strong> For iterative development, use objective metrics. For final evaluation and paper, conduct formal MOS test.</li>
                                </ul>
                                <p><strong>Recommended minimal pipeline for Uzbek:</strong> Rule-based text normalizer &rarr; Rule-based G2P &rarr; VITS (end-to-end, pre-trained on Turkish, fine-tuned on Uzbek). This minimizes the number of components and leverages transfer learning for the data-scarce setting.</p>
                            </div>
                        </div>
                    </div>

                </div>
            </article>

        </main>

        <!-- TOC (Right Side) -->
        <aside class="toc-container">
            <h4 class="toc-title">Contents</h4>
            <nav class="toc-list">
                <a href="#what-is-tts" class="toc-link">What Is Text-to-Speech?</a>
                <a href="#text-analysis" class="toc-link">Text Analysis &amp; Frontend</a>
                <a href="#key-representations" class="toc-link">Key Representations</a>
                <a href="#concatenative-synthesis" class="toc-link">Concatenative Synthesis</a>
                <a href="#parametric-synthesis" class="toc-link">Parametric Synthesis</a>
                <a href="#neural-revolution" class="toc-link">The Neural Revolution</a>
                <a href="#evaluation-metrics" class="toc-link">MOS and Beyond</a>
                <a href="#tts-landscape" class="toc-link">Modern TTS Landscape</a>
            </nav>
        </aside>
    </div>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-brand">
                    <span class="logo-symbol">&nabla;</span>
                    <span>ML Fundamentals</span>
                </div>
                <p class="footer-tagline">Deep understanding through first principles.</p>
            </div>
            <div class="footer-links">
                <a href="../../../index.html">Home</a>
                <a href="https://github.com/ml-entropy/ml-entropy.github.io" target="_blank">GitHub</a>
            </div>
        </div>
    </footer>

    <script src="../../../js/main.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            if (typeof renderMathInElement === 'function') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }

            const tabs = document.querySelectorAll('.tutorial-tab');
            const articles = document.querySelectorAll('.article-content');

            function switchTab(targetId) {
                if (!targetId || targetId === '#') targetId = '#theory';
                tabs.forEach(tab => {
                    if (tab.getAttribute('href') === targetId) {
                        tab.classList.add('active');
                    } else {
                        tab.classList.remove('active');
                    }
                });
                articles.forEach(article => {
                    const articleId = '#' + article.id;
                    if (articleId === targetId) {
                        article.style.display = 'block';
                    } else {
                        article.style.display = 'none';
                    }
                });
                if (typeof renderMathInElement === 'function') {
                    renderMathInElement(document.body, {
                        delimiters: [
                            {left: '$$', right: '$$', display: true},
                            {left: '$', right: '$', display: false},
                            {left: '\\[', right: '\\]', display: true},
                            {left: '\\(', right: '\\)', display: false}
                        ],
                        throwOnError: false
                    });
                }
                const toc = document.querySelector('.toc-container');
                if (toc) {
                    if (targetId === '#theory') {
                        toc.style.display = 'block';
                        setTimeout(() => toc.classList.add('visible'), 100);
                    } else {
                        toc.classList.remove('visible');
                        setTimeout(() => toc.style.display = 'none', 300);
                    }
                }
            }

            tabs.forEach(tab => {
                tab.addEventListener('click', (e) => {
                    e.preventDefault();
                    const targetId = tab.getAttribute('href');
                    history.pushState(null, null, targetId);
                    switchTab(targetId);
                });
            });

            window.addEventListener('popstate', () => {
                switchTab(window.location.hash);
            });

            switchTab(window.location.hash);
        });
    </script>
</body>
</html>