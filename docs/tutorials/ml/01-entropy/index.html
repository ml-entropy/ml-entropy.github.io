<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Information Entropy | ML Fundamentals</title>
    <meta name="description" content="Understanding Shannon entropy from first principles. Derive the formula, explore Huffman coding, and see why entropy measures surprise.">
    
    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&family=Playfair+Display:wght@400;500;600;700&display=swap" rel="stylesheet">
    
    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}, {left: '\\\\[', right: '\\\\]', display: true}, {left: '\\\\(', right: '\\\\)', display: false}], throwOnError: false});"></script>
    
    <!-- Styles -->
    <link rel="stylesheet" href="../../../css/main.css">
    <link rel="stylesheet" href="../../../css/components.css">
    
    
    
    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>∞</text></svg>">
<link rel="stylesheet" href="../../../css/sidebar.css">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar" id="navbar">
        <div class="nav-container">
            <a href="../../../index.html" class="nav-logo">
                <span class="logo-symbol">∇</span>
                <span class="logo-text">ML Fundamentals</span>
            </a>
            
            <button class="nav-toggle" id="navToggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            
            <div class="nav-menu" id="navMenu">
                <div class="nav-links">
                    <div class="nav-dropdown">
                        <button class="nav-link dropdown-trigger">
                            Tutorials <span class="dropdown-arrow">▾</span>
                        </button>
                        <div class="dropdown-content">
                            <a href="../../ml/index.html" class="active">Machine Learning</a>
                            <a href="../../linear-algebra/index.html">Linear Algebra</a>
                            <a href="../../calculus/index.html">Calculus</a>
                            <a href="../../physics/index.html">Physics</a>
                        </div>
                    </div>
                    <a href="../../../index.html#philosophy" class="nav-link">Philosophy</a>
                    <a href="../../../index.html#roadmap" class="nav-link">Roadmap</a>
                    <a href="https://github.com/ml-entropy/ml-entropy.github.io" class="nav-link" target="_blank">GitHub</a>
                </div>
                
                <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
                    <svg class="sun-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="5"/>
                        <path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"/>
                    </svg>
                    <svg class="moon-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                    </svg>
                </button>
            </div>
        </div>
    </nav>

    <!-- Tutorial Header -->
    <header class="tutorial-content-header">
        <div class="container">
            <nav class="breadcrumb">
                <a href="../../../index.html">Home</a>
                <span class="breadcrumb-separator">→</span>
                <a href="../index.html">Machine Learning</a>
                <span class="breadcrumb-separator">→</span>
                <span>Entropy</span>
            </nav>
            
            <h1>01. Information Entropy</h1>
            <p class="lead">
                Why does entropy have the form $H(X) = -\sum p(x) \log p(x)$? 
                Let's derive it from first principles and discover why entropy measures "surprise."
            </p>
            
            <div class="tutorial-tabs">
                <a href="#theory" class="tutorial-tab active">Theory</a>
                <a href="#code" class="tutorial-tab">Code</a>
                <a href="#exercises" class="tutorial-tab">Exercises</a>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <div class="tutorial-wrapper">
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <aside class="tutorial-sidebar">
            <div class="sidebar-section">
                <h3 class="sidebar-section-title">Machine Learning</h3>
                <nav class="sidebar-nav">
                        <a href="../00-probability/index.html" class="sidebar-link">00. Probability Foundations</a>
                    <a href="../01-entropy/index.html" class="sidebar-link active">01. Entropy Fundamentals</a>
                    <a href="../02-kl-divergence/index.html" class="sidebar-link">02. KL Divergence</a>
                    <a href="../03-distributions/index.html" class="sidebar-link">03. Normal Distributions</a>
                    <a href="../04-logarithms/index.html" class="sidebar-link">04. Why Logarithms?</a>
                    <a href="../05-combinatorics/index.html" class="sidebar-link">05. Combinatorics</a>
                    <a href="../06-backpropagation/index.html" class="sidebar-link">06. Backpropagation</a>
                    <a href="../07-regularization/index.html" class="sidebar-link">07. Regularization</a>
                    <a href="../08-batch-normalization/index.html" class="sidebar-link">08. Batch Normalization</a>
                    <a href="../09-learning-rate/index.html" class="sidebar-link">09. Learning Rate</a>
                    <a href="../10-cnn/index.html" class="sidebar-link">10. CNNs</a>
                    <a href="../11-rnn/index.html" class="sidebar-link">11. RNNs</a>
                    <a href="../12-vae/index.html" class="sidebar-link">12. VAE</a>
                    <a href="../13-variational-inference/index.html" class="sidebar-link">13. Variational Inference</a>
                </nav>
            </div>
            
            <div class="sidebar-section" style="margin-top: 2rem;">
                <h3 class="sidebar-section-title">Related Subjects</h3>
                <nav class="sidebar-nav">
                        <a href="../../linear-algebra/index.html" class="sidebar-link">Linear Algebra</a>
                    <a href="../../calculus/index.html" class="sidebar-link">Calculus</a>
                    <a href="../../physics/index.html" class="sidebar-link">Physics</a>
                </nav>
            </div>
        </aside>

        <!-- Main Article -->
        <main class="tutorial-main">
            <article class="article-content" id="theory">
                
                <!-- Section 1 -->
                <h2 id="intuition">The Intuition: Measuring Surprise</h2>
                
                <p>
                    Imagine you're watching a coin flip. If it's a fair coin, you're equally uncertain about 
                    heads or tails—maximum uncertainty. But if the coin has a 99% chance of heads, you're 
                    barely uncertain at all. <strong>Entropy quantifies this uncertainty.</strong>
                </p>
                
                <div class="definition-box">
                    <div class="box-title">Core Intuition</div>
                    <p style="margin-bottom: 0;">
                        <strong>Entropy = Expected Surprise</strong>. Events that rarely happen are very surprising 
                        when they do. Entropy averages this surprise over all possible outcomes, weighted by 
                        their probabilities.
                    </p>
                </div>
                
                <p>
                    Before we derive the formula, let's ask: what properties should a "surprise" function have?
                </p>
                
                <ul>
                    <li><strong>Rare events are more surprising:</strong> If $p(x)$ is small, surprise should be large.</li>
                    <li><strong>Certain events have zero surprise:</strong> If $p(x) = 1$, no surprise.</li>
                    <li><strong>Independent events combine additively:</strong> Surprise of seeing A and B should be the sum of individual surprises.</li>
                </ul>
                
                <!-- Section 2: Derivation -->
                <h2 id="derivation">Deriving the Entropy Formula</h2>
                
                <h3>Step 1: The Surprise Function</h3>
                
                <p>
                    Let's call the surprise of an event with probability $p$ the function $I(p)$. 
                    From our requirements:
                </p>
                
                <div class="math-derivation">
                    <div class="math-derivation-title">Requirements for Surprise</div>
                    
                    <div class="math-step">
                        <div class="math-step-number">1</div>
                        <div class="math-step-content">
                            $I(p)$ should be <em>decreasing</em> in $p$ (rare events are more surprising)
                        </div>
                    </div>
                    
                    <div class="math-step">
                        <div class="math-step-number">2</div>
                        <div class="math-step-content">
                            $I(1) = 0$ (certain events have zero surprise)
                        </div>
                    </div>
                    
                    <div class="math-step">
                        <div class="math-step-number">3</div>
                        <div class="math-step-content">
                            $I(p_1 \cdot p_2) = I(p_1) + I(p_2)$ (surprises of independent events add)
                        </div>
                    </div>
                </div>
                
                <p>
                    The third requirement is crucial. What function satisfies $f(xy) = f(x) + f(y)$? 
                    The <strong>logarithm</strong>!
                </p>
                
                <div class="math-block">
                    $$I(p) = -\log_2(p) = \log_2\left(\frac{1}{p}\right)$$
                </div>
                
                <p>
                    The negative sign ensures that $I(p)$ is positive (since $\log$ of a number less than 1 
                    is negative). We use base 2 to measure information in <em>bits</em>.
                </p>
                
                <div class="note-box">
                    <div class="box-title">Why Logarithm?</div>
                    <p style="margin-bottom: 0;">
                        The logarithm is the <em>only</em> function (up to a constant) that converts multiplication 
                        to addition: $\log(ab) = \log(a) + \log(b)$. This is why information content must be 
                        logarithmic—it's a mathematical necessity, not an arbitrary choice.
                    </p>
                </div>
                
                <h3>Step 2: From Surprise to Entropy</h3>
                
                <p>
                    Now we have the surprise for a single event. But we want to measure the uncertainty 
                    of an entire random variable $X$. The natural approach: take the <em>expected</em> surprise.
                </p>
                
                <div class="math-derivation">
                    <div class="math-derivation-title">Deriving Shannon Entropy</div>
                    
                    <div class="math-step">
                        <div class="math-step-number">1</div>
                        <div class="math-step-content">
                            Start with expected value of surprise: $H(X) = \mathbb{E}[I(X)]$
                        </div>
                    </div>
                    
                    <div class="math-step">
                        <div class="math-step-number">2</div>
                        <div class="math-step-content">
                            Expand the expectation: $H(X) = \sum_{x} p(x) \cdot I(p(x))$
                        </div>
                    </div>
                    
                    <div class="math-step">
                        <div class="math-step-number">3</div>
                        <div class="math-step-content">
                            Substitute $I(p) = -\log_2(p)$: 
                            $$H(X) = \sum_{x} p(x) \cdot (-\log_2 p(x))$$
                        </div>
                    </div>
                    
                    <div class="math-step">
                        <div class="math-step-number">4</div>
                        <div class="math-step-content">
                            <strong>Shannon Entropy:</strong>
                            $$\boxed{H(X) = -\sum_{x \in \mathcal{X}} p(x) \log_2 p(x)}$$
                        </div>
                    </div>
                </div>
                
                <!-- Section 3: Examples -->
                <h2 id="examples">Concrete Examples</h2>
                
                <h3>Example 1: Fair Coin</h3>
                
                <p>A fair coin has $p(\text{heads}) = p(\text{tails}) = 0.5$.</p>
                
                <div class="math-block">
                    $$H = -\left(0.5 \log_2(0.5) + 0.5 \log_2(0.5)\right) = -2 \times 0.5 \times (-1) = 1 \text{ bit}$$
                </div>
                
                <p>
                    One bit of entropy means we need one binary question to determine the outcome. 
                    Makes sense: "Is it heads?" fully resolves the uncertainty.
                </p>
                
                <h3>Example 2: Biased Coin</h3>
                
                <p>Consider a coin with $p(\text{heads}) = 0.99$, $p(\text{tails}) = 0.01$.</p>
                
                <div class="math-block">
                    $$H = -(0.99 \log_2(0.99) + 0.01 \log_2(0.01)) \approx 0.08 \text{ bits}$$
                </div>
                
                <p>
                    Only 0.08 bits! There's almost no uncertainty—you can predict heads with 99% accuracy.
                </p>
                
                <h3>Example 3: Eight-Sided Die</h3>
                
                <p>A fair 8-sided die has $p(i) = 1/8$ for each face.</p>
                
                <div class="math-block">
                    $$H = -\sum_{i=1}^{8} \frac{1}{8} \log_2\left(\frac{1}{8}\right) = -8 \times \frac{1}{8} \times (-3) = 3 \text{ bits}$$
                </div>
                
                <p>
                    Three bits! This matches our intuition: we need 3 binary questions to identify one of 8 equally likely outcomes 
                    (think binary search).
                </p>
                
                <!-- Section 4: Connection to Coding -->
                <h2 id="coding">Entropy and Optimal Coding</h2>
                
                <p>
                    Here's where entropy becomes practical. Shannon proved that entropy equals the minimum 
                    average number of bits needed to encode messages from a source.
                </p>
                
                <div class="definition-box">
                    <div class="box-title">Shannon's Source Coding Theorem</div>
                    <p style="margin-bottom: 0;">
                        For any prefix-free code, the average code length $\bar{L}$ satisfies 
                        $\bar{L} \geq H(X)$. Huffman coding achieves $\bar{L} < H(X) + 1$.
                    </p>
                </div>
                
                <h3 id="learning-is-compression">Deep Dive: Learning IS Compression</h3>
                <p>
                    This is one of the most profound insights in modern machine learning. 
                    <strong>The goal of learning is to find the shortest program (model) that reproduces your data.</strong>
                </p>
                
                <p>
                    Think about it:
                </p>
                <ul>
                    <li><strong>Memorization:</strong> You write down every single training example. This program is huge (length = size of data). You haven't "learned" anything; you've just stored data.</li>
                    <li><strong>Generalization:</strong> You find a simple rule (like $y = 2x + 1$) that generates the data. This program is tiny. You have successfully "compressed" the data into a rule.</li>
                </ul>
                
                <div class="note-box">
                    <div class="box-title">Minimum Description Length (MDL) Principle</div>
                    <p>
                        The MDL principle formalizes Occam's Razor. It states that the best hypothesis $H$ for data $D$ is the one that minimizes the sum of:
                        $$L(H) + L(D | H)$$
                    </p>
                    <ul>
                        <li><strong>$L(H)$ - Model Cost:</strong> The number of bits needed to describe the model (e.g., weights, architecture). A complex model (like a deep neural net) has a high $L(H)$.</li>
                        <li><strong>$L(D | H)$ - Data Cost:</strong> The number of bits needed to describe the data <em>given</em> the model. This is equivalent to the log-likelihood (or loss). A model that fits well has a low $L(D | H)$.</li>
                    </ul>
                    
                    <h4>Example: Curve Fitting</h4>
                    <p>Imagine fitting points with a polynomial:</p>
                    <ul>
                        <li><strong>Line ($y=mx+b$):</strong> Very simple model (Low $L(H)$), but might miss the curve (High error/surprise $\rightarrow$ High $L(D|H)$).</li>
                        <li><strong>Degree-100 Polynomial:</strong> Passes through every point perfectly (Zero $L(D|H)$), but requires sending 101 coefficients (Huge $L(H)$).</li>
                        <li><strong>MDL Winner:</strong> A parabola ($y=ax^2+bx+c$) that captures the trend with few parameters. It minimizes the <em>sum</em> of both costs.</li>
                    </ul>
                </div>

                <p>
                    <strong>Kolmogorov Complexity</strong> takes this to the limit: the complexity of a string is the length of the <em>shortest possible computer program</em> that produces that string.
                    While uncomputable, it's the theoretical bedrock of learning. When we train a Neural Network, we are searching for a set of weights (a program) that compresses the training data (by predicting it accurately). 
                    A "generalizable" pattern is simply one that can be described more concisely than the data itself.
                </p>

                <h3 id="free-energy">Deep Dive: The Free Energy Principle</h3>
                <p>
                    Entropy is also central to the <strong>Free Energy Principle (FEP)</strong>, a unified theory of brain function proposed by Karl Friston. 
                    It states that all self-organizing systems (like brains or cells) must minimize their <strong>Variational Free Energy</strong> to survive.
                </p>

                <div class="definition-box">
                    <div class="box-title">Variational Free Energy ($F$)</div>
                    <p>
                        Free Energy is an upper bound on "surprise" (negative log-evidence). Minimizing it entails two conflicting goals:
                        $$F = \underbrace{D_{KL}(Q(z|x) \| P(z))}_{\text{Complexity}} - \underbrace{\mathbb{E}_{Q}[\log P(x|z)]}_{\text{Accuracy}}$$
                    </p>
                    <ul>
                        <li><strong>Accuracy:</strong> The model should explain the sensory data well (minimize prediction error).</li>
                        <li><strong>Complexity:</strong> The model's internal states ($Q$) should not diverge too much from its prior beliefs ($P$). It should keep its worldview simple.</li>
                    </ul>
                </div>

                <p>
                    This is mathematically identical to the objective function of a <strong>Variational Autoencoder (VAE)</strong>!
                    In machine learning, we maximize the Evidence Lower Bound (ELBO), which is just negative Free Energy ($ELBO = -F$).
                </p>
                <ul>
                    <li><strong>Perception:</strong> Updating internal states ($z$) to minimize $F$ given sensory input.</li>
                    <li><strong>Action:</strong> Changing the world (sensory input) to minimize $F$ given expectations.</li>
                    <li><strong>Learning:</strong> Updating the model parameters to minimize average $F$ over time.</li>
                </ul>
                <p>
                    In this view, <strong>Entropy minimization is the driving force of life and intelligence</strong>. We act to ensure our sensory inputs fall within the narrow, low-entropy states consistent with our survival.
                </p>
                
                <!-- Section 5: ML Connection -->
                <h2 id="ml-connection">Entropy in Machine Learning</h2>
                
                <p>
                    Entropy isn't just a loss function; it's a fundamental way to understand what a model <em>knows</em>.
                </p>

                <h3>1. Entropy as a Measure of "Clumpiness"</h3>
                <p>
                    Think of a probability distribution as a pile of sand.
                </p>
                <ul>
                    <li><strong>Low Entropy:</strong> The sand is piled high in one spot. The model is <em>certain</em> (confident).</li>
                    <li><strong>High Entropy:</strong> The sand is spread out flat. The model is <em>uncertain</em> (confused).</li>
                </ul>
                <p>
                    When a model starts training, its weights are random, so it outputs a nearly uniform distribution (High Entropy). 
                    As it learns, it starts assigning higher probability to specific classes, reducing entropy.
                </p>

                <h3>2. Why "Surprise" Drives Learning (Gradients)</h3>
                <p>
                    Why do we use Cross-Entropy loss instead of Mean Squared Error (MSE) for classification?
                    <strong>Because gradients should be proportional to surprise.</strong>
                </p>
                
                <p>
                    If a model is 100% wrong (predicts 0% for the correct class), it should be <em>infinitely</em> surprised. 
                    The slope of $-\log(p)$ at $p \to 0$ goes to infinity. This creates a massive gradient that "shoves" the weights 
                    strongly in the right direction. MSE, on the other hand, saturates and learns slowly when completely wrong.
                </p>

                <h3>3. Cross-Entropy Loss</h3>
                
                <p>
                    When training classifiers, we minimize <em>cross-entropy</em> between the true 
                    distribution $p$ and our model's predictions $q$:
                </p>
                
                <div class="math-block">
                    $$H(p, q) = -\sum_{x} p(x) \log q(x)$$
                </div>
                
                <p>
                    In classification, $p$ is one-hot (true label) and $q$ is the softmax output. 
                    Cross-entropy simplifies to $-\log q(\text{correct class})$.
                </p>
                
                <div class="warning-box">
                    <div class="box-title">Key Insight</div>
                    <p style="margin-bottom: 0;">
                        Minimizing cross-entropy is equivalent to maximizing log-likelihood. 
                        The model learns to assign high probability to the correct answers, 
                        which minimizes the "surprise" when seeing training data.
                    </p>
                </div>
                
                <h3>4. Maximum Entropy Principle & Regularization</h3>
                
                <p>
                    <strong>Why do we sometimes WANT high entropy?</strong>
                </p>
                <p>
                    If a model is <em>too</em> certain (Entropy $\approx$ 0), it might be overfitting. 
                    Techniques like <strong>Label Smoothing</strong> explicitly prevent the model from reaching 0 entropy.
                    Instead of targeting $p=[1, 0]$, we target $p=[0.9, 0.1]$. This forces the model to remain slightly "open-minded," 
                    improving generalization.
                </p>
                
                <p>
                    When you don't know which distribution to use, choose the one with maximum entropy 
                    subject to your constraints. This is the <em>least biased</em> choice.
                    <ul>
                        <li>Constrained only to sum to 1? → Uniform distribution</li>
                        <li>Constrained to have specific mean and variance? → Gaussian distribution</li>
                    </ul>
                </p>
            </article>
            
            <!-- Code Section -->
            <section id="code" style="padding-top: 2rem; margin-top: 2rem; border-top: 1px solid #e5e7eb;">
                <h2 style="margin-bottom: 1.5rem;">Hands-on Code</h2>
                <p>Explore entropy, Huffman coding, and cross-entropy in the interactive Jupyter Notebook.</p>
                
                <div class="code-preview">
                    <h3>entropy_huffman.ipynb</h3>
                    <p style="margin-bottom: 1.5rem; color: #666;">
                        Python implementation of Entropy, Huffman Coding, and Visualization of distributions.
                    </p>
                    <a href="https://github.com/ml-entropy/ml-entropy.github.io/blob/main/tutorials/01_entropy_fundamentals/entropy_huffman.ipynb" class="btn-code" target="_blank">
                        <svg height="20" width="20" viewBox="0 0 16 16" fill="currentColor">
                            <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                        </svg>
                        View Notebook on GitHub
                    </a>
                </div>
            </section>
            
                        <!-- Exercises Section -->
            <section id="exercises" style="padding-top: 2rem; margin-top: 2rem; border-top: 1px solid #e5e7eb;">
                <h2 style="margin-bottom: 1.5rem;">30 Practice Exercises</h2>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>E1. Entropy of Uniform Distribution</h3>
                        <span class="difficulty-badge diff-easy">Easy</span>
                    </div>
                    <p>Calculate the entropy (in bits) of a uniform distribution over 16 outcomes.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>$H(X) = \log_2(N) = \log_2(16) = 4$ bits.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>E2. Surprise Calculation</h3>
                        <span class="difficulty-badge diff-easy">Easy</span>
                    </div>
                    <p>An event has a probability of $p=0.01$. What is the 'surprise' or information content (in bits) of observing this event?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>$I(x) = -\log_2(0.01) \approx 6.64$ bits.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>E3. Deterministic Entropy</h3>
                        <span class="difficulty-badge diff-easy">Easy</span>
                    </div>
                    <p>What is the entropy of a random variable that always takes the value 5?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>$H(X) = 0$. There is no uncertainty.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>E4. Bits vs Nats</h3>
                        <span class="difficulty-badge diff-easy">Easy</span>
                    </div>
                    <p>If an event has 1 bit of information, how many 'nats' of information does it have? (Hint: $\ln 2 \approx 0.693$)</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>1 bit = $\ln 2$ nats $\approx 0.693$ nats.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>E5. Independent Events</h3>
                        <span class="difficulty-badge diff-easy">Easy</span>
                    </div>
                    <p>You flip a fair coin 3 times. What is the total entropy of the sequence of outcomes?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>Since flips are independent, $H(X_1, X_2, X_3) = H(X_1) + H(X_2) + H(X_3) = 1 + 1 + 1 = 3$ bits.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>E6. Specific Distribution</h3>
                        <span class="difficulty-badge diff-easy">Easy</span>
                    </div>
                    <p>Calculate the entropy of $P = [0.5, 0.25, 0.25]$.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>
    $$H(P) = -(0.5 \log_2 0.5 + 0.25 \log_2 0.25 + 0.25 \log_2 0.25)$$
    $$= - (0.5(-1) + 0.25(-2) + 0.25(-2))$$
    $$= 0.5 + 0.5 + 0.5 = 1.5 \text{ bits}$$
    </p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>E7. Max Entropy Binary</h3>
                        <span class="difficulty-badge diff-easy">Easy</span>
                    </div>
                    <p>For a binary random variable with $P(X=1) = p$, for what value of $p$ is entropy maximized?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>$p=0.5$. The uniform distribution maximizes entropy.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>E8. Cross-Entropy Identity</h3>
                        <span class="difficulty-badge diff-easy">Easy</span>
                    </div>
                    <p>If the predicted distribution $Q$ is exactly equal to the true distribution $P$, what is the cross-entropy $H(P, Q)$ equal to?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>It is equal to the entropy $H(P)$. Since $D_{KL}(P||Q) = 0$.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>E9. KL Divergence Minimum</h3>
                        <span class="difficulty-badge diff-easy">Easy</span>
                    </div>
                    <p>What is the minimum possible value for KL Divergence $D_{KL}(P||Q)$?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>0. It is always non-negative (Gibbs' inequality).</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>E10. Negative Entropy?</h3>
                        <span class="difficulty-badge diff-easy">Easy</span>
                    </div>
                    <p>True or False: The entropy of a discrete random variable can be negative.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>False. Since $0 \le p \le 1$, $\log p \le 0$, so $-\sum p \log p \ge 0$. (Note: Differential entropy for continuous variables <em>can</em> be negative).</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>M1. Joint Entropy</h3>
                        <span class="difficulty-badge diff-medium">Medium</span>
                    </div>
                    <p>Let $X$ be the outcome of a fair coin flip (H/T). Let $Y=X$ (copy of X). What is the joint entropy $H(X, Y)$?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>Since $X$ and $Y$ are perfectly correlated (dependent):</p>
    <p>$H(X, Y) = H(X) + H(Y|X) = H(X) + 0 = 1$ bit.</p>
    <p>Alternatively, there are only 2 possible joint outcomes: (H,H) and (T,T), each with prob 0.5.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>M2. Conditional Entropy</h3>
                        <span class="difficulty-badge diff-medium">Medium</span>
                    </div>
                    <p>If $Y = f(X)$ is a deterministic function of $X$, what is $H(Y|X)$?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>$H(Y|X) = 0$. If you know $X$, you know $Y$ exactly, so there is no remaining uncertainty.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>M3. Huffman Lower Bound</h3>
                        <span class="difficulty-badge diff-medium">Medium</span>
                    </div>
                    <p>Can the average length of a Huffman code be strictly less than the entropy $H(X)$?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>No. Shannon's Source Coding Theorem states $L \ge H(X)$. It can only be equal if all probabilities are powers of 2.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>M4. KL Asymmetry</h3>
                        <span class="difficulty-badge diff-medium">Medium</span>
                    </div>
                    <p>Let $P=[1, 0]$ and $Q=[0.5, 0.5]$. Calculate $D_{KL}(P||Q)$ and $D_{KL}(Q||P)$. What does this show?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>
    $D_{KL}(P||Q) = 1 \log_2(1/0.5) + 0 = 1$ bit.<br>
    $D_{KL}(Q||P) = 0.5 \log_2(0.5/1) + 0.5 \log_2(0.5/0) = \infty$ (undefined/infinite).<br>
    This shows KL Divergence is <strong>not symmetric</strong>.
    </p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>M5. Cross-Entropy vs Entropy</h3>
                        <span class="difficulty-badge diff-medium">Medium</span>
                    </div>
                    <p>Prove that Cross-Entropy $H(P, Q)$ is always greater than or equal to Entropy $H(P)$.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>$H(P, Q) = H(P) + D_{KL}(P||Q)$. Since $D_{KL} \ge 0$, it follows that $H(P, Q) \ge H(P)$.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>M6. Entropy of Sum</h3>
                        <span class="difficulty-badge diff-medium">Medium</span>
                    </div>
                    <p>Let $X_1, X_2$ be independent fair binary variables (0 or 1). Let $Y = X_1 + X_2$. What is $H(Y)$?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>Possible values for Y: 0, 1, 2.<br>
    P(0) = 0.25 (0+0)<br>
    P(1) = 0.50 (0+1 or 1+0)<br>
    P(2) = 0.25 (1+1)<br>
    $H(Y) = -(0.25 \log 0.25 + 0.5 \log 0.5 + 0.25 \log 0.25) = -(-0.5 - 0.5 - 0.5) = 1.5$ bits.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>M7. Mutual Information</h3>
                        <span class="difficulty-badge diff-medium">Medium</span>
                    </div>
                    <p>Define Mutual Information $I(X; Y)$ in terms of Entropy $H(X)$ and Conditional Entropy $H(X|Y)$. Explain intuitively.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>$I(X; Y) = H(X) - H(X|Y)$. It represents the reduction in uncertainty about $X$ gained by observing $Y$.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>M8. Perplexity</h3>
                        <span class="difficulty-badge diff-medium">Medium</span>
                    </div>
                    <p>In NLP, models are evaluated on 'Perplexity', defined as $2^{H(P)}$. If a model has a perplexity of 8, what does this intuitively mean?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>It means the model is as confused as if it were choosing uniformly at random from 8 equally likely words.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>M9. Chain Rule</h3>
                        <span class="difficulty-badge diff-medium">Medium</span>
                    </div>
                    <p>Write the Chain Rule for Entropy for three variables $H(X, Y, Z)$.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>$H(X, Y, Z) = H(X) + H(Y|X) + H(Z|X, Y)$. Uncertainty sums up: uncertainty of X, plus uncertainty of Y given X, etc.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>M10. Concavity</h3>
                        <span class="difficulty-badge diff-medium">Medium</span>
                    </div>
                    <p>Is the entropy function $H(p)$ concave or convex? Why is this important for optimization?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>It is <strong>concave</strong>. This guarantees that a local maximum (uniform distribution) is the global maximum.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>H1. Geometric Distribution</h3>
                        <span class="difficulty-badge diff-hard">Hard</span>
                    </div>
                    <p>Derive the entropy of a geometric distribution $P(k) = (1-p)^{k-1}p$ for $k=1, 2, \dots$.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>
    $H(X) = -\sum p(k) \log p(k) = -\sum (1-p)^{k-1}p [\log p + (k-1)\log(1-p)]$
    After simplifications using expected value $E[X] = 1/p$:
    $$H(X) = \frac{-(1-p)\log_2(1-p) - p\log_2 p}{p} \text{ bits}$$
    </p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>H2. Independence Bound</h3>
                        <span class="difficulty-badge diff-hard">Hard</span>
                    </div>
                    <p>Prove that $H(X, Y) \le H(X) + H(Y)$. When does equality hold?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>This follows from $I(X; Y) \ge 0$. Since $I(X; Y) = H(X) + H(Y) - H(X, Y)$, non-negativity implies $H(X, Y) \le H(X) + H(Y)$. Equality holds iff X and Y are independent.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>H3. Differential Entropy Uniform</h3>
                        <span class="difficulty-badge diff-hard">Hard</span>
                    </div>
                    <p>Calculate the differential entropy of a Continuous Uniform distribution on $[0, a]$.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>
    PDF $p(x) = 1/a$ for $0 \le x \le a$.
    $$h(X) = -\int_0^a \frac{1}{a} \log \frac{1}{a} dx = - \log \frac{1}{a} \int_0^a \frac{1}{a} dx = \log a$$
    Note: If $a < 1$, entropy is negative!
    </p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>H4. Exponential Entropy</h3>
                        <span class="difficulty-badge diff-hard">Hard</span>
                    </div>
                    <p>Calculate the differential entropy of an Exponential distribution $\lambda e^{-\lambda x}$.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>
    $h(X) = 1 - \ln \lambda$ (nats).
    Derivation involves $\ln p(x) = \ln \lambda - \lambda x$ and taking expectation.
    </p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>H5. Gaussian KL</h3>
                        <span class="difficulty-badge diff-hard">Hard</span>
                    </div>
                    <p>Write the formula for KL Divergence between two univariate Gaussians $P \sim \mathcal{N}(\mu_1, \sigma_1^2)$ and $Q \sim \mathcal{N}(\mu_2, \sigma_2^2)$.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>
    $$D_{KL}(P||Q) = \ln\frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2 + (\mu_1 - \mu_2)^2}{2\sigma_2^2} - \frac{1}{2}$$
    This is crucial for Variational Autoencoders (VAEs).
    </p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>H6. Max Entropy Variance</h3>
                        <span class="difficulty-badge diff-hard">Hard</span>
                    </div>
                    <p>Why does the Gaussian distribution have the maximum entropy among all distributions with a fixed mean and variance?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>Using Lagrange multipliers with constraints $\int p(x)dx=1$, $\int xp(x)dx=\mu$, $\int (x-\mu)^2 p(x)dx=\sigma^2$ yields the form of the Gaussian PDF.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>H7. MLE vs KL</h3>
                        <span class="difficulty-badge diff-hard">Hard</span>
                    </div>
                    <p>Formal proof: Show that maximizing Likelihood is equivalent to minimizing KL divergence from the empirical distribution to the model.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>
    Let $P_{data}$ be the empirical distribution. Max $\sum \log Q(x_i)$ is equivalent to Max $E_{P_{data}}[\log Q(x)]$.
    Min $D_{KL}(P_{data}||Q) = \sum P_{data} \log P_{data} - \sum P_{data} \log Q$.
    The first term is constant wrt Q. So Min KL $\iff$ Max $\sum P_{data} \log Q$.
    </p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>H8. Entropy Rate</h3>
                        <span class="difficulty-badge diff-hard">Hard</span>
                    </div>
                    <p>For a stochastic process (like a Markov chain), what is the Entropy Rate?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>$\lim_{n \to \infty} \frac{1}{n} H(X_1, \dots, X_n)$. It measures the average new information per step in the long run.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>H9. Data Processing Inequality</h3>
                        <span class="difficulty-badge diff-hard">Hard</span>
                    </div>
                    <p>State the Data Processing Inequality for a Markov chain $X \to Y \to Z$.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>$I(X; Z) \le I(X; Y)$. Processing data (Y to Z) cannot create new information about the source X.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>H10. Fano's Inequality</h3>
                        <span class="difficulty-badge diff-hard">Hard</span>
                    </div>
                    <p>What does Fano's Inequality relate?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>It relates the probability of error $P_e$ in estimating $X$ from $Y$ to the conditional entropy $H(X|Y)$. It sets a lower bound on error probability based on uncertainty.</p>
                        </div>
                    </details>
                </div>
                
                <h3 style="margin-top: 2rem; margin-bottom: 1rem;">Free Energy Exercises</h3>
                
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>F1. Free Energy Bound</h3>
                        <span class="difficulty-badge diff-hard">Hard</span>
                    </div>
                    <p>Prove that Variational Free Energy $F$ is an upper bound on surprise $-\log P(x)$.</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>
    $$F = D_{KL}(Q(z) || P(z|x)) - \log P(x)$$
    Since KL divergence is always non-negative ($D_{KL} \ge 0$):
    $$F \ge -\log P(x)$$
    Minimizing $F$ pushes it down towards the true surprise.
    </p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>F2. The Dark Room Problem</h3>
                        <span class="difficulty-badge diff-medium">Medium</span>
                    </div>
                    <p>If agents minimize the entropy of their sensory states, why don't they just stay in a dark, silent room where inputs are perfectly predictable (low entropy)?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>Because they minimize <strong>Variational Free Energy</strong> relative to a <em>prior model</em> of what states they <em>should</em> be in. 
    If your internal model (DNA/evolution) expects you to be fed and warm, staying in a dark room creates a large gap (prediction error) between expected states (fed) and actual states (starving), resulting in high Free Energy.</p>
                        </div>
                    </details>
                </div>
                <div class="exercise-card">
                    <div class="exercise-header">
                        <h3>F3. Accuracy-Complexity Tradeoff</h3>
                        <span class="difficulty-badge diff-medium">Medium</span>
                    </div>
                    <p>How does minimizing Free Energy $F = \text{Complexity} - \text{Accuracy}$ prevent overfitting?</p>
                    <details>
                        <summary>Show Solution</summary>
                        <div class="solution-content">
                            <p>
    Overfitting improves Accuracy (second term) but usually requires a complex posterior $Q$ that diverges far from the simple prior $P$. 
    This increases the Complexity cost (first term, KL divergence). Minimizing $F$ forces the model to find the simplest explanation (lowest KL) that still explains the data well (high accuracy).
    </p>
                        </div>
                    </details>
                </div>
            </section>


            <!-- Navigation Buttons -->
            <div class="tutorial-nav" style="margin-top: 3rem;">
                <a href="../00-probability/index.html" class="tutorial-nav-link prev">
                    <span class="nav-label">Previous</span>
                    <span class="nav-title">← Probability Concepts</span>
                </a>
                <a href="../02-kl-divergence/index.html" class="tutorial-nav-link next">
                    <span class="nav-label">Next</span>
                    <span class="nav-title">KL Divergence →</span>
                </a>
            </div>
        </main>

        <!-- TOC (Right Side) -->
        <aside class="toc-container">
            <h4 class="toc-title">Contents</h4>
            <nav class="toc-list">
                <a href="#intuition" class="toc-link">The Intuition</a>
                <a href="#derivation" class="toc-link">Deriving the Formula</a>
                <a href="#examples" class="toc-link">Concrete Examples</a>
                <a href="#coding" class="toc-link">Entropy and Coding</a>
                <a href="#learning-is-compression" class="toc-link">Learning is Compression</a>
                <a href="#ml-connection" class="toc-link">Entropy in ML</a>
                <a href="#code" class="toc-link">Code</a>
                <a href="#exercises" class="toc-link">Exercises</a>
            </nav>
        </aside>
    </div>


    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-brand">
                    <span class="logo-symbol">∇</span>
                    <span>ML Fundamentals</span>
                </div>
                <p class="footer-tagline">Deep understanding through first principles.</p>
            </div>
            <div class="footer-links">
                <a href="../../../index.html">Home</a>
                <a href="https://github.com/ml-entropy/ml-entropy.github.io" target="_blank">GitHub</a>
            </div>
        </div>
    </footer>

    <!-- Scripts -->
    <script src="../../../js/main.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            if (typeof renderMathInElement === 'function') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }
        });
    </script>
</body>
</html>
