<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Neural-Symbolic Hybrids & When to Choose FSTs | ML Fundamentals</title>
    <meta name="description" content="Positioning FSTs in the modern ML landscape: hybrid architectures, neural transducers, decision frameworks, and when to choose FSTs over neural networks.">

    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&family=Playfair+Display:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}, {left: '\\\\[', right: '\\\\]', display: true}, {left: '\\\\(', right: '\\\\)', display: false}], throwOnError: false});"></script>

    <!-- Styles -->
    <link rel="stylesheet" href="../../../css/main.css">
    <link rel="stylesheet" href="../../../css/components.css">
    <link rel="stylesheet" href="../../../css/sidebar.css">

    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>&#x221E;</text></svg>">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar" id="navbar">
        <div class="nav-container">
            <a href="../../../index.html" class="nav-logo">
                <span class="logo-symbol">&nabla;</span>
                <span class="logo-text">ML Fundamentals</span>
            </a>

            <button class="nav-toggle" id="navToggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>

            <div class="nav-menu" id="navMenu">
                <div class="nav-links">
                    <a href="../../../tutorials/ml/index.html" class="nav-link active">Machine Learning</a>
                    <a href="../../../tutorials/linear-algebra/index.html" class="nav-link">Linear Algebra</a>
                    <a href="../../../tutorials/calculus/index.html" class="nav-link">Calculus</a>
                    <a href="../../../tutorials/physics/index.html" class="nav-link">Physics</a>
                    <a href="../../../index.html#philosophy" class="nav-link">Philosophy</a>
                    <a href="../../../index.html#roadmap" class="nav-link">Roadmap</a>
                    <a href="https://github.com/ml-entropy/ml-entropy.github.io" class="nav-link" target="_blank">GitHub</a>
                </div>

                <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
                    <svg class="sun-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="5"/>
                        <path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"/>
                    </svg>
                    <svg class="moon-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                    </svg>
                </button>
            </div>
        </div>
    </nav>

    <!-- Tutorial Header -->
    <header class="tutorial-content-header">
        <div class="container">
            <nav class="breadcrumb">
                <a href="../../../index.html">Home</a>
                <span class="breadcrumb-separator">&rarr;</span>
                <a href="../index.html">Machine Learning</a>
                <span class="breadcrumb-separator">&rarr;</span>
                <span>Neural-Symbolic Hybrids</span>
            </nav>

            <div class="tutorial-tabs">
                <a href="#theory" class="tutorial-tab active">Theory</a>
                <a href="#code" class="tutorial-tab">Code</a>
                <a href="#exercises" class="tutorial-tab">Exercises</a>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <div class="tutorial-wrapper">

        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <aside class="tutorial-sidebar">
            <div class="sidebar-section">
                <h3 class="sidebar-section-title">Machine Learning</h3>
                <nav class="sidebar-nav">
                        <a href="../00-probability/index.html" class="sidebar-link">00. Probability Foundations</a>
                    <a href="../04-logarithms/index.html" class="sidebar-link">01. Why Logarithms?</a>
                    <a href="../05-combinatorics/index.html" class="sidebar-link">02. Combinatorics</a>
                    <a href="../03-distributions/index.html" class="sidebar-link">03. Normal Distributions</a>
                    <a href="../01-entropy/index.html" class="sidebar-link">04. Entropy Fundamentals</a>
                    <a href="../02-cross-entropy/index.html" class="sidebar-link">05. Cross-Entropy</a>
                    <a href="../02-kl-divergence/index.html" class="sidebar-link">06. KL Divergence</a>
                    <a href="../14-entropy-connections/index.html" class="sidebar-link">07. Entropy Connections</a>
                    <a href="../06-backpropagation/index.html" class="sidebar-link">08. Backpropagation</a>
                    <a href="../07-regularization/index.html" class="sidebar-link">09. Regularization</a>
                    <a href="../08-batch-normalization/index.html" class="sidebar-link">10. Batch Normalization</a>
                    <a href="../09-learning-rate/index.html" class="sidebar-link">11. Learning Rate</a>
                    <a href="../10-cnn/index.html" class="sidebar-link">12. CNNs</a>
                    <a href="../11-rnn/index.html" class="sidebar-link">13. RNNs</a>
                    <a href="../15-autoencoder/index.html" class="sidebar-link">14. Autoencoders</a>
                    <a href="../13-variational-inference/index.html" class="sidebar-link">15. Variational Inference</a>
                    <a href="../12-vae/index.html" class="sidebar-link">16. VAE</a>
                    <a href="../16-inductive-bias/index.html" class="sidebar-link">17. Inductive Bias</a>
                    <a href="../17-architectural-biases/index.html" class="sidebar-link">18. Architectural Biases</a>
                    <a href="../18-designing-biases/index.html" class="sidebar-link">19. Designing Biases</a>
                    <a href="../19-fst-fundamentals/index.html" class="sidebar-link">20. FST Fundamentals</a>
                    <a href="../20-weighted-fsts/index.html" class="sidebar-link">21. Weighted FSTs</a>
                    <a href="../21-fst-libraries/index.html" class="sidebar-link">22. FST Libraries</a>
                    <a href="../22-fst-applications/index.html" class="sidebar-link">23. FST Applications</a>
                    <a href="../23-neural-symbolic/index.html" class="sidebar-link active">24. Neural-Symbolic Hybrids</a>
                    <a href="../24-sequence-alignment/index.html" class="sidebar-link">25. Sequence Alignment</a>
                    <a href="../25-mas-algorithm/index.html" class="sidebar-link">26. MAS Algorithm</a>
                    <a href="../26-forced-alignment/index.html" class="sidebar-link">27. Forced Alignment & MFA</a>
                </nav>
            </div>
            
            <div class="sidebar-section" style="margin-top: 2rem;">
                <h3 class="sidebar-section-title">Related Subjects</h3>
                <nav class="sidebar-nav">
                        <a href="../../linear-algebra/index.html" class="sidebar-link">Linear Algebra</a>
                    <a href="../../calculus/index.html" class="sidebar-link">Calculus</a>
                    <a href="../../physics/index.html" class="sidebar-link">Physics</a>
                </nav>
            </div>
        </aside>

        <!-- Main Article -->
        <main class="tutorial-main">

            <!-- ==================== THEORY TAB ==================== -->
            <article class="article-content" id="theory">

                <!-- Section 1: FST vs Neural -->
                <h2 id="fst-vs-neural">FST vs Neural: The Tradeoff</h2>

                <p>
                    Throughout this series, we have built a thorough understanding of Finite State Transducers &mdash; their formalism, weighted variants, software libraries, and real-world applications. Now it is time to ask the most practical question of all: <strong>when should you actually use an FST instead of a neural network, and when should you combine both?</strong>
                </p>

                <p>
                    The tension between symbolic and neural approaches is one of the oldest debates in AI. FSTs represent the symbolic tradition: explicit rules, provable properties, transparent behavior. Neural networks represent the statistical tradition: learned representations, flexible approximation, data-driven adaptation. Neither is universally superior. The right choice depends on your problem, your data, and your constraints.
                </p>

                <p>
                    The following table summarizes the key tradeoffs across six critical dimensions:
                </p>

                <div class="definition-box">
                    <div class="box-title">FST vs Neural Network: Comparison Table</div>
                    <table style="width: 100%; border-collapse: collapse; margin-top: 0.5rem;">
                        <thead>
                            <tr style="border-bottom: 2px solid var(--color-border);">
                                <th style="text-align: left; padding: 0.5rem;">Dimension</th>
                                <th style="text-align: left; padding: 0.5rem;">FSTs</th>
                                <th style="text-align: left; padding: 0.5rem;">Neural Networks</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr style="border-bottom: 1px solid var(--color-border);">
                                <td style="padding: 0.5rem;"><strong>Interpretability</strong></td>
                                <td style="padding: 0.5rem;">Fully transparent &mdash; every transition is inspectable and explainable</td>
                                <td style="padding: 0.5rem;">Opaque &mdash; learned weights are difficult to interpret</td>
                            </tr>
                            <tr style="border-bottom: 1px solid var(--color-border);">
                                <td style="padding: 0.5rem;"><strong>Data Efficiency</strong></td>
                                <td style="padding: 0.5rem;">Zero data needed &mdash; rules are hand-crafted from expert knowledge</td>
                                <td style="padding: 0.5rem;">Requires labeled data; performance scales with dataset size</td>
                            </tr>
                            <tr style="border-bottom: 1px solid var(--color-border);">
                                <td style="padding: 0.5rem;"><strong>Flexibility</strong></td>
                                <td style="padding: 0.5rem;">Rigid &mdash; only handles patterns anticipated by the designer</td>
                                <td style="padding: 0.5rem;">Highly flexible &mdash; generalizes to unseen patterns from data</td>
                            </tr>
                            <tr style="border-bottom: 1px solid var(--color-border);">
                                <td style="padding: 0.5rem;"><strong>Generalization</strong></td>
                                <td style="padding: 0.5rem;">No generalization beyond explicit rules; no ability to handle novel inputs gracefully</td>
                                <td style="padding: 0.5rem;">Learns abstract representations that generalize to new examples</td>
                            </tr>
                            <tr style="border-bottom: 1px solid var(--color-border);">
                                <td style="padding: 0.5rem;"><strong>Speed</strong></td>
                                <td style="padding: 0.5rem;">$O(n)$ in input length with small constant; extremely fast at runtime</td>
                                <td style="padding: 0.5rem;">Variable &mdash; depends on model size; can be slow for large models</td>
                            </tr>
                            <tr>
                                <td style="padding: 0.5rem;"><strong>Debugging</strong></td>
                                <td style="padding: 0.5rem;">Straightforward &mdash; trace the state path to find the bug</td>
                                <td style="padding: 0.5rem;">Difficult &mdash; requires probing activations, gradient analysis, ablation studies</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <div class="note-box">
                    <div class="box-title">Task-Level Comparison: Which Approach Wins?</div>
                    <table style="width: 100%; border-collapse: collapse; margin-top: 0.5rem;">
                        <thead>
                            <tr style="border-bottom: 2px solid var(--color-border);">
                                <th style="text-align: left; padding: 0.5rem;">Task</th>
                                <th style="text-align: left; padding: 0.5rem;">Best Approach</th>
                                <th style="text-align: left; padding: 0.5rem;">Why</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr style="border-bottom: 1px solid var(--color-border);">
                                <td style="padding: 0.5rem;">Spell correction with known rules</td>
                                <td style="padding: 0.5rem;"><strong>FST</strong></td>
                                <td style="padding: 0.5rem;">Edit-distance transducer covers all errors within distance $k$; no training data needed, guaranteed coverage of the dictionary.</td>
                            </tr>
                            <tr style="border-bottom: 1px solid var(--color-border);">
                                <td style="padding: 0.5rem;">Machine translation (high-resource)</td>
                                <td style="padding: 0.5rem;"><strong>Neural</strong></td>
                                <td style="padding: 0.5rem;">Requires long-range context, disambiguation, and fluency that cannot be captured by finite-state rules; abundant parallel data available.</td>
                            </tr>
                            <tr style="border-bottom: 1px solid var(--color-border);">
                                <td style="padding: 0.5rem;">Low-resource grapheme-to-phoneme (G2P)</td>
                                <td style="padding: 0.5rem;"><strong>Hybrid</strong></td>
                                <td style="padding: 0.5rem;">FST encodes known pronunciation rules; neural model learns exceptions from limited data. The combination outperforms either alone.</td>
                            </tr>
                            <tr style="border-bottom: 1px solid var(--color-border);">
                                <td style="padding: 0.5rem;">Phone number formatting</td>
                                <td style="padding: 0.5rem;"><strong>FST</strong></td>
                                <td style="padding: 0.5rem;">Purely deterministic pattern rewriting; a neural model would be wasteful overkill.</td>
                            </tr>
                            <tr style="border-bottom: 1px solid var(--color-border);">
                                <td style="padding: 0.5rem;">Sentiment analysis</td>
                                <td style="padding: 0.5rem;"><strong>Neural</strong></td>
                                <td style="padding: 0.5rem;">Context-dependent, requires understanding irony, negation scope, and implicit meaning &mdash; impossible to enumerate as rules.</td>
                            </tr>
                            <tr>
                                <td style="padding: 0.5rem;">Speech recognition (voice commands)</td>
                                <td style="padding: 0.5rem;"><strong>Hybrid</strong></td>
                                <td style="padding: 0.5rem;">Neural acoustic model handles audio variability; FST decoder constrains output to valid commands, guaranteeing no hallucinated words.</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <p><strong>When FSTs win decisively:</strong></p>
                <ul>
                    <li><strong>Low-data regimes:</strong> When you have little or no training data, but you do have expert knowledge of the transformation rules.</li>
                    <li><strong>Rules are known and fixed:</strong> Text normalization, morphological analysis, phone number formatting &mdash; patterns that are well-understood and unlikely to change.</li>
                    <li><strong>Interpretability is required:</strong> Medical systems, legal applications, safety-critical pipelines where every decision must be auditable.</li>
                    <li><strong>Latency constraints:</strong> Edge devices, real-time systems where you cannot afford the cost of neural inference.</li>
                </ul>

                <p><strong>When neural networks win decisively:</strong></p>
                <ul>
                    <li><strong>Learning from data:</strong> When the transformation is too complex to articulate as rules, but you have abundant labeled examples.</li>
                    <li><strong>Long-range context:</strong> Transformations that depend on distant parts of the input (e.g., coreference resolution, document-level translation).</li>
                    <li><strong>Ambiguity and noise:</strong> When input is noisy or the correct output depends on probabilistic context (e.g., speech recognition in noisy environments).</li>
                    <li><strong>Evolving patterns:</strong> When the underlying distribution changes over time, and the system must adapt through retraining rather than manual rule updates.</li>
                </ul>

                <!-- Section 2: Hybrid Architectures -->
                <h2 id="hybrid-architectures">Hybrid Architectures</h2>

                <p>
                    The most powerful insight from decades of practical NLP and speech processing is this: <strong>you do not have to choose one or the other</strong>. The best systems often combine FSTs and neural networks, using each where it excels and letting them compensate for each other's weaknesses.
                </p>

                <p>
                    There are three dominant hybrid architecture patterns:
                </p>

                <p><strong>Pattern 1: Neural Encoder &rarr; FST Decoder</strong></p>
                <p>
                    A neural network processes raw input (audio, images, noisy text) and produces a structured intermediate representation. An FST then decodes this representation into a valid output sequence according to known constraints. The neural network handles the messy, ambiguous perception problem; the FST ensures the output conforms to known rules.
                </p>
                <ul>
                    <li><strong>Example:</strong> In speech recognition, a neural acoustic model scores phoneme hypotheses, and a WFST decoder combines these scores with a pronunciation lexicon and language model to produce valid word sequences.</li>
                    <li><strong>Architecture:</strong> Raw audio &rarr; Neural acoustic model &rarr; Phoneme posteriors &rarr; WFST (lexicon + grammar) &rarr; Word sequence</li>
                </ul>

                <p><strong>Pattern 2: FST Preprocessor &rarr; Neural Model</strong></p>
                <p>
                    An FST handles deterministic, rule-based preprocessing (normalization, tokenization, known pattern extraction), and a neural model handles the remaining ambiguous, context-dependent task. The FST reduces the complexity of the input that the neural network must handle.
                </p>
                <ul>
                    <li><strong>Example:</strong> In text classification, an FST normalizes dates, numbers, URLs, and abbreviations before a neural classifier processes the cleaned text.</li>
                    <li><strong>Architecture:</strong> Raw text &rarr; FST (normalize, tokenize) &rarr; Cleaned tokens &rarr; Neural classifier &rarr; Label</li>
                </ul>

                <p><strong>Pattern 3: Lattice Rescoring with Neural LM</strong></p>
                <p>
                    An FST generates a lattice (a compact representation of many possible output sequences with scores), and a neural language model rescores this lattice to select the best output. The FST efficiently constrains the search space, and the neural model provides fine-grained disambiguation.
                </p>
                <ul>
                    <li><strong>Example:</strong> In machine translation, a phrase-based FST generates a translation lattice, and a neural LM rescores to select the most fluent translation.</li>
                    <li><strong>Architecture:</strong> Input &rarr; FST (generate n-best lattice) &rarr; Neural LM (rescore) &rarr; Best output</li>
                </ul>

                <div class="definition-box">
                    <div class="box-title">Lattice Rescoring</div>
                    <p>
                        <strong>Lattice rescoring</strong> is a two-pass decoding strategy. In the first pass, a fast model (typically an FST or n-gram language model) generates a <em>lattice</em> &mdash; a directed acyclic graph that compactly encodes many candidate output sequences along with their scores. In the second pass, a more powerful model (typically a neural language model) re-evaluates each candidate and selects the best one according to a combined score.
                    </p>
                    <p style="margin-bottom: 0;">
                        Formally, the rescored probability of a word sequence $w$ is a weighted interpolation of the FST score and the neural score:
                    </p>
                </div>

                <div class="math-derivation">
                    <div class="math-derivation-title">Lattice Rescoring Formula</div>

                    <div class="math-step">
                        <div class="math-step-number">1</div>
                        <div class="math-step-content">
                            <p>The FST generates an n-best list of candidates $\{w_1, w_2, \ldots, w_N\}$, each with score $P_{\text{FST}}(w_i)$ from the first-pass model (acoustic + language model).</p>
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">2</div>
                        <div class="math-step-content">
                            <p>A neural language model computes $P_{\text{neural}}(w_i)$ for each candidate, capturing long-range fluency and contextual plausibility.</p>
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">3</div>
                        <div class="math-step-content">
                            <p>The hybrid score is computed as a log-linear interpolation:</p>
                            $$\log P_{\text{hybrid}}(w) = \alpha \cdot \log P_{\text{FST}}(w) + (1 - \alpha) \cdot \log P_{\text{neural}}(w)$$
                            <p>where $\alpha \in [0, 1]$ controls the balance. Setting $\alpha = 1$ recovers the pure FST ranking; $\alpha = 0$ uses only the neural model. In practice, $\alpha$ is tuned on a development set.</p>
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">4</div>
                        <div class="math-step-content">
                            <p>The final output is the candidate that maximizes the hybrid score:</p>
                            $$\hat{w} = \arg\max_{w \in \text{lattice}} \left[\alpha \cdot \log P_{\text{FST}}(w) + (1 - \alpha) \cdot \log P_{\text{neural}}(w)\right]$$
                        </div>
                    </div>
                </div>

                <div class="note-box">
                    <div class="box-title">Best of Both Worlds</div>
                    <p style="margin-bottom: 0;">
                        Hybrid architectures achieve what neither approach can alone: the <strong>guarantees and efficiency of FSTs</strong> combined with the <strong>flexibility and learning ability of neural networks</strong>. The FST constrains the search space and enforces structural validity, while the neural component handles ambiguity and generalization. This division of labor is not a compromise &mdash; it is a principled design pattern that leverages the strengths of both paradigms.
                    </p>
                </div>

                <!-- Section 3: Neural Transducers -->
                <h2 id="neural-transducers">Neural Transducers</h2>

                <p>
                    A fascinating line of research blurs the boundary between FSTs and neural networks entirely, creating models that have the <em>structure</em> of transducers but the <em>learning ability</em> of neural networks.
                </p>

                <p><strong>RNN Transducers (RNN-T)</strong></p>
                <p>
                    The RNN Transducer, introduced by Graves (2012), is a neural sequence-to-sequence model that explicitly factors the output generation into a transcription network (encoder) and a prediction network (decoder), combined through a joint network. At each step, the model decides whether to emit an output symbol or consume another input symbol &mdash; exactly like an FST transition that may consume input, produce output, or both.
                </p>
                <p>
                    The key difference from a classical FST: the "states" are continuous hidden vectors rather than discrete symbols, and the "transition function" is a learned neural network rather than a lookup table. RNN-T has become one of the dominant architectures for streaming speech recognition precisely because it preserves the online, left-to-right processing property of FSTs while learning from data.
                </p>

                <p><strong>Attention as "Soft" FST</strong></p>
                <p>
                    Attention mechanisms can be viewed as a soft, differentiable generalization of FST operations. In a classical FST, at each step you are in exactly one state and follow exactly one transition. In an attention-based model, you maintain a <em>distribution</em> over positions (analogous to being in a superposition of states) and "transition" by reweighting this distribution. The hard alignment of FSTs becomes the soft alignment of attention.
                </p>

                <p><strong>Differentiable FSTs</strong></p>
                <p>
                    The most direct bridge between FSTs and neural networks is the idea of making FST operations differentiable. Recall that a WFST maps paths to weights via the semiring. If we choose the weight domain to be real numbers and use operations compatible with automatic differentiation (real-valued addition and multiplication instead of tropical min/+), the FST becomes a differentiable computation graph. Gradients can flow through the FST, enabling end-to-end training.
                </p>

                <div class="definition-box">
                    <div class="box-title">Neural Transducer</div>
                    <p>
                        A <strong>Neural Transducer</strong> is a neural network architecture that preserves the input-output sequential transformation structure of classical transducers while replacing discrete states and transition tables with continuous representations and learned functions. Formally:
                    </p>
                    <p>
                        A neural transducer learns a function $f: \Sigma^* \to \Gamma^*$ similar to an FST, but uses continuous representations instead of discrete states. Where a classical FST defines a transition function $\delta: Q \times \Sigma \to Q \times \Gamma^*$ over a finite state set $Q$, a neural transducer defines $\delta_\theta: \mathbb{R}^d \times \Sigma \to \mathbb{R}^d \times \Delta(\Gamma)$ where $\mathbb{R}^d$ is a continuous state space and $\Delta(\Gamma)$ is a probability distribution over output symbols. The parameters $\theta$ are learned from data via gradient descent.
                    </p>
                    <p style="margin-bottom: 0;">
                        The model maintains the ability to process input and produce output incrementally (token by token), enabling streaming applications and compositional reasoning, while gaining the ability to learn complex patterns from data through gradient-based optimization.
                    </p>
                </div>

                <div class="note-box">
                    <div class="box-title">Key Insight: Attention as Soft Alignment</div>
                    <p style="margin-bottom: 0;">
                        Attention mechanisms can be viewed as a <strong>soft, differentiable analogue of FST state transitions</strong>. In a classical FST, at each step you are in exactly one state and follow exactly one transition &mdash; this is a <em>hard</em> alignment between input and output positions. In an attention-based model, the decoder maintains a <em>distribution</em> over all encoder positions (analogous to being in a superposition of FST states) and "transitions" by reweighting this distribution at each step. The hard alignment $\alpha_{ij} \in \{0, 1\}$ of FSTs becomes the soft alignment $\alpha_{ij} \in [0, 1]$ of attention, where $\sum_j \alpha_{ij} = 1$. This relaxation is precisely what makes attention differentiable and trainable, while FST alignment is combinatorial and discrete.
                    </p>
                </div>

                <!-- Section 4: Kaldi Case Study -->
                <h2 id="kaldi-case-study">Case Study: Kaldi (Speech)</h2>

                <p>
                    The Kaldi speech recognition toolkit is perhaps the best real-world example of FST-neural hybrid architecture at scale. For over a decade, Kaldi was the dominant open-source speech recognition system, used in both research and production. Its architecture is deeply instructive for understanding how FSTs and neural networks complement each other in practice.
                </p>

                <p><strong>The Kaldi Pipeline</strong></p>
                <p>
                    Kaldi's decoding pipeline composes four WFSTs into a single search graph:
                </p>
                <ol>
                    <li><strong>H (HMM topology):</strong> Maps context-dependent HMM state sequences to context-dependent phones. This FST encodes the acoustic model topology &mdash; which sequences of sub-phonetic states correspond to each phone.</li>
                    <li><strong>C (Context-dependency):</strong> Maps context-dependent phones to context-independent phones. A triphone system, for example, has separate models for /k/ in "ski" vs /k/ in "cat" &mdash; this FST collapses them back to the base phone.</li>
                    <li><strong>L (Lexicon):</strong> Maps phone sequences to words. This is the pronunciation dictionary &mdash; the FST that knows "cat" is pronounced /k ae t/.</li>
                    <li><strong>G (Grammar):</strong> Maps word sequences to valid sentences. This is the language model, represented as a WFST (often an n-gram model compiled into FST form).</li>
                </ol>
                <p>
                    These four FSTs are composed into a single WFST: $HCLG = H \circ C \circ L \circ G$. This composed FST maps directly from HMM states to word sequences, allowing a single Viterbi or beam search to find the best path.
                </p>

                <div class="definition-box">
                    <div class="box-title">The HCLG Cascade</div>
                    <p>The HCLG cascade is a composition of four WFSTs, each encoding a distinct level of linguistic knowledge:</p>
                    <ul>
                        <li><strong>H</strong> (HMM topology): Maps sub-phonetic HMM state IDs to context-dependent phone labels.</li>
                        <li><strong>C</strong> (Context-dependency): Maps context-dependent phones (e.g., triphones) to context-independent phones.</li>
                        <li><strong>L</strong> (Lexicon): Maps phone sequences to words &mdash; the pronunciation dictionary.</li>
                        <li><strong>G</strong> (Grammar): Maps word sequences to valid sentences &mdash; the language model.</li>
                    </ul>
                    <p style="margin-bottom: 0;">The composition $HCLG = H \circ C \circ L \circ G$ creates a single transducer that maps directly from acoustic HMM states to word sequences, enabling a unified search.</p>
                </div>

                <div class="math-derivation">
                    <div class="math-derivation-title">Kaldi Decoding Formula</div>

                    <div class="math-step">
                        <div class="math-step-number">1</div>
                        <div class="math-step-content">
                            <p>Given acoustic observations $O = (o_1, o_2, \ldots, o_T)$, the goal of speech recognition is to find the most probable word sequence $\hat{w}$:</p>
                            $$\hat{w} = \arg\max_w \; P(w \mid O) = \arg\max_w \; P(O \mid w) \cdot P(w)$$
                            <p>by Bayes' rule (dropping $P(O)$ since it is constant across hypotheses).</p>
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">2</div>
                        <div class="math-step-content">
                            <p>The HCLG cascade encodes $P(O \mid w)$ and $P(w)$ jointly. The decoding becomes a shortest-path problem on the composed WFST:</p>
                            $$\hat{w} = \arg\max_w \; [H \circ C \circ L \circ G](O)$$
                            <p>where $[H \circ C \circ L \circ G](O)$ denotes the weight of the best path through the composed transducer given observation sequence $O$.</p>
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">3</div>
                        <div class="math-step-content">
                            <p>In the tropical semiring $(\mathbb{R} \cup \{+\infty\}, \min, +, +\infty, 0)$, this is equivalent to finding the minimum-cost path through the composed graph, which is solved efficiently by the Viterbi algorithm or beam search.</p>
                        </div>
                    </div>
                </div>

                <div class="note-box">
                    <div class="box-title">Why WFST Composition Makes This Tractable</div>
                    <p style="margin-bottom: 0;">
                        Without composition, decoding would require searching over all possible HMM state sequences, phone sequences, word sequences, and sentence structures simultaneously &mdash; a combinatorial explosion. WFST composition pre-computes the interactions between these levels, and determinization and minimization compress the result into a compact graph. The composed $HCLG$ transducer is typically optimized offline (taking minutes to hours), but decoding through it at runtime takes milliseconds per utterance. This separation of offline compilation and online search is the key engineering insight that made real-time speech recognition possible.
                    </p>
                </div>

                <p><strong>Where Neural Networks Enter</strong></p>
                <p>
                    The neural component in Kaldi is the <strong>acoustic model</strong> &mdash; a deep neural network (DNN, TDNN, or LSTM) that takes acoustic features (e.g., MFCCs or filterbank energies) as input and produces posterior probabilities over HMM states. These posteriors are converted to likelihoods and used as the "acoustic scores" on the arcs of the H transducer.
                </p>
                <p>
                    This separation is elegant: the neural network handles the hardest perceptual problem (mapping messy, variable audio to phonetic categories), while the FST cascade handles the structural knowledge (pronunciation rules, grammar, valid word sequences). Neither component could do the other's job well.
                </p>

                <p><strong>Why This Works</strong></p>
                <ul>
                    <li><strong>Modularity:</strong> Each FST encodes a distinct, independently verifiable piece of knowledge. You can update the lexicon without retraining the acoustic model. You can swap language models without touching anything else.</li>
                    <li><strong>Interpretability:</strong> When the system makes an error, you can trace the path through $HCLG$ to determine exactly which component caused it. Was it a bad acoustic score? A missing pronunciation? A low-probability word sequence?</li>
                    <li><strong>Efficient search:</strong> The composed WFST supports efficient beam search, pruning, and lattice generation. This is critical for real-time speech recognition.</li>
                    <li><strong>Formal guarantees:</strong> The FST framework guarantees that the output is always a valid word sequence from the vocabulary, unlike end-to-end neural models that can hallucinate non-existent words.</li>
                </ul>

                <div class="note-box">
                    <div class="box-title">Kaldi's Legacy</div>
                    <p style="margin-bottom: 0;">
                        While modern end-to-end models (CTC, RNN-T, attention-based encoder-decoder) have largely replaced the Kaldi-style pipeline in production, Kaldi's architectural insights remain relevant. Many production systems still use FST-based decoding on top of neural encoders, especially when they need constrained decoding (e.g., voice commands from a fixed vocabulary) or when they need the modularity to swap components independently. The Kaldi design pattern &mdash; neural perception + symbolic reasoning &mdash; is a template for hybrid AI systems far beyond speech recognition.
                    </p>
                </div>

                <!-- Section 5: Decision Framework -->
                <h2 id="decision-framework">Decision Framework</h2>

                <p>
                    Given a new sequence transformation problem, how should you decide between an FST, a neural network, or a hybrid? The following decision framework distills the practical wisdom from the preceding tutorials into actionable guidance.
                </p>

                <div class="definition-box">
                    <div class="box-title">The FST/Neural/Hybrid Decision Framework</div>
                    <p><strong>Start with these questions:</strong></p>
                    <ol>
                        <li><strong>Can you write down the transformation rules explicitly?</strong>
                            <ul>
                                <li>Yes, completely &rarr; <strong>Use an FST.</strong></li>
                                <li>Partially &rarr; <strong>Consider a hybrid.</strong></li>
                                <li>No &rarr; Go to question 2.</li>
                            </ul>
                        </li>
                        <li><strong>Do you have abundant labeled training data?</strong>
                            <ul>
                                <li>Yes &rarr; <strong>Use a neural network.</strong></li>
                                <li>No &rarr; Go to question 3.</li>
                            </ul>
                        </li>
                        <li><strong>Can you decompose the problem into a rule-based part and a learned part?</strong>
                            <ul>
                                <li>Yes &rarr; <strong>Use a hybrid:</strong> FST for the rule-based part, neural for the rest.</li>
                                <li>No &rarr; Collect more data or consult domain experts to articulate rules.</li>
                            </ul>
                        </li>
                    </ol>
                    <p style="margin-bottom: 0;"><strong>Refining criteria:</strong> Even after the initial decision, consider whether interpretability is required (favors FST), whether the system must handle novel inputs gracefully (favors neural), whether latency is critical (favors FST), and whether the transformation involves long-range dependencies (favors neural).</p>
                </div>

                <div class="math-derivation">
                    <div class="math-derivation-title">Stepped Decision Process</div>

                    <div class="math-step">
                        <div class="math-step-number">1</div>
                        <div class="math-step-content">
                            <p><strong>"Do you have explicit, complete rules for the transformation?"</strong></p>
                            <p>If <em>Yes</em>: strongly consider a <strong>pure FST</strong>. If the rules fully describe the mapping and are unlikely to change, an FST provides guaranteed correctness, full interpretability, and minimal computational cost. Examples: date formatting, unit conversion, regular expression replacement.</p>
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">2</div>
                        <div class="math-step-content">
                            <p><strong>"Do you need to learn the transformation from data?"</strong></p>
                            <p>If <em>Yes</em>: consider a <strong>neural model</strong>. If the transformation cannot be articulated as rules and you have sufficient labeled data (typically thousands of examples or more), a neural approach will discover the patterns. Examples: machine translation, image captioning, sentiment analysis.</p>
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">3</div>
                        <div class="math-step-content">
                            <p><strong>"Do you need interpretability or auditability?"</strong></p>
                            <p>If <em>Yes</em>: lean toward <strong>FST or Hybrid</strong>. In regulated domains (medical, legal, financial), every decision must be traceable. An FST provides a complete audit trail. A hybrid allows the neural component to handle ambiguity while the FST enforces verifiable constraints.</p>
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">4</div>
                        <div class="math-step-content">
                            <p><strong>"Is your labeled data fewer than ~1,000 examples?"</strong></p>
                            <p>If <em>Yes</em>: <strong>FST or rule-based</strong> approach. Neural models typically need thousands of examples to generalize well. With very limited data, expert-crafted rules (FSTs) or few-shot approaches are more reliable. Even 50 hand-written rules can outperform a neural model trained on 200 examples.</p>
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">5</div>
                        <div class="math-step-content">
                            <p><strong>"Does the transformation require long-range context?"</strong></p>
                            <p>If <em>Yes</em>: <strong>neural (with attention)</strong>. FSTs have finite memory ($|Q|$ states), so they cannot model dependencies that span unbounded distances. Tasks like document summarization, coreference resolution, or translation of long sentences require attention mechanisms or recurrence. This is a fundamental limitation of FSTs, not just an engineering constraint.</p>
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">6</div>
                        <div class="math-step-content">
                            <p><strong>"Do you need both rules AND data-driven learning?"</strong></p>
                            <p>If <em>Yes</em>: <strong>Hybrid architecture</strong>. Many real-world systems have a deterministic component (formatting, known vocabulary, structural constraints) and a learned component (disambiguation, ranking, generation). Use FSTs for the rule-based parts and neural models for the learned parts. Examples: speech recognition (neural acoustics + FST decoding), constrained text generation (neural LM + FST grammar filter).</p>
                        </div>
                    </div>
                </div>

                <p>
                    <strong>Additional nuanced criteria:</strong>
                </p>
                <ul>
                    <li><strong>Error tolerance:</strong> If errors are catastrophic (medical, legal, financial), lean toward FSTs or FST-constrained decoding that guarantees valid output.</li>
                    <li><strong>Maintenance burden:</strong> FSTs require manual updates when rules change. Neural models require retraining when data distribution shifts. Estimate which maintenance cost is lower for your use case.</li>
                    <li><strong>Team expertise:</strong> An FST built by a linguist who understands the rules may outperform a neural model built by an ML engineer with no domain knowledge, and vice versa. The best approach often depends on what expertise is available.</li>
                    <li><strong>Compositionality:</strong> If the problem naturally decomposes into a pipeline of independent transformations, FSTs (which compose elegantly) may be preferable even if each step could in principle be learned.</li>
                    <li><strong>Deployment constraints:</strong> FSTs compile to compact, fast automata. Neural models may require GPUs, large memory footprints, or specialized hardware. For embedded or edge deployment, FSTs have a significant advantage.</li>
                </ul>

                <!-- Section 6: FST Limitations Revisited -->
                <h2 id="limitations-revisited">FST Limitations Revisited</h2>

                <p>
                    Throughout this series, we have been honest about what FSTs can and cannot do. As we conclude, it is worth consolidating these limitations into a clear, unflinching assessment. Understanding limitations is not a weakness &mdash; it is the foundation of good engineering judgment.
                </p>

                <div class="warning-box">
                    <div class="box-title">Fundamental Limitations of FSTs</div>
                    <p><strong>1. Non-regular patterns:</strong> FSTs operate on regular languages and compute regular relations. Any pattern requiring unbounded counting ($a^n b^n$), recursive nesting (balanced parentheses), or cross-serial dependencies cannot be handled by any FST, no matter how many states it has. Natural language syntax, in particular, is not regular.</p>
                    <p><strong>2. Context beyond a fixed window:</strong> An FST's "memory" is its current state. With $|Q|$ states, the FST can distinguish at most $|Q|$ different histories of the input. Any dependency that requires remembering more than a fixed amount of context will exceed the FST's capacity. Neural networks with attention or recurrence can, in principle, maintain unbounded context.</p>
                    <p><strong>3. Learning implicit patterns from data:</strong> FSTs do not learn. If the transformation rules are not known a priori, an FST cannot discover them from examples. This is the most fundamental gap: the patterns that humans can articulate are a tiny fraction of the patterns that exist in data.</p>
                    <p><strong>4. Handling noise and variability:</strong> Real-world input is noisy &mdash; misspellings, disfluencies, unexpected formatting. An FST either handles a pattern (because a rule was written for it) or fails entirely. Neural networks degrade gracefully, producing reasonable outputs even for inputs that differ from training examples.</p>
                    <p style="margin-bottom: 0;"><strong>5. Scalability of rule authoring:</strong> For simple transformations, writing FST rules is easy. But as the number of rules grows, interactions between rules become complex and hard to manage. A system with 10,000 FST rules is extremely difficult to maintain, debug, and extend. Neural models scale with data, not with manual engineering effort.</p>
                </div>

                <div class="definition-box">
                    <div class="box-title">Concrete Examples of FST Limitations</div>
                    <p><strong>Counting (balanced parentheses):</strong> Consider the language $\{(^n )^n \mid n \geq 0\}$ &mdash; strings of $n$ opening parentheses followed by $n$ closing ones: <code>()</code>, <code>(())</code>, <code>((()))</code>, etc. An FST cannot verify that the number of opening and closing parentheses match, because doing so requires counting to an unbounded $n$, which exceeds any finite number of states. After reading $k$ opening parentheses (where $k > |Q|$), the FST must revisit a state by the pigeonhole principle, losing track of the exact count.</p>
                    <p style="margin-bottom: 0;"><strong>Long-range agreement:</strong> Consider English sentences like "The cat that the dog that the rat bit chased ran." Verifying subject-verb agreement across multiple levels of embedding requires tracking which noun goes with which verb across arbitrary distances. An FST with $|Q|$ states can only distinguish $|Q|$ different embedding depths. Natural language regularly exceeds this limit, which is why syntax is context-free (at minimum), not regular.</p>
                </div>

                <div class="math-derivation">
                    <div class="math-derivation-title">Pumping Lemma Intuition</div>

                    <div class="math-step">
                        <div class="math-step-number">1</div>
                        <div class="math-step-content">
                            <p>The <strong>Pumping Lemma for regular languages</strong> gives a formal tool for proving that a language is <em>not</em> regular (and hence cannot be recognized by any FSA or transduced by any FST).</p>
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">2</div>
                        <div class="math-step-content">
                            <p>The core idea: if a language $L$ is regular, then for any sufficiently long string $s \in L$ (specifically, $|s| \geq p$ where $p$ is the "pumping length"), we can split $s = xyz$ such that:</p>
                            $$|y| > 0, \quad |xy| \leq p, \quad \text{and} \quad xy^i z \in L \;\; \forall \, i \geq 0$$
                            <p>That is, the middle portion $y$ can be "pumped" (repeated any number of times) and the result is still in $L$.</p>
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">3</div>
                        <div class="math-step-content">
                            <p><strong>Application to $a^n b^n$:</strong> Suppose $a^n b^n$ is regular with pumping length $p$. Take $s = a^p b^p$. Any split $s = xyz$ with $|xy| \leq p$ means $y = a^k$ for some $k > 0$. Pumping: $xy^2z = a^{p+k} b^p$, but $p + k \neq p$, so $xy^2z \notin L$. Contradiction &mdash; hence $a^n b^n$ is not regular.</p>
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">4</div>
                        <div class="math-step-content">
                            <p><strong>Practical significance:</strong> This is not merely a theoretical curiosity. Any task that requires unbounded counting, nesting, or cross-serial matching is provably beyond FSTs. Recognizing this early saves engineering effort &mdash; no amount of clever FST design can overcome a fundamental expressivity barrier.</p>
                        </div>
                    </div>
                </div>

                <p>
                    These limitations are real, but they do not make FSTs obsolete. They make FSTs a <em>specialized tool</em> &mdash; superb within their domain of applicability, and best used in combination with other approaches outside that domain.
                </p>

                <!-- Section 7: Future Directions -->
                <h2 id="future-directions">Future Directions</h2>

                <p>
                    The boundary between symbolic and neural computation is not static. Active research is pushing toward tighter integration, creating systems that combine the guarantees of FSTs with the adaptability of neural networks in ways that go beyond simple pipelines.
                </p>

                <p><strong>Differentiable FSTs</strong></p>
                <p>
                    By implementing FST operations (composition, shortest path, determinization) using differentiable primitives, researchers have created FSTs that can be trained end-to-end with gradient descent. The FST structure provides an inductive bias (the assumption that the transformation is transducer-like), while the weights are learned from data. Libraries like <code>k2</code> (a successor to Kaldi's FST framework) and <code>gtn</code> (from Facebook AI Research) implement this idea.
                </p>

                <p><strong>Neural-Guided FST Induction</strong></p>
                <p>
                    Instead of hand-crafting FST rules, can we learn the FST structure itself? Neural-guided FST induction uses neural networks to propose candidate FST transitions, which are then evaluated and refined. This is a form of program synthesis: the output is not a set of weights but a symbolic FST that can be inspected, verified, and composed with other FSTs.
                </p>

                <p><strong>FSTs for Interpretability and Safety in Neural Systems</strong></p>
                <p>
                    As neural models are deployed in high-stakes applications, there is growing interest in using FSTs as <em>guardrails</em> &mdash; constraining neural model output to a set of valid responses defined by an FST. For example, a neural text generation model might produce fluent text, but an FST filter ensures the output conforms to required formatting, avoids forbidden patterns, or stays within a sanctioned vocabulary. This "neural generation + FST constraint" pattern is increasingly common in production systems.
                </p>

                <div class="note-box">
                    <div class="box-title">Differentiable Relaxations: Bridging Discrete and Continuous</div>
                    <p>
                        The key technical challenge in making FSTs trainable is that their core operations are <strong>discrete</strong>: $\arg\min$ over paths (Viterbi decoding), hard composition of automata, and deterministic state transitions. These operations have zero gradients almost everywhere. The solution is to replace each discrete operation with a continuous, differentiable analogue:
                    </p>
                    <ul>
                        <li><strong>$\arg\min$ (shortest path)</strong> is relaxed to <strong>$\text{softmin}$</strong>: instead of selecting the single best path, compute a weighted average over all paths using the forward algorithm in the log semiring. Temperature parameter $\tau$ controls the sharpness: as $\tau \to 0$, softmin approaches hard argmin.</li>
                        <li><strong>Composition</strong> (intersection of two automata) is relaxed to <strong>matrix multiplication</strong>: transition matrices of two FSTs are multiplied, producing a soft composed automaton where every pair of transitions contributes with a real-valued weight.</li>
                        <li><strong>Deterministic transitions</strong> (one-hot state vectors) are relaxed to <strong>soft state distributions</strong>: the "current state" is a probability vector over all states, and transitions are matrix-vector products.</li>
                    </ul>
                    <p style="margin-bottom: 0;">
                        This relaxation framework, implemented in libraries like <code>gtn</code> and <code>k2</code>, allows gradients to flow through FST operations, enabling end-to-end training of hybrid systems where FST structure provides the inductive bias and neural networks provide the learning signal.
                    </p>
                </div>

                <div class="note-box">
                    <div class="box-title">An Emerging Research Frontier</div>
                    <p style="margin-bottom: 0;">
                        The integration of symbolic and neural computation is one of the most active areas in AI research. Differentiable programming, neurosymbolic AI, and structured prediction all draw on the same insight: combining the learnability of neural networks with the structure and guarantees of formal models like FSTs. The tools and ideas from this tutorial series &mdash; semirings, composition, weighted paths, lattice rescoring &mdash; are the vocabulary of this emerging field. Mastering them positions you to contribute to the next generation of AI systems.
                    </p>
                </div>

                <!-- Section 8: Summary -->
                <h2 id="summary">Series Summary</h2>

                <p>
                    This tutorial concludes our five-part series on Finite State Transducers. Let us look back at the journey and distill the key lessons.
                </p>

                <p><strong>Tutorial 20 &mdash; FST Fundamentals:</strong> We defined FSTs formally as 6-tuples, distinguished them from FSMs, explored deterministic vs non-deterministic variants, and learned that FSTs compute regular relations. We saw composition as the key operation and established the fundamental limitation: FSTs cannot count.</p>

                <p><strong>Tutorial 21 &mdash; Weighted FSTs:</strong> We added weights to FSTs via semirings, enabling probabilistic reasoning. The tropical semiring gave us shortest-path / Viterbi decoding. The log semiring gave us efficient marginalization. We learned the weight-pushing and epsilon-removal optimizations that make WFSTs practical at scale.</p>

                <p><strong>Tutorial 22 &mdash; FST Libraries:</strong> We surveyed the software ecosystem: OpenFst for C++ production use, Pynini for Python prototyping, k2 for differentiable FSTs with GPU support. We learned how to build, compose, optimize, and query FSTs programmatically.</p>

                <p><strong>Tutorial 23 &mdash; FST Applications:</strong> We walked through real-world applications: text normalization, morphological analysis, speech recognition decoding, and computational biology. Each application illustrated a different FST design pattern and motivated different optimization strategies.</p>

                <p><strong>Tutorial 24 &mdash; Neural-Symbolic Hybrids (this tutorial):</strong> We positioned FSTs honestly within the modern ML ecosystem. We explored hybrid architectures, neural transducers, the Kaldi case study, and a decision framework for choosing between FSTs, neural networks, and hybrid systems.</p>

                <div class="math-derivation">
                    <div class="math-derivation-title">Final Decision Guide</div>

                    <div class="math-step">
                        <div class="math-step-number">1</div>
                        <div class="math-step-content">
                            <p><strong>Use a pure FST when:</strong> Rules are known, data is scarce, interpretability is critical, or latency is paramount. Text normalization, morphological analysis, constrained decoding, and format conversion are canonical FST tasks.</p>
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">2</div>
                        <div class="math-step-content">
                            <p><strong>Use a pure neural model when:</strong> Rules are unknown, data is abundant, the task involves long-range context or ambiguity, and you can tolerate occasional errors. Machine translation, open-domain QA, and image captioning are canonical neural tasks.</p>
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">3</div>
                        <div class="math-step-content">
                            <p><strong>Use a hybrid when:</strong> Part of the problem is rule-based and part is learned. Speech recognition, constrained text generation, and any pipeline with deterministic preprocessing followed by neural inference are natural hybrid candidates.</p>
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">4</div>
                        <div class="math-step-content">
                            <p><strong>Use differentiable FSTs when:</strong> You want FST structure (compositionality, interpretability) but need to learn weights from data. Low-resource ASR, morphological inflection, and structured prediction are emerging applications.</p>
                        </div>
                    </div>
                </div>

                <p>
                    <strong>Closing thoughts:</strong> FSTs are not a relic of a pre-neural era. They are a mature, principled formalism for sequence transformations that remains indispensable in production systems. The best ML engineers do not ask "FST or neural?" &mdash; they ask "where in my system does each approach belong?" The ability to answer that question, grounded in the formal understanding developed across these five tutorials, is what separates a practitioner who builds robust systems from one who chases the latest model architecture.
                </p>

                <p>
                    The mathematics of finite-state machines &mdash; states, transitions, semirings, composition &mdash; is a language for thinking precisely about sequential computation. Whether you ultimately implement your solution as a classical WFST, a neural transducer, or a hybrid pipeline, this language will serve you. It is one of the deep structures of computer science, and now it is part of your toolkit.
                </p>

                <!-- Navigation -->
                <div class="tutorial-footer-summary" style="margin: 3rem 0; padding: 2rem; background: #f8f9fa; border-radius: 8px; border-left: 4px solid #3b82f6;">
                    <h1>24. Neural-Symbolic Hybrids</h1>
                    <p class="lead">
                        Positioning FSTs in the modern ML landscape: hybrid architectures, neural transducers, decision frameworks, and an honest assessment of when to choose FSTs, neural networks, or both.
                    </p>
                </div>
                <div class="tutorial-nav">
                    <a href="../22-fst-applications/index.html" class="tutorial-nav-link prev">
                        <span class="nav-label">Previous</span>
                        <span class="nav-title">&larr; FST Applications</span>
                    </a>
                    <a href="../24-sequence-alignment/index.html" class="tutorial-nav-link next">
                        <span class="nav-label">Next</span>
                        <span class="nav-title">Sequence Alignment &rarr;</span>
                    </a>
                </div>

            </article>

            <!-- ==================== CODE TAB ==================== -->
            <article class="article-content" id="code" style="display: none;">
                <h2>Python Code Examples</h2>
                <p>Three code examples: a hybrid FST + neural classifier, lattice generation with neural rescoring, and a differentiable toy FST that supports backpropagation.</p>

                <!-- Code Example 1 -->
                <h3>1. Hybrid: FST Text Preprocessing + Neural Classifier</h3>
                <p>
                    This example demonstrates the FST Preprocessor &rarr; Neural Model pattern. The FST handles deterministic text normalization (lowercasing, abbreviation expansion, punctuation removal), and a simple numpy-based neural network classifies the cleaned text.
                </p>

                <div class="code-block">
                    <div class="code-block-header">
                        <span class="code-block-lang">python</span>
                        <button class="code-block-copy">Copy</button>
                    </div>
                    <pre><code class="language-python">import numpy as np

# ============================================================
# Part 1: FST Text Preprocessor
# ============================================================
# This simulates a finite-state text normalizer. In production,
# you would use Pynini or OpenFst to compile these rules into
# an actual WFST for optimal performance. Here we implement
# the same logic in Python for clarity.

class TextNormalizerFST:
    """
    FST-based text preprocessor with four normalization stages:
      Stage 1 (lowercase):    Each character maps to its lowercase form
      Stage 2 (abbreviation): Multi-pattern transducer expands abbreviations
      Stage 3 (punctuation):  Punctuation symbols map to epsilon (deleted)
      Stage 4 (whitespace):   Multiple spaces collapse to single space

    Each stage is a separate FST; the full normalizer is their composition.
    """

    def __init__(self):
        # Abbreviation expansion rules -- each is an FST arc:
        #   input:"dr." / output:"doctor", input:"mr." / output:"mister", etc.
        self.abbreviations = {
            "dr.": "doctor", "mr.": "mister", "mrs.": "missus",
            "st.": "street", "govt": "government", "govt.": "government",
            "approx": "approximately", "dept": "department",
            "info": "information", "asap": "as soon as possible",
            "avg": "average", "est": "estimated",
        }
        self.punctuation = set('.,!?;:()[]{}"\'-')

    def transduce(self, text):
        """Apply the full FST normalization pipeline."""
        # Stage 1: Lowercase (single-state FST: sigma -> lower(sigma))
        text = text.lower()

        # Stage 2: Expand abbreviations (multi-pattern Aho-Corasick FST)
        words = text.split()
        expanded = []
        for word in words:
            clean_word = word.strip(''.join(self.punctuation))
            if clean_word in self.abbreviations:
                expanded.append(self.abbreviations[clean_word])
            else:
                expanded.append(word)
        text = ' '.join(expanded)

        # Stage 3: Remove punctuation (FST: punct -> epsilon)
        text = ''.join(c for c in text if c not in self.punctuation)

        # Stage 4: Normalize whitespace (FST: space+ -> space)
        text = ' '.join(text.split())
        return text


# ============================================================
# Part 2: Simple Neural Classifier (numpy only)
# ============================================================
# A tiny 2-layer neural network for text classification.
# In production you would use PyTorch/TensorFlow, but numpy
# demonstrates the core mechanics without dependencies.

class SimpleNeuralClassifier:
    """
    Bag-of-words neural classifier.
    Architecture: BoW(V) -> Dense(H, ReLU) -> Dense(C, softmax)
    Trained with cross-entropy loss and SGD.
    """

    def __init__(self, vocab_size, hidden_size=16, num_classes=3):
        np.random.seed(42)
        # Xavier initialization for stable gradient flow
        self.W1 = np.random.randn(vocab_size, hidden_size) * np.sqrt(2.0 / vocab_size)
        self.b1 = np.zeros(hidden_size)
        self.W2 = np.random.randn(hidden_size, num_classes) * np.sqrt(2.0 / hidden_size)
        self.b2 = np.zeros(num_classes)

    def forward(self, x):
        """Forward pass: BoW vector -> class probabilities."""
        self._x = x  # cache for backprop
        self._h = np.maximum(0, x @ self.W1 + self.b1)  # ReLU hidden layer
        logits = self._h @ self.W2 + self.b2
        exp_logits = np.exp(logits - np.max(logits))   # stable softmax
        self._probs = exp_logits / exp_logits.sum()
        return self._probs

    def backward(self, target_idx, lr=0.01):
        """Backward pass: compute gradients and update weights."""
        # Gradient of cross-entropy + softmax
        d_logits = self._probs.copy()
        d_logits[target_idx] -= 1.0  # d(loss)/d(logits)

        # Gradients for W2, b2
        dW2 = np.outer(self._h, d_logits)
        db2 = d_logits

        # Backprop through ReLU
        d_h = d_logits @ self.W2.T
        d_h[self._h <= 0] = 0  # ReLU derivative

        # Gradients for W1, b1
        dW1 = np.outer(self._x, d_h)
        db1 = d_h

        # SGD update
        self.W2 -= lr * dW2
        self.b2 -= lr * db2
        self.W1 -= lr * dW1
        self.b1 -= lr * db1

        # Return loss for monitoring
        return -np.log(self._probs[target_idx] + 1e-10)


# ============================================================
# Part 3: Data Generation and Hybrid Pipeline
# ============================================================

# Training data: texts with abbreviations, varied formatting
# Each text belongs to one of 5 categories
training_data = [
    ("The Dr. said the patient needs info about the dept.", 0),  # medical
    ("Patient has an ASAP appointment with Dr. Wilson!", 0),      # medical
    ("Mr. Smith lives on Oak St. in the govt. district.", 1),     # location
    ("Dept. of Transport office is on Main St.", 1),              # location
    ("Approx. 500 people attended the meeting yesterday!", 2),    # event
    ("EST. 300 attendees at the annual conference.", 2),          # event
    ("The stock price dropped 15% after the announcement.", 3),   # finance
    ("Avg. revenue increased by approx. 20% this quarter.", 3),   # finance
    ("BREAKING: New govt. policy on healthcare announced.", 4),    # politics
    ("Govt. dept. releases new regulation info.", 4),              # politics
]
label_names = ["medical", "location", "event", "finance", "politics"]

# ------ Step 1: FST Preprocessing (deterministic, no training) ------
fst = TextNormalizerFST()
print("=" * 65)
print("STEP 1: FST Preprocessing (deterministic, rule-based)")
print("=" * 65)
for text, label in training_data[:4]:
    cleaned = fst.transduce(text)
    print(f"  Original: {text}")
    print(f"  Cleaned:  {cleaned}")
    print(f"  Label:    {label_names[label]}")
    print()

# ------ Step 2: Build vocabulary from FST-cleaned texts ------
cleaned_texts = [fst.transduce(text) for text, _ in training_data]
all_words = set()
for text in cleaned_texts:
    all_words.update(text.split())
vocab = sorted(all_words)
word_to_idx = {w: i for i, w in enumerate(vocab)}
print(f"Vocabulary size after FST normalization: {len(vocab)} words")
print(f"(Without FST, abbreviations like 'dr', 'dr.', 'Doctor' would")
print(f" be separate tokens, inflating vocab and hurting generalization)")
print()

# ------ Step 3: Feature extraction (bag-of-words) ------
def text_to_bow(text, word_to_idx):
    bow = np.zeros(len(word_to_idx))
    for word in text.split():
        if word in word_to_idx:
            bow[word_to_idx[word]] += 1.0  # count occurrences
    return bow

# Prepare training arrays
X_train = np.array([text_to_bow(t, word_to_idx) for t in cleaned_texts])
y_train = np.array([label for _, label in training_data])

# ------ Step 4: Train the neural classifier ------
print("=" * 65)
print("STEP 2: Neural Classifier Training (learned, data-driven)")
print("=" * 65)
classifier = SimpleNeuralClassifier(
    vocab_size=len(vocab), hidden_size=16, num_classes=len(label_names)
)

# Training loop
for epoch in range(200):
    epoch_loss = 0.0
    for i in range(len(X_train)):
        probs = classifier.forward(X_train[i])
        loss = classifier.backward(y_train[i], lr=0.05)
        epoch_loss += loss
    if (epoch + 1) % 50 == 0:
        # Compute training accuracy
        correct = sum(
            np.argmax(classifier.forward(X_train[i])) == y_train[i]
            for i in range(len(X_train))
        )
        acc = correct / len(X_train)
        print(f"  Epoch {epoch+1:3d}: loss={epoch_loss/len(X_train):.4f}, "
              f"accuracy={acc:.0%}")

# ------ Step 5: Test the full hybrid pipeline ------
print()
print("=" * 65)
print("STEP 3: Hybrid Pipeline on New Inputs")
print("=" * 65)
test_inputs = [
    "The Dr. needs more info about the patient ASAP!",
    "Govt. announces new policy on St. funding.",
    "Approx. 200 people at the event on Oak St.",
    "Avg. stock returns were est. at 12% this year.",
]

for text in test_inputs:
    # FST: deterministic preprocessing (guaranteed correct)
    cleaned = fst.transduce(text)
    # Feature extraction
    bow = text_to_bow(cleaned, word_to_idx)
    # Neural: learned classification (data-driven)
    probs = classifier.forward(bow)
    predicted_idx = np.argmax(probs)

    print(f"  Input:     {text}")
    print(f"  Cleaned:   {cleaned}")
    print(f"  Predicted: {label_names[predicted_idx]} "
          f"(confidence: {probs[predicted_idx]:.1%})")
    print(f"  Scores:    {', '.join(f'{n}={p:.1%}' for n, p in zip(label_names, probs))}")
    print()

print("KEY INSIGHT: The FST handles normalization perfectly -- no training")
print("data needed, guaranteed correct for all known abbreviations. The")
print("neural classifier handles the ambiguous categorization task, which")
print("would be extremely tedious to hand-code as rules. Together, the")
print("hybrid outperforms either component alone.")</code></pre>
                </div>

                <!-- Code Example 2 -->
                <h3>2. Lattice Generation and Neural Rescoring</h3>
                <p>
                    This example demonstrates the Lattice Rescoring pattern: an FST generates multiple candidate outputs (an n-best list), and a simple neural language model rescores them to select the best one.
                </p>

                <div class="code-block">
                    <div class="code-block-header">
                        <span class="code-block-lang">python</span>
                        <button class="code-block-copy">Copy</button>
                    </div>
                    <pre><code class="language-python">import numpy as np
from collections import defaultdict

# ============================================================
# Part 1: FST Lattice Generator
# ============================================================
# In speech recognition, a WFST (H o C o L o G) generates a
# lattice: a DAG of word sequence hypotheses, each scored by
# the acoustic model + first-pass language model. This class
# simulates that process with explicit transitions and weights
# in the tropical semiring (lower weight = better).

class LatticeFST:
    """
    A WFST lattice generator using the tropical semiring.
    Transitions: (state, input) -> [(next_state, output, weight)]
    Weights are negative log-probabilities: lower = more probable.
    """

    def __init__(self):
        self.transitions = defaultdict(list)
        self.start_state = 'start'
        self.final_states = {}  # state -> final weight

    def add_transition(self, src, dst, output, weight):
        """Add a weighted transition (arc) to the lattice."""
        self.transitions[src].append((dst, output, weight))

    def set_final(self, state, weight=0.0):
        """Mark a state as final with an optional final weight."""
        self.final_states[state] = weight

    def generate_nbest(self, n=10):
        """
        Generate the n-best paths through the lattice using
        depth-first enumeration. In production, you would use
        Yen's k-shortest-paths algorithm for efficiency.

        Returns: list of (hypothesis_string, total_weight) tuples,
                 sorted by weight (best first).
        """
        # Stack-based DFS: (state, output_words, accumulated_weight)
        stack = [(self.start_state, [], 0.0)]
        completed = []

        while stack and len(completed) < n * 5:  # oversample then prune
            state, words, weight = stack.pop()

            if state in self.final_states:
                total = weight + self.final_states[state]
                completed.append((' '.join(words), total))

            for dst, output, arc_weight in self.transitions[state]:
                stack.append((dst, words + [output], weight + arc_weight))

        completed.sort(key=lambda x: x[1])  # tropical: lower is better
        return completed[:n]


# ============================================================
# Part 2: Build a realistic lattice for ambiguous speech
# ============================================================
# Scenario: The audio sounds like "I scream for ice cream" but
# several acoustic confusions create alternative hypotheses.
# This is exactly the situation where lattice rescoring shines.

lattice = LatticeFST()

# --- Hypotheses starting with "I" (acoustic score: good) ---
lattice.add_transition('start', 'heard_I', 'I', 1.2)

# "I scream ..." vs "I stream ..." (scream/stream confusion)
lattice.add_transition('heard_I', 'I_verb', 'scream', 2.1)
lattice.add_transition('heard_I', 'I_verb', 'stream', 2.8)  # worse acoustic

# "... for ..."
lattice.add_transition('I_verb', 'for', 'for', 0.5)

# --- Hypotheses starting with "ice" (acoustic score: slightly worse) ---
lattice.add_transition('start', 'heard_ice', 'ice', 1.5)
lattice.add_transition('heard_ice', 'ice_cream', 'cream', 1.0)
lattice.add_transition('ice_cream', 'for', 'for', 0.5)

# --- Second half: "... ice cream" vs "... I scream" ---
lattice.add_transition('for', 'final_ice', 'ice', 1.0)
lattice.add_transition('final_ice', 'end', 'cream', 0.8)

lattice.add_transition('for', 'final_I', 'I', 1.8)
lattice.add_transition('final_I', 'end', 'scream', 2.3)

lattice.set_final('end', weight=0.0)

# Generate n-best list from the FST lattice
nbest = lattice.generate_nbest(n=10)

print("PHASE 1: FST Lattice -- N-Best Hypotheses (acoustic + LM1 scores)")
print("=" * 70)
print(f"{'Rank':<6} {'FST Score':<12} {'Hypothesis'}")
print("-" * 70)
for i, (hyp, score) in enumerate(nbest):
    print(f"  {i+1:<4} {score:<12.3f} {hyp}")
print()
print(f"  Total hypotheses in lattice: {len(nbest)}")
print(f"  Best FST hypothesis: \"{nbest[0][0]}\" (score: {nbest[0][1]:.3f})")
print()


# ============================================================
# Part 3: Neural Language Model for Rescoring
# ============================================================
# In practice this would be a Transformer LM or LSTM LM.
# Here we use a hand-crafted bigram model to illustrate the
# rescoring mechanics. The key idea: the neural LM captures
# linguistic plausibility that the acoustic model alone misses.

class BigramLM:
    """
    Bigram language model with log-probability scoring.
    P(w1 w2 ... wn) = P(w1|<s>) * P(w2|w1) * ... * P(</s>|wn)
    Score = sum of log-probabilities (higher = more probable).
    """

    def __init__(self):
        # Hand-crafted bigram log-probs reflecting English fluency.
        # "I scream for ice cream" is a known phrase -> high LM score.
        self.bigrams = {
            ('<s>', 'I'):       -1.0,   # common sentence start
            ('<s>', 'ice'):     -2.0,   # less common start
            ('I', 'scream'):    -2.5,   # uncommon but valid
            ('I', 'stream'):    -4.0,   # rare as a sentence
            ('ice', 'cream'):   -0.5,   # very common collocation
            ('ice', 'scream'):  -4.5,   # very unlikely
            ('scream', 'for'):  -1.5,   # "scream for" is natural
            ('stream', 'for'):  -2.0,   # less natural
            ('cream', 'for'):   -1.8,   # "cream for" is ok
            ('for', 'ice'):     -1.0,   # "for ice" is natural
            ('for', 'I'):       -2.5,   # "for I" is ungrammatical
            ('cream', '</s>'):  -0.3,   # good sentence ending
            ('scream', '</s>'): -0.8,   # ok sentence ending
        }
        self.default = -5.0  # heavy penalty for unknown bigrams

    def score(self, sentence):
        """Compute log-probability of a sentence under the bigram model."""
        words = sentence.split()
        logprob = 0.0
        prev = '<s>'
        for w in words:
            logprob += self.bigrams.get((prev, w), self.default)
            prev = w
        logprob += self.bigrams.get((prev, '</s>'), self.default)
        return logprob

    def score_breakdown(self, sentence):
        """Show per-bigram scores for analysis."""
        words = sentence.split()
        breakdown = []
        prev = '<s>'
        for w in words:
            score = self.bigrams.get((prev, w), self.default)
            breakdown.append(((prev, w), score))
            prev = w
        score = self.bigrams.get((prev, '</s>'), self.default)
        breakdown.append(((prev, '</s>'), score))
        return breakdown


# ============================================================
# Part 4: Lattice Rescoring with Interpolation
# ============================================================
# Combined score: alpha * FST_score + (1-alpha) * (-LM_score)
# (We negate LM score because FST uses "lower is better" but
# LM uses "higher is better".)

lm = BigramLM()
alpha = 0.6  # weight for FST score (tuned on dev set)

print("PHASE 2: Neural LM Rescoring")
print("=" * 70)
print(f"Interpolation: score = {alpha} * FST + {1-alpha} * (-LM)")
print()

rescored = []
for hyp, fst_score in nbest:
    lm_score = lm.score(hyp)
    # Combine: both in "lower is better" convention
    combined = alpha * fst_score + (1 - alpha) * (-lm_score)
    rescored.append((hyp, fst_score, lm_score, combined))

# Sort by combined score (lower is better)
rescored.sort(key=lambda x: x[3])

print(f"{'Rank':<6} {'FST':>7} {'LM':>7} {'Combined':>9}  {'Hypothesis'}")
print("-" * 70)
for i, (hyp, fst_s, lm_s, comb) in enumerate(rescored):
    marker = " *** BEST ***" if i == 0 else ""
    print(f"  {i+1:<4} {fst_s:>7.3f} {lm_s:>7.3f} {comb:>9.3f}  {hyp}{marker}")

# Show detailed rescoring for the top hypothesis
print()
print("DETAILED ANALYSIS: Why the best hypothesis wins")
print("-" * 70)
best_hyp = rescored[0][0]
breakdown = lm.score_breakdown(best_hyp)
print(f"  Hypothesis: \"{best_hyp}\"")
print(f"  LM score breakdown:")
for (w1, w2), score in breakdown:
    print(f"    P({w2} | {w1}) = exp({score:.1f}) = {np.exp(score):.4f}")
print(f"  Total LM log-prob: {sum(s for _, s in breakdown):.3f}")

# Show what changed between FST ranking and combined ranking
print()
print("RANKING CHANGES after rescoring:")
print("-" * 70)
fst_ranking = {hyp: i+1 for i, (hyp, _) in enumerate(nbest)}
for i, (hyp, _, _, _) in enumerate(rescored):
    old_rank = fst_ranking.get(hyp, '?')
    new_rank = i + 1
    change = old_rank - new_rank if isinstance(old_rank, int) else 0
    arrow = f"(+{change})" if change > 0 else f"({change})" if change < 0 else "(=)"
    print(f"  #{new_rank}: \"{hyp}\"  was #{old_rank} {arrow}")

print()
print("KEY INSIGHT: The FST lattice efficiently constrains the search")
print("space to acoustically plausible hypotheses. The neural LM then")
print("reranks based on linguistic fluency. This two-pass approach is")
print("faster than running the neural LM over all possible word sequences.")</code></pre>
                </div>

                <!-- Code Example 3 -->
                <h3>3. Differentiable FST (Toy Implementation)</h3>
                <p>
                    This example shows how FST-like operations can be implemented with matrices that support gradient computation. The "states" become probability distributions, "transitions" become matrix multiplications, and the whole system is differentiable &mdash; enabling training with backpropagation.
                </p>

                <div class="code-block">
                    <div class="code-block-header">
                        <span class="code-block-lang">python</span>
                        <button class="code-block-copy">Copy</button>
                    </div>
                    <pre><code class="language-python">import numpy as np

# ============================================================
# Differentiable FST: Making FST operations gradient-friendly
# ============================================================
#
# CORE IDEA: Replace every discrete FST operation with a
# continuous, differentiable analogue:
#
#   Classical FST              Differentiable FST
#         
#   State: one-hot vector      State: probability distribution
#   Transition: lookup table   Transition: matrix multiplication
#   Output: one symbol         Output: distribution over symbols
#   argmin over paths          softmin (log-sum-exp)
#
# The result: an FST that is also a neural computation graph.
# Gradients flow through it, enabling weight learning via SGD.

class DifferentiableFST:
    """
    A differentiable FST where states, transitions, and outputs
    are represented as continuous matrices with learnable parameters.

    Architecture:
      - State vector s_t  R^|Q| (probability distribution over states)
      - For each input symbol a  :
          * Transition matrix T[a]  R^{|Q| x |Q|}  (row-stochastic)
          * Output matrix    O[a]  R^{|Q| x ||}   (row-stochastic)
      - Forward step: s_{t+1} = s_t @ T[a_t]
      - Output:       o_t    = s_{t+1} @ O[a_t]

    The raw parameters are logits; softmax converts them to
    proper probability matrices.
    """

    def __init__(self, num_states, input_vocab, output_vocab):
        self.num_states = num_states
        self.input_vocab = input_vocab
        self.output_vocab = output_vocab
        self.input_to_idx = {s: i for i, s in enumerate(input_vocab)}
        self.output_to_idx = {s: i for i, s in enumerate(output_vocab)}

        np.random.seed(0)

        # LEARNABLE PARAMETERS (stored as logits, not probabilities)
        # Transition logits: one |Q|x|Q| matrix per input symbol
        # These are the "soft" transition tables of the FST.
        self.T_logits = {
            a: np.random.randn(num_states, num_states) * 0.5
            for a in input_vocab
        }

        # Output logits: one |Q|x|| matrix per input symbol
        # Row i gives the output distribution when in state i.
        self.O_logits = {
            a: np.random.randn(num_states, len(output_vocab)) * 0.5
            for a in input_vocab
        }

        # Final state logits: which states are accepting
        self.final_logits = np.random.randn(num_states) * 0.5

    @staticmethod
    def softmax(x, axis=-1):
        """Numerically stable softmax: logits -> probabilities."""
        e = np.exp(x - np.max(x, axis=axis, keepdims=True))
        return e / e.sum(axis=axis, keepdims=True)

    def get_transition_matrix(self, symbol):
        """
        Get the row-stochastic transition matrix for input symbol.
        T[i,j] = P(next_state=j | current_state=i, input=symbol)
        """
        return self.softmax(self.T_logits[symbol], axis=1)

    def get_output_matrix(self, symbol):
        """
        Get the row-stochastic output matrix for input symbol.
        O[i,k] = P(output=k | state=i, input=symbol)
        """
        return self.softmax(self.O_logits[symbol], axis=1)

    def forward(self, input_sequence):
        """
        Process input sequence through the differentiable FST.

        At each step t:
          1. Soft transition: s_{t+1} = s_t @ T[a_t]
             (matrix-vector product replaces discrete state lookup)
          2. Soft output:     o_t = s_{t+1} @ O[a_t]
             (weighted mixture of output distributions)

        Returns: (state_history, output_history, final_score)
        """
        # Start state: deterministically in state 0 (one-hot)
        state = np.zeros(self.num_states)
        state[0] = 1.0

        state_history = [state.copy()]
        output_history = []

        for symbol in input_sequence:
            # --- Soft transition (replaces discrete delta function) ---
            T = self.get_transition_matrix(symbol)
            state = state @ T  # s_{t+1} = s_t * T[a_t]

            # --- Soft output emission ---
            O = self.get_output_matrix(symbol)
            output_dist = state @ O  # mixture of per-state outputs

            state_history.append(state.copy())
            output_history.append(output_dist.copy())

        # Final score: sigmoid of logits, weighted by final state dist
        final_probs = 1.0 / (1.0 + np.exp(-self.final_logits))
        final_score = np.dot(state, final_probs)

        return state_history, output_history, final_score

    def decode(self, output_dist):
        """Decode: pick the most probable output symbol."""
        return self.output_vocab[np.argmax(output_dist)]

    def compute_loss(self, input_seq, target_seq):
        """
        Cross-entropy loss between predicted and target output.
        loss = -(1/T) * sum_t log P(target_t | input_1..t)
        This is differentiable w.r.t. all parameters.
        """
        _, output_history, _ = self.forward(input_seq)
        loss = 0.0
        for output_dist, target in zip(output_history, target_seq):
            target_idx = self.output_to_idx[target]
            loss -= np.log(output_dist[target_idx] + 1e-10)
        return loss / len(target_seq)

    def numerical_gradient(self, input_seq, target_seq, param_dict, key,
                           epsilon=1e-5):
        """
        Compute gradient via finite differences (for verification).
        In production, use PyTorch autograd or JAX autodiff.
        dL/dW  (L(W+) - L(W-)) / (2)
        """
        params = param_dict[key]
        grad = np.zeros_like(params)
        for idx in np.ndindex(params.shape):
            orig = params[idx]
            params[idx] = orig + epsilon
            loss_plus = self.compute_loss(input_seq, target_seq)
            params[idx] = orig - epsilon
            loss_minus = self.compute_loss(input_seq, target_seq)
            grad[idx] = (loss_plus - loss_minus) / (2 * epsilon)
            params[idx] = orig
        return grad


# ============================================================
# TRAINING: Learn a simple character cipher via gradient descent
# ============================================================
# Task: learn the mapping a->x, b->y from examples only.
# The FST structure (states, transitions) is fixed; only the
# weights (transition probabilities, output probabilities) are
# learned. This is analogous to learning FST arc weights in
# the log semiring via the forward-backward algorithm.

input_vocab = ['a', 'b']
output_vocab = ['x', 'y']
NUM_STATES = 4  # more states than needed -- model must learn to ignore extras

dfst = DifferentiableFST(NUM_STATES, input_vocab, output_vocab)

# Training data: input-output pairs
train_data = [
    (list('ab'),   list('xy')),
    (list('ba'),   list('yx')),
    (list('aa'),   list('xx')),
    (list('bb'),   list('yy')),
    (list('aba'),  list('xyx')),
    (list('bab'),  list('yxy')),
    (list('aabb'), list('xxyy')),
    (list('abba'), list('xyyx')),
]

print("DIFFERENTIABLE FST: Learning a Character Cipher")
print("=" * 65)
print(f"Task:         a -> x, b -> y")
print(f"States:       {NUM_STATES} (model must learn which to use)")
print(f"Train pairs:  {len(train_data)}")
print()

# --- Before training: random predictions ---
print("BEFORE TRAINING (random weights):")
for inp, tgt in train_data[:4]:
    _, outs, _ = dfst.forward(inp)
    pred = [dfst.decode(o) for o in outs]
    loss = dfst.compute_loss(inp, tgt)
    match = "correct" if pred == tgt else "WRONG"
    print(f"  {''.join(inp)} -> {''.join(pred)}  "
          f"(target: {''.join(tgt)})  loss={loss:.4f}  [{match}]")
print()

# --- Training loop with SGD ---
lr = 0.5
print(f"Training with SGD, lr={lr}, 150 epochs...")
print("-" * 65)

for epoch in range(150):
    total_loss = 0.0
    np.random.shuffle(train_data)  # shuffle each epoch

    for inp, tgt in train_data:
        # Compute and apply gradients for transition weights
        for sym in input_vocab:
            grad_T = dfst.numerical_gradient(inp, tgt, dfst.T_logits, sym)
            dfst.T_logits[sym] -= lr * grad_T

            grad_O = dfst.numerical_gradient(inp, tgt, dfst.O_logits, sym)
            dfst.O_logits[sym] -= lr * grad_O

        total_loss += dfst.compute_loss(inp, tgt)

    avg_loss = total_loss / len(train_data)

    # Report every 30 epochs
    if (epoch + 1) % 30 == 0:
        # Compute accuracy
        correct = 0
        for inp, tgt in train_data:
            _, outs, _ = dfst.forward(inp)
            pred = [dfst.decode(o) for o in outs]
            if pred == tgt:
                correct += 1
        acc = correct / len(train_data)
        print(f"  Epoch {epoch+1:3d}: loss={avg_loss:.4f}  "
              f"accuracy={acc:.0%}  ({correct}/{len(train_data)})")

# --- After training ---
print()
print("AFTER TRAINING:")
for inp, tgt in train_data[:4]:
    _, outs, _ = dfst.forward(inp)
    pred = [dfst.decode(o) for o in outs]
    loss = dfst.compute_loss(inp, tgt)
    match = "correct" if pred == tgt else "WRONG"
    print(f"  {''.join(inp)} -> {''.join(pred)}  "
          f"(target: {''.join(tgt)})  loss={loss:.4f}  [{match}]")

# --- Generalization to unseen inputs ---
print()
print("GENERALIZATION (unseen inputs):")
test_inputs = [list('aab'), list('bba'), list('abab'), list('bbaa')]
for inp in test_inputs:
    _, outs, _ = dfst.forward(inp)
    pred = [dfst.decode(o) for o in outs]
    expected = ['x' if c == 'a' else 'y' for c in inp]
    match = "correct" if pred == expected else "WRONG"
    print(f"  {''.join(inp)} -> {''.join(pred)}  "
          f"(expected: {''.join(expected)})  [{match}]")

# --- Inspect learned transition matrices ---
print()
print("LEARNED TRANSITION MATRICES:")
print("(Row i = distribution over next states given current state i)")
for sym in input_vocab:
    T = dfst.get_transition_matrix(sym)
    print(f"\n  T['{sym}'] (transition probs on input '{sym}'):")
    for i in range(NUM_STATES):
        row = '  '.join(f'{T[i,j]:.3f}' for j in range(NUM_STATES))
        print(f"    state {i}: [{row}]")

print()
print("LEARNED OUTPUT MATRICES:")
for sym in input_vocab:
    O = dfst.get_output_matrix(sym)
    print(f"\n  O['{sym}'] (output probs per state on input '{sym}'):")
    for i in range(NUM_STATES):
        row = '  '.join(f'{output_vocab[j]}={O[i,j]:.3f}'
                        for j in range(len(output_vocab)))
        print(f"    state {i}: [{row}]")

print()
print("KEY INSIGHT: The FST structure (states, transitions, output")
print("matrices) provides an inductive bias -- the model 'knows' that")
print("the task is a sequential transduction. But the actual mapping")
print("is learned from data via gradient descent. This is the essence")
print("of differentiable FSTs: symbolic structure + neural learning.")</code></pre>
                </div>

            </article>

            <!-- ==================== EXERCISES TAB ==================== -->
            <article class="article-content" id="exercises" style="display: none;">
                <h2>Exercises</h2>
                <p>Test your understanding of neural-symbolic hybrids and the decision framework for choosing between FSTs, neural networks, and hybrid systems. Exercises range from conceptual reasoning to system design and research analysis.</p>

                <div class="exercise-list">

                    <!-- Easy -->
                    <h3 style="margin-top: 1rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">Easy</h3>

                    <!-- Exercise 1 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">1. Choose the Right Approach</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>For each scenario below, decide whether you would use an FST, a neural network, or a hybrid system. Justify your choice in one or two sentences.</p>
                            <ol type="a">
                                <li>Converting US phone numbers from "(555) 123-4567" to "+1-555-123-4567".</li>
                                <li>Translating English sentences to French.</li>
                                <li>A voice assistant that accepts commands from a fixed vocabulary of 50 commands in a noisy environment.</li>
                                <li>Detecting sarcasm in social media posts.</li>
                                <li>Normalizing chemical formulas from informal notation to IUPAC standard.</li>
                            </ol>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>(a) FST.</strong> Phone number reformatting is a deterministic, rule-based transformation with a known, finite set of input patterns. No training data is needed, and the rules are simple and fixed.</p>
                                <p><strong>(b) Neural network.</strong> Machine translation requires understanding context, handling ambiguity, and generating fluent output in the target language. The transformation rules are far too complex to articulate manually, and the task requires long-range contextual dependencies.</p>
                                <p><strong>(c) Hybrid.</strong> A neural acoustic model handles the noisy audio perception, while an FST constrains the output to one of the 50 valid commands. The FST guarantees the system never produces an invalid command, while the neural model handles acoustic variability.</p>
                                <p><strong>(d) Neural network.</strong> Sarcasm detection depends on subtle contextual cues, tone, world knowledge, and pragmatic inference. There are no simple rules for detecting sarcasm, and the patterns are learned best from large labeled datasets.</p>
                                <p><strong>(e) FST or Hybrid.</strong> If IUPAC normalization rules are well-documented and cover the input space, a pure FST works. If informal notation is highly variable and ambiguous, a hybrid (neural to parse ambiguous notation + FST to enforce IUPAC formatting) may be needed.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 2 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">2. Three Advantages of FSTs over Neural Networks</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>List three concrete advantages that FSTs have over neural networks. For each advantage, give a specific real-world scenario where this advantage is decisive.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>1. Interpretability and auditability.</strong> Every decision an FST makes can be traced to a specific state and transition. <em>Scenario:</em> A medical text normalization system that converts drug dosage abbreviations (e.g., "bid" to "twice daily"). In a healthcare setting, regulators may require that every transformation is explainable and verifiable. An FST provides a complete audit trail; a neural model does not.</p>
                                <p><strong>2. Zero-data operation.</strong> FSTs encode expert knowledge directly and require no training data. <em>Scenario:</em> Building a text normalization system for a newly documented endangered language. There may be only a few hundred written examples total &mdash; far too little to train any neural model. A linguist can write FST rules from grammatical descriptions to perform morphological analysis.</p>
                                <p><strong>3. Guaranteed behavior and no hallucination.</strong> An FST will never produce output outside its defined rule set. <em>Scenario:</em> A financial system that formats currency amounts for regulatory reports. The system must guarantee that "$1,234.56" is always formatted as "USD 1234.56" and never produces unexpected variations. An FST guarantees this by construction; a neural model might occasionally produce malformed output.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 3 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">3. Three Advantages of Neural Networks over FSTs</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>List three concrete advantages that neural networks have over FSTs. For each advantage, give a specific real-world scenario where this advantage is decisive.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>1. Learning from data without explicit rules.</strong> Neural networks discover patterns automatically from labeled examples. <em>Scenario:</em> Sentiment analysis of product reviews. The relationship between words and sentiment is nuanced, context-dependent, and constantly evolving with new slang. No set of hand-crafted rules could capture this; a neural model trained on thousands of labeled reviews learns these patterns implicitly.</p>
                                <p><strong>2. Handling long-range dependencies.</strong> Neural networks (especially Transformers) can model dependencies across hundreds or thousands of tokens. <em>Scenario:</em> Document summarization. The model must understand relationships between sentences in different paragraphs, track which entities are most important across the entire document, and generate a coherent summary. An FST's fixed-state memory cannot represent these long-range relationships.</p>
                                <p><strong>3. Graceful degradation with noisy input.</strong> Neural networks produce reasonable outputs even for inputs that differ from training examples. <em>Scenario:</em> OCR post-correction. Scanned documents contain noise, artifacts, and unusual fonts that produce garbled text. A neural sequence-to-sequence model trained on (noisy, clean) text pairs can correct errors it has never seen before by generalizing from similar patterns. An FST would need an explicit rule for every possible error, which is infeasible.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Medium -->
                    <h3 style="margin-top: 2rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">Medium</h3>

                    <!-- Exercise 4 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">4. Design a Hybrid System for Named Entity Recognition</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Design a hybrid FST + neural system for Named Entity Recognition (NER) in financial news articles. Your system must identify person names, company names, monetary amounts, and dates. Specify:</p>
                            <ol type="a">
                                <li>Which entity types would you handle with FSTs and which with neural networks? Why?</li>
                                <li>Draw the architecture diagram (describe it in words).</li>
                                <li>How do the FST and neural components interact?</li>
                                <li>What happens when the components disagree?</li>
                            </ol>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>(a) Component assignment:</strong></p>
                                <ul>
                                    <li><strong>FST:</strong> Monetary amounts (e.g., "$1.2 billion", "EUR 500M", "42 cents") and dates (e.g., "January 15, 2026", "Q3 2025", "2024-01-01"). These have regular, well-defined patterns that can be captured by finite-state rules.</li>
                                    <li><strong>Neural:</strong> Person names and company names. These are open-class categories with no fixed pattern &mdash; new names appear constantly, and disambiguation requires context ("Apple" the company vs "apple" the fruit).</li>
                                </ul>
                                <p><strong>(b) Architecture:</strong> Raw text &rarr; Tokenizer &rarr; [FST: pattern-match monetary amounts and dates] &rarr; [Neural: BiLSTM-CRF or Transformer for person/company names, with FST-detected entities masked or provided as features] &rarr; Merge &rarr; Final NER output.</p>
                                <p><strong>(c) Interaction:</strong> The FST runs first and identifies high-confidence monetary and date entities. These are passed to the neural model as additional features (e.g., special token embeddings for FST-detected spans). The neural model can use these as context clues: if "Apple" appears near a monetary amount detected by the FST, it is more likely to be a company name.</p>
                                <p><strong>(d) Disagreements:</strong> When the FST and neural model identify overlapping spans with different entity types, use the following priority: FST-detected monetary amounts and dates take precedence (since the FST is deterministic and highly precise for these patterns). For spans where only the neural model has a prediction, use the neural prediction with a confidence threshold. Log all disagreements for human review and iterative system improvement.</p>
                                <p><strong>Code skeleton for the FST gazetteer + neural classifier pipeline:</strong></p>
<pre><code class="language-python"># --- FST Gazetteer: deterministic pattern matching ---
class EntityFST:
    """FST-based entity recognizer for monetary amounts and dates."""

    # Money patterns: $X, $X.XX, X million, EUR X, etc.
    money_patterns = [
        r'\$[\d,]+(\.\d{2})?',             # $1,234.56
        r'\$[\d.]+ (billion|million|k)',    # $1.2 billion
        r'(EUR|GBP|USD|JPY)\s*[\d,.]+',    # EUR 500
    ]

    # Date patterns: Jan 15 2026, 2024-01-01, Q3 2025, etc.
    date_patterns = [
        r'(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\w*\s+\d{1,2},?\s+\d{4}',
        r'\d{4}-\d{2}-\d{2}',
        r'Q[1-4]\s+\d{4}',
    ]

    def extract(self, text):
        """Return list of (start, end, entity_type) spans."""
        spans = []
        for pattern in self.money_patterns:
            for match in re.finditer(pattern, text):
                spans.append((match.start(), match.end(), 'MONEY'))
        for pattern in self.date_patterns:
            for match in re.finditer(pattern, text):
                spans.append((match.start(), match.end(), 'DATE'))
        return spans

# --- Neural NER: learned from data ---
class NeuralNER:
    """BiLSTM-CRF or Transformer-based NER for PERSON and ORG."""
    def predict(self, tokens, fst_features):
        # fst_features[i] = entity type if FST detected, else None
        # The neural model uses FST detections as extra input features
        # ... (standard BiLSTM-CRF forward pass) ...
        return predicted_labels

# --- Hybrid Pipeline ---
def hybrid_ner(text):
    tokens = tokenize(text)
    fst = EntityFST()
    neural = NeuralNER()

    # Step 1: FST extracts MONEY and DATE (high precision)
    fst_spans = fst.extract(text)

    # Step 2: Pass FST detections as features to neural model
    fst_features = mark_fst_spans(tokens, fst_spans)
    neural_labels = neural.predict(tokens, fst_features)

    # Step 3: Merge -- FST takes priority for MONEY/DATE
    final_labels = merge(fst_spans, neural_labels, priority='fst')
    return final_labels</code></pre>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 5 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">5. Data Requirements: FST vs Neural Spell Correction</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Estimate the data and engineering requirements for building a spell-correction system using (a) a pure FST approach and (b) a pure neural approach. Consider:</p>
                            <ul>
                                <li>Training data needed</li>
                                <li>Expert knowledge needed</li>
                                <li>Development time</li>
                                <li>Coverage of edge cases</li>
                                <li>Adaptation to new domains (e.g., medical terminology)</li>
                            </ul>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>(a) FST approach:</strong></p>
                                <ul>
                                    <li><strong>Training data:</strong> Essentially none &mdash; you need a dictionary (word list) and a set of edit-distance rules, not labeled correction pairs.</li>
                                    <li><strong>Expert knowledge:</strong> High. You need to define the error model (which edits are common: insertion, deletion, substitution, transposition), craft phonetic similarity rules (e.g., Soundex), and handle context-dependent corrections.</li>
                                    <li><strong>Development time:</strong> Moderate for a basic system (days to weeks using libraries like Pynini). High for comprehensive coverage (months of rule refinement).</li>
                                    <li><strong>Edge cases:</strong> Each edge case requires a specific rule. Coverage is proportional to engineering effort.</li>
                                    <li><strong>Domain adaptation:</strong> Add domain-specific dictionary entries and possibly new edit rules. Relatively straightforward but manual.</li>
                                </ul>
                                <p><strong>(b) Neural approach:</strong></p>
                                <ul>
                                    <li><strong>Training data:</strong> Large &mdash; typically millions of (misspelled, correct) pairs. Can be synthetically generated by applying random edits to correct text.</li>
                                    <li><strong>Expert knowledge:</strong> Low for the core model (standard seq2seq architecture). Some expertise needed for data generation and evaluation.</li>
                                    <li><strong>Development time:</strong> Fast to prototype (hours with modern frameworks). Longer to achieve high quality (weeks of training, hyperparameter tuning, evaluation).</li>
                                    <li><strong>Edge cases:</strong> Handled implicitly through data coverage. The model generalizes to errors it has not seen, but may hallucinate corrections for unusual words.</li>
                                    <li><strong>Domain adaptation:</strong> Fine-tune on domain-specific text. Requires domain data but no rule engineering.</li>
                                </ul>
                                <p><strong>Conclusion:</strong> The FST approach has lower data requirements but higher expert-knowledge requirements. The neural approach scales better with data but may produce unexpected errors. A practical system might use an FST for known patterns (edit distance up to 2 within a dictionary) and a neural model for handling unknown words and context-dependent corrections.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 6 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">6. Critique: "FSTs Are Obsolete"</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>A colleague claims: "Now that we have large language models, FSTs are completely obsolete. There is no reason to use them in any modern system." Write a thoughtful critique of this claim, addressing both the kernel of truth in it and the ways in which it is wrong. Provide at least three concrete counterexamples.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>The kernel of truth:</strong> Large language models have indeed replaced FSTs in many tasks where FSTs were previously dominant. Machine translation, speech recognition, and text generation are now overwhelmingly neural. For open-ended, creative, or highly ambiguous tasks, LLMs are categorically superior to any rule-based system. The scope of what FSTs are "best at" has genuinely narrowed.</p>
                                <p><strong>Why the claim is wrong:</strong></p>
                                <p><strong>Counterexample 1: Constrained decoding.</strong> When a neural model must produce output that conforms to a specific grammar (e.g., generating valid SQL queries, formatting structured data, or producing text that matches a template), FSTs are used as constraints during decoding. The LLM generates candidates, and the FST ensures validity. This is not a niche use case &mdash; it is used in production systems at major tech companies.</p>
                                <p><strong>Counterexample 2: Edge/embedded deployment.</strong> An LLM requires gigabytes of memory and significant compute. A text normalization FST for a smart thermostat's display (converting "72F" to "72 degrees Fahrenheit") runs in kilobytes with microsecond latency. Using an LLM for this task would be absurdly wasteful and often infeasible on the target hardware.</p>
                                <p><strong>Counterexample 3: Safety-critical systems.</strong> In medical device software, aviation systems, or financial transaction processing, every transformation must be provably correct and fully auditable. FSTs provide formal guarantees that no neural model can match. Regulators may require this level of verifiability.</p>
                                <p><strong>Counterexample 4 (bonus): Preprocessing for LLMs.</strong> Ironically, many LLM pipelines use FST-like preprocessing (tokenization, text normalization, format conversion) before the text even reaches the LLM. The BPE tokenizer used by most LLMs is itself based on finite-state operations.</p>
                                <p><strong>The nuanced position:</strong> FSTs are not obsolete; their role has shifted from being the primary modeling tool to being a component in larger systems &mdash; handling preprocessing, constrained decoding, and safety enforcement. Claiming they are obsolete is like claiming screwdrivers are obsolete because we have power drills. Different tools for different jobs.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 7 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">7. FST for a Safety-Critical Application</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Design an FST-based safety layer for a neural text generation system used in a children's educational application. The FST should filter the neural model's output to ensure it never contains:</p>
                            <ul>
                                <li>Profanity or inappropriate language</li>
                                <li>URLs or email addresses</li>
                                <li>Phone numbers or other personal information patterns</li>
                            </ul>
                            <p>Describe the FST's input alphabet, output alphabet, states, and transition logic. Discuss the tradeoffs of this approach.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Design:</strong></p>
                                <ul>
                                    <li><strong>Input alphabet:</strong> Characters (or tokens) from the neural model's output.</li>
                                    <li><strong>Output alphabet:</strong> Same characters, plus a replacement token (e.g., "[filtered]").</li>
                                    <li><strong>Architecture:</strong> Compose three FSTs: $T_{profanity} \circ T_{url} \circ T_{pii}$.</li>
                                </ul>
                                <p><strong>$T_{profanity}$:</strong> An Aho-Corasick-style multi-pattern matcher compiled as an FST. States represent partial matches of profanity words. When a full match is found, output "[filtered]" instead of the matched word. Non-matching characters pass through unchanged. Must handle common evasion patterns (letter substitution, spacing tricks).</p>
                                <p><strong>$T_{url}$:</strong> Matches patterns like "http://", "https://", "www.", "@" followed by domain-like patterns. On match, replace with "[link removed]". States: initial, after "h", after "ht", after "htt", after "http", etc.</p>
                                <p><strong>$T_{pii}$:</strong> Matches phone number patterns (sequences of digits with separators), SSN-like patterns (NNN-NN-NNNN), email patterns. On match, replace with "[info removed]".</p>
                                <p><strong>Tradeoffs:</strong></p>
                                <ul>
                                    <li><strong>Pros:</strong> Deterministic, fast, guaranteed to catch all patterns in the rule set. No false negatives for known patterns. Fully auditable.</li>
                                    <li><strong>Cons:</strong> Cannot catch novel inappropriate content not in the rule set. May produce false positives (e.g., "Scunthorpe" contains a profanity substring). Requires ongoing maintenance as new inappropriate patterns emerge. Does not understand context ("the word 'kill' in 'kill the process'" is acceptable).</li>
                                </ul>
                                <p><strong>Recommended approach:</strong> Use the FST as a hard safety layer (guaranteed minimum protection) and a neural classifier as a soft layer (context-aware filtering) in sequence. The FST catches known patterns with zero latency; the neural classifier handles novel and context-dependent cases.</p>
                                <p><strong>Bonus &mdash; Medical safety example (drug interaction checking):</strong> An FST-based safety layer for a prescription system could enforce known drug interaction rules as a hard constraint. For example:</p>
<pre><code class="language-python"># FST rules for dangerous drug interactions
# Each rule: if (drug_A, drug_B) co-occur in prescription -> BLOCK
interaction_rules = {
    # (Drug A, Drug B): severity, reason
    ("warfarin", "aspirin"):     ("CRITICAL", "increased bleeding risk"),
    ("metformin", "alcohol"):    ("WARNING",  "risk of lactic acidosis"),
    ("ssri", "maoi"):            ("CRITICAL", "serotonin syndrome risk"),
    ("ace_inhibitor", "potassium"): ("WARNING", "hyperkalemia risk"),
    ("statin", "grapefruit"):    ("CAUTION",  "increased statin levels"),
}

# FST states: IDLE -> DRUG_A_SEEN -> CHECK_INTERACTION
# Transition on seeing drug_A: move to DRUG_A_SEEN
# Transition on seeing drug_B while in DRUG_A_SEEN:
#   If (A,B) in interaction_rules -> emit BLOCK signal
#   Else -> emit PASS, return to IDLE
# This runs in O(n) time and guarantees no known interaction
# is ever missed, regardless of what the neural model suggests.</code></pre>
                            </div>
                        </div>
                    </div>

                    <!-- Hard -->
                    <h3 style="margin-top: 2rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">Hard</h3>

                    <!-- Exercise 8 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">8. Design a Neural-FST Architecture for Low-Resource MT</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>You are tasked with building a machine translation system for a low-resource language pair (e.g., English to a language with only 50,000 parallel sentences available). Design a neural-FST hybrid architecture that leverages both the available data and linguistic knowledge. Address:</p>
                            <ol type="a">
                                <li>What linguistic knowledge can be encoded as FSTs? (Think morphology, transliteration, number/date conversion.)</li>
                                <li>How does the neural component interact with the FST components?</li>
                                <li>How would you train this system end-to-end?</li>
                                <li>What happens at inference time?</li>
                                <li>How does this compare to a pure neural baseline with the same data?</li>
                            </ol>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>(a) FST-encodable knowledge:</strong></p>
                                <ul>
                                    <li><strong>Morphological analysis/generation:</strong> An FST that analyzes source words into stems + morphological features (e.g., "running" &rarr; "run" + VERB + PRESENT_PARTICIPLE) and generates target words from stems + features. This is especially valuable for morphologically rich target languages.</li>
                                    <li><strong>Transliteration:</strong> An FST that maps named entities between scripts (e.g., Latin to Cyrillic, or to a local script). Names often follow predictable phonetic mapping rules.</li>
                                    <li><strong>Number and date conversion:</strong> Deterministic mapping of numerical expressions between languages ("January 15" &rarr; target equivalent).</li>
                                    <li><strong>Known vocabulary mapping:</strong> A lexicon FST for unambiguous one-to-one word translations (technical terms, function words).</li>
                                </ul>
                                <p><strong>(b) Architecture:</strong></p>
                                <p>Source text &rarr; [FST: morphological analysis, number/date normalization] &rarr; Normalized source &rarr; [Neural: small Transformer encoder-decoder] &rarr; Normalized target &rarr; [FST: morphological generation, transliteration, number/date denormalization] &rarr; Final target text.</p>
                                <p>The FSTs reduce the vocabulary size the neural model must handle (by factoring out morphology), handle deterministic patterns (numbers, dates, names), and ensure morphological correctness in the output.</p>
                                <p><strong>(c) Training:</strong> Pre-compose the FSTs and create a training pipeline: apply source FSTs to the source side of parallel data, apply target FSTs inversely to the target side, then train the neural model on the normalized parallel corpus. For end-to-end training with differentiable FSTs, use frameworks like k2 to backpropagate through the FST operations.</p>
                                <p><strong>(d) Inference:</strong> Source &rarr; source FSTs &rarr; neural model (beam search) &rarr; target FSTs &rarr; output. The FSTs run in $O(n)$ time and add negligible latency. The neural model's beam search is the bottleneck.</p>
                                <p><strong>(e) Comparison to pure neural:</strong> With only 50K parallel sentences, a pure neural model will struggle with rare words, morphological variation, and number/date expressions. The hybrid system effectively increases the training data by factoring out deterministic patterns that the neural model would otherwise have to learn from examples. Expected improvement: 3-8 BLEU points on morphologically rich language pairs, with the largest gains on rare and out-of-vocabulary words.</p>
                                <p><strong>Detailed Architecture Diagram (text form):</strong></p>
<pre><code>
                    SOURCE LANGUAGE SIDE                      
                                                             
  Raw source text                                            
                                                            
                                                            
                                     
    FST: Morphological    "running"  "run" + VERB+PROG   
         Analysis         "cats"     "cat" + NOUN+PL     
                                     
                                                            
                                                            
                                     
    FST: Number/Date      "January 15"  NUM_DATE_0001    
         Normalization    "$1.2M"       NUM_MONEY_0002   
                                     
                                                            
                                                            
  Normalized source: "run +VERB+PROG cat +NOUN+PL ..."      
  (Smaller vocabulary, regularized morphology)               

                         
                         
          
             NEURAL TRANSFORMER         
             (Encoder-Decoder)          
             Trained on normalized      
             parallel corpus (50K)      
             Vocab: ~8K BPE (vs ~30K   
             without FST normalization) 
          
                         
                         

                    TARGET LANGUAGE SIDE                      
                                                             
  Normalized target: "lauf +VERB+PROG Katze +NOUN+PL ..."   
                                                            
                                                            
                                     
    FST: Morphological    "lauf" + VERB+PROG  "laufend"  
         Generation       "Katze" + NOUN+PL   "Katzen"   
                                     
                                                            
                                                            
                                     
    FST: Transliterate    Named entities: script mapping   
       + Denormalize      NUM_DATE_0001  "15. Januar"     
                                     
                                                            
                                                            
  Final target text (fully inflected, properly formatted)    
</code></pre>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 9 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">9. Computational Complexity: FST vs RNN vs Transformer</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Analyze and compare the computational complexity of sequence transduction using three approaches. For an input sequence of length $n$:</p>
                            <ol type="a">
                                <li>A deterministic FST with $|Q|$ states and alphabet size $|\Sigma|$.</li>
                                <li>An RNN (LSTM) with hidden dimension $d$.</li>
                                <li>A Transformer with $L$ layers, hidden dimension $d$, and $h$ attention heads.</li>
                            </ol>
                            <p>For each, analyze: (1) time complexity for processing the full sequence, (2) space complexity, (3) parallelizability, and (4) how each scales as $n$ grows very large.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>(a) Deterministic FST:</strong></p>
                                <ul>
                                    <li><strong>Time:</strong> $O(n)$. Each input symbol requires one transition lookup. If transitions are stored as a hash table, each lookup is $O(1)$. If stored as a sorted array, each lookup is $O(\log |\Sigma|)$. Total: $O(n)$ or $O(n \log |\Sigma|)$.</li>
                                    <li><strong>Space:</strong> $O(|Q| \cdot |\Sigma|)$ for the transition table (static, does not depend on $n$). Runtime memory is $O(1)$ &mdash; only the current state needs to be stored.</li>
                                    <li><strong>Parallelizability:</strong> Inherently sequential &mdash; each step depends on the previous state. However, for composition of FSTs, the intermediate representations can be computed in parallel.</li>
                                    <li><strong>Scaling:</strong> Linear in $n$ with a very small constant. FSTs can process millions of tokens per second on a single CPU core. No GPU required.</li>
                                </ul>
                                <p><strong>(b) RNN (LSTM):</strong></p>
                                <ul>
                                    <li><strong>Time:</strong> $O(n \cdot d^2)$. Each timestep involves matrix-vector multiplications of size $d \times d$ (the gate computations). There are 4 gates, so the constant is approximately $4d^2$ per step.</li>
                                    <li><strong>Space:</strong> $O(d^2)$ for parameters (weight matrices). $O(n \cdot d)$ for storing hidden states during backpropagation (can be reduced to $O(d)$ for inference only).</li>
                                    <li><strong>Parallelizability:</strong> Sequential within a sequence (like FSTs). Can parallelize across batch elements. Cannot parallelize across timesteps within a single sequence.</li>
                                    <li><strong>Scaling:</strong> Linear in $n$ but with a much larger constant than FSTs (each step costs $O(d^2)$ where $d$ is typically 256-1024). Benefits from GPU acceleration.</li>
                                </ul>
                                <p><strong>(c) Transformer:</strong></p>
                                <ul>
                                    <li><strong>Time:</strong> $O(L \cdot n^2 \cdot d)$ for self-attention (each layer computes $n \times n$ attention matrix and multiplies by values of dimension $d$). Feed-forward layers add $O(L \cdot n \cdot d^2)$.</li>
                                    <li><strong>Space:</strong> $O(L \cdot d^2)$ for parameters. $O(n^2 + n \cdot d)$ for attention matrices and activations during forward pass.</li>
                                    <li><strong>Parallelizability:</strong> Highly parallelizable &mdash; all positions can be processed simultaneously within each layer. This is the Transformer's key advantage over RNNs.</li>
                                    <li><strong>Scaling:</strong> Quadratic in $n$ due to self-attention. For very long sequences ($n > 10{,}000$), the $O(n^2)$ attention becomes a bottleneck. Efficient variants (linear attention, sparse attention) reduce this to $O(n \log n)$ or $O(n)$.</li>
                                </ul>
                                <p><strong>Summary table:</strong></p>
                                <table style="width: 100%; border-collapse: collapse;">
                                    <tr style="border-bottom: 2px solid var(--color-border);">
                                        <th style="padding: 0.3rem; text-align: left;">Model</th>
                                        <th style="padding: 0.3rem; text-align: left;">Time</th>
                                        <th style="padding: 0.3rem; text-align: left;">Space</th>
                                        <th style="padding: 0.3rem; text-align: left;">Parallel?</th>
                                    </tr>
                                    <tr style="border-bottom: 1px solid var(--color-border);">
                                        <td style="padding: 0.3rem;">FST</td>
                                        <td style="padding: 0.3rem;">$O(n)$</td>
                                        <td style="padding: 0.3rem;">$O(1)$ runtime</td>
                                        <td style="padding: 0.3rem;">No</td>
                                    </tr>
                                    <tr style="border-bottom: 1px solid var(--color-border);">
                                        <td style="padding: 0.3rem;">RNN</td>
                                        <td style="padding: 0.3rem;">$O(n d^2)$</td>
                                        <td style="padding: 0.3rem;">$O(n d)$</td>
                                        <td style="padding: 0.3rem;">No</td>
                                    </tr>
                                    <tr>
                                        <td style="padding: 0.3rem;">Transformer</td>
                                        <td style="padding: 0.3rem;">$O(L n^2 d)$</td>
                                        <td style="padding: 0.3rem;">$O(n^2)$</td>
                                        <td style="padding: 0.3rem;">Yes</td>
                                    </tr>
                                </table>
                                <p>The key insight: FSTs are asymptotically the most efficient but the least expressive. Transformers are the most expressive and parallelizable but the most expensive. The right choice depends on whether the task requires the expressive power of the more expensive models.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 10 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">10. Research Paper Analysis</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Read (or review the abstract and key results of) one of the following papers and answer the questions below:</p>
                            <ul>
                                <li>Mohri, Pereira, Riley (2002): "Weighted Finite-State Transducers in Speech Recognition"</li>
                                <li>Graves (2012): "Sequence Transduction with Recurrent Neural Networks"</li>
                                <li>Hannun et al. (2020): "Differentiable Weighted Finite-State Transducers" (the <code>gtn</code> paper)</li>
                            </ul>
                            <p>Questions:</p>
                            <ol type="a">
                                <li>What problem does the paper address, and why is it important?</li>
                                <li>How does the paper use FSTs (classical, weighted, or differentiable)?</li>
                                <li>What are the key results or contributions?</li>
                                <li>How does this paper relate to the concepts we covered in Tutorials 20-24?</li>
                                <li>What are the limitations of the proposed approach?</li>
                            </ol>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Example analysis: Hannun et al. (2020), "Differentiable Weighted Finite-State Transducers"</strong></p>
                                <p><strong>(a) Problem:</strong> The paper addresses the gap between classical WFST-based systems (which are modular and interpretable but not trainable end-to-end) and neural systems (which are trainable but monolithic). The importance lies in enabling gradient-based optimization of systems that incorporate structured knowledge through WFSTs.</p>
                                <p><strong>(b) FST usage:</strong> The paper makes WFST operations (composition, union, concatenation, closure, shortest-path/forward-score) differentiable by implementing them as operations in a computation graph with well-defined gradients. The WFSTs use the log semiring, which is naturally compatible with gradient computation. The key insight is that the forward algorithm on a WFST is analogous to a forward pass in a neural network, and the backward algorithm provides gradients.</p>
                                <p><strong>(c) Key results:</strong> The paper introduces the <code>gtn</code> library, which implements differentiable WFSTs on GPU. They demonstrate that end-to-end training through WFST operations matches or exceeds the performance of CTC (Connectionist Temporal Classification) on speech recognition benchmarks, while providing the modularity benefits of the WFST framework. The system can learn alignment, language model integration, and acoustic modeling jointly.</p>
                                <p><strong>(d) Relation to Tutorials 20-24:</strong> This paper directly builds on Tutorial 20 (FST formalism), Tutorial 21 (weighted FSTs and semirings &mdash; the log semiring is central), Tutorial 22 (FST libraries &mdash; gtn is one of the libraries we discussed), and this tutorial's discussion of differentiable FSTs and neural-symbolic hybrids. The paper is a concrete realization of the "differentiable FSTs" future direction we described.</p>
                                <p><strong>(e) Limitations:</strong> The approach is limited to tasks where the FST structure is a good inductive bias (sequence transduction with alignment). For tasks requiring complex, non-sequential reasoning, the FST structure may be too restrictive. The computational cost of WFST operations on large graphs can be high, even with GPU acceleration. The approach requires designing the WFST topology, which still requires human expertise &mdash; only the weights are learned automatically.</p>
                                <p><strong>Recommended reading and analysis framework:</strong></p>
                                <p>If you are choosing which paper to analyze, we recommend starting with <strong>Hannun et al. (2020)</strong> because it directly connects all five tutorials in this series. The paper is available on arXiv (arXiv:2010.01003). When reading any of these papers, use this structured framework:</p>
                                <ol>
                                    <li><strong>Problem framing (1 paragraph):</strong> What specific gap in the literature does this paper address? Why did prior approaches fail or fall short?</li>
                                    <li><strong>Technical contribution (2-3 paragraphs):</strong> What is the core technical idea? Map it to concepts from Tutorials 20-24. For example: which semiring is used? What FST operations are central? How does composition appear?</li>
                                    <li><strong>Experimental validation (1 paragraph):</strong> What benchmarks are used? What baselines are compared? Are the improvements statistically significant? Are ablations provided?</li>
                                    <li><strong>Connections to our series (1 paragraph):</strong> Explicitly identify at least 3 concepts from Tutorials 20-24 that appear in the paper. For Hannun et al., these would include: log semiring (Tutorial 21), FST composition (Tutorial 20), differentiable shortest path (this tutorial), and the gtn library (Tutorial 22).</li>
                                    <li><strong>Critical assessment (1 paragraph):</strong> What are the unstated assumptions? What tasks would this approach fail on? What would you do differently?</li>
                                </ol>
                            </div>
                        </div>
                    </div>

                </div>
            </article>

        </main>

        <!-- TOC (Right Side) -->
        <aside class="toc-container">
            <h4 class="toc-title">Contents</h4>
            <nav class="toc-list">
                <a href="#fst-vs-neural" class="toc-link">FST vs Neural: The Tradeoff</a>
                <a href="#hybrid-architectures" class="toc-link">Hybrid Architectures</a>
                <a href="#neural-transducers" class="toc-link">Neural Transducers</a>
                <a href="#kaldi-case-study" class="toc-link">Case Study: Kaldi</a>
                <a href="#decision-framework" class="toc-link">Decision Framework</a>
                <a href="#limitations-revisited" class="toc-link">FST Limitations Revisited</a>
                <a href="#future-directions" class="toc-link">Future Directions</a>
                <a href="#summary" class="toc-link">Series Summary</a>
            </nav>
        </aside>
    </div>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-brand">
                    <span class="logo-symbol">&nabla;</span>
                    <span>ML Fundamentals</span>
                </div>
                <p class="footer-tagline">Deep understanding through first principles.</p>
            </div>
            <div class="footer-links">
                <a href="../../../index.html">Home</a>
                <a href="https://github.com/ml-entropy/ml-entropy.github.io" target="_blank">GitHub</a>
            </div>
        </div>
    </footer>

    <!-- Scripts -->
    <script src="../../../js/main.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // KaTeX Rendering
            if (typeof renderMathInElement === 'function') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }

            // Tab Switching Logic
            const tabs = document.querySelectorAll('.tutorial-tab');
            const articles = document.querySelectorAll('.article-content');

            function switchTab(targetId) {
                if (!targetId || targetId === '#') targetId = '#theory';

                tabs.forEach(tab => {
                    if (tab.getAttribute('href') === targetId) {
                        tab.classList.add('active');
                    } else {
                        tab.classList.remove('active');
                    }
                });

                articles.forEach(article => {
                    const articleId = '#' + article.id;
                    if (articleId === targetId) {
                        article.style.display = 'block';
                    } else {
                        article.style.display = 'none';
                    }
                });

                if (typeof renderMathInElement === 'function') {
                    renderMathInElement(document.body, {
                        delimiters: [
                            {left: '$$', right: '$$', display: true},
                            {left: '$', right: '$', display: false},
                            {left: '\\[', right: '\\]', display: true},
                            {left: '\\(', right: '\\)', display: false}
                        ],
                        throwOnError: false
                    });
                }

                const toc = document.querySelector('.toc-container');
                if (toc) {
                    if (targetId === '#theory') {
                        toc.style.display = 'block';
                        setTimeout(() => toc.classList.add('visible'), 100);
                    } else {
                        toc.classList.remove('visible');
                        setTimeout(() => toc.style.display = 'none', 300);
                    }
                }
            }

            tabs.forEach(tab => {
                tab.addEventListener('click', (e) => {
                    e.preventDefault();
                    const targetId = tab.getAttribute('href');
                    history.pushState(null, null, targetId);
                    switchTab(targetId);
                });
            });

            window.addEventListener('popstate', () => {
                switchTab(window.location.hash);
            });

            switchTab(window.location.hash);
        });
    </script>
</body>
</html>
