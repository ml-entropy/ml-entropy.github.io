<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Exercises: Probability Concepts | ML Fundamentals</title>
    <meta name="description" content="37 practice exercises for probability theory, Bayes' theorem, continuous distributions, and density functions for machine learning.">
    
    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&family=Playfair+Display:wght@400;500;600;700&display=swap" rel="stylesheet">
    
    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}, {left: '\\[', right: '\\]', display: true}, {left: '\\(', right: '\\)', display: false}], throwOnError: false});"></script>
    
    <!-- Styles -->
    <link rel="stylesheet" href="../../../css/main.css">
    <link rel="stylesheet" href="../../../css/components.css">
    <link rel="stylesheet" href="../../../css/sidebar.css">
    
    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>∞</text></svg>">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar" id="navbar">
        <div class="nav-container">
            <a href="../../../index.html" class="nav-logo">
                <span class="logo-symbol">∇</span>
                <span class="logo-text">ML Fundamentals</span>
            </a>
            
            <button class="nav-toggle" id="navToggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            
            <div class="nav-menu" id="navMenu">
                <div class="nav-links">
                    <a href="../../../tutorials/ml/index.html" class="nav-link active">Machine Learning</a>
                    <a href="../../../tutorials/linear-algebra/index.html" class="nav-link">Linear Algebra</a>
                    <a href="../../../tutorials/calculus/index.html" class="nav-link">Calculus</a>
                    <a href="../../../tutorials/physics/index.html" class="nav-link">Physics</a>
                    <a href="../../../index.html#philosophy" class="nav-link">Philosophy</a>
                    <a href="../../../index.html#roadmap" class="nav-link">Roadmap</a>
                    <a href="https://github.com/ml-entropy/ml-entropy.github.io" class="nav-link" target="_blank">GitHub</a>
                </div>
                
                <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
                    <svg class="sun-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="5"/>
                        <path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"/>
                    </svg>
                    <svg class="moon-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                    </svg>
                </button>
            </div>
        </div>
    </nav>

    <!-- Tutorial Header -->
    <header class="tutorial-content-header">
        <div class="container">
            <nav class="breadcrumb">
                <a href="../../../index.html">Home</a>
                <span class="breadcrumb-separator">→</span>
                <a href="../index.html">Machine Learning</a>
                <span class="breadcrumb-separator">→</span>
                <span>Probability Exercises</span>
            </nav>
            
            <div class="tutorial-tabs">
                <a href="index.html" class="tutorial-tab">Theory</a>
                <a href="index.html#code" class="tutorial-tab">Code</a>
                <a href="#" class="tutorial-tab active">Exercises</a>
            </div>
        </div>
    </header>

    <div class="tutorial-wrapper">
        <aside class="tutorial-sidebar">
            <div class="sidebar-section">
                <h3 class="sidebar-section-title">Machine Learning</h3>
                <nav class="sidebar-nav">
                        <a href="../00-probability/index.html" class="sidebar-link active">00. Probability Foundations</a>
                    <a href="../04-entropy/index.html" class="sidebar-link">01. Entropy Fundamentals</a>
                    <a href="../05-cross-entropy/index.html" class="sidebar-link">02. Cross-Entropy</a>
                    <a href="../06-kl-divergence/index.html" class="sidebar-link">03. KL Divergence</a>
                    <a href="../03-distributions/index.html" class="sidebar-link">04. Normal Distributions</a>
                    <a href="../01-logarithms/index.html" class="sidebar-link">05. Why Logarithms?</a>
                    <a href="../02-combinatorics/index.html" class="sidebar-link">06. Combinatorics</a>
                    <a href="../08-backpropagation/index.html" class="sidebar-link">07. Backpropagation</a>
                    <a href="../09-regularization/index.html" class="sidebar-link">08. Regularization</a>
                    <a href="../10-batch-normalization/index.html" class="sidebar-link">09. Batch Normalization</a>
                    <a href="../11-learning-rate/index.html" class="sidebar-link">10. Learning Rate</a>
                    <a href="../12-cnn/index.html" class="sidebar-link">11. CNNs</a>
                    <a href="../13-rnn/index.html" class="sidebar-link">12. RNNs</a>
                    <a href="../17-vae/index.html" class="sidebar-link">13. VAE</a>
                    <a href="../16-variational-inference/index.html" class="sidebar-link">14. Variational Inference</a>
                    <a href="../07-entropy-connections/index.html" class="sidebar-link">15. Entropy Connections</a>
                </nav>
            </div>
            
            <div class="sidebar-section" style="margin-top: 2rem;">
                <h3 class="sidebar-section-title">Related Subjects</h3>
                <nav class="sidebar-nav">
                        <a href="../../linear-algebra/index.html" class="sidebar-link">Linear Algebra</a>
                    <a href="../../calculus/index.html" class="sidebar-link">Calculus</a>
                    <a href="../../physics/index.html" class="sidebar-link">Physics</a>
                </nav>
            </div>
        </aside>

        <main class="tutorial-main">
            <article class="article-content">
                <h1>Probability Exercises</h1>
                <p class="lead">37 exercises covering probability fundamentals, Bayes' theorem, continuous distributions, high-dimensional probability, and generative models. Work through them sequentially or jump to the section that challenges you.</p>

                <h2>Part A: Fundamentals</h2>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>1. Probability Basics</h3>
        <span class="difficulty-badge diff-easy">Easy</span>
    </div>
    <p>A bag has 5 red, 3 blue, 2 green balls. You draw one ball at random.</p>
<p>a) What is $P(\text{red})$?</p>
<p>b) What is $P(\text{not green})$?</p>
<p>c) You draw two balls <em>without replacement</em>. What is $P(\text{both red})$?</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>a) $P(\text{red}) = \frac{5}{10} = 0.5$.</p>
<p>b) $P(\text{not green}) = 1 - \frac{2}{10} = 0.8$.</p>
<p>c) $P(\text{both red}) = \frac{5}{10} \cdot \frac{4}{9} = \frac{20}{90} = \frac{2}{9} \approx 0.222$.</p>
        </div>
    </details>
</div>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>2. Joint Probability Table</h3>
        <span class="difficulty-badge diff-easy">Easy</span>
    </div>
    <p>A survey of 200 students found: 80 study math, 60 study physics, 30 study both.</p>
<p>a) Construct a joint probability table.</p>
<p>b) Find $P(\text{math} \mid \text{physics})$.</p>
<p>c) Are studying math and physics independent?</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <table class="data-table">
<thead><tr><th></th><th>Physics</th><th>Not Physics</th><th>Total</th></tr></thead>
<tbody>
<tr><td><strong>Math</strong></td><td>0.15</td><td>0.25</td><td>0.40</td></tr>
<tr><td><strong>Not Math</strong></td><td>0.15</td><td>0.45</td><td>0.60</td></tr>
<tr><td><strong>Total</strong></td><td>0.30</td><td>0.70</td><td>1.00</td></tr>
</tbody>
</table>
<p>b) $P(M|P) = \frac{0.15}{0.30} = 0.50$.</p>
<p>c) Not independent: $P(M \cap P) = 0.15 \neq P(M) \cdot P(P) = 0.40 \times 0.30 = 0.12$.</p>
        </div>
    </details>
</div>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>3. Dice Probability</h3>
        <span class="difficulty-badge diff-easy">Easy</span>
    </div>
    <p>Roll two fair dice.</p>
<p>a) What is $P(\text{sum} = 7)$?</p>
<p>b) What is $P(\text{sum} = 7 \mid \text{first die} = 3)$?</p>
<p>c) What is $P(\text{at least one 6})$?</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>a) $\frac{6}{36} = \frac{1}{6}$.</p>
<p>b) Need second die $= 4$, so $\frac{1}{6}$.</p>
<p>c) $1 - P(\text{no 6}) = 1 - \left(\frac{5}{6}\right)^2 = \frac{11}{36} \approx 0.306$.</p>
        </div>
    </details>
</div>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>4. Complementary Counting</h3>
        <span class="difficulty-badge diff-easy">Easy</span>
    </div>
    <p>A password is 4 digits (0–9).</p>
<p>a) How many total passwords are possible?</p>
<p>b) What is $P(\text{all digits different})$?</p>
<p>c) What is $P(\text{at least two digits the same})$?</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>a) $10^4 = 10{,}000$.</p>
<p>b) $\frac{10 \times 9 \times 8 \times 7}{10^4} = \frac{5040}{10000} = 0.504$.</p>
<p>c) $1 - 0.504 = 0.496$.</p>
        </div>
    </details>
</div>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>5. Expected Value</h3>
        <span class="difficulty-badge diff-easy">Easy</span>
    </div>
    <p>A lottery ticket costs $2. With probability 0.001 you win $500, with probability 0.01 you win $10, otherwise you win nothing.</p>
<p>a) What is $E[\text{profit}]$?</p>
<p>b) If you buy 1000 tickets, what is your expected total profit?</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>a) $E[\text{profit}] = 0.001(500) + 0.01(10) + 0.989(0) - 2 = 0.5 + 0.1 - 2 = -\$1.40$.</p>
<p>b) $1000 \times (-1.40) = -\$1{,}400$. The house always wins.</p>
        </div>
    </details>
</div>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>6. Variance Computation</h3>
        <span class="difficulty-badge diff-medium">Medium</span>
    </div>
    <p>$X$ takes values $\{1, 2, 3, 4\}$ each with probability $1/4$.</p>
<p>a) Find $E[X]$.</p>
<p>b) Find $E[X^2]$.</p>
<p>c) Find $\operatorname{Var}(X)$.</p>
<p>d) Let $Y = 2X + 3$. Find $E[Y]$ and $\operatorname{Var}(Y)$.</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>a) $E[X] = \frac{1+2+3+4}{4} = 2.5$.</p>
<p>b) $E[X^2] = \frac{1+4+9+16}{4} = 7.5$.</p>
<p>c) $\operatorname{Var}(X) = 7.5 - 6.25 = 1.25$.</p>
<p>d) $E[Y] = 2(2.5) + 3 = 8$. $\operatorname{Var}(Y) = 4 \cdot \operatorname{Var}(X) = 5.0$ (the $+3$ doesn't affect variance).</p>
        </div>
    </details>
</div>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>7. Marginalization in Practice</h3>
        <span class="difficulty-badge diff-easy">Easy</span>
    </div>
    <p>A classifier outputs $P(\text{cat}|\text{img}) = 0.7$, $P(\text{dog}|\text{img}) = 0.3$. We also know $P(\text{orange}|\text{cat}) = 0.6$, $P(\text{orange}|\text{dog}) = 0.1$.</p>
<p>a) What is $P(\text{orange}|\text{img})$?</p>
<p>b) What is $P(\text{cat}|\text{orange}, \text{img})$?</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>a) $P(\text{orange}|\text{img}) = 0.6 \times 0.7 + 0.1 \times 0.3 = 0.42 + 0.03 = 0.45$.</p>
<p>b) By Bayes' theorem: $\frac{0.42}{0.45} \approx 0.933$.</p>
        </div>
    </details>
</div>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>8. Independence Testing</h3>
        <span class="difficulty-badge diff-medium">Medium</span>
    </div>
    <p>1000 website visitors. 600 use Chrome, 300 Firefox, 100 Safari. Of these, 120 Chrome users bought something, 75 Firefox users bought, and 15 Safari users bought.</p>
<p>a) What is $P(\text{buy})$?</p>
<p>b) What is $P(\text{buy}|\text{Chrome})$?</p>
<p>c) Are browser choice and purchase independent?</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>a) $P(\text{buy}) = \frac{210}{1000} = 0.21$.</p>
<p>b) $P(\text{buy}|\text{Chrome}) = \frac{120}{600} = 0.20$.</p>
<p>c) $P(\text{buy} \cap \text{Chrome}) = \frac{120}{1000} = 0.12$. $P(\text{buy}) \cdot P(\text{Chrome}) = 0.21 \times 0.60 = 0.126$. Close but not equal. $P(\text{buy}|\text{Firefox}) = 0.25$, $P(\text{buy}|\text{Safari}) = 0.15$. Different rates &rarr; not independent.</p>
        </div>
    </details>
</div>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>9. Chain Rule Application</h3>
        <span class="difficulty-badge diff-medium">Medium</span>
    </div>
    <p>In a language model: $P(\text{word}_1 = \text{"the"}) = 0.07$. $P(\text{word}_2 = \text{"cat"} \mid \text{word}_1 = \text{"the"}) = 0.02$. $P(\text{word}_3 = \text{"sat"} \mid \text{word}_1 = \text{"the"}, \text{word}_2 = \text{"cat"}) = 0.05$.</p>
<p>What is the probability of the sequence "the cat sat"?</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>$P(\text{"the cat sat"}) = 0.07 \times 0.02 \times 0.05 = 0.00007 = 7 \times 10^{-5}$.</p>
<p>This is exactly how autoregressive language models (like GPT) compute sequence probabilities.</p>
        </div>
    </details>
</div>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>10. Total Probability</h3>
        <span class="difficulty-badge diff-medium">Medium</span>
    </div>
    <p>A factory has three machines: A produces 50% of items (2% defect rate), B produces 30% (3% defect rate), C produces 20% (5% defect rate).</p>
<p>a) What is $P(\text{defective})$?</p>
<p>b) Given an item is defective, what is $P(\text{from machine C})$?</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>a) $P(D) = 0.50(0.02) + 0.30(0.03) + 0.20(0.05) = 0.01 + 0.009 + 0.01 = 0.029$.</p>
<p>b) $P(C|D) = \frac{0.05 \times 0.20}{0.029} = \frac{0.01}{0.029} \approx 0.345$.</p>
<p>Machine C produces only 20% of items but is responsible for ~34.5% of defects.</p>
        </div>
    </details>
</div>

                <h2>Part B: Bayes' Theorem &amp; Conditional Probability</h2>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>11. Spam Filter</h3>
        <span class="difficulty-badge diff-medium">Medium</span>
    </div>
    <p>20% of emails are spam. The word "free" appears in 80% of spam and 10% of legitimate emails.</p>
<p>a) What is $P(\text{spam} \mid \text{"free"})$?</p>
<p>b) What is $P(\text{spam} \mid \text{no "free"})$?</p>
<p>c) What if 50% of emails are spam?</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>a) $P(\text{spam}|\text{free}) = \frac{0.80 \times 0.20}{0.80 \times 0.20 + 0.10 \times 0.80} = \frac{0.16}{0.24} \approx 0.667$.</p>
<p>b) $P(\text{spam}|\neg\text{free}) = \frac{0.20 \times 0.20}{0.20 \times 0.20 + 0.90 \times 0.80} = \frac{0.04}{0.76} \approx 0.053$.</p>
<p>c) $\frac{0.80 \times 0.50}{0.80 \times 0.50 + 0.10 \times 0.50} = \frac{0.40}{0.45} \approx 0.889$.</p>
        </div>
    </details>
</div>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>12. Medical Testing Revisited</h3>
        <span class="difficulty-badge diff-medium">Medium</span>
    </div>
    <p>Disease prevalence: 0.1%. Test sensitivity: 99%. Test specificity: 99%.</p>
<p>a) What is $P(\text{disease} \mid \text{positive})$?</p>
<p>b) What if prevalence is 5%?</p>
<p>c) What specificity is needed for $P(\text{disease} \mid +) > 0.5$ at 0.1% prevalence?</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>a) $P(D|+) = \frac{0.99 \times 0.001}{0.99 \times 0.001 + 0.01 \times 0.999} = \frac{0.00099}{0.01098} \approx 0.090 = 9.0\%$.</p>
<p>b) $\frac{0.99 \times 0.05}{0.99 \times 0.05 + 0.01 \times 0.95} = \frac{0.0495}{0.059} \approx 0.839 = 83.9\%$.</p>
<p>c) Need $(1 - \text{sp}) &lt; \frac{0.00099}{0.999} \approx 0.00099$, so specificity $&gt; 99.9\%$.</p>
        </div>
    </details>
</div>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>13. Two-Test Problem</h3>
        <span class="difficulty-badge diff-medium">Medium</span>
    </div>
    <p>Same setup as exercise 12 (0.1% prevalence, 99% sensitivity, 99% specificity). A person tests positive on <em>two independent tests</em>.</p>
<p>What is $P(\text{disease} \mid \text{both positive})$?</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>After first positive, prior updates to 0.090. Second test:</p>
<div class="math-block">$$P(D|{+}{+}) = \frac{0.99 \times 0.090}{0.99 \times 0.090 + 0.01 \times 0.910} = \frac{0.0891}{0.0891 + 0.0091} = \frac{0.0891}{0.0982} \approx 0.907$$</div>
<p>Two positive tests: 90.7% vs only 9% with one test. Sequential Bayesian updating!</p>
        </div>
    </details>
</div>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>14. Naive Bayes Classifier</h3>
        <span class="difficulty-badge diff-medium">Medium</span>
    </div>
    <p>$P(\text{spam}) = 0.4$. Word likelihoods: $P(\text{"buy"}|\text{spam}) = 0.8$, $P(\text{"buy"}|\text{ham}) = 0.1$, $P(\text{"cheap"}|\text{spam}) = 0.6$, $P(\text{"cheap"}|\text{ham}) = 0.05$.</p>
<p>Assuming conditional independence, find $P(\text{spam} \mid \text{"buy"}, \text{"cheap"})$.</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>$P(\text{"buy","cheap"}|\text{spam}) = 0.8 \times 0.6 = 0.48$.</p>
<p>$P(\text{"buy","cheap"}|\text{ham}) = 0.1 \times 0.05 = 0.005$.</p>
<div class="math-block">$$P(\text{spam}|\text{both}) = \frac{0.48 \times 0.4}{0.48 \times 0.4 + 0.005 \times 0.6} = \frac{0.192}{0.195} \approx 0.985$$</div>
        </div>
    </details>
</div>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>15. Prosecutor's Fallacy</h3>
        <span class="difficulty-badge diff-medium">Medium</span>
    </div>
    <p>A DNA match probability for a random person is 1 in 1 million. The city has 5 million people.</p>
<p>a) What is $P(\text{match} \mid \text{innocent})$?</p>
<p>b) What is $P(\text{innocent} \mid \text{match})$?</p>
<p>c) Are these the same?</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>a) $P(\text{match}|\text{innocent}) = 10^{-6}$.</p>
<p>b) Prior $P(\text{guilty}) \approx \frac{1}{5{,}000{,}000}$. By Bayes:</p>
<div class="math-block">$$P(\text{guilty}|\text{match}) \approx \frac{2 \times 10^{-7}}{2 \times 10^{-7} + 10^{-6}} \approx 0.167$$</div>
<p>c) $P(\text{match}|\text{innocent}) = 0.000001$ but $P(\text{innocent}|\text{match}) \approx 0.833$. The prosecutor's fallacy is confusing $P(\text{evidence}|\text{innocent})$ with $P(\text{innocent}|\text{evidence})$. About 5 people in the city would match.</p>
        </div>
    </details>
</div>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>16. Prior Sensitivity</h3>
        <span class="difficulty-badge diff-medium">Medium</span>
    </div>
    <p>You want to estimate a coin's bias $\theta$. You observe 6 heads in 10 flips. Compute the posterior mean under three priors:</p>
<p>a) Uniform $\operatorname{Beta}(1,1)$.</p>
<p>b) $\operatorname{Beta}(10,10)$.</p>
<p>c) $\operatorname{Beta}(1,5)$.</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>Posterior is $\operatorname{Beta}(\alpha+h, \beta+t)$ where $h=6$, $t=4$.</p>
<p>a) $\operatorname{Beta}(7,5)$: mean $= 7/12 \approx 0.583$.</p>
<p>b) $\operatorname{Beta}(16,14)$: mean $= 16/30 \approx 0.533$.</p>
<p>c) $\operatorname{Beta}(7,9)$: mean $= 7/16 = 0.4375$.</p>
<p>Strong prior toward fair (b) pulls toward 0.5. Strong prior toward tails (c) overrides the data. With only 10 flips, the prior has significant influence.</p>
        </div>
    </details>
</div>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>17. Bayesian Model Comparison</h3>
        <span class="difficulty-badge diff-hard">Hard</span>
    </div>
    <p>$P(\text{data}|A) = 0.003$, $P(\text{data}|B) = 0.0001$. Prior: $P(A) = P(B) = 0.5$.</p>
<p>a) What is the Bayes factor?</p>
<p>b) What is $P(A \mid \text{data})$?</p>
<p>c) What if $P(A) = 0.1$?</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>a) $BF = \frac{P(\text{data}|A)}{P(\text{data}|B)} = \frac{0.003}{0.0001} = 30$.</p>
<p>b) $P(A|\text{data}) = \frac{0.003 \times 0.5}{0.003 \times 0.5 + 0.0001 \times 0.5} = \frac{0.0015}{0.00155} \approx 0.968$.</p>
<p>c) $P(A|\text{data}) = \frac{0.003 \times 0.1}{0.003 \times 0.1 + 0.0001 \times 0.9} = \frac{0.0003}{0.00039} \approx 0.769$.</p>
<p>Still prefer A. Bayes factor 30 overcomes the skeptical prior.</p>
        </div>
    </details>
</div>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>18. From Prior to Posterior</h3>
        <span class="difficulty-badge diff-medium">Medium</span>
    </div>
    <p>Prior: $\theta \sim \operatorname{Uniform}[0,1]$. You observe 7 heads in 10 flips.</p>
<p>a) What is the likelihood $P(7H \mid \theta)$?</p>
<p>b) What is the posterior $\propto$ ?</p>
<p>c) Why is the posterior mean $\approx 0.667$, not 0.70?</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>a) $P(7H|\theta) = \binom{10}{7}\theta^7(1-\theta)^3 = 120\theta^7(1-\theta)^3$.</p>
<p>b) $p(\theta|\text{data}) \propto \theta^7(1-\theta)^3 \times 1$. This is a $\operatorname{Beta}(8,4)$ kernel.</p>
<p>c) Posterior mean $= \frac{7+1}{7+1+3+1} = \frac{8}{12}$. The uniform prior adds 1 phantom head + 1 phantom tail. This pulls the estimate toward 0.5. This is Bayesian shrinkage.</p>
        </div>
    </details>
</div>

                <h2>Part C: Continuous Distributions &amp; Density</h2>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>19. Density vs Probability</h3>
        <span class="difficulty-badge diff-medium">Medium</span>
    </div>
    <p>A random variable has PDF $p(x) = 3x^2$ for $x \in [0,1]$.</p>
<p>a) Verify this is a valid density.</p>
<p>b) Find $P(0.5 \leq X \leq 1)$.</p>
<p>c) Find $E[X]$ and $\operatorname{Var}(X)$.</p>
<p>d) $p(1) = 3$. Does this mean $P(X=1) = 3$?</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>a) $\int_0^1 3x^2\,dx = [x^3]_0^1 = 1$ ✓.</p>
<p>b) $[x^3]_{0.5}^1 = 1 - 0.125 = 0.875$.</p>
<p>c) $E[X] = 3\int_0^1 x^3\,dx = \frac{3}{4} = 0.75$. $E[X^2] = 3\int_0^1 x^4\,dx = \frac{3}{5} = 0.6$. $\operatorname{Var}(X) = 0.6 - 0.5625 = 0.0375$.</p>
<p>d) No! $P(X=1) = 0$. Density 3 means probability is concentrated near $x=1$, but probability of any exact point is zero.</p>
        </div>
    </details>
</div>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>20. Exponential Distribution</h3>
        <span class="difficulty-badge diff-medium">Medium</span>
    </div>
    <p>$X \sim \operatorname{Exp}(\lambda=2)$, so $p(x) = 2e^{-2x}$ for $x \geq 0$.</p>
<p>a) Find $P(X &gt; 1)$.</p>
<p>b) Find $P(X &gt; 2 \mid X &gt; 1)$.</p>
<p>c) Find $E[X]$ and $\operatorname{Var}(X)$.</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>a) $P(X&gt;1) = e^{-2} \approx 0.135$.</p>
<p>b) $P(X&gt;2|X&gt;1) = \frac{P(X&gt;2)}{P(X&gt;1)} = \frac{e^{-4}}{e^{-2}} = e^{-2} \approx 0.135$. Same as $P(X&gt;1)$! This is the memoryless property.</p>
<p>c) $E[X] = \frac{1}{\lambda} = 0.5$. $\operatorname{Var}(X) = \frac{1}{\lambda^2} = 0.25$.</p>
        </div>
    </details>
</div>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>21. CDF and Quantiles</h3>
        <span class="difficulty-badge diff-medium">Medium</span>
    </div>
    <p>A random variable has PDF $p(x) = 2x$ for $x \in [0,1]$.</p>
<p>a) Find the CDF.</p>
<p>b) Find the median.</p>
<p>c) Find the 90th percentile.</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>a) $F(x) = \int_0^x 2t\,dt = x^2$ for $x \in [0,1]$.</p>
<p>b) $x^2 = 0.5 \Rightarrow m = \sqrt{0.5} \approx 0.707$.</p>
<p>c) $x^2 = 0.9 \Rightarrow \sqrt{0.9} \approx 0.949$.</p>
        </div>
    </details>
</div>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>22. Uniform Distribution Density</h3>
        <span class="difficulty-badge diff-easy">Easy</span>
    </div>
    <p>$X \sim \operatorname{Uniform}[a,b]$.</p>
<p>a) What is the PDF?</p>
<p>b) What is the density for $\operatorname{Uniform}[0, 0.1]$?</p>
<p>c) Find $E[X]$ and $\operatorname{Var}(X)$ for $\operatorname{Uniform}[2, 8]$.</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>a) $p(x) = \frac{1}{b-a}$ for $x \in [a,b]$.</p>
<p>b) $p(x) = \frac{1}{0.1} = 10$ for $x \in [0, 0.1]$. Density $= 10 &gt; 1$, perfectly valid! $\int_0^{0.1} 10\,dx = 1$.</p>
<p>c) $E[X] = \frac{2+8}{2} = 5$. $\operatorname{Var}(X) = \frac{(8-2)^2}{12} = \frac{36}{12} = 3$.</p>
        </div>
    </details>
</div>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>23. Gaussian Density</h3>
        <span class="difficulty-badge diff-medium">Medium</span>
    </div>
    <p>$X \sim N(\mu=5, \sigma^2=4)$.</p>
<p>a) Write the PDF formula.</p>
<p>b) Where is the density maximized? What is the maximum value?</p>
<p>c) Find $P(3 \leq X \leq 7)$.</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>a) $p(x) = \frac{1}{2\sqrt{2\pi}} \exp\left(-\frac{(x-5)^2}{8}\right)$.</p>
<p>b) Maximum at $x = \mu = 5$. Max density $= \frac{1}{\sigma\sqrt{2\pi}} = \frac{1}{2 \times 2.507} \approx 0.199$.</p>
<p>c) $P\left(\frac{3-5}{2} \leq Z \leq \frac{7-5}{2}\right) = P(-1 \leq Z \leq 1) = \Phi(1) - \Phi(-1) \approx 0.6826 \approx 68.3\%$.</p>
        </div>
    </details>
</div>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>24. Mixture of Gaussians</h3>
        <span class="difficulty-badge diff-medium">Medium</span>
    </div>
    <p>$p(x) = 0.3 \cdot N(x; 0, 1) + 0.7 \cdot N(x; 5, 1)$.</p>
<p>a) Is this a valid PDF?</p>
<p>b) Find $E[X]$.</p>
<p>c) Is the distribution unimodal or bimodal?</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>a) Yes. $\int p(x)\,dx = 0.3(1) + 0.7(1) = 1$ and $p(x) \geq 0$.</p>
<p>b) $E[X] = 0.3(0) + 0.7(5) = 3.5$.</p>
<p>c) Bimodal — peaks near $x=0$ and $x=5$. Components are well-separated (distance 5 &gt;&gt; $\sigma=1$).</p>
        </div>
    </details>
</div>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>25. Joint Continuous Density</h3>
        <span class="difficulty-badge diff-hard">Hard</span>
    </div>
    <p>Joint density $p(x,y) = 6x$ for $0 \leq x \leq 1$ and $0 \leq y \leq 1-x$.</p>
<p>a) Verify the density integrates to 1.</p>
<p>b) Find the marginal $p_X(x)$.</p>
<p>c) Find $E[X]$.</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>a) $\int_0^1\int_0^{1-x} 6x\,dy\,dx = \int_0^1 6x(1-x)\,dx = 6\left[\frac{x^2}{2} - \frac{x^3}{3}\right]_0^1 = 6\left(\frac{1}{2} - \frac{1}{3}\right) = 6 \cdot \frac{1}{6} = 1$ ✓.</p>
<p>b) $p_X(x) = \int_0^{1-x} 6x\,dy = 6x(1-x)$ for $x \in [0,1]$.</p>
<p>c) $E[X] = \int_0^1 x \cdot 6x(1-x)\,dx = 6\int_0^1(x^2-x^3)\,dx = 6\left(\frac{1}{3}-\frac{1}{4}\right) = \frac{6}{12} = 0.5$.</p>
        </div>
    </details>
</div>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>26. Maximum of Random Variables</h3>
        <span class="difficulty-badge diff-hard">Hard</span>
    </div>
    <p>$X_1, X_2 \sim \operatorname{Uniform}[0,1]$ independently. Let $M = \max(X_1, X_2)$.</p>
<p>a) Find the CDF of $M$.</p>
<p>b) Find the PDF of $M$.</p>
<p>c) Find $E[M]$.</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>a) $P(M \leq m) = P(X_1 \leq m)P(X_2 \leq m) = m^2$ for $m \in [0,1]$.</p>
<p>b) $p_M(m) = \frac{d}{dm}m^2 = 2m$ for $m \in [0,1]$.</p>
<p>c) $E[M] = \int_0^1 m \cdot 2m\,dm = \frac{2}{3}$.</p>
<p>Generalizes: for $n$ iid $\operatorname{Uniform}[0,1]$, $E[\max] = \frac{n}{n+1}$.</p>
        </div>
    </details>
</div>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>27. Change of Variables</h3>
        <span class="difficulty-badge diff-hard">Hard</span>
    </div>
    <p>$X \sim N(0,1)$. Define $Y = X^2$.</p>
<p>a) Is $P(X=1) = P(Y=1)$?</p>
<p>b) Is $p_X(1) = p_Y(1)$?</p>
<p>c) Why does density need the Jacobian?</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>a) Yes: both $= 0$ (continuous random variables).</p>
<p>b) No! $p_X(1) = \frac{1}{\sqrt{2\pi}}e^{-1/2} \approx 0.242$. For $Y=X^2$, $y=1$ means $x=\pm 1$.</p>
<div class="math-block">$$p_Y(1) = \frac{p_X(1)}{|2 \cdot 1|} + \frac{p_X(-1)}{|2 \cdot (-1)|} = \frac{0.242}{2} + \frac{0.242}{2} = 0.242$$</div>
<p>Coincidentally equal here but for different reasons.</p>
<p>c) The Jacobian $|g'(x)|^{-1}$ corrects for stretching/compressing. $p_Y(y)\,dy = p_X(x)\,dx$ gives $p_Y(y) = p_X(x)/|g'(x)|$. Essential for normalizing flows.</p>
        </div>
    </details>
</div>

                <h2>Part D: High-Dimensional Probability &amp; Generative Models</h2>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>28. High-Dimensional Gaussian Peak</h3>
        <span class="difficulty-badge diff-hard">Hard</span>
    </div>
    <p>Consider a standard $N(0, I)$ in $d$ dimensions.</p>
<p>a) What is the peak density $p(0)$ for $d=1, 10, 100$?</p>
<p>b) A generative model reports $\log p(x) = -800$ for 28&times;28 images. Is this high or low?</p>
<p>c) Why is sampling from the mode atypical?</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>a) $p(0) = (2\pi)^{-d/2}$. $d=1$: &asymp; 0.399. $d=10$: &asymp; $1.02 \times 10^{-4}$. $d=100$: &asymp; $5.2 \times 10^{-40}$.</p>
<p>b) $p(x) = e^{-800} \approx 10^{-347}$. Peak in $d=784$: $(2\pi)^{-392} \approx 10^{-362}$. So $\log p(x) = -800$ is $10^{15}$ times higher than the Gaussian peak — very high.</p>
<p>c) The mode has highest density per point, but the shell at radius $\sqrt{d}$ has exponentially more volume. Typical samples have $\|x\| \approx \sqrt{d} = 28$, not 0.</p>
        </div>
    </details>
</div>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>29. Bits Per Dimension</h3>
        <span class="difficulty-badge diff-medium">Medium</span>
    </div>
    <p>A generative model over 32&times;32&times;3 images reports $\log_2 p(x) = -9{,}000$.</p>
<p>a) How many total dimensions?</p>
<p>b) What is the bits-per-dimension?</p>
<p>c) Compare to JPEG (~2 bits/pixel).</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>a) $d = 32 \times 32 \times 3 = 3{,}072$.</p>
<p>b) $\text{BPD} = \frac{9{,}000}{3{,}072} \approx 2.93$ bits/dim.</p>
<p>c) At ~2.93 bpd vs JPEG's ~2 bpd, the model is worse. State-of-the-art achieves ~3 bpd on CIFAR-10.</p>
        </div>
    </details>
</div>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>30. Why Density Comparisons Matter</h3>
        <span class="difficulty-badge diff-medium">Medium</span>
    </div>
    <p>A model assigns: $\log p(x_1) = -1200$, $\log p(x_2) = -1350$, $\log p(x_3) = -45000$.</p>
<p>a) Rank by typicality.</p>
<p>b) What is $p(x_1)/p(x_2)$?</p>
<p>c) Why not just say "p(x) is basically zero"?</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>a) $x_1$ most typical, then $x_2$, then $x_3$.</p>
<p>b) $p(x_1)/p(x_2) = e^{150} \approx 10^{65}$.</p>
<p>c) In high dimensions, ALL densities are tiny. What matters is relative ranking. $\log p(x_1) = -1200$ could be near the model's peak.</p>
        </div>
    </details>
</div>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>31. Anomaly Detection with Density</h3>
        <span class="difficulty-badge diff-medium">Medium</span>
    </div>
    <p>A model is trained on 10,000 normal scans. Average $\log p(x) = -500$ (std $= 50$). A new scan has $\log p(y) = -12{,}000$.</p>
<p>a) Is the new scan likely anomalous?</p>
<p>b) What is the z-score?</p>
<p>c) What is a limitation of this approach?</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>a) Yes, very likely anomalous.</p>
<p>b) $z = \frac{-12{,}000 - (-500)}{50} = -230$. Extreme outlier.</p>
<p>c) A simple image (all-black) might get high density under a poorly calibrated model. Also, models can assign high density to out-of-distribution data.</p>
        </div>
    </details>
</div>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>32. Typical Set Volume</h3>
        <span class="difficulty-badge diff-hard">Hard</span>
    </div>
    <p>Consider a standard Gaussian in $d$ dimensions.</p>
<p>a) What is $E[\|X\|^2]$ for $d=100$?</p>
<p>b) What is the standard deviation of $\|X\|^2$?</p>
<p>c) What fraction of the total volume does the typical shell occupy?</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>a) $E[\|X\|^2] = \sum E[X_i^2] = d = 100$.</p>
<p>b) $\operatorname{Var}(\|X\|^2) = 2d = 200$. Std $= \sqrt{200} \approx 14.1$.</p>
<p>c) Shell at radius $\sqrt{100} = 10$, width $\approx 14.1/10 \approx 1.41$ in radius. An incredibly thin fraction of the ball's volume, yet contains essentially all probability mass.</p>
        </div>
    </details>
</div>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>33. Likelihood-Based Generation</h3>
        <span class="difficulty-badge diff-hard">Hard</span>
    </div>
    <p>Explain the following:</p>
<p>a) Why does maximizing $\log p(x)$ teach a model what "typical" looks like?</p>
<p>b) Why does perfect log-likelihood imply perfect samples?</p>
<p>c) What do GANs trade away?</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>a) Maximum likelihood finds $\theta$ making observed data most probable: $\arg\max_\theta \sum \log p(x_i|\theta)$. This forces the model to concentrate density on training-like images.</p>
<p>b) If $p_{\text{model}} = p_{\text{data}}$, samples from $p_{\text{model}}$ are indistinguishable from real data by definition.</p>
<p>c) GANs trade density estimation for sample quality. Cost: no $p(x)$ evaluation (no anomaly detection, no compression), mode collapse, unstable training, no principled interpolation.</p>
        </div>
    </details>
</div>

                <h2>Part E: Challenge Problems</h2>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>34. Monty Hall via Bayes</h3>
        <span class="difficulty-badge diff-hard">Hard</span>
    </div>
    <p>Three doors, one car, two goats. You pick door 1. The host opens door 3 (reveals a goat). Should you switch to door 2?</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>Let $C_i$ = car behind door $i$. Prior: $P(C_1) = P(C_2) = P(C_3) = 1/3$.</p>
<p>Let $H_3$ = host opens door 3.</p>
<p>$P(H_3|C_1) = 1/2$ (host can open door 2 or 3).</p>
<p>$P(H_3|C_2) = 1$ (host must open door 3).</p>
<p>$P(H_3|C_3) = 0$ (host won't reveal the car).</p>
<div class="math-block">$$P(H_3) = \frac{1}{2} \cdot \frac{1}{3} + 1 \cdot \frac{1}{3} + 0 = \frac{1}{2}$$</div>
<div class="math-block">$$P(C_1|H_3) = \frac{1/2 \cdot 1/3}{1/2} = \frac{1}{3}$$</div>
<div class="math-block">$$P(C_2|H_3) = \frac{1 \cdot 1/3}{1/2} = \frac{2}{3}$$</div>
<p><strong>Switch!</strong> You double your chances from $1/3$ to $2/3$.</p>
        </div>
    </details>
</div>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>35. Conditional Independence Paradox</h3>
        <span class="difficulty-badge diff-hard">Hard</span>
    </div>
    <p>Let $X$ = "it rained" and $Y$ = "sprinkler was on". Suppose $X$ and $Y$ are marginally independent. Let $Z$ = "grass is wet" (caused by both).</p>
<p>a) Are $X$ and $Y$ independent given $Z$?</p>
<p>b) If $Z=1$ and $X=1$, how does $P(Y=1 \mid Z=1)$ change?</p>
<p>c) What is this phenomenon called?</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>a) No! Given wet grass, learning it rained makes the sprinkler less likely ("explaining away"). $P(Y|Z,X) \neq P(Y|Z)$.</p>
<p>b) $P(\text{sprinkler}|\text{wet, rained}) &lt; P(\text{sprinkler}|\text{wet})$. Rain "explains" the wetness.</p>
<p>c) "Explaining away" or Berkson's paradox. Conditioning on a common effect of two independent causes makes them dependent. Crucial for Bayesian networks.</p>
        </div>
    </details>
</div>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>36. KL Divergence Intuition</h3>
        <span class="difficulty-badge diff-hard">Hard</span>
    </div>
    <p>$P = [0.25, 0.25, 0.25, 0.25]$ and $Q = [0.5, 0.25, 0.125, 0.125]$.</p>
<p>a) Compute $D_{KL}(P\|Q)$.</p>
<p>b) Compute $D_{KL}(Q\|P)$.</p>
<p>c) Are they equal? Is this always the case?</p>
<p>d) Which direction does maximum likelihood minimize?</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>a) $D_{KL}(P\|Q) = 0.25\ln\frac{0.25}{0.5} + 0.25\ln\frac{0.25}{0.25} + 0.25\ln\frac{0.25}{0.125} + 0.25\ln\frac{0.25}{0.125}$</p>
<p>$= 0.25(-\ln 2) + 0 + 0.25(\ln 2) + 0.25(\ln 2) = 0.25\ln 2 \approx 0.173$ nats.</p>
<p>b) $D_{KL}(Q\|P) = 0.5\ln 2 + 0 - 0.125\ln 2 - 0.125\ln 2 = 0.25\ln 2 \approx 0.173$ nats.</p>
<p>c) Here they happen to be equal, but in general $D_{KL}(P\|Q) \neq D_{KL}(Q\|P)$ — KL divergence is asymmetric.</p>
<p>d) Maximum likelihood minimizes $D_{KL}(P_{\text{data}}\|P_{\text{model}})$ — it penalizes the model for assigning low probability where data is common (mode-covering).</p>
        </div>
    </details>
</div>

                <div class="exercise-card">
    <div class="exercise-header">
        <h3>37. Information-Theoretic Optimal Code</h3>
        <span class="difficulty-badge diff-hard">Hard</span>
    </div>
    <p>Distribution: $p(x_1)=0.5$, $p(x_2)=0.25$, $p(x_3)=0.125$, $p(x_4)=0.125$.</p>
<p>a) What are the optimal code lengths?</p>
<p>b) Design a Huffman code.</p>
<p>c) What is the expected code length (entropy)?</p>
<p>d) How much is wasted compared to a uniform code?</p>
    <details>
        <summary>Show Solution</summary>
        <div class="solution-content">
            <p>a) $L(x_1) = -\log_2(0.5) = 1$, $L(x_2) = 2$, $L(x_3) = 3$, $L(x_4) = 3$ bits.</p>
<p>b) Huffman code: $x_1 \to$ "0", $x_2 \to$ "10", $x_3 \to$ "110", $x_4 \to$ "111". Prefix-free.</p>
<p>c) $H = 0.5(1) + 0.25(2) + 0.125(3) + 0.125(3) = 1.75$ bits/symbol.</p>
<p>d) Uniform code: 2 bits. Waste $= 2 - 1.75 = 0.25$ bits $= 14.3\%$ overhead $= D_{KL}(P\|\text{Uniform}) = \log_2 4 - H(P)$.</p>
        </div>
    </details>
</div>

            </article>
        </main>
    </div>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-brand">
                    <span class="logo-symbol">∇</span>
                    <span>ML Fundamentals</span>
                </div>
                <p class="footer-tagline">Deep understanding through first principles.</p>
            </div>
            <div class="footer-links">
                <a href="../../../index.html">Home</a>
                <a href="https://github.com/ml-entropy/ml-entropy.github.io" target="_blank">GitHub</a>
            </div>
        </div>
    </footer>

    <!-- Scripts -->
    <script src="../../../js/main.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            if (typeof renderMathInElement === 'function') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }
        });
    </script>
</body>
</html>