<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Probability Concepts for ML | ML Fundamentals</title>
    <meta name="description" content="Essential probability theory for machine learning. Master Bayes' theorem, conditional probability, and probabilistic thinking.">
    
    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&family=Playfair+Display:wght@400;500;600;700&display=swap" rel="stylesheet">
    
    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}, {left: '\\\\[', right: '\\\\]', display: true}, {left: '\\\\(', right: '\\\\)', display: false}], throwOnError: false});"></script>
    
    <!-- Styles -->
    <link rel="stylesheet" href="../../../css/main.css">
    <link rel="stylesheet" href="../../../css/components.css">
    
    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>âˆž</text></svg>">
<link rel="stylesheet" href="../../../css/sidebar.css">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar" id="navbar">
        <div class="nav-container">
            <a href="../../../index.html" class="nav-logo">
                <span class="logo-symbol">âˆ‡</span>
                <span class="logo-text">ML Fundamentals</span>
            </a>
            
            <button class="nav-toggle" id="navToggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            
            <div class="nav-menu" id="navMenu">
                <div class="nav-links">
                    <a href="../../../tutorials/ml/index.html" class="nav-link active">Machine Learning</a>
                    <a href="../../../tutorials/linear-algebra/index.html" class="nav-link">Linear Algebra</a>
                    <a href="../../../tutorials/calculus/index.html" class="nav-link">Calculus</a>
                    <a href="../../../tutorials/physics/index.html" class="nav-link">Physics</a>
                    <a href="../../../index.html#philosophy" class="nav-link">Philosophy</a>
                    <a href="../../../index.html#roadmap" class="nav-link">Roadmap</a>
                    <a href="https://github.com/ml-entropy/ml-entropy.github.io" class="nav-link" target="_blank">GitHub</a>
                </div>
                
                <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
                    <svg class="sun-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="5"/>
                        <path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"/>
                    </svg>
                    <svg class="moon-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                    </svg>
                </button>
            </div>
        </div>
    </nav>

    <!-- Tutorial Header -->
    <header class="tutorial-content-header">
        <div class="container">
            <nav class="breadcrumb">
                <a href="../../../index.html">Home</a>
                <span class="breadcrumb-separator">â†’</span>
                <a href="../index.html">Machine Learning</a>
                <span class="breadcrumb-separator">â†’</span>
                <span>Probability Concepts</span>
            </nav>
            
            
            
            
            <div class="tutorial-tabs">
                <a href="#theory" class="tutorial-tab active">Theory</a>
                <a href="#code" class="tutorial-tab">Code</a>
                <a href="#exercises" class="tutorial-tab">Exercises</a>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    
    <!-- Main Content -->
    <div class="tutorial-wrapper">
        
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <aside class="tutorial-sidebar">
            <div class="sidebar-section">
                <h3 class="sidebar-section-title">Machine Learning</h3>
                <nav class="sidebar-nav">
                        <a href="../00-probability/index.html" class="sidebar-link active">00. Probability Foundations</a>
                    <a href="../01-entropy/index.html" class="sidebar-link">01. Entropy Fundamentals</a>
                    <a href="../02-cross-entropy/index.html" class="sidebar-link">02. Cross-Entropy</a>
                    <a href="../02-kl-divergence/index.html" class="sidebar-link">03. KL Divergence</a>
                    <a href="../03-distributions/index.html" class="sidebar-link">04. Normal Distributions</a>
                    <a href="../04-logarithms/index.html" class="sidebar-link">05. Why Logarithms?</a>
                    <a href="../05-combinatorics/index.html" class="sidebar-link">06. Combinatorics</a>
                    <a href="../06-backpropagation/index.html" class="sidebar-link">07. Backpropagation</a>
                    <a href="../07-regularization/index.html" class="sidebar-link">08. Regularization</a>
                    <a href="../08-batch-normalization/index.html" class="sidebar-link">09. Batch Normalization</a>
                    <a href="../09-learning-rate/index.html" class="sidebar-link">10. Learning Rate</a>
                    <a href="../10-cnn/index.html" class="sidebar-link">11. CNNs</a>
                    <a href="../11-rnn/index.html" class="sidebar-link">12. RNNs</a>
                    <a href="../12-vae/index.html" class="sidebar-link">13. VAE</a>
                    <a href="../13-variational-inference/index.html" class="sidebar-link">14. Variational Inference</a>
                    <a href="../14-entropy-connections/index.html" class="sidebar-link">15. Entropy Connections</a>
                </nav>
            </div>
            
            <div class="sidebar-section" style="margin-top: 2rem;">
                <h3 class="sidebar-section-title">Related Subjects</h3>
                <nav class="sidebar-nav">
                        <a href="../../linear-algebra/index.html" class="sidebar-link">Linear Algebra</a>
                    <a href="../../calculus/index.html" class="sidebar-link">Calculus</a>
                    <a href="../../physics/index.html" class="sidebar-link">Physics</a>
                </nav>
            </div>
        </aside>

        <!-- Main Article -->
        <main class="tutorial-main">
            
            <article class="article-content" id="theory">

                <div class="tutorial-footer-summary" style="margin: 0 0 3rem 0; padding: 2rem; background: #f8f9fa; border-radius: 8px; border-left: 4px solid #3b82f6;">
                    <h1>00. Probability Concepts for ML</h1>
                    <p class="lead">
                        Probability is the language of uncertainty. Before diving into ML,
                        we need to master the foundations: random variables, Bayes' theorem,
                        continuous distributions, probability density, and the art of probabilistic reasoning.
                    </p>
                </div>

                <!-- =============================== -->
                <!-- Section 1: Random Variables     -->
                <!-- =============================== -->
                <h2 id="random-variables">Random Variables</h2>

                <p>
                    Imagine you roll a die. Before it lands, the outcome is uncertainâ€”it could be any
                    number from 1 to 6. A <strong>random variable</strong> is simply a way to attach
                    numbers to uncertain outcomes so we can do math with them.
                </p>

                <div class="definition-box">
                    <div class="box-title">Definition: Random Variable</div>
                    <p style="margin-bottom: 0;">
                        A random variable $X: \Omega \to \mathbb{R}$ assigns a numerical value to each
                        outcome in the sample space $\Omega$. We write $P(X = x)$ for the probability
                        that $X$ takes value $x$.
                    </p>
                </div>

                <p>
                    <strong>Example:</strong> You flip two coins. The sample space is
                    $\Omega = \{HH, HT, TH, TT\}$. Define $X$ = "number of heads."
                    Then $X(HH) = 2$, $X(HT) = 1$, $X(TH) = 1$, $X(TT) = 0$, giving us:
                </p>

                <table class="data-table" style="max-width: 400px;">
                    <thead>
                        <tr><th>$x$</th><th>$P(X = x)$</th></tr>
                    </thead>
                    <tbody>
                        <tr><td>0</td><td>$1/4 = 0.25$</td></tr>
                        <tr><td>1</td><td>$2/4 = 0.50$</td></tr>
                        <tr><td>2</td><td>$1/4 = 0.25$</td></tr>
                    </tbody>
                </table>

                <p>
                    Notice: probabilities sum to 1. This is always trueâ€”something <em>must</em> happen.
                </p>

                <p>Random variables come in two fundamental flavors:</p>

                <ul>
                    <li><strong>Discrete:</strong> Takes countable values (coin flips, dice rolls, word counts, pixel classes)</li>
                    <li><strong>Continuous:</strong> Takes values in an interval (height, temperature, neural network weights, pixel intensity)</li>
                </ul>

                <p>
                    This distinction matters enormously. For discrete variables, we can ask
                    "what's the probability of <em>exactly</em> this value?" For continuous variables,
                    that question breaks downâ€”and understanding <em>why</em> is one of the most
                    important insights in probability. We'll tackle this in depth
                    <a href="#continuous-probability">below</a>.
                </p>


                <!-- =============================== -->
                <!-- Section 2: Rules of Probability -->
                <!-- =============================== -->
                <h2 id="probability-rules">The Rules of Probability</h2>

                <p>
                    All of probability theory rests on just a few rules. Master these, and you can
                    derive almost everything else.
                </p>

                <h3>The Sum Rule (Marginalization)</h3>

                <p>
                    Suppose you track two variables: $X$ = "weather" (sunny, rainy) and
                    $Y$ = "mood" (happy, sad). You have the full joint table:
                </p>

                <table class="data-table" style="max-width: 450px;">
                    <thead>
                        <tr><th></th><th>$Y$=Happy</th><th>$Y$=Sad</th><th>$P(X)$</th></tr>
                    </thead>
                    <tbody>
                        <tr><td><strong>$X$=Sunny</strong></td><td>0.40</td><td>0.10</td><td style="background-color: #f0fdf4;"><strong>0.50</strong></td></tr>
                        <tr><td><strong>$X$=Rainy</strong></td><td>0.15</td><td>0.35</td><td style="background-color: #f0fdf4;"><strong>0.50</strong></td></tr>
                        <tr><td>$P(Y)$</td><td style="background-color: #f0fdf4;"><strong>0.55</strong></td><td style="background-color: #f0fdf4;"><strong>0.45</strong></td><td>1.00</td></tr>
                    </tbody>
                </table>

                <p>
                    To find $P(X = \text{Sunny})$, sum across the row: $0.40 + 0.10 = 0.50$.
                    We "marginalize out" $Y$. In general:
                </p>

                <div class="math-block">
                    $$P(X = x) = \sum_{y} P(X = x, Y = y)$$
                </div>

                <div class="note-box">
                    <div class="box-title">Why "marginalize"?</div>
                    <p style="margin-bottom: 0;">
                        The name comes from writing sums in the <em>margins</em> of a tableâ€”exactly like
                        the highlighted column and row above. You collapse away one variable to learn
                        about the other.
                    </p>
                </div>

                <h3>The Product Rule (Chain Rule)</h3>

                <p>
                    Joint probabilities decompose into conditionals. From the table above:
                    $P(\text{Sunny, Happy}) = P(\text{Happy} | \text{Sunny}) \cdot P(\text{Sunny})
                    = \frac{0.40}{0.50} \times 0.50 = 0.80 \times 0.50 = 0.40$. âœ“
                </p>

                <div class="math-block">
                    $$P(X, Y) = P(X | Y) \cdot P(Y) = P(Y | X) \cdot P(X)$$
                </div>

                <div class="note-box">
                    <div class="box-title">Chain Rule Extension</div>
                    <p style="margin-bottom: 0;">
                        For multiple variables:
                        $P(X_1, X_2, \ldots, X_n) = P(X_1) \prod_{i=2}^{n} P(X_i | X_1, \ldots, X_{i-1})$.
                        This is the foundation of autoregressive models (GPT generates text one token at a time,
                        each conditioned on all previous tokens).
                    </p>
                </div>


                <!-- =============================== -->
                <!-- Section 3: Bayes' Theorem       -->
                <!-- =============================== -->
                <h2 id="bayes-theorem">Bayes' Theorem: The Heart of ML</h2>

                <p>
                    From the product rule, we can derive the most important equation in machine learning.
                    The key insight: we can write $P(A, B)$ in two ways and set them equal.
                </p>

                <div class="math-derivation">
                    <div class="math-derivation-title">Deriving Bayes' Theorem</div>

                    <div class="math-step">
                        <div class="math-step-number">1</div>
                        <div class="math-step-content">
                            Start with product rule: $P(A, B) = P(A|B)P(B) = P(B|A)P(A)$
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">2</div>
                        <div class="math-step-content">
                            Equate the two expressions: $P(A|B)P(B) = P(B|A)P(A)$
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">3</div>
                        <div class="math-step-content">
                            Solve for $P(A|B)$:
                            $$\boxed{P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}}$$
                        </div>
                    </div>
                </div>

                <h3>Concrete Example: Medical Testing</h3>

                <p>
                    A disease affects 1% of the population. A test detects the disease 95% of the time
                    (sensitivity), but has a 5% false positive rate. You test positiveâ€”what's the
                    probability you actually have the disease?
                </p>

                <p><strong>Your gut says:</strong> "The test is 95% accurate, so about 95%." <strong>Your gut is wrong.</strong></p>

                <div class="math-derivation">
                    <div class="math-derivation-title">Applying Bayes' Theorem</div>

                    <div class="math-step">
                        <div class="math-step-number">1</div>
                        <div class="math-step-content">
                            <strong>Prior:</strong> $P(\text{Disease}) = 0.01$, $P(\text{Healthy}) = 0.99$
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">2</div>
                        <div class="math-step-content">
                            <strong>Likelihood:</strong> $P(+|\text{Disease}) = 0.95$, $P(+|\text{Healthy}) = 0.05$
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">3</div>
                        <div class="math-step-content">
                            <strong>Evidence:</strong> $P(+) = 0.95 \times 0.01 + 0.05 \times 0.99 = 0.0095 + 0.0495 = 0.059$
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">4</div>
                        <div class="math-step-content">
                            <strong>Posterior:</strong>
                            $P(\text{Disease}|+) = \frac{0.95 \times 0.01}{0.059} \approx 0.161 = 16.1\%$
                        </div>
                    </div>
                </div>

                <div class="warning-box">
                    <div class="box-title">Base Rate Neglect</div>
                    <p style="margin-bottom: 0;">
                        Despite a 95% accurate test, a positive result only means ~16% chance of disease!
                        The low base rate (1%) means most positives are false positives. This is why Bayes'
                        theorem is so importantâ€”it forces you to account for <strong>prior probabilities</strong>,
                        which your intuition often ignores.
                    </p>
                </div>

                <p>In machine learning terms:</p>

                <div class="math-block">
                    $$\underbrace{P(\theta | \mathcal{D})}_{\text{Posterior}} = \frac{\overbrace{P(\mathcal{D} | \theta)}^{\text{Likelihood}} \cdot \overbrace{P(\theta)}^{\text{Prior}}}{\underbrace{P(\mathcal{D})}_{\text{Evidence}}}$$
                </div>

                <ul>
                    <li><strong>Prior $P(\theta)$:</strong> What we believe about model parameters before seeing data (e.g., "weights should be small" â†’ L2 regularization)</li>
                    <li><strong>Likelihood $P(\mathcal{D}|\theta)$:</strong> How probable is the observed data under these parameters</li>
                    <li><strong>Posterior $P(\theta|\mathcal{D})$:</strong> Updated belief after seeing dataâ€”this is what we want</li>
                    <li><strong>Evidence $P(\mathcal{D})$:</strong> Normalizing constant (often intractable, which motivates variational inference)</li>
                </ul>


                <!-- =============================== -->
                <!-- Section 4: Expectation          -->
                <!-- =============================== -->
                <h2 id="expectation">Expectation and Variance</h2>

                <p>
                    The <strong>expected value</strong> answers: "If I repeated this experiment forever,
                    what would the average outcome be?"
                </p>

                <p>
                    <strong>Example:</strong> A game costs $1 to play. You roll a die: if you get a 6,
                    you win $4; otherwise you win nothing. Should you play?
                </p>

                <div class="math-block">
                    $$\mathbb{E}[\text{profit}] = \frac{1}{6}(\$4) + \frac{5}{6}(\$0) - \$1 = \$0.67 - \$1 = -\$0.33$$
                </div>

                <p>
                    On average you lose 33 cents per game. The formal definition:
                </p>

                <div class="math-block">
                    $$\mathbb{E}[X] = \sum_{x} x \cdot P(X = x) \quad \text{(discrete)} \qquad \mathbb{E}[X] = \int x \cdot p(x) \, dx \quad \text{(continuous)}$$
                </div>

                <p>
                    The <strong>variance</strong> measures how spread out the values are around the mean.
                    Two investments might have the same expected return, but very different variancesâ€”that
                    difference is <em>risk</em>.
                </p>

                <div class="math-block">
                    $$\text{Var}(X) = \mathbb{E}[(X - \mathbb{E}[X])^2] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2$$
                </div>

                <div class="definition-box">
                    <div class="box-title">Linearity of Expectation</div>
                    <p style="margin-bottom: 0;">
                        $\mathbb{E}[aX + bY] = a\mathbb{E}[X] + b\mathbb{E}[Y]$, alwaysâ€”even when $X$ and $Y$
                        are dependent! This is one of the most powerful properties in probability.
                        It's why the expected number of heads in 100 coin flips is exactly 50, regardless
                        of any correlations between flips.
                    </p>
                </div>


                <!-- =============================== -->
                <!-- Section 5: Independence         -->
                <!-- =============================== -->
                <h2 id="independence">Independence</h2>

                <p>
                    Two events are <strong>independent</strong> if knowing one tells you nothing about the other:
                </p>

                <div class="math-block">
                    $$P(A \cap B) = P(A) \cdot P(B) \quad \Leftrightarrow \quad P(A|B) = P(A)$$
                </div>

                <p>
                    <strong>Example:</strong> Two dice rolls are independentâ€”knowing the first die landed
                    on 3 tells you nothing about the second die. But "rain today" and "carry umbrella"
                    are <em>not</em> independent.
                </p>

                <p>
                    <strong>Conditional independence</strong> is crucial for graphical models:
                </p>

                <div class="math-block">
                    $$X \perp Y \mid Z \quad \Leftrightarrow \quad P(X, Y \mid Z) = P(X|Z) \cdot P(Y|Z)$$
                </div>

                <p>
                    <strong>Example:</strong> Whether two students get an A ($X$ and $Y$) may be correlated
                    (both depend on exam difficulty). But <em>given</em> the exam difficulty ($Z$), their
                    grades become independentâ€”each student's performance depends only on their own ability.
                </p>

                <div class="warning-box">
                    <div class="box-title">Common Pitfall</div>
                    <p style="margin-bottom: 0;">
                        Independence and conditional independence are different! $X \perp Y$ does NOT imply
                        $X \perp Y \mid Z$, and vice versa. This subtlety is essential for understanding
                        Bayesian networks and Naive Bayes classifiers.
                    </p>
                </div>


                <!-- ============================================ -->
                <!-- Section 6: Continuous Probability & Density  -->
                <!-- ============================================ -->
                <h2 id="continuous-probability">Continuous Probability & Probability Density</h2>

                <p>
                    This is where most beginners get confusedâ€”and where deep understanding
                    separates good ML practitioners from great ones.
                </p>

                <h3 id="discrete-vs-continuous">The Fundamental Problem with Continuous Variables</h3>

                <p>
                    With a fair die, $P(X = 3) = 1/6$. Simple. Now consider a spinner that can land
                    on any real number between 0 and 1. What is $P(X = 0.5)$?
                </p>

                <p>
                    There are <em>uncountably infinitely many</em> points between 0 and 1. If each
                    had positive probability, the sum would be infiniteâ€”violating the axiom that
                    all probabilities must sum to 1. So the probability of any <em>exact</em> point must be:
                </p>

                <div class="math-block">
                    $$P(X = x) = 0 \quad \text{for any specific } x$$
                </div>

                <div class="warning-box">
                    <div class="box-title">This is not a technicalityâ€”it's profound</div>
                    <p style="margin-bottom: 0;">
                        Your height is exactly 170.000000... cm with probability zero. The temperature
                        right now is exactly 22.000000...Â°C with probability zero. Every continuous measurement
                        you've ever taken has probability zero of being exactly what it was.
                        Yet these things happen! This paradox is resolved by probability <em>density</em>.
                    </p>
                </div>

                <h3 id="probability-density">Probability Density Functions (PDFs)</h3>

                <p>
                    Since we can't assign probability to individual points, we assign probability to
                    <strong>intervals</strong>. The <strong>probability density function</strong> $p(x)$
                    tells you how probability is "concentrated" around each point:
                </p>

                <div class="definition-box">
                    <div class="box-title">Definition: Probability Density Function</div>
                    <p>
                        A function $p(x) \geq 0$ is a probability density function if:
                    </p>
                    <div class="math-block">
                        $$\int_{-\infty}^{\infty} p(x) \, dx = 1$$
                    </div>
                    <p style="margin-bottom: 0;">
                        The probability that $X$ falls in interval $[a, b]$ is:
                        $P(a \leq X \leq b) = \int_{a}^{b} p(x) \, dx$
                    </p>
                </div>

                <p>
                    <strong>Key insight:</strong> $p(x)$ is <em>not</em> a probability. It's a <strong>density</strong>â€”probability
                    per unit length. Just like mass density (kg/mÂ³) isn't mass, probability density isn't probability.
                    You need to multiply by a "volume" (integrate over an interval) to get actual probability.
                </p>

                <p>
                    <strong>Analogy:</strong> Think of a long rope with varying thickness. The density at
                    a point tells you how thick the rope is there, but you can't weigh a single point of rope.
                    To get actual weight, you need to integrate density over a length of rope.
                </p>

                <h3 id="density-can-exceed-one">Density Can Exceed 1!</h3>

                <p>
                    Because $p(x)$ is not a probability, it is <em>not</em> bounded by 1. Consider a
                    uniform distribution on $[0, 0.5]$:
                </p>

                <div class="math-block">
                    $$p(x) = \begin{cases} 2 & \text{if } 0 \leq x \leq 0.5 \\ 0 & \text{otherwise} \end{cases}$$
                </div>

                <p>
                    Here $p(x) = 2$ for all $x$ in the interval, and $\int_0^{0.5} 2 \, dx = 1$. âœ“ Perfectly valid.
                    The density is 2, but every interval probability is at most 1.
                </p>

                <div class="note-box">
                    <div class="box-title">Rule of Thumb</div>
                    <p style="margin-bottom: 0;">
                        $p(x) > 1$ simply means probability is concentrated in a small region.
                        A Gaussian with $\sigma = 0.1$ has $p(\mu) \approx 4.0$ at its peak.
                        The narrower the distribution, the taller the density peak.
                    </p>
                </div>


                <!-- ============================================ -->
                <!-- Section 7: p(x) = 0.01 for images           -->
                <!-- ============================================ -->
                <h2 id="density-images">Is $p(x) = 0.01$ High or Low? It Depends.</h2>

                <p>
                    This is one of the most important questions in ML, and the answer reveals a deep truth
                    about high-dimensional probability.
                </p>

                <h3>The 1D Case: A Simple Start</h3>

                <p>
                    If $X$ is a single number (say, a person's height in meters), and $p(x) = 0.01$,
                    what does that tell us? It means: in a tiny interval $[x, x + dx]$, the probability
                    of landing there is approximately $0.01 \cdot dx$.
                </p>

                <p>
                    For a standard Gaussian $\mathcal{N}(0, 1)$, the peak density is $p(0) = \frac{1}{\sqrt{2\pi}} \approx 0.399$.
                    So $p(x) = 0.01$ means $x$ is way out in the tailsâ€”rare. In 1D, $0.01$ is indeed a "low" density.
                </p>

                <h3>The Image Case: Where Intuition Breaks</h3>

                <p>
                    Now suppose $x$ is a $28 \times 28$ grayscale image (like MNIST digits). Each pixel
                    is a continuous value, so $x \in \mathbb{R}^{784}$. The density $p(x)$ now lives in
                    a <strong>784-dimensional space</strong>.
                </p>

                <p>
                    Is $p(x) = 0.01$ high or low for an image? <strong>It's actually extremely high.</strong>
                </p>

                <div class="definition-box">
                    <div class="box-title">Why? The Curse of Dimensionality</div>
                    <p>
                        In $d$ dimensions, the density at a point tells you the probability per unit
                        <em>hypervolume</em>. To get actual probability, you integrate over some
                        $d$-dimensional region:
                    </p>
                    <div class="math-block">
                        $$P(x \in R) = \int_R p(x) \, dx_1 \, dx_2 \cdots dx_d$$
                    </div>
                    <p style="margin-bottom: 0;">
                        Even a tiny interval $[x_i - \epsilon, x_i + \epsilon]$ per dimension creates
                        a hypervolume of $(2\epsilon)^d$. For $d = 784$ and $\epsilon = 0.01$,
                        that's $(0.02)^{784} \approx 10^{-1332}$. The volume is unimaginably small.
                    </p>
                </div>

                <p>
                    Let's build intuition step by step. Consider a standard Gaussian $\mathcal{N}(0, I)$ in $d$ dimensions.
                    The peak density (at the origin) is:
                </p>

                <div class="math-block">
                    $$p(0) = \frac{1}{(2\pi)^{d/2}} = \frac{1}{(2\pi)^{392}} \approx 10^{-362}$$
                </div>

                <p>
                    For a 784-dimensional Gaussian, even the <em>most likely</em> point has a density of $10^{-362}$.
                    So $p(x) = 0.01 = 10^{-2}$ would be <strong>about 10<sup>360</sup> times larger than
                    the peak of a standard Gaussian</strong>â€”meaning the distribution is incredibly
                    concentrated around that image. That's a model that is extremely confident about
                    what images look like.
                </p>

                <h3>A More Practical Perspective</h3>

                <p>
                    In practice, when ML researchers say "$p(x)$ is high," they usually mean relative to
                    other images under the same model. What matters is the <strong>log-likelihood</strong>:
                </p>

                <div class="math-block">
                    $$\log p(x) = -2 \quad \text{vs.} \quad \log p(x') = -500$$
                </div>

                <p>
                    Image $x$ is far more "likely" (typical) under the model than $x'$. The absolute value
                    of $p(x)$ in high dimensions is essentially meaninglessâ€”only <strong>comparisons</strong> matter.
                </p>

                <div class="note-box">
                    <div class="box-title">Bits Per Dimension</div>
                    <p style="margin-bottom: 0;">
                        To make log-likelihoods comparable across different image sizes, researchers often
                        report <strong>bits per dimension</strong>: $\frac{-\log_2 p(x)}{d}$. This normalizes
                        by the number of dimensions, giving you the average number of bits needed to encode
                        each pixel. Lower is better. State-of-the-art generative models achieve ~3 bits/dim
                        on natural images.
                    </p>
                </div>

                <h3>The Typical Set: Where Probability Actually Lives</h3>

                <p>
                    Here's a mind-bending fact: in high dimensions, the most probable <em>point</em> (the mode)
                    is <em>not</em> where most of the probability mass lives. This seems contradictory, but
                    the explanation is geometric.
                </p>

                <p>
                    For a $d$-dimensional Gaussian, the mode is at the origin, where density is highest.
                    But the origin is a single point, while there are exponentially more points at distance
                    $\sqrt{d}$ from the origin. The thin "shell" at radius $\approx \sqrt{d}$ has enormous
                    surface areaâ€”enough to dominate despite having lower density per point. This shell
                    is called the <strong>typical set</strong>.
                </p>

                <div class="warning-box">
                    <div class="box-title">Critical for Generative Models</div>
                    <p style="margin-bottom: 0;">
                        When sampling images from a VAE or diffusion model, you should sample from the
                        <em>typical set</em>, not the mode. Sampling from the mode (maximum density point)
                        often gives blurry, "average" images. This is why truncation tricks and temperature
                        scaling existâ€”they adjust <em>where</em> in the distribution you sample from.
                    </p>
                </div>

                <h3>Summary: Interpreting Density Values</h3>

                <table class="data-table">
                    <thead>
                        <tr><th>Dimension</th><th>Peak of $\mathcal{N}(0,I)$</th><th>Is $p(x)=0.01$ high?</th></tr>
                    </thead>
                    <tbody>
                        <tr><td>$d = 1$</td><td>$\approx 0.40$</td><td>No â€” in the tails</td></tr>
                        <tr><td>$d = 10$</td><td>$\approx 1.6 \times 10^{-4}$</td><td>Very high â€” 60Ã— above peak</td></tr>
                        <tr><td>$d = 100$</td><td>$\approx 10^{-40}$</td><td>Astronomically high</td></tr>
                        <tr><td>$d = 784$ (MNIST)</td><td>$\approx 10^{-362}$</td><td>Absurdly high â€” model is incredibly concentrated</td></tr>
                    </tbody>
                </table>

                <div class="definition-box">
                    <div class="box-title">The Takeaway</div>
                    <p style="margin-bottom: 0;">
                        <strong>Never interpret density values without considering dimensionality.</strong>
                        In 1D, $p(x) = 0.01$ is low. In 784D, it's astronomically high. Always compare
                        densities to other points under the same model and in the same space.
                        When in doubt, use log-likelihoods and bits-per-dimension.
                    </p>
                </div>


                <!-- =============================== -->
                <!-- Section 8: Exercises            -->
                <!-- =============================== -->
                <h2 id="exercises">Exercises</h2>

                <!-- Exercise 1 -->
                <div class="exercise-card" style="margin-bottom: 1.5rem; padding: 1.5rem; border: 1px solid var(--color-border, #e5e7eb); border-radius: 8px;">
                    <div class="exercise-header" style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 1rem;">
                        <h3 style="margin: 0;">1. Joint Probability Table</h3>
                        <span class="difficulty-badge diff-easy" style="background: #dcfce7; color: #166534; padding: 2px 10px; border-radius: 12px; font-size: 0.85rem;">ðŸŸ¢ Easy</span>
                    </div>
                    <p>
                        A survey of 200 students found: 80 study math, 60 study physics, and 30 study both.
                        A student is chosen at random.
                    </p>
                    <p>
                        <strong>a)</strong> Construct the full joint probability table for $X$ = "studies math" (yes/no)
                        and $Y$ = "studies physics" (yes/no).<br>
                        <strong>b)</strong> Find $P(\text{math} | \text{physics})$.<br>
                        <strong>c)</strong> Are studying math and physics independent?
                    </p>
                    <details style="margin-top: 1rem;">
                        <summary style="cursor: pointer; color: var(--color-primary, #3b82f6); font-weight: 500;">Show Solution</summary>
                        <div class="solution-content" style="margin-top: 1rem; padding-top: 1rem; border-top: 1px dashed var(--color-border, #e5e7eb);">
                            <p><strong>a)</strong> From the data: Mathâˆ©Physics = 30, Mathâˆ©Â¬Physics = 50, Â¬Mathâˆ©Physics = 30, Â¬Mathâˆ©Â¬Physics = 90. Dividing by 200:</p>
                            <table class="data-table" style="max-width: 400px;">
                                <thead><tr><th></th><th>Physics</th><th>Â¬Physics</th><th>Total</th></tr></thead>
                                <tbody>
                                    <tr><td><strong>Math</strong></td><td>0.15</td><td>0.25</td><td>0.40</td></tr>
                                    <tr><td><strong>Â¬Math</strong></td><td>0.15</td><td>0.45</td><td>0.60</td></tr>
                                    <tr><td><strong>Total</strong></td><td>0.30</td><td>0.70</td><td>1.00</td></tr>
                                </tbody>
                            </table>
                            <p><strong>b)</strong> $P(\text{math} | \text{physics}) = \frac{P(\text{math} \cap \text{physics})}{P(\text{physics})} = \frac{0.15}{0.30} = 0.50$</p>
                            <p><strong>c)</strong> Independence requires $P(\text{math} \cap \text{physics}) = P(\text{math}) \cdot P(\text{physics})$. We have $0.15$ vs $0.40 \times 0.30 = 0.12$. Since $0.15 \neq 0.12$, they are <strong>not independent</strong>. Studying physics makes studying math more likely (positive correlation).</p>
                        </div>
                    </details>
                </div>

                <!-- Exercise 2 -->
                <div class="exercise-card" style="margin-bottom: 1.5rem; padding: 1.5rem; border: 1px solid var(--color-border, #e5e7eb); border-radius: 8px;">
                    <div class="exercise-header" style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 1rem;">
                        <h3 style="margin: 0;">2. Bayes in Practice: Spam Filter</h3>
                        <span class="difficulty-badge diff-medium" style="background: #fef9c3; color: #854d0e; padding: 2px 10px; border-radius: 12px; font-size: 0.85rem;">ðŸŸ¡ Medium</span>
                    </div>
                    <p>
                        An email spam filter works as follows: 20% of all emails are spam. The word "free"
                        appears in 80% of spam emails and 10% of legitimate emails.
                    </p>
                    <p>
                        <strong>a)</strong> An email contains "free." What's the probability it's spam?<br>
                        <strong>b)</strong> An email does NOT contain "free." What's the probability it's spam?<br>
                        <strong>c)</strong> How does the answer to (a) change if 50% of emails are spam?
                    </p>
                    <details style="margin-top: 1rem;">
                        <summary style="cursor: pointer; color: var(--color-primary, #3b82f6); font-weight: 500;">Show Solution</summary>
                        <div class="solution-content" style="margin-top: 1rem; padding-top: 1rem; border-top: 1px dashed var(--color-border, #e5e7eb);">
                            <p><strong>a)</strong> Using Bayes' theorem:</p>
                            <div class="math-block">
                                $$P(\text{spam}|\text{free}) = \frac{P(\text{free}|\text{spam}) \cdot P(\text{spam})}{P(\text{free})} = \frac{0.80 \times 0.20}{0.80 \times 0.20 + 0.10 \times 0.80} = \frac{0.16}{0.16 + 0.08} = \frac{0.16}{0.24} \approx 0.667$$
                            </div>
                            <p><strong>b)</strong> $P(\text{spam}|\text{Â¬free}) = \frac{P(\text{Â¬free}|\text{spam}) \cdot P(\text{spam})}{P(\text{Â¬free})} = \frac{0.20 \times 0.20}{0.20 \times 0.20 + 0.90 \times 0.80} = \frac{0.04}{0.04 + 0.72} = \frac{0.04}{0.76} \approx 0.053$</p>
                            <p><strong>c)</strong> With $P(\text{spam}) = 0.50$: $P(\text{spam}|\text{free}) = \frac{0.80 \times 0.50}{0.80 \times 0.50 + 0.10 \times 0.50} = \frac{0.40}{0.45} \approx 0.889$. The higher prior dramatically increases the posteriorâ€”this shows how prior beliefs shape conclusions.</p>
                        </div>
                    </details>
                </div>

                <!-- Exercise 3 -->
                <div class="exercise-card" style="margin-bottom: 1.5rem; padding: 1.5rem; border: 1px solid var(--color-border, #e5e7eb); border-radius: 8px;">
                    <div class="exercise-header" style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 1rem;">
                        <h3 style="margin: 0;">3. Density vs. Probability</h3>
                        <span class="difficulty-badge diff-medium" style="background: #fef9c3; color: #854d0e; padding: 2px 10px; border-radius: 12px; font-size: 0.85rem;">ðŸŸ¡ Medium</span>
                    </div>
                    <p>
                        A continuous random variable $X$ has the following PDF:
                    </p>
                    <div class="math-block">
                        $$p(x) = \begin{cases} 3x^2 & \text{if } 0 \leq x \leq 1 \\ 0 & \text{otherwise} \end{cases}$$
                    </div>
                    <p>
                        <strong>a)</strong> Verify this is a valid PDF (integrates to 1).<br>
                        <strong>b)</strong> Find $P(0.5 \leq X \leq 1)$.<br>
                        <strong>c)</strong> Find $\mathbb{E}[X]$ and $\text{Var}(X)$.<br>
                        <strong>d)</strong> The density at $x = 1$ is $p(1) = 3$. Is the probability of $X = 1$ equal to 3? Explain.
                    </p>
                    <details style="margin-top: 1rem;">
                        <summary style="cursor: pointer; color: var(--color-primary, #3b82f6); font-weight: 500;">Show Solution</summary>
                        <div class="solution-content" style="margin-top: 1rem; padding-top: 1rem; border-top: 1px dashed var(--color-border, #e5e7eb);">
                            <p><strong>a)</strong> $\int_0^1 3x^2 \, dx = [x^3]_0^1 = 1 - 0 = 1$ âœ“</p>
                            <p><strong>b)</strong> $P(0.5 \leq X \leq 1) = \int_{0.5}^{1} 3x^2 \, dx = [x^3]_{0.5}^{1} = 1 - 0.125 = 0.875$</p>
                            <p>So 87.5% of the probability mass is in the upper half of $[0, 1]$â€”the distribution is skewed right.</p>
                            <p><strong>c)</strong> $\mathbb{E}[X] = \int_0^1 x \cdot 3x^2 \, dx = 3\int_0^1 x^3 \, dx = 3 \cdot \frac{1}{4} = 0.75$</p>
                            <p>$\mathbb{E}[X^2] = \int_0^1 x^2 \cdot 3x^2 \, dx = 3 \cdot \frac{1}{5} = 0.6$</p>
                            <p>$\text{Var}(X) = 0.6 - 0.75^2 = 0.6 - 0.5625 = 0.0375$</p>
                            <p><strong>d)</strong> No! $P(X = 1) = 0$ for any continuous random variable. The value $p(1) = 3$ is a <em>density</em>, not a probability. It means probability is highly concentrated near $x = 1$, but the probability of the exact point $x = 1$ is zero. You can only get probabilities by integrating density over an interval.</p>
                        </div>
                    </details>
                </div>

                <!-- Exercise 4 -->
                <div class="exercise-card" style="margin-bottom: 1.5rem; padding: 1.5rem; border: 1px solid var(--color-border, #e5e7eb); border-radius: 8px;">
                    <div class="exercise-header" style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 1rem;">
                        <h3 style="margin: 0;">4. High-Dimensional Density Intuition</h3>
                        <span class="difficulty-badge diff-hard" style="background: #fee2e2; color: #991b1b; padding: 2px 10px; border-radius: 12px; font-size: 0.85rem;">ðŸ”´ Hard</span>
                    </div>
                    <p>
                        Consider a standard multivariate Gaussian $\mathcal{N}(0, I)$ in $d$ dimensions.
                    </p>
                    <p>
                        <strong>a)</strong> Write the formula for the peak density $p(0)$ as a function of $d$. Compute it for $d = 1, 10, 100$.<br>
                        <strong>b)</strong> If a generative model over $28 \times 28$ images reports $\log p(x) = -800$, what is $p(x)$?
                            Is this high or low relative to a standard Gaussian in the same space?<br>
                        <strong>c)</strong> Explain why sampling from the mode of a high-dimensional Gaussian gives an
                            atypical sample. Where does a typical sample lie?
                    </p>
                    <details style="margin-top: 1rem;">
                        <summary style="cursor: pointer; color: var(--color-primary, #3b82f6); font-weight: 500;">Show Solution</summary>
                        <div class="solution-content" style="margin-top: 1rem; padding-top: 1rem; border-top: 1px dashed var(--color-border, #e5e7eb);">
                            <p><strong>a)</strong> The peak of the $d$-dimensional standard Gaussian is:</p>
                            <div class="math-block">$$p(0) = \frac{1}{(2\pi)^{d/2}}$$</div>
                            <ul>
                                <li>$d = 1$: $p(0) = \frac{1}{\sqrt{2\pi}} \approx 0.399$</li>
                                <li>$d = 10$: $p(0) = \frac{1}{(2\pi)^5} \approx 1.02 \times 10^{-4}$</li>
                                <li>$d = 100$: $p(0) = \frac{1}{(2\pi)^{50}} \approx 5.2 \times 10^{-40}$</li>
                            </ul>
                            <p><strong>b)</strong> $p(x) = e^{-800} \approx 10^{-347}$. The peak of a standard Gaussian in $d = 784$ is $p(0) = (2\pi)^{-392} \approx 10^{-362}$. So $\log p(x) = -800$ gives a density about $10^{15}$ times <em>higher</em> than the Gaussian peakâ€”meaning this model is much more concentrated than a standard Gaussian. This is <strong>high</strong>.</p>
                            <p><strong>c)</strong> The mode (origin) has the highest density per point, but there's only one such point. At distance $r = \sqrt{d}$, the density per point is lower, but the surface area of the sphere scales as $r^{d-1}$â€”exponentially more points. The total probability mass at radius $\sqrt{d}$ vastly exceeds the mass near the origin. So a typical sample from a 784-dimensional Gaussian has $\|x\| \approx \sqrt{784} = 28$, not $\|x\| \approx 0$. This is the <strong>typical set</strong> â€” the "shell" where volume and density jointly maximize probability mass.</p>
                        </div>
                    </details>
                </div>

                <!-- Exercise 5 -->
                <div class="exercise-card" style="margin-bottom: 1.5rem; padding: 1.5rem; border: 1px solid var(--color-border, #e5e7eb); border-radius: 8px;">
                    <div class="exercise-header" style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 1rem;">
                        <h3 style="margin: 0;">5. From Prior to Posterior</h3>
                        <span class="difficulty-badge diff-medium" style="background: #fef9c3; color: #854d0e; padding: 2px 10px; border-radius: 12px; font-size: 0.85rem;">ðŸŸ¡ Medium</span>
                    </div>
                    <p>
                        You believe a coin is fair: your prior is $P(\text{heads}) = \theta$, where
                        $\theta$ is uniformly distributed on $[0, 1]$ (maximum ignorance).
                        You flip the coin 10 times and get 7 heads.
                    </p>
                    <p>
                        <strong>a)</strong> Write down the likelihood $P(\text{7 heads in 10}|\theta)$ as a function of $\theta$.<br>
                        <strong>b)</strong> The posterior is proportional to prior Ã— likelihood. Write $p(\theta | \text{data}) \propto \text{what?}$<br>
                        <strong>c)</strong> The posterior turns out to be a Beta(8, 4) distribution with mean $\frac{8}{12} \approx 0.667$.
                            Why isn't the posterior mean exactly $0.70$ (= 7/10)?
                    </p>
                    <details style="margin-top: 1rem;">
                        <summary style="cursor: pointer; color: var(--color-primary, #3b82f6); font-weight: 500;">Show Solution</summary>
                        <div class="solution-content" style="margin-top: 1rem; padding-top: 1rem; border-top: 1px dashed var(--color-border, #e5e7eb);">
                            <p><strong>a)</strong> By the binomial distribution:</p>
                            <div class="math-block">$$P(\text{7H in 10}|\theta) = \binom{10}{7} \theta^7 (1 - \theta)^3 = 120 \, \theta^7(1 - \theta)^3$$</div>
                            <p><strong>b)</strong> With a uniform prior ($p(\theta) = 1$ for $\theta \in [0,1]$):</p>
                            <div class="math-block">$$p(\theta | \text{data}) \propto p(\text{data}|\theta) \cdot p(\theta) = \theta^7(1 - \theta)^3 \cdot 1 = \theta^7(1 - \theta)^3$$</div>
                            <p>This is the kernel of a Beta(8, 4) distribution.</p>
                            <p><strong>c)</strong> The posterior mean is $\frac{\alpha}{\alpha + \beta} = \frac{7 + 1}{7 + 1 + 3 + 1} = \frac{8}{12} \approx 0.667$. The uniform prior is equivalent to having seen 1 "phantom" head and 1 "phantom" tail before the experiment. This pulls the estimate toward 0.5 (the prior mean). This is <strong>shrinkage</strong>â€”Bayesian estimates are always a compromise between prior and data. With more data, the likelihood dominates and the posterior approaches the MLE (0.70).</p>
                        </div>
                    </details>
                </div>

                <!-- Exercise 6 -->
                <div class="exercise-card" style="margin-bottom: 1.5rem; padding: 1.5rem; border: 1px solid var(--color-border, #e5e7eb); border-radius: 8px;">
                    <div class="exercise-header" style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 1rem;">
                        <h3 style="margin: 0;">6. Marginalization in Practice</h3>
                        <span class="difficulty-badge diff-easy" style="background: #dcfce7; color: #166534; padding: 2px 10px; border-radius: 12px; font-size: 0.85rem;">ðŸŸ¢ Easy</span>
                    </div>
                    <p>
                        A classifier outputs $P(\text{cat}|\text{image}) = 0.7$ and $P(\text{dog}|\text{image}) = 0.3$.
                        Your model also predicts color: $P(\text{orange}|\text{cat}) = 0.6$, $P(\text{orange}|\text{dog}) = 0.1$.
                    </p>
                    <p>
                        <strong>a)</strong> What is $P(\text{orange}|\text{image})$?<br>
                        <strong>b)</strong> Given the animal IS orange, what's the probability it's a cat?
                    </p>
                    <details style="margin-top: 1rem;">
                        <summary style="cursor: pointer; color: var(--color-primary, #3b82f6); font-weight: 500;">Show Solution</summary>
                        <div class="solution-content" style="margin-top: 1rem; padding-top: 1rem; border-top: 1px dashed var(--color-border, #e5e7eb);">
                            <p><strong>a)</strong> Marginalize over animal type:</p>
                            <div class="math-block">$$P(\text{orange}|\text{img}) = P(\text{orange}|\text{cat})P(\text{cat}|\text{img}) + P(\text{orange}|\text{dog})P(\text{dog}|\text{img}) = 0.6 \times 0.7 + 0.1 \times 0.3 = 0.42 + 0.03 = 0.45$$</div>
                            <p><strong>b)</strong> By Bayes: $P(\text{cat}|\text{orange, img}) = \frac{P(\text{orange}|\text{cat})P(\text{cat}|\text{img})}{P(\text{orange}|\text{img})} = \frac{0.42}{0.45} \approx 0.933$. Orange strongly suggests cat!</p>
                        </div>
                    </details>
                </div>

                <!-- Exercise 7 -->
                <div class="exercise-card" style="margin-bottom: 1.5rem; padding: 1.5rem; border: 1px solid var(--color-border, #e5e7eb); border-radius: 8px;">
                    <div class="exercise-header" style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 1rem;">
                        <h3 style="margin: 0;">7. The Change of Variables Trap</h3>
                        <span class="difficulty-badge diff-hard" style="background: #fee2e2; color: #991b1b; padding: 2px 10px; border-radius: 12px; font-size: 0.85rem;">ðŸ”´ Hard</span>
                    </div>
                    <p>
                        Let $X \sim \mathcal{N}(0, 1)$ and define $Y = X^2$.
                    </p>
                    <p>
                        <strong>a)</strong> Is $P(X = 1) = P(Y = 1)$? Explain carefully.<br>
                        <strong>b)</strong> Is $p_X(1) = p_Y(1)$? (Compare the density functions.)<br>
                        <strong>c)</strong> Explain in words why the density of a transformed variable isn't just $p_X$ evaluated at the
                            back-transformed point. What extra factor appears, and why?
                    </p>
                    <details style="margin-top: 1rem;">
                        <summary style="cursor: pointer; color: var(--color-primary, #3b82f6); font-weight: 500;">Show Solution</summary>
                        <div class="solution-content" style="margin-top: 1rem; padding-top: 1rem; border-top: 1px dashed var(--color-border, #e5e7eb);">
                            <p><strong>a)</strong> Yes: $P(X = 1) = P(Y = 1) = 0$. Both are continuous random variables, so the probability of any exact value is zero.</p>
                            <p><strong>b)</strong> No! $p_X(1) = \frac{1}{\sqrt{2\pi}}e^{-1/2} \approx 0.242$. For $Y = X^2$, $Y = 1$ means $X = 1$ or $X = -1$, and the density of $Y$ at $y = 1$ is:
                            </p>
                            <div class="math-block">$$p_Y(1) = \frac{p_X(1)}{|2 \cdot 1|} + \frac{p_X(-1)}{|2 \cdot (-1)|} = \frac{0.242}{2} + \frac{0.242}{2} = 0.242$$</div>
                            <p>(Coincidentally equal in this case, but for a different reason!)</p>
                            <p><strong>c)</strong> When you transform $X$ to $Y = g(X)$, the density changes because the transformation stretches or compresses the $x$-axis. A small interval $dx$ around $x$ maps to an interval $dy = |g'(x)| \, dx$ around $y$. Since probability must be preserved ($p_Y(y) \, dy = p_X(x) \, dx$), we get $p_Y(y) = p_X(x) / |g'(x)|$. The Jacobian factor $|g'(x)|^{-1}$ corrects for how much the transformation stretches space. This is the <strong>change of variables formula</strong>â€”essential for normalizing flows and reparameterization tricks in VAEs.</p>
                        </div>
                    </details>
                </div>

                <!-- Navigation -->
                <div class="tutorial-nav">
                    <div class="tutorial-nav-link prev disabled">
                        <span class="nav-label">Previous</span>
                        <span class="nav-title">â† Start of Series</span>
                    </div>
                    <a href="../01-entropy/index.html" class="tutorial-nav-link next">
                        <span class="nav-label">Next</span>
                        <span class="nav-title">Information Entropy â†’</span>
                    </a>
                </div>
                
            </article>
        
        </main>

        <!-- TOC (Right Side) -->
        <aside class="toc-container">
        <h4 class="toc-title">Contents</h4>
        <nav class="toc-list">
            <a href="#random-variables" class="toc-link">Random Variables</a>
            <a href="#probability-rules" class="toc-link">Rules of Probability</a>
            <a href="#bayes-theorem" class="toc-link">Bayes' Theorem</a>
            <a href="#expectation" class="toc-link">Expectation & Variance</a>
            <a href="#independence" class="toc-link">Independence</a>
            <a href="#continuous-probability" class="toc-link">Continuous Probability</a>
            <a href="#density-images" class="toc-link">p(x) = 0.01 for Images?</a>
            <a href="#exercises" class="toc-link">Exercises</a>
        </nav>
    </aside>
    </div>
    

    <!-- Table of Contents (floating) -->
    

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-brand">
                    <span class="logo-symbol">âˆ‡</span>
                    <span>ML Fundamentals</span>
                </div>
                <p class="footer-tagline">Deep understanding through first principles.</p>
            </div>
            <div class="footer-links">
                <a href="../../../index.html">Home</a>
                <a href="https://github.com/ml-entropy/ml-entropy.github.io" target="_blank">GitHub</a>
            </div>
        </div>
    </footer>

    <!-- Scripts -->
    <script src="../../../js/main.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            if (typeof renderMathInElement === 'function') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }
        });
    </script>
</body>
</html>
