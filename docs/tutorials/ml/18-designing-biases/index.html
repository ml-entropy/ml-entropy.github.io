<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Designing Inductive Biases | ML Fundamentals</title>
    <meta name="description" content="How to choose, design, and evaluate inductive biases: regularization, data augmentation, transfer learning, and the scaling debate.">

    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&family=Playfair+Display:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}, {left: '\\\\[', right: '\\\\]', display: true}, {left: '\\\\(', right: '\\\\)', display: false}], throwOnError: false});"></script>

    <!-- Styles -->
    <link rel="stylesheet" href="../../../css/main.css">
    <link rel="stylesheet" href="../../../css/components.css">
    <link rel="stylesheet" href="../../../css/sidebar.css">

    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>&#x221E;</text></svg>">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar" id="navbar">
        <div class="nav-container">
            <a href="../../../index.html" class="nav-logo">
                <span class="logo-symbol">&nabla;</span>
                <span class="logo-text">ML Fundamentals</span>
            </a>

            <button class="nav-toggle" id="navToggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>

            <div class="nav-menu" id="navMenu">
                <div class="nav-links">
                    <a href="../../../tutorials/ml/index.html" class="nav-link active">Machine Learning</a>
                    <a href="../../../tutorials/linear-algebra/index.html" class="nav-link">Linear Algebra</a>
                    <a href="../../../tutorials/calculus/index.html" class="nav-link">Calculus</a>
                    <a href="../../../tutorials/physics/index.html" class="nav-link">Physics</a>
                    <a href="../../../index.html#philosophy" class="nav-link">Philosophy</a>
                    <a href="../../../index.html#roadmap" class="nav-link">Roadmap</a>
                    <a href="https://github.com/ml-entropy/ml-entropy.github.io" class="nav-link" target="_blank">GitHub</a>
                </div>

                <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
                    <svg class="sun-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="5"/>
                        <path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"/>
                    </svg>
                    <svg class="moon-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                    </svg>
                </button>
            </div>
        </div>
    </nav>

    <!-- Tutorial Header -->
    <header class="tutorial-content-header">
        <div class="container">
            <nav class="breadcrumb">
                <a href="../../../index.html">Home</a>
                <span class="breadcrumb-separator">&rarr;</span>
                <a href="../index.html">Machine Learning</a>
                <span class="breadcrumb-separator">&rarr;</span>
                <span>Designing Biases</span>
            </nav>

            <div class="tutorial-tabs">
                <a href="#theory" class="tutorial-tab active">Theory</a>
                <a href="#code" class="tutorial-tab">Code</a>
                <a href="#exercises" class="tutorial-tab">Exercises</a>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <div class="tutorial-wrapper">

        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <aside class="tutorial-sidebar">
            <div class="sidebar-section">
                <h3 class="sidebar-section-title">Machine Learning</h3>
                <nav class="sidebar-nav">
                        <a href="../00-probability/index.html" class="sidebar-link">00. Probability Foundations</a>
                    <a href="../04-logarithms/index.html" class="sidebar-link">01. Why Logarithms?</a>
                    <a href="../05-combinatorics/index.html" class="sidebar-link">02. Combinatorics</a>
                    <a href="../03-distributions/index.html" class="sidebar-link">03. Normal Distributions</a>
                    <a href="../01-entropy/index.html" class="sidebar-link">04. Entropy Fundamentals</a>
                    <a href="../02-cross-entropy/index.html" class="sidebar-link">05. Cross-Entropy</a>
                    <a href="../02-kl-divergence/index.html" class="sidebar-link">06. KL Divergence</a>
                    <a href="../14-entropy-connections/index.html" class="sidebar-link">07. Entropy Connections</a>
                    <a href="../06-backpropagation/index.html" class="sidebar-link">08. Backpropagation</a>
                    <a href="../07-regularization/index.html" class="sidebar-link">09. Regularization</a>
                    <a href="../08-batch-normalization/index.html" class="sidebar-link">10. Batch Normalization</a>
                    <a href="../09-learning-rate/index.html" class="sidebar-link">11. Learning Rate</a>
                    <a href="../10-cnn/index.html" class="sidebar-link">12. CNNs</a>
                    <a href="../11-rnn/index.html" class="sidebar-link">13. RNNs</a>
                    <a href="../15-autoencoder/index.html" class="sidebar-link">14. Autoencoders</a>
                    <a href="../13-variational-inference/index.html" class="sidebar-link">15. Variational Inference</a>
                    <a href="../12-vae/index.html" class="sidebar-link">16. VAE</a>
                    <a href="../16-inductive-bias/index.html" class="sidebar-link">17. Inductive Bias</a>
                    <a href="../17-architectural-biases/index.html" class="sidebar-link">18. Architectural Biases</a>
                    <a href="../18-designing-biases/index.html" class="sidebar-link active">19. Designing Biases</a>
                    <a href="../19-fst-fundamentals/index.html" class="sidebar-link">20. FST Fundamentals</a>
                    <a href="../20-weighted-fsts/index.html" class="sidebar-link">21. Weighted FSTs</a>
                    <a href="../21-fst-libraries/index.html" class="sidebar-link">22. FST Libraries</a>
                    <a href="../22-fst-applications/index.html" class="sidebar-link">23. FST Applications</a>
                    <a href="../23-neural-symbolic/index.html" class="sidebar-link">24. Neural-Symbolic Hybrids</a>
                    <a href="../24-sequence-alignment/index.html" class="sidebar-link">25. Sequence Alignment</a>
                    <a href="../25-mas-algorithm/index.html" class="sidebar-link">26. MAS Algorithm</a>
                    <a href="../26-forced-alignment/index.html" class="sidebar-link">27. Forced Alignment & MFA</a>
                </nav>
            </div>
            
            <div class="sidebar-section" style="margin-top: 2rem;">
                <h3 class="sidebar-section-title">Related Subjects</h3>
                <nav class="sidebar-nav">
                        <a href="../../linear-algebra/index.html" class="sidebar-link">Linear Algebra</a>
                    <a href="../../calculus/index.html" class="sidebar-link">Calculus</a>
                    <a href="../../physics/index.html" class="sidebar-link">Physics</a>
                </nav>
            </div>
        </aside>

        <!-- Main Article -->
        <main class="tutorial-main">

            <!-- ==================== THEORY TAB ==================== -->
            <article class="article-content" id="theory">

                <!-- Section 1: The Practitioner's Dilemma -->
                <h2 id="practitioners-dilemma">The Practitioner's Dilemma</h2>

                <p>
                    You've built a model and it's not working. Before tuning hyperparameters or gathering more data, ask: "Does my model's inductive bias match the structure of my data?" This single question is often more important than any amount of hyperparameter search.
                </p>

                <div class="note-box">
                    <div class="box-title">The Fundamental Question</div>
                    <p style="margin-bottom: 0;">
                        Every ML design choice is an answer to: "What structure does my data have?" The right bias turns an impossible learning problem into a tractable one. The wrong bias turns a simple problem into an impossible one.
                    </p>
                </div>

                <p>
                    Inductive bias can take many forms: architecture choice (Tutorial 18), regularization, data augmentation, pre-training, and custom constraints. This tutorial covers the practical tools beyond architecture &mdash; the techniques you can combine, tune, and design to match any problem.
                </p>

                <!-- Section 2: Regularization as Inductive Bias -->
                <h2 id="regularization-as-bias">Regularization as Inductive Bias</h2>

                <p>
                    Regularization adds a preference bias without changing the hypothesis space. It tells the optimizer: "among all functions that fit the data, prefer these ones." This is a soft constraint &mdash; the model <em>can</em> learn any function, but it is <em>encouraged</em> to learn certain kinds.
                </p>

                <div class="math-derivation">
                    <div class="math-derivation-title">Bayesian Interpretation of Regularization</div>

                    <div class="math-step">
                        <div class="math-step-number">1</div>
                        <div class="math-step-content">
                            <p>MAP estimation:</p>
                            <div class="math-block">
                                $$\theta^* = \arg\max_\theta P(\theta|D) = \arg\max_\theta P(D|\theta)P(\theta)$$
                            </div>
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">2</div>
                        <div class="math-step-content">
                            <p>Taking negative log:</p>
                            <div class="math-block">
                                $$\theta^* = \arg\min_\theta [-\log P(D|\theta) - \log P(\theta)]$$
                            </div>
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">3</div>
                        <div class="math-step-content">
                            <p>If prior $P(\theta) = \mathcal{N}(0, \sigma^2 I)$ (Gaussian): $-\log P(\theta) \propto \|\theta\|_2^2$. This gives <strong>L2 regularization</strong>.</p>
                        </div>
                    </div>

                    <div class="math-step">
                        <div class="math-step-number">4</div>
                        <div class="math-step-content">
                            <p>If prior $P(\theta) = \text{Laplace}(0, b)$: $-\log P(\theta) \propto \|\theta\|_1$. This gives <strong>L1 regularization</strong>.</p>
                        </div>
                    </div>
                </div>

                <table style="width: 100%; border-collapse: collapse; margin: 1.5rem 0;">
                    <thead>
                        <tr style="border-bottom: 2px solid var(--color-border);">
                            <th style="text-align: left; padding: 0.75rem;">Regularizer</th>
                            <th style="text-align: left; padding: 0.75rem;">Inductive Bias</th>
                            <th style="text-align: left; padding: 0.75rem;">Bayesian Prior</th>
                            <th style="text-align: left; padding: 0.75rem;">Effect</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.75rem;">L2 (Ridge)</td>
                            <td style="padding: 0.75rem;">Smoothness, small weights</td>
                            <td style="padding: 0.75rem;">Gaussian prior</td>
                            <td style="padding: 0.75rem;">Weights shrink toward zero uniformly</td>
                        </tr>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.75rem;">L1 (Lasso)</td>
                            <td style="padding: 0.75rem;">Sparsity</td>
                            <td style="padding: 0.75rem;">Laplace prior</td>
                            <td style="padding: 0.75rem;">Many weights become exactly zero</td>
                        </tr>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.75rem;">Dropout</td>
                            <td style="padding: 0.75rem;">Ensemble of subnetworks</td>
                            <td style="padding: 0.75rem;">Spike-and-slab prior (approx.)</td>
                            <td style="padding: 0.75rem;">No single neuron is critical</td>
                        </tr>
                        <tr>
                            <td style="padding: 0.75rem;">Batch Normalization</td>
                            <td style="padding: 0.75rem;">Scale invariance</td>
                            <td style="padding: 0.75rem;">Implicit prior on activation distributions</td>
                            <td style="padding: 0.75rem;">Stabilizes training, enables higher learning rates</td>
                        </tr>
                    </tbody>
                </table>

                <div class="note-box">
                    <div class="box-title">Connection to Tutorial 09 (Regularization)</div>
                    <p style="margin-bottom: 0;">
                        Tutorial 09 covered the mechanics of L1, L2, and dropout. Here we see the deeper picture: each regularizer encodes a specific belief about the true function. L2 says "the true function has small, distributed weights." L1 says "only a few features matter." Dropout says "the representation should be redundant." Choosing a regularizer is choosing what you believe about the world.
                    </p>
                </div>

                <!-- Section 3: Data Augmentation as Inductive Bias -->
                <h2 id="augmentation-as-bias">Data Augmentation as Inductive Bias</h2>

                <p>
                    Data augmentation is a powerful way to encode invariances without modifying the architecture. By showing the model transformed versions of the data, we tell it: "these transformations don't change the label."
                </p>

                <div class="definition-box">
                    <div class="box-title">Augmentation = Encoded Invariance</div>
                    <p>
                        If we augment with transformation $T$, we implicitly require $f(T(x)) = f(x)$. This is an invariance constraint on the learned function.
                    </p>
                    <ul style="margin-bottom: 0;">
                        <li><strong>Horizontal flip</strong> = mirror symmetry</li>
                        <li><strong>Random crop</strong> = translation invariance</li>
                        <li><strong>Color jitter</strong> = illumination invariance</li>
                    </ul>
                </div>

                <div class="warning-box">
                    <div class="box-title">Wrong Augmentations Hurt</div>
                    <p style="margin-bottom: 0;">
                        Augmentation must match the true invariances of your task. Flipping digit '6' produces '9' &mdash; a different class. Rotating satellite images may be valid, but rotating text is not. Always verify that your augmentations preserve the label.
                    </p>
                </div>

                <p>
                    Connection to equivariance: data augmentation achieves approximately what equivariant architectures achieve exactly. Augmentation is a "soft" bias (statistical) &mdash; the model sees many transformed examples and <em>learns</em> to be invariant. Architectural equivariance is a "hard" bias (mathematical guarantee) &mdash; invariance is built into the computation. The soft version requires more data but is more flexible; the hard version is more sample-efficient but less adaptable.
                </p>

                <!-- Section 4: Transfer Learning & Pre-training -->
                <h2 id="transfer-learning">Transfer Learning &amp; Pre-training</h2>

                <p>
                    Pre-trained weights are a form of inductive bias learned from data rather than designed by humans. Instead of hand-crafting assumptions about the data structure, we let a model discover useful structure from a large dataset and then transfer that knowledge.
                </p>

                <div class="definition-box">
                    <div class="box-title">Transfer Learning</div>
                    <p>
                        Using knowledge from a source task to improve learning on a target task. The pre-trained weights encode:
                    </p>
                    <ol style="margin-bottom: 0;">
                        <li>General feature hierarchies (edges &rarr; textures &rarr; objects)</li>
                        <li>Statistical regularities of the source domain</li>
                        <li>Implicit biases from the source architecture and training procedure</li>
                    </ol>
                </div>

                <p>
                    Foundation models (BERT, GPT, CLIP) take this further &mdash; they provide strong biases from massive pre-training that can be fine-tuned for diverse downstream tasks. The pre-training data distribution becomes part of the inductive bias. A model pre-trained on English text has a bias toward English linguistic structure; a model pre-trained on ImageNet has a bias toward natural image statistics.
                </p>

                <div class="note-box">
                    <div class="box-title">Architecture Bias vs Data Bias</div>
                    <p style="margin-bottom: 0;">
                        A CNN's translation equivariance is an architectural bias &mdash; it holds regardless of training data. ImageNet pre-training is a data bias &mdash; it provides knowledge about natural images. The strongest systems combine both: architectural biases that match the domain structure + pre-trained weights that encode domain knowledge.
                    </p>
                </div>

                <!-- Section 5: The Bitter Lesson & Scaling Laws -->
                <h2 id="bitter-lesson">The Bitter Lesson &amp; Scaling Laws</h2>

                <p>
                    Rich Sutton's "Bitter Lesson" (2019) argues that hand-crafted biases consistently lose to methods that leverage computation and scale. The history of AI, he argues, repeatedly shows that general methods win in the long run.
                </p>

                <div class="definition-box">
                    <div class="box-title">The Bitter Lesson (Sutton, 2019)</div>
                    <p style="margin-bottom: 0;">
                        "The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin."
                    </p>
                </div>

                <div class="math-block">
                    $$L(N) \propto N^{-\alpha}$$
                </div>

                <p>
                    where $L$ is loss, $N$ is model size/data/compute, and $\alpha > 0$ is the scaling exponent. Neural scaling laws show that performance improves as a power law with scale &mdash; suggesting that bigger models with more data can discover structure that hand-crafted biases encode.
                </p>

                <p>
                    Counter-arguments to the Bitter Lesson are substantial: (1) Scaling alone can't discover all structure &mdash; physics simulations still need physics. (2) Biases dramatically improve sample efficiency when data is limited. (3) Many real-world problems have small datasets where scaling isn't an option. (4) Architecture biases like convolution actually help at ALL scales &mdash; ConvNets outperform MLPs even at massive scale for vision tasks.
                </p>

                <div class="note-box">
                    <div class="box-title">The Pragmatic View</div>
                    <p style="margin-bottom: 0;">
                        The truth likely lies between extremes. Hand-crafted biases provide valuable initialization and structure, especially with limited data. But as data and compute grow, the flexibility of less-biased architectures allows them to discover structure that hand-crafted biases might miss. The best practitioners use both: architectural biases for known structure + scale for discovering unknown patterns.
                    </p>
                </div>

                <!-- Section 6: When Inductive Bias Fails -->
                <h2 id="when-bias-fails">When Inductive Bias Fails</h2>

                <p>
                    Mismatched biases are worse than no bias at all. Using the wrong architecture for your data actively fights against learning. The model wastes capacity trying to work around the imposed structure instead of learning the actual patterns.
                </p>

                <div class="warning-box">
                    <div class="box-title">Wrong Bias Is Worse Than No Bias</div>
                    <p style="margin-bottom: 0;">
                        A CNN on tabular data imposes locality and translation equivariance where none exists. An RNN on non-sequential data imposes ordering where there is none. The model wastes capacity encoding irrelevant structure instead of learning the actual patterns.
                    </p>
                </div>

                <p>Examples of bias mismatch:</p>
                <ol>
                    <li><strong>CNN on tabular data:</strong> Features have no spatial relationship, so local receptive fields are meaningless. Feature 1 is no more "local" to feature 2 than to feature 500.</li>
                    <li><strong>RNN on set-structured data:</strong> Imposing order on an orderless set introduces spurious dependencies. The model's output changes when you permute the input, even though the answer shouldn't.</li>
                    <li><strong>Transformer without enough data:</strong> The minimal bias means the model can't generalize from few examples. Without strong assumptions, it needs massive data to discover structure on its own.</li>
                    <li><strong>GNN on dense graphs:</strong> Message passing on fully-connected graphs loses the sparsity bias that makes GNNs effective. Every node attends to every other node, collapsing to an expensive version of a standard neural network.</li>
                </ol>

                <p>
                    <strong>How to diagnose:</strong> If your model performs worse than a simpler baseline (e.g., logistic regression), the bias may be mismatched. If adding complexity doesn't improve validation performance, you may be fighting the wrong battle. Always compare against minimal-bias baselines before concluding that you need more data or more compute.
                </p>

                <!-- Section 7: Designing Custom Biases -->
                <h2 id="designing-custom">Designing Custom Biases</h2>

                <p>
                    For specialized domains, you can design custom biases that encode domain knowledge directly into the architecture or loss function. This is where machine learning meets domain science.
                </p>

                <div class="definition-box">
                    <div class="box-title">Physics-Informed Neural Networks (PINNs)</div>
                    <p>
                        Add a physics loss term:
                    </p>
                    <div class="math-block">
                        $$\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{data}} + \lambda \mathcal{L}_{\text{physics}}$$
                    </div>
                    <p style="margin-bottom: 0;">
                        where $\mathcal{L}_{\text{physics}} = \|\mathcal{F}[u](x) - 0\|^2$ and $\mathcal{F}$ is a differential operator encoding a physical law (e.g., Navier-Stokes, Maxwell's equations). The network is free to fit the data, but is penalized for violating known physics.
                    </p>
                </div>

                <p>
                    <strong>Neural ODEs:</strong> Parameterize continuous dynamics $\frac{dh}{dt} = f_\theta(h, t)$, encoding the bias that the system evolves continuously (no discrete jumps). Perfect for physical systems, time series, and normalizing flows. The architecture itself enforces that the learned dynamics are smooth and reversible.
                </p>

                <p>
                    <strong>SE(3)-equivariant networks:</strong> For molecular and protein data, encode equivariance to 3D rotations and translations. The bias: physics doesn't depend on the coordinate frame. Rotating a molecule doesn't change its energy, so the network should give the same prediction regardless of orientation.
                </p>

                <div class="note-box">
                    <div class="box-title">The Design Principle</div>
                    <p style="margin-bottom: 0;">
                        To design a custom bias: (1) Identify the symmetries and constraints of your domain. (2) Choose an architecture or loss function that respects those constraints. (3) Verify that your bias holds on real data (not just theoretical assumptions). A bias derived from theory is only useful if the theory accurately describes your data.
                    </p>
                </div>

                <!-- Section 8: Summary & Guidelines -->
                <h2 id="guidelines">Summary &amp; Guidelines</h2>

                <p>
                    Choosing the right inductive bias is a decision process: identify your data's structure, select an architecture that matches, add regularization that encodes your beliefs, and validate against baselines. The following table summarizes the key recommendations.
                </p>

                <table style="width: 100%; border-collapse: collapse; margin: 1.5rem 0;">
                    <thead>
                        <tr style="border-bottom: 2px solid var(--color-border);">
                            <th style="text-align: left; padding: 0.75rem;">Data Structure</th>
                            <th style="text-align: left; padding: 0.75rem;">Suggested Architecture</th>
                            <th style="text-align: left; padding: 0.75rem;">Key Bias</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.75rem;">Grid/spatial (images)</td>
                            <td style="padding: 0.75rem;">CNN</td>
                            <td style="padding: 0.75rem;">Translation equivariance, locality</td>
                        </tr>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.75rem;">Sequential (text, audio)</td>
                            <td style="padding: 0.75rem;">RNN / Transformer</td>
                            <td style="padding: 0.75rem;">Temporal order / attention</td>
                        </tr>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.75rem;">Graph-structured</td>
                            <td style="padding: 0.75rem;">GNN</td>
                            <td style="padding: 0.75rem;">Node permutation equivariance</td>
                        </tr>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.75rem;">Tabular (no structure)</td>
                            <td style="padding: 0.75rem;">MLP / Gradient Boosting</td>
                            <td style="padding: 0.75rem;">Minimal bias</td>
                        </tr>
                        <tr style="border-bottom: 1px solid var(--color-border);">
                            <td style="padding: 0.75rem;">Physics-governed</td>
                            <td style="padding: 0.75rem;">PINN / Neural ODE</td>
                            <td style="padding: 0.75rem;">Physical laws as constraints</td>
                        </tr>
                        <tr>
                            <td style="padding: 0.75rem;">3D geometry</td>
                            <td style="padding: 0.75rem;">SE(3)-equivariant net</td>
                            <td style="padding: 0.75rem;">Rotation + translation equivariance</td>
                        </tr>
                    </tbody>
                </table>

                <p><strong>Key principles:</strong></p>
                <ol>
                    <li><strong>Match bias to data structure.</strong> This is the single most important design decision.</li>
                    <li><strong>When unsure, start with less bias and add more if needed.</strong> It's easier to add constraints than to remove them.</li>
                    <li><strong>Validate with baselines</strong> &mdash; if a simple model wins, your bias may be wrong.</li>
                    <li><strong>Combine biases:</strong> architecture + regularization + augmentation. Each addresses a different aspect of the problem.</li>
                    <li><strong>More data can compensate for less bias, but not for wrong bias.</strong> A mismatched bias actively hurts regardless of dataset size.</li>
                </ol>

                <!-- Navigation -->
                <div class="tutorial-footer-summary" style="margin: 3rem 0; padding: 2rem; background: #f8f9fa; border-radius: 8px; border-left: 4px solid #3b82f6;">
                    <h1>19. Designing Inductive Biases</h1>
                    <p class="lead">
                        A practical guide to choosing, combining, and designing the right inductive biases for your machine learning problems.
                    </p>
                </div>
                <div class="tutorial-nav">
                    <a href="../17-architectural-biases/index.html" class="tutorial-nav-link prev">
                        <span class="nav-label">Previous</span>
                        <span class="nav-title">&larr; Architectural Biases</span>
                    </a>
                    <a href="../19-fst-fundamentals/index.html" class="tutorial-nav-link next">
                        <span class="nav-label">Next</span>
                        <span class="nav-title">FST Fundamentals &rarr;</span>
                    </a>
                </div>

            </article>

            <!-- ==================== CODE TAB ==================== -->
            <article class="article-content" id="code" style="display: none;">
                <h2>Python Code Examples</h2>
                <p>Four code examples demonstrating practical inductive bias techniques: regularization comparison, data augmentation impact, wrong bias diagnosis, and transfer learning benefits.</p>

                <!-- Code Example 1 -->
                <h3>1. Regularization Comparison: Weight Distributions</h3>
                <p>
                    Compare OLS, L2 (Ridge), and L1 (Lasso) regression on a sparse problem to see how each regularizer encodes a different inductive bias about the weight distribution.
                </p>

                <div class="code-block">
                    <div class="code-block-header">
                        <span class="code-block-lang">python</span>
                        <button class="code-block-copy">Copy</button>
                    </div>
                    <pre><code class="language-python">import numpy as np

np.random.seed(42)

# Generate synthetic regression data
n = 50
X = np.random.randn(n, 20)  # 20 features, many irrelevant
true_weights = np.zeros(20)
true_weights[:5] = [3.0, -2.0, 1.5, -1.0, 0.5]  # Only 5 relevant features
y = X @ true_weights + np.random.randn(n) * 0.5

# Add intercept
X_b = np.column_stack([np.ones(n), X])

def ridge_regression(X, y, lam):
    """L2 regularization: minimize ||y - Xw||^2 + lambda * ||w||^2"""
    n_features = X.shape[1]
    I = np.eye(n_features)
    I[0, 0] = 0  # Don't regularize intercept
    w = np.linalg.solve(X.T @ X + lam * I, X.T @ y)
    return w

def lasso_coordinate_descent(X, y, lam, n_iter=1000):
    """L1 regularization via coordinate descent."""
    n_samples, n_features = X.shape
    w = np.zeros(n_features)
    for _ in range(n_iter):
        for j in range(n_features):
            residual = y - X @ w + X[:, j] * w[j]
            rho = X[:, j] @ residual
            if j == 0:  # Don't regularize intercept
                w[j] = rho / (X[:, j] @ X[:, j])
            else:
                w[j] = np.sign(rho) * max(abs(rho) - lam * n_samples, 0) / (X[:, j] @ X[:, j])
    return w

# Compare regularizers
lam = 1.0
w_ols = np.linalg.lstsq(X_b, y, rcond=None)[0]
w_ridge = ridge_regression(X_b, y, lam)
w_lasso = lasso_coordinate_descent(X_b, y, lam * 0.1)

print("Regularization as Inductive Bias: Weight Distributions")
print("=" * 55)
print(f"\n{'Feature':<10} {'True':<10} {'OLS':<10} {'L2/Ridge':<10} {'L1/Lasso':<10}")
print("-" * 50)
for i in range(21):
    true_w = true_weights[i-1] if i > 0 else 0.0
    feat_name = f"bias" if i == 0 else f"x_{i}"
    print(f"{feat_name:<10} {true_w:<10.2f} {w_ols[i]:<10.2f} {w_ridge[i]:<10.2f} {w_lasso[i]:<10.2f}")

print(f"\nL2 bias: All weights shrink toward zero (Gaussian prior)")
print(f"L1 bias: Sparse weights — many exactly zero (Laplace prior)")
print(f"\nNon-zero weights in L1: {np.sum(np.abs(w_lasso[1:]) > 0.01)}/20")
print(f"True non-zero features: 5/20")</code></pre>
                </div>

                <!-- Code Example 2 -->
                <h3>2. Data Augmentation Impact</h3>
                <p>
                    Demonstrate how data augmentation (random shifts) encodes translation invariance, dramatically improving performance on a pattern-detection task with limited data.
                </p>

                <div class="code-block">
                    <div class="code-block-header">
                        <span class="code-block-lang">python</span>
                        <button class="code-block-copy">Copy</button>
                    </div>
                    <pre><code class="language-python">import numpy as np

np.random.seed(42)

def generate_shifted_pattern_data(n_samples, pattern_length=5, signal_length=20):
    """Classification: detect if a specific pattern exists (at any position)."""
    pattern = np.array([1.0, -1.0, 1.0, -1.0, 1.0])
    X = np.random.randn(n_samples, signal_length) * 0.3
    y = np.zeros(n_samples, dtype=int)

    for i in range(n_samples):
        if np.random.rand() > 0.5:
            pos = np.random.randint(0, signal_length - pattern_length + 1)
            X[i, pos:pos+pattern_length] += pattern
            y[i] = 1
    return X, y

def augment_with_shifts(X, y, n_augmented_per_sample=3):
    """Augment by randomly shifting signals (translation invariance)."""
    X_aug, y_aug = [X], [y]
    for _ in range(n_augmented_per_sample):
        shift = np.random.randint(-3, 4, size=len(X))
        X_shifted = np.zeros_like(X)
        for i in range(len(X)):
            X_shifted[i] = np.roll(X[i], shift[i])
        X_aug.append(X_shifted)
        y_aug.append(y)
    return np.vstack(X_aug), np.concatenate(y_aug)

def simple_classifier(X_train, y_train, X_test):
    """Simple linear classifier via least squares."""
    X_b = np.column_stack([np.ones(len(X_train)), X_train])
    X_test_b = np.column_stack([np.ones(len(X_test)), X_test])
    w = np.linalg.lstsq(X_b, y_train, rcond=None)[0]
    return (X_test_b @ w > 0.5).astype(int)

# Generate test set
X_test, y_test = generate_shifted_pattern_data(500)

print("Data Augmentation as Inductive Bias")
print("=" * 50)
print(f"\nTask: detect a pattern at any position in a 1D signal")
print(f"Augmentation: random shifts (encodes translation invariance)\n")

train_sizes = [20, 50, 100, 200]
for n_train in train_sizes:
    # Without augmentation
    accs_no_aug = []
    accs_aug = []

    for trial in range(20):
        X_train, y_train = generate_shifted_pattern_data(n_train)

        # No augmentation
        preds = simple_classifier(X_train, y_train, X_test)
        accs_no_aug.append(np.mean(preds == y_test))

        # With augmentation
        X_aug, y_aug = augment_with_shifts(X_train, y_train)
        preds_aug = simple_classifier(X_aug, y_aug, X_test)
        accs_aug.append(np.mean(preds_aug == y_test))

    print(f"  n={n_train:<5}  No aug: {np.mean(accs_no_aug):.1%} +/- {np.std(accs_no_aug):.1%}  "
          f"With aug: {np.mean(accs_aug):.1%} +/- {np.std(accs_aug):.1%}  "
          f"Improvement: {np.mean(accs_aug) - np.mean(accs_no_aug):+.1%}")

print(f"\nAugmentation helps most with small datasets — it encodes the")
print(f"translation invariance that the model can't learn from limited data.")</code></pre>
                </div>

                <!-- Code Example 3 -->
                <h3>3. Wrong Bias Demo: CNN vs DeepSets on Permutation-Invariant Task</h3>
                <p>
                    Show that a CNN (which imposes ordering) gives different predictions when inputs are permuted, while DeepSets (which is permutation-invariant by construction) does not.
                </p>

                <div class="code-block">
                    <div class="code-block-header">
                        <span class="code-block-lang">python</span>
                        <button class="code-block-copy">Copy</button>
                    </div>
                    <pre><code class="language-python">import numpy as np

np.random.seed(42)

def generate_set_task(n_samples, set_size=10, dim=3):
    """
    Task: predict the VARIANCE of a set of vectors.
    This is permutation-invariant — order doesn't matter.
    """
    X = np.random.randn(n_samples, set_size, dim)
    y = np.var(X, axis=(1, 2))  # Scalar output per set
    return X, y

def deep_sets_predict(X, w_phi, w_rho):
    """
    DeepSets: f(X) = rho(SUM(phi(x_i)))
    Permutation invariant by design (sum is invariant).
    """
    # phi: per-element transform
    transformed = np.tanh(X @ w_phi)  # (n, set_size, hidden)
    # Sum pooling (permutation invariant)
    pooled = transformed.sum(axis=1)  # (n, hidden)
    # rho: transform pooled representation
    return pooled @ w_rho  # (n,)

def cnn_1d_predict(X, w_conv, w_fc):
    """
    1D CNN: treats set as a sequence (imposes ordering bias).
    """
    n, set_size, dim = X.shape
    X_flat = X.reshape(n, set_size * dim)

    # Simple 1D conv (kernel size 3*dim to cover 3 elements)
    k = w_conv.shape[0]
    out_len = X_flat.shape[1] - k + 1
    conv_out = np.zeros((n, out_len))
    for i in range(out_len):
        conv_out[:, i] = np.tanh(X_flat[:, i:i+k] @ w_conv)

    # Global average pooling + FC
    pooled = conv_out.mean(axis=1, keepdims=True)
    return (pooled @ w_fc).ravel()

# Generate data
X_train, y_train = generate_set_task(200)
X_test, y_test = generate_set_task(500)
n, set_size, dim = X_train.shape

# Initialize random weights (not training, just comparing architecture behavior)
hidden = 8
w_phi = np.random.randn(dim, hidden) * 0.3
w_rho = np.random.randn(hidden, 1) * 0.3
w_conv = np.random.randn(dim * 3, 1) * 0.3
w_fc = np.random.randn(1, 1) * 0.3

# Test permutation sensitivity
n_test = 100
X_sample = X_test[:n_test]

print("Wrong Bias Demo: Permutation-Invariant Task")
print("=" * 50)
print(f"\nTask: predict variance of a set (order doesn't matter)")

# Check if predictions change when we permute the set elements
ds_diffs = []
cnn_diffs = []

for i in range(n_test):
    perm = np.random.permutation(set_size)
    X_orig = X_sample[i:i+1]
    X_perm = X_sample[i:i+1, perm, :]

    ds_orig = deep_sets_predict(X_orig, w_phi, w_rho)[0]
    ds_perm = deep_sets_predict(X_perm, w_phi, w_rho)[0]
    ds_diffs.append(abs(ds_orig - ds_perm))

    cnn_orig = cnn_1d_predict(X_orig, w_conv, w_fc)[0]
    cnn_perm = cnn_1d_predict(X_perm, w_conv, w_fc)[0]
    cnn_diffs.append(abs(cnn_orig - cnn_perm))

print(f"\nPermutation sensitivity (should be 0 for invariant model):")
print(f"  DeepSets: mean diff = {np.mean(ds_diffs):.6f} (invariant by construction)")
print(f"  CNN:      mean diff = {np.mean(cnn_diffs):.6f} (NOT invariant)")

print(f"\nDeepSets has the RIGHT bias (permutation invariance).")
print(f"CNN has the WRONG bias (assumes spatial/sequential order).")
print(f"The CNN must learn invariance from data — wasteful and fragile.")</code></pre>
                </div>

                <!-- Code Example 4 -->
                <h3>4. Transfer Learning: Pre-trained vs Random Init</h3>
                <p>
                    Demonstrate how pre-trained weights serve as an inductive bias that dramatically improves performance when target data is limited.
                </p>

                <div class="code-block">
                    <div class="code-block-header">
                        <span class="code-block-lang">python</span>
                        <button class="code-block-copy">Copy</button>
                    </div>
                    <pre><code class="language-python">import numpy as np

np.random.seed(42)

def generate_feature_data(n, n_features=50, n_informative=10):
    """Generate data where first n_informative features matter."""
    X = np.random.randn(n, n_features)
    w_true = np.zeros(n_features)
    w_true[:n_informative] = np.random.randn(n_informative)
    y = (X @ w_true > 0).astype(float)
    return X, y, w_true

def train_linear_model(X, y, w_init, lr=0.01, n_steps=200):
    """Train a linear model with gradient descent from given initialization."""
    w = w_init.copy()
    n = len(X)
    losses = []
    for step in range(n_steps):
        logits = X @ w
        preds = 1 / (1 + np.exp(-logits))
        preds = np.clip(preds, 1e-7, 1 - 1e-7)
        loss = -np.mean(y * np.log(preds) + (1 - y) * np.log(1 - preds))
        losses.append(loss)
        grad = X.T @ (preds - y) / n
        w -= lr * grad
    return w, losses

# Phase 1: Pre-train on a large "source" dataset
print("Transfer Learning: Pre-trained vs Random Initialization")
print("=" * 55)

X_source, y_source, w_source = generate_feature_data(5000)
w_pretrained, _ = train_linear_model(
    X_source, y_source, np.zeros(50), lr=0.01, n_steps=500
)

# Phase 2: Fine-tune on small "target" dataset (related but different)
# Target uses similar features but with different weights
X_target, y_target, w_target_true = generate_feature_data(50)  # Only 50 samples!
X_test, y_test, _ = generate_feature_data(1000)

print(f"\nSource dataset: 5000 samples (pre-training)")
print(f"Target dataset: 50 samples (fine-tuning)")
print(f"Test dataset: 1000 samples\n")

# Compare: random init vs pre-trained init
results = {}
for init_name, w_init in [("Random init", np.random.randn(50) * 0.01),
                            ("Pre-trained", w_pretrained.copy())]:
    w_final, losses = train_linear_model(X_target, y_target, w_init, lr=0.01, n_steps=200)

    # Evaluate
    test_preds = (X_test @ w_final > 0).astype(float)
    accuracy = np.mean(test_preds == y_test)
    results[init_name] = (accuracy, losses)

    print(f"{init_name}:")
    print(f"  Final training loss: {losses[-1]:.4f}")
    print(f"  Test accuracy: {accuracy:.1%}")
    print(f"  Initial loss: {losses[0]:.4f}")
    print()

improvement = results["Pre-trained"][0] - results["Random init"][0]
print(f"Pre-training improvement: {improvement:+.1%}")
print(f"\nWith only 50 target samples, pre-trained weights provide")
print(f"a strong inductive bias — knowledge transferred from the source")
print(f"domain compensates for limited target data.")

# Show effect at different target dataset sizes
print(f"\n--- Accuracy vs Target Dataset Size ---")
for n_target in [10, 25, 50, 100, 500, 2000]:
    X_t, y_t, _ = generate_feature_data(n_target)

    w_r, _ = train_linear_model(X_t, y_t, np.random.randn(50) * 0.01, n_steps=200)
    w_p, _ = train_linear_model(X_t, y_t, w_pretrained.copy(), n_steps=200)

    acc_r = np.mean((X_test @ w_r > 0).astype(float) == y_test)
    acc_p = np.mean((X_test @ w_p > 0).astype(float) == y_test)

    print(f"  n={n_target:<6} Random: {acc_r:.1%}  Pre-trained: {acc_p:.1%}  Gap: {acc_p - acc_r:+.1%}")

print(f"\nAs target data grows, the gap shrinks — more data compensates for less bias.")</code></pre>
                </div>

            </article>

            <!-- ==================== EXERCISES TAB ==================== -->
            <article class="article-content" id="exercises" style="display: none;">
                <h2>Exercises</h2>
                <p>Test your understanding of designing, choosing, and evaluating inductive biases. Exercises range from identifying regularization biases to designing custom architectures for novel problems.</p>

                <div class="exercise-list">

                    <!-- Easy -->
                    <h3 style="margin-top: 1rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">Easy</h3>

                    <!-- Exercise 1 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">1. Identify Regularization Biases</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>For each regularizer, state the inductive bias it encodes:</p>
                            <ol type="a">
                                <li>L2 regularization</li>
                                <li>L1 regularization</li>
                                <li>Dropout</li>
                                <li>Early stopping</li>
                            </ol>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>(a) L2 regularization:</strong> Smoothness bias &mdash; prefers small, distributed weights (Gaussian prior). The model believes no single feature should dominate; the true function is smooth.</p>
                                <p><strong>(b) L1 regularization:</strong> Sparsity bias &mdash; prefers few non-zero weights (Laplace prior). The model believes only a small subset of features are relevant.</p>
                                <p><strong>(c) Dropout:</strong> Ensemble bias &mdash; no single neuron should be critical; prefers redundant representations. The model should work even when parts of the network are missing.</p>
                                <p><strong>(d) Early stopping:</strong> Simplicity bias &mdash; prefers functions reachable in fewer optimization steps (equivalent to constraining the effective model complexity). The model believes simpler functions generalize better.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 2 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">2. Design Augmentations</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Design appropriate data augmentations for each task, and identify augmentations that would be harmful:</p>
                            <ol type="a">
                                <li>Chest X-ray classification</li>
                                <li>Sentiment analysis of tweets</li>
                                <li>Music genre classification</li>
                            </ol>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>(a) Chest X-rays:</strong> Small rotations (&lt; 10 degrees &mdash; lungs are roughly symmetric but not freely rotatable), brightness/contrast changes (different X-ray machines), slight scaling. <strong>NOT:</strong> large rotations (anatomy has orientation), horizontal flip (heart is on the left).</p>
                                <p><strong>(b) Tweets:</strong> Synonym replacement, random deletion of words, back-translation. <strong>NOT:</strong> character-level shuffling (destroys meaning), removing hashtags/mentions (may carry sentiment).</p>
                                <p><strong>(c) Music:</strong> Time stretching (tempo invariance), pitch shifting (key invariance), adding background noise (environment invariance). <strong>NOT:</strong> reversing the audio (destroys temporal structure).</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 3 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">3. When to Fine-tune</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>You have a pre-trained ImageNet model and a new task with 500 labeled images of skin lesions. Should you:</p>
                            <ol type="a">
                                <li>Train from scratch</li>
                                <li>Fine-tune the whole network</li>
                                <li>Freeze early layers and fine-tune later layers</li>
                            </ol>
                            <p>Explain your reasoning.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>(c) Freeze early layers + fine-tune later layers.</strong></p>
                                <p><strong>Reasoning:</strong> Early layers learn general features (edges, textures) that transfer well across image domains. Later layers learn task-specific features. With only 500 images:</p>
                                <ul>
                                    <li><strong>Training from scratch</strong> will overfit badly &mdash; 500 images are far too few to learn general visual features from random initialization.</li>
                                    <li><strong>Fine-tuning everything</strong> risks destroying useful pre-trained features &mdash; the gradients from 500 samples may corrupt the well-learned early-layer representations.</li>
                                    <li><strong>Freezing early layers</strong> preserves general features (edges, textures, shapes) while adapting the final layers to skin lesion classification. This is the most sample-efficient approach.</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <!-- Medium -->
                    <h3 style="margin-top: 2rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">Medium</h3>

                    <!-- Exercise 4 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">4. Argue Both Sides of the Bitter Lesson</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Present a strong argument FOR the Bitter Lesson and a strong argument AGAINST it. Include specific examples for each side.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>FOR the Bitter Lesson:</strong></p>
                                <ul>
                                    <li><strong>Chess:</strong> Deep Blue's hand-crafted evaluation function was surpassed by AlphaZero's pure learning approach.</li>
                                    <li><strong>Go:</strong> Hand-crafted features and heuristics couldn't compete with Monte Carlo Tree Search + neural networks.</li>
                                    <li><strong>NLP:</strong> Hand-crafted parsers, POS taggers, and linguistic features lost to Transformers trained on raw text.</li>
                                </ul>
                                <p>The pattern: human knowledge is consistently outperformed by learning + compute as scale increases.</p>
                                <p><strong>AGAINST the Bitter Lesson:</strong></p>
                                <ul>
                                    <li><strong>PINNs for physics:</strong> Encoding Navier-Stokes into the loss dramatically reduces data needs for fluid simulation.</li>
                                    <li><strong>Drug discovery:</strong> SE(3)-equivariant architectures achieve state-of-the-art with less data by encoding 3D symmetries.</li>
                                    <li><strong>Few-shot learning:</strong> Without strong biases, you simply cannot learn from 5 examples &mdash; no amount of architecture scale helps.</li>
                                </ul>
                                <p>The counter-pattern: when data is limited or domain structure is well-understood, domain knowledge is essential and more efficient than pure scale.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 5 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">5. Diagnose Mismatched Biases</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>A practitioner trains a CNN on a tabular dataset (500 features, 10000 samples) and gets 72% accuracy. A random forest achieves 85%. Explain what went wrong and propose fixes.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p>The CNN imposes <strong>locality and translation equivariance</strong> &mdash; meaningful for images but harmful for tabular data. Feature 1 is no more "local" to feature 2 than to feature 500. The CNN's convolutional filters look for spatial patterns that don't exist in tabular data.</p>
                                <p>The random forest has an <strong>axis-aligned split bias</strong> which matches tabular data better &mdash; it can independently assess each feature's contribution.</p>
                                <p><strong>Fixes:</strong></p>
                                <ol>
                                    <li><strong>Use an MLP instead</strong> &mdash; no spatial bias. Fully-connected layers treat all feature pairs equally.</li>
                                    <li><strong>Try gradient boosting (XGBoost/LightGBM)</strong> &mdash; strong tabular data bias with axis-aligned splits and additive structure.</li>
                                    <li><strong>If you must use a CNN,</strong> consider treating each feature as a "channel" with 1x1 spatial extent (removes locality bias).</li>
                                    <li><strong>Use TabNet or FT-Transformer</strong> &mdash; architectures specifically designed for tabular data with appropriate inductive biases.</li>
                                </ol>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 6 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">6. Prove Augmentation = Invariance</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Show that training with data augmentation under transformation $T$ is equivalent (in expectation) to adding an invariance regularizer to the loss.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p>Original loss:</p>
                                <div class="math-block">
                                    $$\mathcal{L}(\theta) = \mathbb{E}_{(x,y)}[\ell(f_\theta(x), y)]$$
                                </div>
                                <p>With augmentation:</p>
                                <div class="math-block">
                                    $$\mathcal{L}_{\text{aug}}(\theta) = \mathbb{E}_{(x,y)} \mathbb{E}_{T \sim \mathcal{T}}[\ell(f_\theta(T(x)), y)]$$
                                </div>
                                <p>The expected augmented loss can be decomposed:</p>
                                <div class="math-block">
                                    $$\mathcal{L}_{\text{aug}} = \mathcal{L}(\theta) + \mathbb{E}_{(x,y)}\mathbb{E}_T[\ell(f_\theta(T(x)), y) - \ell(f_\theta(x), y)]$$
                                </div>
                                <p>The second term penalizes sensitivity to transformation $T$ &mdash; this IS an invariance regularizer. It is minimized when $f_\theta(T(x)) = f_\theta(x)$ for all $T \in \mathcal{T}$, i.e., when $f_\theta$ is invariant to $\mathcal{T}$.</p>
                                <p>Therefore, data augmentation is equivalent to the original loss plus a penalty for non-invariance &mdash; a soft invariance constraint.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 7 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">7. PINN Loss Function</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Write the PINN loss function for learning the solution to the 1D heat equation $\frac{\partial u}{\partial t} = \alpha \frac{\partial^2 u}{\partial x^2}$ with initial condition $u(x, 0) = \sin(\pi x)$ and boundary conditions $u(0, t) = u(1, t) = 0$.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p>The total PINN loss has three components:</p>
                                <div class="math-block">
                                    $$\mathcal{L}_{\text{total}} = \lambda_{\text{pde}} \mathcal{L}_{\text{pde}} + \lambda_{\text{ic}} \mathcal{L}_{\text{ic}} + \lambda_{\text{bc}} \mathcal{L}_{\text{bc}}$$
                                </div>
                                <p><strong>PDE residual</strong> (evaluated at random collocation points in the domain):</p>
                                <div class="math-block">
                                    $$\mathcal{L}_{\text{pde}} = \frac{1}{N_r} \sum_{i=1}^{N_r} \left|\frac{\partial u_\theta}{\partial t}(x_i, t_i) - \alpha \frac{\partial^2 u_\theta}{\partial x^2}(x_i, t_i)\right|^2$$
                                </div>
                                <p><strong>Initial condition:</strong></p>
                                <div class="math-block">
                                    $$\mathcal{L}_{\text{ic}} = \frac{1}{N_0} \sum_{i=1}^{N_0} |u_\theta(x_i, 0) - \sin(\pi x_i)|^2$$
                                </div>
                                <p><strong>Boundary conditions:</strong></p>
                                <div class="math-block">
                                    $$\mathcal{L}_{\text{bc}} = \frac{1}{N_b} \sum_{i=1}^{N_b} [|u_\theta(0, t_i)|^2 + |u_\theta(1, t_i)|^2]$$
                                </div>
                                <p>The PINN encodes the heat equation as an inductive bias through $\mathcal{L}_{\text{pde}}$. The network is free to learn any function, but is strongly penalized for violating the known physics. The derivatives $\frac{\partial u_\theta}{\partial t}$ and $\frac{\partial^2 u_\theta}{\partial x^2}$ are computed via automatic differentiation.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Hard -->
                    <h3 style="margin-top: 2rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">Hard</h3>

                    <!-- Exercise 8 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">8. Choose Architectures for Novel Problems</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>For each problem, recommend an architecture and justify your choice by identifying the relevant inductive bias:</p>
                            <ol type="a">
                                <li>Predicting protein folding from amino acid sequence</li>
                                <li>Weather forecasting from satellite images over time</li>
                                <li>Recommending products based on user purchase history</li>
                                <li>Controlling a robotic arm</li>
                            </ol>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>(a) Protein folding:</strong> SE(3)-equivariant GNN (e.g., AlphaFold-style). <strong>Bias:</strong> 3D structure is equivariant to rotations/translations; graph structure captures amino acid interactions. Amino acids interact through spatial proximity, not just sequential position, making graph structure essential.</p>
                                <p><strong>(b) Weather forecasting:</strong> 3D CNN or Vision Transformer with temporal attention. <strong>Bias:</strong> spatial locality (nearby regions affect each other) + temporal dynamics (weather evolves continuously). Consider physics-informed constraints (conservation of energy, mass, momentum).</p>
                                <p><strong>(c) Product recommendations:</strong> Graph neural network on user-item bipartite graph, or Transformer on purchase sequences. <strong>Bias:</strong> GNN captures collaborative filtering structure (similar users like similar items); Transformer captures sequential purchase patterns (temporal dependencies in buying behavior).</p>
                                <p><strong>(d) Robotic arm control:</strong> Recurrent policy network (RNN/LSTM) or Transformer for decision-making. Consider equivariance to workspace transformations. Physics-informed reward shaping for dynamics constraints. <strong>Bias:</strong> temporal structure in control sequences + physical dynamics constraints.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 9 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">9. Design a Scale-Equivariant Layer</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Many natural images contain objects at different scales. Design a convolutional layer that is equivariant to scaling (dilation) in addition to translation. Describe:</p>
                            <ol type="a">
                                <li>The key idea</li>
                                <li>How to implement it</li>
                                <li>The computational cost compared to standard convolution</li>
                            </ol>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>(a) Key idea:</strong> Apply the same filter at multiple scales by using dilated convolutions with different dilation rates. A scale transformation of the input should produce a correspondingly scaled transformation of the output.</p>
                                <p><strong>(b) Implementation:</strong> For a filter $w$ and dilation rates $r \in \{1, 2, 4, 8\}$, compute:</p>
                                <div class="math-block">
                                    $$f_r(x)[n] = \sum_k w[k] \cdot x[n - r \cdot k]$$
                                </div>
                                <p>Stack outputs across scales as a "scale dimension." The layer is equivariant: scaling the input by factor 2 shifts the response from dilation rate $r$ to rate $2r$.</p>
                                <p><strong>(c) Computational cost:</strong> If we use $S$ scales, cost is $S \times$ standard convolution. But parameters are shared (only one filter $w$), so parameter count is the same as one standard convolution. This is the principle behind scale-equivariant CNNs (Worrall &amp; Welling, 2019) and is related to wavelet transforms.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 10 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">10. The Scaling vs Bias Tradeoff</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Consider a task where a CNN with 1M parameters achieves 90% accuracy with 10K training samples. An MLP with 10M parameters achieves only 80% on the same data but reaches 95% with 1M samples.</p>
                            <ol type="a">
                                <li>Plot the qualitative learning curves for both models.</li>
                                <li>At what dataset size does the MLP overtake the CNN?</li>
                                <li>If you have 50K samples, which model would you use?</li>
                                <li>Relate this to the Bitter Lesson.</li>
                            </ol>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>(a)</strong> CNN curve: starts high (~70% at n=100), rises steeply to ~90% at n=10K, plateaus around 92&ndash;93%. MLP curve: starts low (~55% at n=100), rises slowly, crosses CNN around n=100K, reaches 95%+ at n=1M.</p>
                                <p><strong>(b)</strong> The crossover point is approximately where the MLP has enough data to compensate for its lack of bias. From the given data points, interpolating: MLP reaches 90% around n=100K. The crossover is approximately at n=100K.</p>
                                <p><strong>(c)</strong> With 50K samples: <strong>still use the CNN</strong>. The CNN's inductive bias gives it an advantage when data is limited. The MLP needs roughly 100K+ to match. At 50K, the CNN likely achieves ~91% while the MLP is around ~85%.</p>
                                <p><strong>(d)</strong> This IS the Bitter Lesson in action: the less-biased model (MLP) eventually wins given enough data, but the biased model (CNN) is better with limited data. The practical question is: do you have enough data to overcome the bias deficit? For most real-world problems with limited labeled data, architectural bias still wins.</p>
                            </div>
                        </div>
                    </div>

                </div>
            </article>

        </main>

        <!-- TOC (Right Side) -->
        <aside class="toc-container">
            <h4 class="toc-title">Contents</h4>
            <nav class="toc-list">
                <a href="#practitioners-dilemma" class="toc-link">The Practitioner's Dilemma</a>
                <a href="#regularization-as-bias" class="toc-link">Regularization as Bias</a>
                <a href="#augmentation-as-bias" class="toc-link">Augmentation as Bias</a>
                <a href="#transfer-learning" class="toc-link">Transfer Learning</a>
                <a href="#bitter-lesson" class="toc-link">The Bitter Lesson</a>
                <a href="#when-bias-fails" class="toc-link">When Bias Fails</a>
                <a href="#designing-custom" class="toc-link">Designing Custom Biases</a>
                <a href="#guidelines" class="toc-link">Summary &amp; Guidelines</a>
            </nav>
        </aside>
    </div>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-brand">
                    <span class="logo-symbol">&nabla;</span>
                    <span>ML Fundamentals</span>
                </div>
                <p class="footer-tagline">Deep understanding through first principles.</p>
            </div>
            <div class="footer-links">
                <a href="../../../index.html">Home</a>
                <a href="https://github.com/ml-entropy/ml-entropy.github.io" target="_blank">GitHub</a>
            </div>
        </div>
    </footer>

    <!-- Scripts -->
    <script src="../../../js/main.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // KaTeX Rendering
            if (typeof renderMathInElement === 'function') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }

            // Tab Switching Logic
            const tabs = document.querySelectorAll('.tutorial-tab');
            const articles = document.querySelectorAll('.article-content');

            function switchTab(targetId) {
                if (!targetId || targetId === '#') targetId = '#theory';

                // Update tab classes
                tabs.forEach(tab => {
                    if (tab.getAttribute('href') === targetId) {
                        tab.classList.add('active');
                    } else {
                        tab.classList.remove('active');
                    }
                });

                // Show/Hide articles
                articles.forEach(article => {
                    const articleId = '#' + article.id;
                    if (articleId === targetId) {
                        article.style.display = 'block';
                    } else {
                        article.style.display = 'none';
                    }
                });

                // Re-render KaTeX for newly visible content
                if (typeof renderMathInElement === 'function') {
                    renderMathInElement(document.body, {
                        delimiters: [
                            {left: '$$', right: '$$', display: true},
                            {left: '$', right: '$', display: false},
                            {left: '\\[', right: '\\]', display: true},
                            {left: '\\(', right: '\\)', display: false}
                        ],
                        throwOnError: false
                    });
                }

                // Update TOC visibility based on tab
                const toc = document.querySelector('.toc-container');
                if (toc) {
                    if (targetId === '#theory') {
                        toc.style.display = 'block';
                        setTimeout(() => toc.classList.add('visible'), 100);
                    } else {
                        toc.classList.remove('visible');
                        setTimeout(() => toc.style.display = 'none', 300);
                    }
                }
            }

            // Event Listeners
            tabs.forEach(tab => {
                tab.addEventListener('click', (e) => {
                    e.preventDefault();
                    const targetId = tab.getAttribute('href');
                    history.pushState(null, null, targetId);
                    switchTab(targetId);
                });
            });

            // Handle browser back/forward buttons
            window.addEventListener('popstate', () => {
                switchTab(window.location.hash);
            });

            // Initial Load
            switchTab(window.location.hash);
        });
    </script>
</body>
</html>
