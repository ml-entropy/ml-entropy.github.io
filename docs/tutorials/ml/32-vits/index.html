<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>VITS: End-to-End Text-to-Speech | ML Fundamentals</title>
    <meta name="description" content="VITS end-to-end text-to-speech synthesis combining VAE, normalizing flows, and adversarial training for high-fidelity speech generation.">
    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&family=Playfair+Display:wght@400;500;600;700&display=swap" rel="stylesheet">
    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}, {left: '\\\\[', right: '\\\\]', display: true}, {left: '\\\\(', right: '\\\\)', display: false}], throwOnError: false});"></script>
    <!-- Styles -->
    <link rel="stylesheet" href="../../../css/main.css">
    <link rel="stylesheet" href="../../../css/components.css">
    <link rel="stylesheet" href="../../../css/sidebar.css">
    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>&#x221E;</text></svg>">
</head>
<body>
    <nav class="navbar" id="navbar">
        <div class="nav-container">
            <a href="../../../index.html" class="nav-logo">
                <span class="logo-symbol">&nabla;</span>
                <span class="logo-text">ML Fundamentals</span>
            </a>
            <button class="nav-toggle" id="navToggle" aria-label="Toggle navigation">
                <span></span><span></span><span></span>
            </button>
            <div class="nav-menu" id="navMenu">
                <div class="nav-links">
                    <a href="../../../tutorials/ml/index.html" class="nav-link active">Machine Learning</a>
                    <a href="../../../tutorials/linear-algebra/index.html" class="nav-link">Linear Algebra</a>
                    <a href="../../../tutorials/calculus/index.html" class="nav-link">Calculus</a>
                    <a href="../../../tutorials/physics/index.html" class="nav-link">Physics</a>
                    <a href="../../../index.html#philosophy" class="nav-link">Philosophy</a>
                    <a href="../../../index.html#roadmap" class="nav-link">Roadmap</a>
                    <a href="https://github.com/ml-entropy/ml-entropy.github.io" class="nav-link" target="_blank">GitHub</a>
                </div>
                <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
                    <svg class="sun-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="5"/><path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"/></svg>
                    <svg class="moon-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/></svg>
                </button>
            </div>
        </div>
    </nav>

    <header class="tutorial-content-header">
        <div class="container">
            <nav class="breadcrumb">
                <a href="../../../index.html">Home</a>
                <span class="breadcrumb-separator">&rarr;</span>
                <a href="../index.html">Machine Learning</a>
                <span class="breadcrumb-separator">&rarr;</span>
                <span>VITS: End-to-End TTS</span>
            </nav>
            <div class="tutorial-tabs">
                <a href="#theory" class="tutorial-tab active">Theory</a>
                <a href="#code" class="tutorial-tab">Code</a>
                <a href="#exercises" class="tutorial-tab">Exercises</a>
            </div>
        </div>
    </header>

    <div class="tutorial-wrapper">

        <!-- Sidebar Navigation -->
        <!-- Sidebar Navigation -->
        <aside class="tutorial-sidebar">
            <div class="sidebar-section">
                <h3 class="sidebar-section-title">Machine Learning</h3>
                <nav class="sidebar-nav">
                        <a href="../00-probability/index.html" class="sidebar-link">00. Probability Foundations</a>
                    <a href="../04-logarithms/index.html" class="sidebar-link">01. Why Logarithms?</a>
                    <a href="../05-combinatorics/index.html" class="sidebar-link">02. Combinatorics</a>
                    <a href="../03-distributions/index.html" class="sidebar-link">03. Normal Distributions</a>
                    <a href="../01-entropy/index.html" class="sidebar-link">04. Entropy Fundamentals</a>
                    <a href="../02-cross-entropy/index.html" class="sidebar-link">05. Cross-Entropy</a>
                    <a href="../02-kl-divergence/index.html" class="sidebar-link">06. KL Divergence</a>
                    <a href="../14-entropy-connections/index.html" class="sidebar-link">07. Entropy Connections</a>
                    <a href="../06-backpropagation/index.html" class="sidebar-link">08. Backpropagation</a>
                    <a href="../07-regularization/index.html" class="sidebar-link">09. Regularization</a>
                    <a href="../08-batch-normalization/index.html" class="sidebar-link">10. Batch Normalization</a>
                    <a href="../09-learning-rate/index.html" class="sidebar-link">11. Learning Rate</a>
                    <a href="../10-cnn/index.html" class="sidebar-link">12. CNNs</a>
                    <a href="../11-rnn/index.html" class="sidebar-link">13. RNNs</a>
                    <a href="../15-autoencoder/index.html" class="sidebar-link">14. Autoencoders</a>
                    <a href="../13-variational-inference/index.html" class="sidebar-link">15. Variational Inference</a>
                    <a href="../12-vae/index.html" class="sidebar-link">16. VAE</a>
                    <a href="../16-inductive-bias/index.html" class="sidebar-link">17. Inductive Bias</a>
                    <a href="../17-architectural-biases/index.html" class="sidebar-link">18. Architectural Biases</a>
                    <a href="../18-designing-biases/index.html" class="sidebar-link">19. Designing Biases</a>
                    <a href="../19-fst-fundamentals/index.html" class="sidebar-link">20. FST Fundamentals</a>
                    <a href="../20-weighted-fsts/index.html" class="sidebar-link">21. Weighted FSTs</a>
                    <a href="../21-fst-libraries/index.html" class="sidebar-link">22. FST Libraries</a>
                    <a href="../22-fst-applications/index.html" class="sidebar-link">23. FST Applications</a>
                    <a href="../23-neural-symbolic/index.html" class="sidebar-link">24. Neural-Symbolic Hybrids</a>
                    <a href="../24-sequence-alignment/index.html" class="sidebar-link">25. Sequence Alignment</a>
                    <a href="../25-mas-algorithm/index.html" class="sidebar-link">26. MAS Algorithm</a>
                    <a href="../26-forced-alignment/index.html" class="sidebar-link">27. Forced Alignment & MFA</a>
                    <a href="../27-tts-fundamentals/index.html" class="sidebar-link">28. TTS Fundamentals</a>
                    <a href="../28-neural-vocoders/index.html" class="sidebar-link">29. Neural Vocoders</a>
                    <a href="../29-tacotron/index.html" class="sidebar-link">30. Tacotron & Attention TTS</a>
                    <a href="../30-fastspeech/index.html" class="sidebar-link">31. FastSpeech & Non-AR TTS</a>
                    <a href="../31-glow-tts/index.html" class="sidebar-link">32. Glow-TTS & Flows</a>
                    <a href="../32-vits/index.html" class="sidebar-link active">33. VITS: End-to-End TTS</a>
                    <a href="../33-bilingual-tts/index.html" class="sidebar-link">34. Bilingual TTS: RU+KY</a>
                </nav>
            </div>
            
            <div class="sidebar-section" style="margin-top: 2rem;">
                <h3 class="sidebar-section-title">Related Subjects</h3>
                <nav class="sidebar-nav">
                        <a href="../../linear-algebra/index.html" class="sidebar-link">Linear Algebra</a>
                    <a href="../../calculus/index.html" class="sidebar-link">Calculus</a>
                    <a href="../../physics/index.html" class="sidebar-link">Physics</a>
                </nav>
            </div>
        </aside>

        <main class="tutorial-main">

            <!-- ==================== THEORY TAB ==================== -->
            <article class="article-content" id="theory">

                <!-- Section 1: The VITS Breakthrough -->
                <h2 id="vits-breakthrough">The VITS Breakthrough</h2>

                <p>
                    VITS (Variational Inference with adversarial learning for end-to-end Text-to-Speech) represents a fundamental shift in speech synthesis. Published at ICML 2021 by Kim, Kong, and Son, VITS is the first model to generate high-fidelity waveforms directly from text in a single end-to-end system &mdash; eliminating the traditional two-stage pipeline of acoustic model followed by vocoder. The result is a system that achieves a Mean Opinion Score (MOS) of <strong>4.43</strong>, remarkably close to the <strong>4.45</strong> ground-truth score on the LJ Speech dataset.
                </p>

                <p>
                    The key innovation of VITS lies in its synthesis of three powerful generative modeling frameworks: <strong>variational autoencoders</strong> (VAEs), <strong>normalizing flows</strong>, and <strong>adversarial training</strong>. Each framework brings a distinct and complementary strength to the system, and their combination overcomes the individual limitations of each approach.
                </p>

                <p>
                    <strong>Why VAEs?</strong> The VAE framework (see <a href="../12-vae/index.html">Tutorial 16: VAE</a>) provides a structured latent space that captures the variation in speech. Given the same text, a speaker can produce many different valid utterances with different prosody, emphasis, and rhythm. The VAE's latent space encodes this variation, enabling stochastic sampling that produces diverse, natural-sounding speech. Without the VAE, the model would lack a principled way to represent the one-to-many mapping from text to speech.
                </p>

                <p>
                    <strong>Why normalizing flows?</strong> A standard VAE suffers from the expressiveness gap: the posterior $q_\phi(z|x)$ is typically a diagonal Gaussian, but the true posterior can be highly complex and multimodal. Normalizing flows (see <a href="../31-glow-tts/index.html">Tutorial 32: Glow-TTS</a>) transform the simple prior distribution into a much more expressive distribution through a series of invertible transformations. In VITS, flows bridge the gap between the text-conditioned prior and the speech-conditioned posterior, dramatically improving the model's ability to represent the complex distribution of speech.
                </p>

                <p>
                    <strong>Why adversarial training?</strong> VAEs are notorious for producing over-smoothed outputs because the reconstruction loss (typically L1 or L2 on mel-spectrograms) penalizes all deviations equally, causing the model to predict the mean of possible outputs rather than a sharp, realistic sample. Adversarial training, borrowed from GANs, replaces this blurry reconstruction with a discriminator that judges whether the generated waveform sounds real. This forces the generator to produce crisp, detailed waveforms with natural-sounding harmonics and transients.
                </p>

                <div class="definition-box">
                    <div class="box-title">Definition: VITS (Variational Inference with adversarial learning for end-to-end Text-to-Speech)</div>
                    <p>
                        VITS is an end-to-end conditional VAE that generates raw waveforms directly from text. It combines three generative frameworks:
                    </p>
                    <ul>
                        <li><strong>VAE:</strong> Provides a structured latent space $z$ for capturing speech variation, trained via the evidence lower bound (ELBO).</li>
                        <li><strong>Normalizing flows:</strong> Transform the simple text-conditioned prior $p_\theta(z|c)$ into a complex distribution that better matches the posterior $q_\phi(z|x)$, reducing the KL divergence.</li>
                        <li><strong>Adversarial training:</strong> A multi-period discriminator judges generated waveforms against real speech, replacing over-smoothed reconstruction with sharp, realistic output.</li>
                    </ul>
                    <p style="margin-bottom: 0;">
                        The model is trained end-to-end: text goes in, waveforms come out, with no intermediate mel-spectrogram representation at inference time.
                    </p>
                </div>

                <p>
                    The combination of these three frameworks is not merely additive &mdash; it is synergistic. The VAE provides the probabilistic framework and latent space structure. Flows enhance the prior to match the complex posterior, reducing the "posterior collapse" problem that plagues standard VAEs. Adversarial training provides the perceptual quality that neither VAEs nor flows can achieve alone, because it optimizes for perceptual realism rather than pixel-level (or spectrogram-level) accuracy. Together, they produce a system that is simultaneously expressive, diverse, and high-fidelity.
                </p>

                <!-- Section 2: Architecture Deep Dive -->
                <h2 id="vits-architecture">Architecture Deep Dive</h2>

                <p>
                    The VITS architecture consists of five main components that work together during training and inference. Understanding each component and how they interact is essential to grasping why VITS achieves such remarkable quality.
                </p>

                <p>
                    <strong>1. Posterior Encoder.</strong> The posterior encoder takes the <strong>linear spectrogram</strong> of the ground-truth audio and produces the parameters of the approximate posterior distribution $q_\phi(z|x)$. It uses a stack of WaveNet-style residual blocks with dilated convolutions, followed by a projection to output the mean $\mu_q$ and log-variance $\log \sigma_q^2$ for each time frame. The latent variable $z$ is then sampled via the reparameterization trick: $z = \mu_q + \sigma_q \odot \epsilon$, where $\epsilon \sim \mathcal{N}(0, I)$. Critically, the posterior encoder operates on the <strong>linear spectrogram</strong> (not the mel-spectrogram), which preserves more spectral detail and avoids the information bottleneck inherent in mel compression.
                </p>

                <p>
                    <strong>2. Prior Encoder.</strong> The prior encoder takes the text (phoneme sequence) and produces the parameters of the prior distribution $p_\theta(z|c)$. It consists of a phoneme embedding layer followed by a transformer encoder (with relative positional encoding) that produces contextualized hidden states $h_{\text{text}}$. These hidden states are projected to produce the mean $\mu_p$ and log-variance $\log \sigma_p^2$ of the prior distribution. Without flows, this prior is a simple diagonal Gaussian conditioned on text &mdash; too restrictive to capture the complexity of speech. The normalizing flow component transforms this simple prior into a much richer distribution.
                </p>

                <p>
                    <strong>3. Normalizing Flow.</strong> The normalizing flow $f_\theta$ is a stack of affine coupling layers (similar to those in Glow-TTS) that transforms the simple prior distribution into a complex distribution that better matches the posterior. The flow operates bidirectionally: during training, it transforms the posterior samples $z$ into a simpler space where they can be compared to the text-conditioned prior; during inference, it transforms samples from the text-conditioned prior into the complex latent space expected by the decoder. The change of variables formula ensures exact density computation:
                </p>
                $$p_\theta(z|c) = p_0(f_\theta^{-1}(z|c)) \cdot \left|\det \frac{\partial f_\theta^{-1}}{\partial z}\right|$$
                <p>
                    where $p_0$ is the simple Gaussian prior from the text encoder and $f_\theta^{-1}$ is the inverse flow.
                </p>

                <p>
                    <strong>4. Decoder (HiFi-GAN Generator).</strong> The decoder takes the latent variable $z$ and generates the raw waveform directly. It uses the HiFi-GAN V1 generator architecture (see <a href="../28-neural-vocoders/index.html">Tutorial 29: Neural Vocoders</a>), which consists of transposed convolutions for upsampling followed by multi-receptive field fusion (MRF) modules. Each MRF module applies dilated convolutions with different kernel sizes and dilation rates to capture patterns at multiple scales. The decoder operates on $z$ (not mel-spectrograms), making this a true end-to-end system.
                </p>

                <p>
                    <strong>5. Discriminator.</strong> The discriminator provides the adversarial training signal. VITS uses the same discriminator architecture as HiFi-GAN: a combination of <strong>multi-period discriminators</strong> (MPD) that reshape the 1D waveform into 2D with different periods [2, 3, 5, 7, 11] to capture periodic patterns, and <strong>multi-scale discriminators</strong> (MSD) that operate on the waveform at different resolutions (original, 2x downsampled, 4x downsampled) to capture patterns at different time scales.
                </p>

                <div class="note-box">
                    <div class="box-title">VITS Architecture: Component Interactions</div>
                    <p><strong>Training data flow:</strong></p>
                    <ol>
                        <li>Text &rarr; Prior Encoder &rarr; $(\mu_p, \sigma_p)$ per phoneme</li>
                        <li>Linear spectrogram &rarr; Posterior Encoder &rarr; $(\mu_q, \sigma_q)$ &rarr; sample $z$</li>
                        <li>$z$ &rarr; Flow (inverse) &rarr; $z'$ (mapped to simple space for KL computation)</li>
                        <li>$z$ &rarr; Decoder &rarr; waveform &rarr; Discriminator (real vs. generated)</li>
                        <li>MAS aligns text prior to posterior using $\log p_\theta(z_j | c_i)$ cost matrix</li>
                    </ol>
                    <p><strong>Inference data flow:</strong></p>
                    <ol>
                        <li>Text &rarr; Prior Encoder &rarr; $(\mu_p, \sigma_p)$ per phoneme</li>
                        <li>Duration Predictor &rarr; expand prior to frame-level</li>
                        <li>Sample $z' \sim \mathcal{N}(\mu_p, \sigma_p)$</li>
                        <li>$z'$ &rarr; Flow (forward) &rarr; $z$ (mapped to complex latent space)</li>
                        <li>$z$ &rarr; Decoder &rarr; waveform</li>
                    </ol>
                    <p style="margin-bottom: 0;">
                        Note the asymmetry: during training, the posterior encoder provides $z$ from real speech; during inference, $z$ comes from the flow-enhanced prior. The flow's job is to ensure these two sources of $z$ produce indistinguishable results.
                    </p>
                </div>

                <!-- Section 3: Training Losses -->
                <h2 id="vits-training-losses">Training Losses</h2>

                <p>
                    VITS is trained with a composite loss function consisting of five terms, each serving a distinct purpose. The full training objective is:
                </p>
                $$\mathcal{L} = \mathcal{L}_{\text{recon}} + \mathcal{L}_{\text{kl}} + \mathcal{L}_{\text{adv}} + \mathcal{L}_{\text{fm}} + \mathcal{L}_{\text{dur}}$$
                <p>
                    Understanding each term and why it is necessary reveals the architectural logic of VITS.
                </p>

                <p>
                    <strong>1. Reconstruction Loss $\mathcal{L}_{\text{recon}}$.</strong> This is the L1 distance between the mel-spectrogram of the generated waveform and the mel-spectrogram of the ground-truth audio. Note that although VITS generates raw waveforms, the reconstruction loss is computed in the mel domain because mel-spectrograms are perceptually meaningful:
                </p>
                $$\mathcal{L}_{\text{recon}} = \| \text{Mel}(\hat{y}) - \text{Mel}(y) \|_1$$
                <p>
                    where $\hat{y} = G(z)$ is the generated waveform and $y$ is the ground truth. The Mel transform is applied as a non-learnable feature extraction step. This loss ensures the generated audio has the correct spectral content (pitch, formants, energy).
                </p>

                <p>
                    <strong>2. KL Divergence Loss $\mathcal{L}_{\text{kl}}$.</strong> This is the KL divergence between the posterior distribution $q_\phi(z|x)$ and the flow-enhanced prior $p_\theta(z|c)$. It measures how well the text-conditioned prior (enhanced by normalizing flows) matches the speech-conditioned posterior:
                </p>
                $$\mathcal{L}_{\text{kl}} = D_{\text{KL}}(q_\phi(z|x) \| p_\theta(z|c))$$
                <p>
                    This is the term that bridges text and speech: it forces the prior (which only sees text) to produce a distribution over $z$ that matches what the posterior (which sees actual speech) produces. The normalizing flow makes this possible by allowing $p_\theta(z|c)$ to be much more expressive than a simple Gaussian. We will derive the full computation in the next section.
                </p>

                <p>
                    <strong>3. Adversarial Loss $\mathcal{L}_{\text{adv}}$.</strong> The adversarial loss uses the hinge loss formulation. For the generator (all components except the discriminator):
                </p>
                $$\mathcal{L}_{\text{adv}}(G) = \mathbb{E}\left[\sum_{k=1}^{K} \max(0, 1 - D_k(\hat{y}))\right]$$
                <p>
                    For the discriminator:
                </p>
                $$\mathcal{L}_{\text{adv}}(D) = \mathbb{E}\left[\sum_{k=1}^{K} \left(\max(0, 1 - D_k(y)) + \max(0, 1 + D_k(\hat{y}))\right)\right]$$
                <p>
                    where $D_k$ are the $K$ sub-discriminators (from MPD and MSD). The adversarial loss is what gives VITS its perceptual quality &mdash; it forces the generated waveforms to be indistinguishable from real speech at multiple temporal scales and periodicities.
                </p>

                <p>
                    <strong>4. Feature Matching Loss $\mathcal{L}_{\text{fm}}$.</strong> Feature matching stabilizes adversarial training by adding an auxiliary loss that matches the intermediate feature representations of the discriminator:
                </p>
                $$\mathcal{L}_{\text{fm}} = \mathbb{E}\left[\sum_{k=1}^{K} \sum_{l=1}^{L_k} \frac{1}{N_l} \| D_k^{(l)}(y) - D_k^{(l)}(\hat{y}) \|_1\right]$$
                <p>
                    where $D_k^{(l)}$ is the $l$-th layer feature map of the $k$-th discriminator and $N_l$ is the number of elements in that feature map. This loss acts as a learned perceptual loss: the discriminator learns to extract features that distinguish real from fake speech, and the feature matching loss forces the generator to match these features. It provides smoother gradients than the adversarial loss alone.
                </p>

                <p>
                    <strong>5. Duration Loss $\mathcal{L}_{\text{dur}}$.</strong> The duration loss trains the stochastic duration predictor to match the durations extracted by MAS:
                </p>
                $$\mathcal{L}_{\text{dur}} = -\log p_d(d | h_{\text{text}}; \theta_d)$$
                <p>
                    Unlike Glow-TTS which uses a deterministic duration predictor with MSE loss, VITS uses a <strong>stochastic duration predictor</strong> based on normalizing flows. The duration predictor models the distribution of durations (rather than predicting a single value), which enables diverse prosody during inference. The loss is the negative log-likelihood of the MAS-derived durations under the flow-based duration model.
                </p>

                <div class="math-derivation">
                    <div class="box-title">Full ELBO Derivation for VITS</div>
                    <div class="math-step">
                        <p><strong>Step 1:</strong> Start from the standard VAE evidence lower bound. For observed audio $x$ and text conditioning $c$:</p>
                        $$\log p(x|c) \geq \mathbb{E}_{q_\phi(z|x)}\left[\log p_\theta(x|z)\right] - D_{\text{KL}}(q_\phi(z|x) \| p_\theta(z|c))$$
                    </div>
                    <div class="math-step">
                        <p><strong>Step 2:</strong> The reconstruction term $\log p_\theta(x|z)$ corresponds to how well the decoder generates waveform $x$ from latent $z$. In VITS, this is replaced by $\mathcal{L}_{\text{recon}}$ (mel L1) plus $\mathcal{L}_{\text{adv}}$ and $\mathcal{L}_{\text{fm}}$ (adversarial losses). The adversarial losses serve as a learned, perceptually-motivated reconstruction criterion that is more effective than pixel-level losses.</p>
                    </div>
                    <div class="math-step">
                        <p><strong>Step 3:</strong> The KL term measures the gap between posterior and prior. Expanding with the flow-enhanced prior:</p>
                        $$D_{\text{KL}}(q_\phi(z|x) \| p_\theta(z|c)) = \mathbb{E}_{q_\phi(z|x)}\left[\log q_\phi(z|x) - \log p_\theta(z|c)\right]$$
                    </div>
                    <div class="math-step">
                        <p><strong>Step 4:</strong> Substituting the flow-enhanced prior $p_\theta(z|c) = p_0(f_\theta^{-1}(z|c)) \cdot |\det \frac{\partial f_\theta^{-1}}{\partial z}|$:</p>
                        $$= \mathbb{E}_{q_\phi(z|x)}\left[\log q_\phi(z|x) - \log p_0(f_\theta^{-1}(z|c)) - \log \left|\det \frac{\partial f_\theta^{-1}}{\partial z}\right|\right]$$
                    </div>
                    <div class="math-step">
                        <p><strong>Step 5:</strong> The full VITS training objective combines all terms:</p>
                        $$\mathcal{L}_{\text{VITS}} = \underbrace{\mathcal{L}_{\text{recon}} + \mathcal{L}_{\text{adv}} + \mathcal{L}_{\text{fm}}}_{\text{reconstruction (perceptual)}} + \underbrace{\mathcal{L}_{\text{kl}}}_{\text{regularization}} + \underbrace{\mathcal{L}_{\text{dur}}}_{\text{alignment}}$$
                        <p style="margin-bottom: 0;">This is a modified ELBO where the reconstruction term is replaced by a combination of spectral and adversarial losses, and the duration loss adds alignment supervision.</p>
                    </div>
                </div>

                <!-- Section 4: KL Divergence with Flows -->
                <h2 id="kl-flow">KL Divergence with Flows</h2>

                <p>
                    The KL divergence term in VITS is arguably the most technically important component, because it is the mechanism through which the text-to-speech mapping is learned. Without the normalizing flow, the prior $p_\theta(z|c)$ is a simple diagonal Gaussian conditioned on text, which is far too restrictive to match the complex posterior $q_\phi(z|x)$ conditioned on real speech. This section explains why flows are essential and how they make the KL computation tractable.
                </p>

                <p>
                    <strong>The problem without flows.</strong> In a standard conditional VAE, both the prior and posterior are diagonal Gaussians:
                </p>
                $$q_\phi(z|x) = \mathcal{N}(z; \mu_q, \sigma_q^2 I), \quad p_\theta(z|c) = \mathcal{N}(z; \mu_p, \sigma_p^2 I)$$
                <p>
                    The KL divergence between two diagonal Gaussians has a closed-form solution:
                </p>
                $$D_{\text{KL}}(q \| p) = \frac{1}{2} \sum_{d=1}^{D} \left[\log \frac{\sigma_{p,d}^2}{\sigma_{q,d}^2} + \frac{\sigma_{q,d}^2 + (\mu_{q,d} - \mu_{p,d})^2}{\sigma_{p,d}^2} - 1\right]$$
                <p>
                    This is simple but severely limiting. The posterior $q_\phi(z|x)$ is forced to be a diagonal Gaussian, which cannot represent correlations between dimensions or multimodal distributions. Speech, however, has complex structure: pitch, energy, and duration are correlated, and many dimensions of variation interact nonlinearly. A diagonal Gaussian prior means the model can only learn text-to-speech mappings where each latent dimension varies independently &mdash; a poor inductive bias for speech (see <a href="../13-variational-inference/index.html">Tutorial 15: Variational Inference</a> for background on variational inference).
                </p>

                <p>
                    <strong>The solution with flows.</strong> VITS introduces a normalizing flow $f_\theta$ that transforms the simple prior into a complex distribution. The flow-enhanced prior is:
                </p>
                $$p_\theta(z|c) = p_0(f_\theta^{-1}(z|c)) \cdot \left|\det \frac{\partial f_\theta^{-1}}{\partial z}\right|$$
                <p>
                    where $p_0(z'|c) = \mathcal{N}(z'; \mu_p, \sigma_p^2 I)$ is the simple Gaussian prior from the text encoder, and $f_\theta^{-1}$ maps from the complex $z$-space to the simple $z'$-space. The KL divergence becomes:
                </p>

                <div class="math-derivation">
                    <div class="box-title">KL with Flow-Enhanced Prior</div>
                    <div class="math-step">
                        <p><strong>Step 1:</strong> Write the KL divergence explicitly:</p>
                        $$D_{\text{KL}}(q_\phi(z|x) \| p_\theta(z|c)) = \mathbb{E}_{z \sim q_\phi}\left[\log q_\phi(z|x) - \log p_\theta(z|c)\right]$$
                    </div>
                    <div class="math-step">
                        <p><strong>Step 2:</strong> Substitute the flow-enhanced prior:</p>
                        $$= \mathbb{E}_{z \sim q_\phi}\left[\log q_\phi(z|x) - \log p_0(f_\theta^{-1}(z|c)) - \log \left|\det \frac{\partial f_\theta^{-1}}{\partial z}\right|\right]$$
                    </div>
                    <div class="math-step">
                        <p><strong>Step 3:</strong> Since $q_\phi(z|x)$ is diagonal Gaussian, $\log q_\phi(z|x)$ is cheap to compute. The term $\log p_0(f_\theta^{-1}(z|c))$ is also cheap because $p_0$ is Gaussian &mdash; we just need to compute $f_\theta^{-1}(z|c)$ (one pass through the inverse flow) and evaluate the Gaussian density. The log-determinant is computed efficiently due to the triangular Jacobian structure of affine coupling layers.</p>
                    </div>
                    <div class="math-step">
                        <p><strong>Step 4:</strong> In practice, VITS computes this as:</p>
                        $$\mathcal{L}_{\text{kl}} = \log q_\phi(z|x) - \log p_0(z') - \log \left|\det \frac{\partial f_\theta^{-1}}{\partial z}\right|$$
                        <p style="margin-bottom: 0;">where $z \sim q_\phi(z|x)$ via reparameterization and $z' = f_\theta^{-1}(z|c)$. This is computed per sample (no expectation needed because the reparameterization trick gives a single-sample Monte Carlo estimate).</p>
                    </div>
                </div>

                <p>
                    <strong>Why is this crucial for quality?</strong> The flow allows the prior to represent complex, correlated distributions over the latent space. Without the flow, the KL term would force the posterior to collapse toward a simple Gaussian, destroying the rich structure that the posterior encoder learns from real speech. With the flow, the prior can flexibly match the posterior's complexity, so the KL term is small even when both distributions are highly structured. This means the model can maintain a rich latent representation without paying a high KL penalty &mdash; avoiding the "posterior collapse" problem that plagues many VAE-based speech models.
                </p>

                <div class="warning-box">
                    <div class="box-title">Flow Direction Convention</div>
                    <p style="margin-bottom: 0;">
                        The flow direction in VITS can be confusing. During <strong>training</strong>, the flow runs in the <strong>inverse</strong> direction: $z' = f_\theta^{-1}(z)$, mapping the posterior sample $z$ to the simple space $z'$ for KL computation. During <strong>inference</strong>, the flow runs in the <strong>forward</strong> direction: $z = f_\theta(z')$, mapping a sample $z'$ from the simple prior to the complex latent space for the decoder. This is the same convention as Glow-TTS (see <a href="../31-glow-tts/index.html">Tutorial 32</a>).
                    </p>
                </div>

                <!-- Section 5: Monotonic Alignment in VITS -->
                <h2 id="mas-in-vits">Monotonic Alignment in VITS</h2>

                <p>
                    VITS uses the same Monotonic Alignment Search (MAS) algorithm as Glow-TTS (see <a href="../25-mas-algorithm/index.html">Tutorial 26: MAS Algorithm</a>), but with an important difference in what is being aligned. In Glow-TTS, MAS operates on latent vectors $z$ that come from running the <em>mel-spectrogram</em> through the flow inverse. In VITS, $z$ comes from the <strong>posterior encoder</strong>, which processes the <em>linear spectrogram</em> of the actual speech. This difference is subtle but significant: VITS's posterior encoder produces latent variables that are explicitly optimized to be useful for waveform generation (via the decoder and adversarial loss), whereas Glow-TTS's flow inverse simply maps mel frames to a latent space for likelihood computation.
                </p>

                <p>
                    <strong>Cost matrix construction.</strong> The MAS cost matrix in VITS is:
                </p>
                $$Q_{j,i} = \log p_\theta(z_j | c_i)$$
                <p>
                    where $z_j$ is the $j$-th latent frame from the posterior encoder and $c_i$ is the $i$-th text hidden state from the prior encoder. Since the prior (before the flow) is Gaussian with parameters $(\mu_{p,i}, \sigma_{p,i})$ for each text position $i$, this becomes:
                </p>
                $$Q_{j,i} = \log \mathcal{N}(z_j; \mu_{p,i}, \sigma_{p,i}^2) = -\frac{1}{2}\left[\log(2\pi) + 2\log \sigma_{p,i} + \frac{(z_j - \mu_{p,i})^2}{\sigma_{p,i}^2}\right]$$
                <p>
                    MAS finds the monotonic alignment $A^*$ that maximizes $\sum_{j} Q_{j, A^*(j)}$ using the same $O(T \times N)$ dynamic programming algorithm. The resulting alignment tells us which phoneme each latent frame corresponds to, and the number of frames assigned to each phoneme gives the durations.
                </p>

                <p>
                    <strong>Stochastic Duration Predictor.</strong> One of VITS's most distinctive innovations is its <strong>stochastic duration predictor</strong>, which replaces the deterministic duration predictor used in Glow-TTS and FastSpeech 2. Instead of predicting a single duration value per phoneme, the stochastic duration predictor models the full <em>distribution</em> of durations using a flow-based model. During training, it maximizes the log-likelihood of the MAS-derived durations:
                </p>
                $$\mathcal{L}_{\text{dur}} = -\log p_d(d | h_{\text{text}}; \theta_d)$$
                <p>
                    where $d$ is the vector of MAS-derived durations and $p_d$ is the flow-based duration distribution. During inference, the stochastic duration predictor <em>samples</em> durations from this learned distribution, producing different speaking rates and rhythms for the same text. A temperature parameter controls the diversity of the sampled durations.
                </p>

                <p>
                    <strong>Why stochastic durations matter.</strong> In natural speech, the same sentence can be spoken with different timing. Consider "I didn't say he stole the money" &mdash; depending on which word is emphasized, the duration pattern changes dramatically. A deterministic duration predictor can only produce one duration pattern, typically the average, which sounds monotonous. The stochastic duration predictor can sample different emphasis patterns, making the speech sound more natural and varied. This is particularly important for conversational TTS applications where robotic-sounding, metronomic speech is unacceptable.
                </p>

                <div class="note-box">
                    <div class="box-title">MAS in VITS vs. Glow-TTS</div>
                    <p>Key differences between how MAS is used in the two models:</p>
                    <ul>
                        <li><strong>Source of $z$:</strong> In Glow-TTS, $z = f^{-1}(y)$ comes from the flow inverse applied to the mel-spectrogram. In VITS, $z \sim q_\phi(z|x)$ comes from the posterior encoder applied to the linear spectrogram.</li>
                        <li><strong>Duration predictor:</strong> Glow-TTS uses a deterministic predictor (MSE loss). VITS uses a stochastic predictor (flow-based, log-likelihood loss).</li>
                        <li><strong>What MAS optimizes:</strong> In both, MAS maximizes $\sum_j \log p(z_j | c_{A(j)})$. But in VITS, the $z_j$ are also being used to generate waveforms (via the decoder), so MAS operates on a richer latent space.</li>
                        <li><strong>Flow role:</strong> In Glow-TTS, the flow IS the generative model (it generates mel-spectrograms). In VITS, the flow transforms the prior to match the posterior &mdash; it is part of the VAE framework, not the output generator.</li>
                    </ul>
                    <p style="margin-bottom: 0;">
                        Despite these differences, the core MAS algorithm is identical: $O(T \times N)$ dynamic programming finding the optimal monotonic alignment.
                    </p>
                </div>

                <!-- Section 6: Why End-to-End Matters -->
                <h2 id="end-to-end-advantage">Why End-to-End Matters</h2>

                <p>
                    Before VITS, the dominant TTS paradigm was <strong>two-stage synthesis</strong>: an acoustic model (Tacotron 2, FastSpeech 2, or Glow-TTS) generates mel-spectrograms, then a vocoder (HiFi-GAN) converts them to waveforms. This approach has three fundamental problems that VITS eliminates.
                </p>

                <div class="definition-box">
                    <div class="box-title">The Three Problems of Two-Stage TTS</div>
                    <p><strong>1. Mel-Spectrogram Bottleneck:</strong> The mel-spectrogram is a lossy representation. It discards phase information and compresses the frequency axis. The vocoder must reconstruct what was lost, but cannot recover information that was never encoded.</p>
                    <p><strong>2. Train-Test Mismatch:</strong> During training, the vocoder sees ground-truth mel-spectrograms. During inference, it sees predicted mel-spectrograms from the acoustic model. These predicted spectrograms contain artifacts (over-smoothing, boundary effects) that the vocoder has never seen, leading to degraded quality.</p>
                    <p style="margin-bottom: 0;"><strong>3. Separate Optimization:</strong> The acoustic model and vocoder are trained independently with different objectives. There is no guarantee that minimizing the acoustic model's loss produces mel-spectrograms that the vocoder can best reconstruct. Joint optimization would find a better overall solution.</p>
                </div>

                <p>
                    VITS eliminates all three problems by operating in a <strong>learned latent space</strong> rather than the mel-spectrogram space. The posterior encoder and decoder jointly learn what information to encode in $z$ &mdash; they can preserve phase-related information, speaker characteristics, and fine acoustic details that mel-spectrograms discard. There is no train-test mismatch because the same model generates and decodes. And all components are optimized jointly.
                </p>

                <div class="note-box">
                    <div class="box-title">Quantitative Comparison: Two-Stage vs End-to-End</div>
                    <table style="width: 100%; border-collapse: collapse; margin: 0.5rem 0;">
                        <thead>
                            <tr style="border-bottom: 2px solid var(--color-border);">
                                <th style="text-align: left; padding: 0.5rem;">System</th>
                                <th style="text-align: center; padding: 0.5rem;">MOS ($\uparrow$)</th>
                                <th style="text-align: center; padding: 0.5rem;">RTF ($\downarrow$)</th>
                                <th style="text-align: center; padding: 0.5rem;">Components</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr style="border-bottom: 1px solid var(--color-border);">
                                <td style="padding: 0.5rem;">Ground Truth</td>
                                <td style="text-align: center; padding: 0.5rem;">4.45</td>
                                <td style="text-align: center; padding: 0.5rem;">&mdash;</td>
                                <td style="text-align: center; padding: 0.5rem;">&mdash;</td>
                            </tr>
                            <tr style="border-bottom: 1px solid var(--color-border);">
                                <td style="padding: 0.5rem;">Tacotron 2 + WaveGlow</td>
                                <td style="text-align: center; padding: 0.5rem;">4.25</td>
                                <td style="text-align: center; padding: 0.5rem;">0.12</td>
                                <td style="text-align: center; padding: 0.5rem;">2</td>
                            </tr>
                            <tr style="border-bottom: 1px solid var(--color-border);">
                                <td style="padding: 0.5rem;">Glow-TTS + HiFi-GAN</td>
                                <td style="text-align: center; padding: 0.5rem;">4.00</td>
                                <td style="text-align: center; padding: 0.5rem;">0.02</td>
                                <td style="text-align: center; padding: 0.5rem;">2</td>
                            </tr>
                            <tr>
                                <td style="padding: 0.5rem;"><strong>VITS</strong></td>
                                <td style="text-align: center; padding: 0.5rem;"><strong>4.43</strong></td>
                                <td style="text-align: center; padding: 0.5rem;"><strong>0.015</strong></td>
                                <td style="text-align: center; padding: 0.5rem;"><strong>1</strong></td>
                            </tr>
                        </tbody>
                    </table>
                    <p style="margin-bottom: 0;">RTF = Real-Time Factor (lower is faster). VITS achieves the best quality AND the fastest speed with a single model.</p>
                </div>

                <!-- Section 7: Why VITS Sounds Natural -->
                <h2 id="why-natural">Why VITS Sounds Natural</h2>

                <p>
                    VITS achieves near-human naturalness through the synergy of three complementary generative frameworks. Each framework addresses a specific limitation that would degrade quality if absent.
                </p>

                <div class="math-derivation">
                    <div class="math-step">
                        <strong>Component 1: Adversarial Training &rarr; Sharp, Detailed Waveforms</strong>
                        <p>The HiFi-GAN discriminator (multi-period + multi-scale) forces the generator to produce waveforms with sharp harmonics, natural noise textures, and correct temporal dynamics. Without adversarial training, the decoder would produce over-smoothed waveforms (the averaging effect of L1/L2 reconstruction losses). The discriminator prevents this by penalizing any distribution mismatch between real and generated waveforms.</p>
                    </div>
                    <div class="math-step">
                        <strong>Component 2: VAE &rarr; Structured Latent Space</strong>
                        <p>The VAE framework provides a <em>structured</em> latent space where nearby points produce similar speech. This is critical for: (1) smooth interpolation between prosodic styles, (2) meaningful sampling that always produces valid speech, and (3) disentangling content from speaker characteristics. Without the VAE, the adversarial-only model would produce high-quality but uncontrollable and often incoherent outputs.</p>
                    </div>
                    <div class="math-step">
                        <strong>Component 3: Normalizing Flows &rarr; Expressive Prior</strong>
                        <p>The normalizing flow transforms the simple text-conditional Gaussian prior into a complex distribution that matches the rich structure of the posterior. Without flows, the KL divergence between a simple Gaussian prior and the complex posterior would be large, forcing either: (a) a weak posterior (poor reconstruction) or (b) posterior collapse (ignoring the prior). The flow resolves this tension by making the prior as expressive as needed.</p>
                    </div>
                </div>

                <div class="warning-box">
                    <div class="box-title">Why Each Component Alone is Insufficient</div>
                    <p><strong>GAN alone</strong> (e.g., GAN-TTS): High-fidelity audio but unstable training, mode collapse, and no structured latent space for control.</p>
                    <p><strong>VAE alone</strong> (e.g., vanilla VAE-TTS): Structured latent space but over-smoothed outputs due to pixel-wise reconstruction loss.</p>
                    <p style="margin-bottom: 0;"><strong>Flow alone</strong> (e.g., Glow-TTS): Exact likelihood and diverse outputs but limited to mel-spectrogram domain, requiring a separate vocoder.</p>
                </div>

                <p>
                    <strong>Temperature sampling</strong> controls the trade-off between naturalness and diversity. At temperature $\tau = 0$, VITS produces deterministic, "average" speech. At $\tau = 0.667$ (the typical setting), it produces natural variation in prosody. At $\tau = 1.0$, maximum diversity is achieved but with occasional artifacts. The stochastic duration predictor adds an orthogonal dimension of variation: even at the same latent temperature, different duration samples produce different speaking rhythms.
                </p>

                <!-- Section 8: VITS Extensions -->
                <h2 id="vits-extensions">VITS Extensions</h2>

                <p>
                    VITS's modular architecture has spawned a rich ecosystem of extensions, each addressing a specific limitation or adding new capabilities.
                </p>

                <p>
                    <strong>VITS2</strong> (2023) improves upon the original with better flow architecture, improved duration modeling, and monotonic alignment with a learned prior. It achieves higher MOS scores and more stable training.
                </p>

                <p>
                    <strong>MB-iSTFT-VITS</strong> replaces the standard HiFi-GAN decoder with a multi-band iSTFT-based decoder. Instead of directly generating waveform samples, it predicts magnitude and phase spectra for multiple frequency sub-bands and reconstructs the waveform via inverse STFT. This reduces computational cost by 2&ndash;4x while maintaining comparable quality.
                </p>

                <p>
                    <strong>YourTTS</strong> extends VITS for zero-shot multi-speaker synthesis: given a short reference audio clip from an unseen speaker, it can generate speech in that speaker's voice. It uses a speaker encoder (pre-trained on speaker verification) to extract a speaker embedding from the reference clip, then conditions VITS on this embedding. This enables voice cloning without any fine-tuning.
                </p>

                <p>
                    <strong>so-VITS-SVC</strong> (Singing Voice Conversion) adapts the VITS framework for converting singing voices. It replaces the text encoder with a content encoder that extracts pitch-independent content from the source singing, then synthesizes the content in the target singer's voice. This has become extremely popular in the AI music community.
                </p>

                <p>
                    <strong>Multilingual VITS</strong> extends the architecture with language embeddings and shared phoneme representations, enabling a single model to synthesize speech in multiple languages. This is the foundation for the bilingual TTS system we will build in the next tutorial.
                </p>

                <div class="note-box">
                    <div class="box-title">Bridge to Tutorial 34</div>
                    <p style="margin-bottom: 0;">In the next tutorial, we will extend VITS for <strong>bilingual synthesis (Russian + Kyrgyz)</strong>. The key challenges are: (1) designing a unified phoneme set for two Cyrillic-script languages with partially overlapping phoneme inventories, (2) handling severe data imbalance (thousands of hours of Russian vs. tens of hours of Kyrgyz), and (3) enabling natural code-switching between languages. VITS's modular architecture makes it ideal for this task &mdash; we can add language conditioning to the text encoder, flow, and duration predictor while sharing the decoder.</p>
                </div>

                <!-- Tutorial Navigation -->
                <div class="tutorial-nav">
                    <a href="../31-glow-tts/index.html" class="tutorial-nav-link prev">
                        <span class="nav-label">Previous</span>
                        <span class="nav-title">&larr; Glow-TTS &amp; Flows</span>
                    </a>
                    <a href="../33-bilingual-tts/index.html" class="tutorial-nav-link next">
                        <span class="nav-label">Next</span>
                        <span class="nav-title">Bilingual TTS: RU+KY &rarr;</span>
                    </a>
                </div>

            </article>

            <!-- Code Tab -->
            <article class="article-content" id="code" style="display: none;">
                <div class="container">

                    <h2>Code Example 1: VITS Posterior Encoder</h2>

                    <p>The posterior encoder is the component that encodes the linear spectrogram into the latent space $z$. It uses WaveNet-style residual blocks to capture temporal dependencies in the acoustic signal.</p>

                    <pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class WaveNetResidualBlock(nn.Module):
    """Single WaveNet residual block with gated activation."""

    def __init__(self, hidden_channels, kernel_size=5, dilation=1):
        super().__init__()
        padding = (kernel_size * dilation - dilation) // 2
        self.dilated_conv = nn.Conv1d(
            hidden_channels, 2 * hidden_channels,
            kernel_size, dilation=dilation, padding=padding
        )
        self.output_conv = nn.Conv1d(hidden_channels, 2 * hidden_channels, 1)

    def forward(self, x):
        residual = x
        h = self.dilated_conv(x)
        # Gated activation: tanh(h1) * sigmoid(h2)
        h_tanh, h_sigmoid = h.chunk(2, dim=1)
        h = torch.tanh(h_tanh) * torch.sigmoid(h_sigmoid)
        h = self.output_conv(h)
        out, skip = h.chunk(2, dim=1)
        return (out + residual) / (2 ** 0.5), skip


class PosteriorEncoder(nn.Module):
    """
    VITS Posterior Encoder: linear spectrogram -> latent z
    Uses WaveNet residual blocks to capture temporal structure.
    """

    def __init__(self, in_channels=513, hidden_channels=192,
                 latent_channels=192, n_layers=16, kernel_size=5):
        super().__init__()
        self.pre = nn.Conv1d(in_channels, hidden_channels, 1)

        # WaveNet residual blocks with increasing dilation
        self.blocks = nn.ModuleList()
        for i in range(n_layers):
            dilation = 2 ** (i % 4)  # Cycle: 1, 2, 4, 8, 1, 2, 4, 8, ...
            self.blocks.append(
                WaveNetResidualBlock(hidden_channels, kernel_size, dilation)
            )

        # Output projection to mean and log-variance
        self.proj = nn.Conv1d(hidden_channels, 2 * latent_channels, 1)

    def forward(self, x, x_lengths=None):
        """
        Args:
            x: linear spectrogram [B, freq_bins, T]
            x_lengths: lengths for masking [B]
        Returns:
            z: sampled latent [B, latent_channels, T]
            mu: mean [B, latent_channels, T]
            log_sigma: log std dev [B, latent_channels, T]
        """
        # Create mask if lengths provided
        if x_lengths is not None:
            mask = torch.arange(x.size(2), device=x.device)[None, :] < x_lengths[:, None]
            mask = mask.unsqueeze(1).float()  # [B, 1, T]
        else:
            mask = torch.ones(x.size(0), 1, x.size(2), device=x.device)

        h = self.pre(x) * mask
        skip_sum = 0
        for block in self.blocks:
            h, skip = block(h)
            h = h * mask
            skip_sum = skip_sum + skip

        stats = self.proj(skip_sum) * mask
        mu, log_sigma = stats.chunk(2, dim=1)

        # Reparameterization trick
        z = mu + torch.randn_like(mu) * torch.exp(log_sigma)
        z = z * mask

        return z, mu, log_sigma


# Demonstration
if __name__ == "__main__":
    encoder = PosteriorEncoder(
        in_channels=513,      # Linear spectrogram bins (1024 FFT / 2 + 1)
        hidden_channels=192,
        latent_channels=192,
        n_layers=16
    )

    # Simulate a batch of linear spectrograms
    batch_size = 4
    max_len = 200
    x = torch.randn(batch_size, 513, max_len)
    lengths = torch.tensor([200, 180, 150, 120])

    z, mu, log_sigma = encoder(x, lengths)
    print(f"Input shape:  {x.shape}")       # [4, 513, 200]
    print(f"Latent shape: {z.shape}")       # [4, 192, 200]
    print(f"Mean shape:   {mu.shape}")      # [4, 192, 200]
    print(f"Log-sigma:    {log_sigma.shape}") # [4, 192, 200]
    print(f"\nParameters: {sum(p.numel() for p in encoder.parameters()):,}")
</code></pre>

                    <h2>Code Example 2: VITS Training Step with All Losses</h2>

                    <p>A complete VITS training step involves computing five losses and updating both generator and discriminator. This is significantly more complex than Glow-TTS's single maximum likelihood objective.</p>

                    <pre><code class="language-python">import torch
import torch.nn.functional as F

def compute_kl_divergence(mu_q, log_sigma_q, mu_p, log_sigma_p, z_mask):
    """
    KL divergence between posterior q and flow-enhanced prior p.
    Both are diagonal Gaussians after the flow transformation.

    KL(q || p) = sum[ log(sigma_p/sigma_q) + (sigma_q^2 + (mu_q-mu_p)^2)/(2*sigma_p^2) - 0.5 ]
    """
    kl = log_sigma_p - log_sigma_q - 0.5
    kl += 0.5 * (torch.exp(2 * log_sigma_q) + (mu_q - mu_p) ** 2) * torch.exp(-2 * log_sigma_p)
    kl = torch.sum(kl * z_mask)
    return kl

def feature_matching_loss(real_features, fake_features):
    """Feature matching loss: L1 between discriminator intermediate features."""
    loss = 0
    for real_feat, fake_feat in zip(real_features, fake_features):
        for r, f in zip(real_feat, fake_feat):
            loss += torch.mean(torch.abs(r.detach() - f))
    return loss * 2  # Scale factor from original VITS

def generator_adversarial_loss(disc_outputs):
    """Hinge-style generator adversarial loss."""
    loss = 0
    for dg in disc_outputs:
        loss += torch.mean((1 - dg) ** 2)
    return loss

def discriminator_loss(real_outputs, fake_outputs):
    """Hinge-style discriminator loss."""
    loss = 0
    for dr, df in zip(real_outputs, fake_outputs):
        loss += torch.mean((1 - dr) ** 2) + torch.mean(df ** 2)
    return loss

def vits_training_step(batch, generator, discriminator, opt_g, opt_d):
    """
    Complete VITS training step with all 5 losses.

    Generator loss: L_recon + L_kl + L_adv + L_fm + L_dur
    Discriminator loss: L_disc
    """
    text, text_lengths, spec, spec_lengths, wav, wav_lengths = batch

    # ============ Generator Forward Pass ============
    # 1. Posterior encoder: spec -> z, mu_q, log_sigma_q
    # 2. Prior encoder: text -> mu_p, log_sigma_p (before flow)
    # 3. Flow: transform prior (mu_p, log_sigma_p) -> flow-enhanced prior
    # 4. MAS: find optimal alignment A*
    # 5. Duration predictor: predict durations from text
    # 6. Decoder (HiFi-GAN): z -> waveform
    gen_output = generator(text, text_lengths, spec, spec_lengths)

    z = gen_output['z']                     # Posterior sample
    mu_q = gen_output['mu_q']               # Posterior mean
    log_sigma_q = gen_output['log_sigma_q'] # Posterior log-std
    mu_p = gen_output['mu_p']               # Flow-enhanced prior mean
    log_sigma_p = gen_output['log_sigma_p'] # Flow-enhanced prior log-std
    wav_hat = gen_output['wav_hat']          # Generated waveform
    dur_loss = gen_output['dur_loss']        # Duration predictor loss
    mel_pred = gen_output['mel_slice']       # Predicted mel (slice)
    mel_real = gen_output['mel_slice_real']  # Ground-truth mel (slice)
    z_mask = gen_output['z_mask']            # Mask for valid frames

    # ============ Discriminator Step ============
    # Detach generated waveform for discriminator training
    real_scores, real_features = discriminator(wav)
    fake_scores, _ = discriminator(wav_hat.detach())

    loss_disc = discriminator_loss(real_scores, fake_scores)

    opt_d.zero_grad()
    loss_disc.backward()
    opt_d.step()

    # ============ Generator Step ============
    # Re-run discriminator on generated (with gradients)
    fake_scores_g, fake_features_g = discriminator(wav_hat)
    _, real_features_g = discriminator(wav)

    # Loss 1: Reconstruction (mel-spectrogram L1)
    loss_recon = F.l1_loss(mel_pred, mel_real) * 45  # Weight from VITS paper

    # Loss 2: KL divergence (posterior vs flow-enhanced prior)
    loss_kl = compute_kl_divergence(
        mu_q, log_sigma_q, mu_p, log_sigma_p, z_mask
    )

    # Loss 3: Adversarial (generator wants to fool discriminator)
    loss_adv = generator_adversarial_loss(fake_scores_g)

    # Loss 4: Feature matching
    loss_fm = feature_matching_loss(real_features_g, fake_features_g)

    # Loss 5: Duration prediction
    loss_dur = dur_loss

    # Total generator loss
    loss_gen = loss_recon + loss_kl + loss_adv + loss_fm + loss_dur

    opt_g.zero_grad()
    loss_gen.backward()
    opt_g.step()

    return {
        'loss_gen': loss_gen.item(),
        'loss_disc': loss_disc.item(),
        'loss_recon': loss_recon.item(),
        'loss_kl': loss_kl.item(),
        'loss_adv': loss_adv.item(),
        'loss_fm': loss_fm.item(),
        'loss_dur': loss_dur.item(),
    }

# Usage example
print("VITS Training Step Summary:")
print("=" * 50)
print("Generator losses:")
print("  L_recon  : Mel-spectrogram L1 (x45 weight)")
print("  L_kl     : KL(posterior || flow-prior)")
print("  L_adv    : Adversarial hinge loss")
print("  L_fm     : Feature matching (x2 weight)")
print("  L_dur    : Duration prediction log-likelihood")
print()
print("Discriminator loss:")
print("  L_disc   : Hinge loss on real/fake")
print()
print("Update order: Discriminator first, then Generator")
</code></pre>

                    <h2>Code Example 3: VITS vs Two-Stage Inference</h2>

                    <p>This comparison demonstrates the simplicity advantage of end-to-end synthesis. VITS requires a single forward pass, while the two-stage approach requires separate acoustic model and vocoder inference.</p>

                    <pre><code class="language-python">import torch
import time

# ============================================================
# Two-Stage Inference (Glow-TTS + HiFi-GAN)
# ============================================================
def two_stage_inference(text, text_lengths, acoustic_model, vocoder):
    """
    Traditional two-stage TTS pipeline:
    Step 1: Text -> Mel-spectrogram (acoustic model)
    Step 2: Mel-spectrogram -> Waveform (vocoder)
    """
    with torch.no_grad():
        # Stage 1: Generate mel-spectrogram
        mel, mel_lengths = acoustic_model.inference(text, text_lengths)
        # mel shape: [B, n_mels, T_mel]

        # Stage 2: Generate waveform from mel
        waveform = vocoder(mel)
        # waveform shape: [B, 1, T_audio]

    return waveform, mel

# ============================================================
# End-to-End Inference (VITS)
# ============================================================
def vits_inference(text, text_lengths, vits_model, temperature=0.667,
                   duration_temperature=1.0):
    """
    End-to-end TTS: Text -> Waveform in a single model.
    No intermediate mel-spectrogram, no separate vocoder.
    """
    with torch.no_grad():
        waveform = vits_model.inference(
            text, text_lengths,
            noise_scale=temperature,           # Latent diversity
            noise_scale_w=duration_temperature  # Duration diversity
        )
        # waveform shape: [B, 1, T_audio]

    return waveform

# ============================================================
# Speed Comparison
# ============================================================
def compare_speed(text, text_lengths, acoustic_model, vocoder, vits_model,
                  n_runs=50):
    """Compare inference speed between two-stage and end-to-end."""

    # Warm-up
    for _ in range(5):
        two_stage_inference(text, text_lengths, acoustic_model, vocoder)
        vits_inference(text, text_lengths, vits_model)

    # Two-stage timing
    start = time.time()
    for _ in range(n_runs):
        two_stage_inference(text, text_lengths, acoustic_model, vocoder)
    two_stage_time = (time.time() - start) / n_runs

    # VITS timing
    start = time.time()
    for _ in range(n_runs):
        vits_inference(text, text_lengths, vits_model)
    vits_time = (time.time() - start) / n_runs

    print(f"Two-stage:  {two_stage_time*1000:.1f} ms/utterance")
    print(f"VITS:       {vits_time*1000:.1f} ms/utterance")
    print(f"Speedup:    {two_stage_time/vits_time:.2f}x")

# ============================================================
# Diversity Comparison
# ============================================================
def demonstrate_diversity(text, text_lengths, vits_model, n_samples=5):
    """Show how temperature affects VITS output diversity."""

    temperatures = [0.0, 0.333, 0.667, 1.0]
    print("\nVITS Diversity Control:")
    print("=" * 50)

    for temp in temperatures:
        waveforms = []
        for _ in range(n_samples):
            wav = vits_inference(text, text_lengths, vits_model,
                              temperature=temp)
            waveforms.append(wav)

        # Compute pairwise variance as diversity metric
        stacked = torch.stack(waveforms)  # [n_samples, B, 1, T]
        variance = stacked.var(dim=0).mean().item()
        print(f"  Temperature {temp:.3f}: variance = {variance:.6f}")

    print("\nHigher temperature -> more diverse outputs")
    print("Lower temperature -> more deterministic outputs")
</code></pre>

                </div>
            </article>

            <!-- Exercises Tab -->
            <article class="article-content" id="exercises" style="display: none;">
                <div class="container">

                    <h2>Exercises</h2>

                    <h3>Easy</h3>

                    <!-- Exercise 1 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">1. Component Identification</span>
                            <span class="difficulty easy">Easy</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Match each VITS component to its primary function: (a) Posterior Encoder, (b) Prior Encoder, (c) Normalizing Flow, (d) HiFi-GAN Decoder, (e) Stochastic Duration Predictor. Functions: (i) Maps text to latent distribution parameters, (ii) Converts latent representations to waveforms, (iii) Encodes linear spectrogram to latent space, (iv) Transforms simple prior to match complex posterior, (v) Models the distribution of phoneme durations.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p>(a) Posterior Encoder &rarr; (iii) Encodes linear spectrogram to latent space. Takes the linear spectrogram (available during training) and produces $\mu_q, \sigma_q$ for the posterior distribution $q_\phi(z|x)$.</p>
                                <p>(b) Prior Encoder &rarr; (i) Maps text to latent distribution parameters. Takes phoneme embeddings and produces $\mu_p, \sigma_p$ for the text-conditional prior $p_\theta(z|c)$.</p>
                                <p>(c) Normalizing Flow &rarr; (iv) Transforms simple prior to match complex posterior. Applies invertible transformations to make the simple Gaussian prior expressive enough to approximate the complex posterior.</p>
                                <p>(d) HiFi-GAN Decoder &rarr; (ii) Converts latent representations to waveforms. Takes latent $z$ and generates the raw audio waveform using transposed convolutions and MRF blocks.</p>
                                <p>(e) Stochastic Duration Predictor &rarr; (v) Models the distribution of phoneme durations. Uses a flow-based model to represent the full distribution of durations, enabling diverse prosody through sampling.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 2 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">2. Loss Function Matching</span>
                            <span class="difficulty easy">Easy</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>VITS uses five losses: $\mathcal{L}_{recon}$, $\mathcal{L}_{kl}$, $\mathcal{L}_{adv}$, $\mathcal{L}_{fm}$, $\mathcal{L}_{dur}$. For each loss, identify: (a) What it compares (input vs. target), (b) Which component(s) it primarily trains, (c) What happens if this loss is removed.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>$\mathcal{L}_{recon}$</strong> (Reconstruction): (a) Predicted mel-spectrogram vs. ground-truth mel-spectrogram (L1). (b) Primarily trains the decoder/generator. (c) Without it, the model relies solely on adversarial and KL losses, leading to slow convergence and potential mode collapse.</p>
                                <p><strong>$\mathcal{L}_{kl}$</strong> (KL Divergence): (a) Posterior $q_\phi(z|x)$ vs. flow-enhanced prior $p_\theta(z|c)$. (b) Trains posterior encoder, prior encoder, and flow jointly. (c) Without it, the prior has no incentive to match the posterior; inference (which uses the prior) produces garbage.</p>
                                <p><strong>$\mathcal{L}_{adv}$</strong> (Adversarial): (a) Discriminator scores on generated vs. real waveforms. (b) Trains the decoder to fool the discriminator. (c) Without it, generated waveforms are over-smoothed and lack fine detail; quality drops significantly.</p>
                                <p><strong>$\mathcal{L}_{fm}$</strong> (Feature Matching): (a) Discriminator intermediate features on generated vs. real waveforms. (b) Stabilizes decoder training. (c) Without it, adversarial training is less stable; quality degrades and training may diverge.</p>
                                <p><strong>$\mathcal{L}_{dur}$</strong> (Duration): (a) Predicted duration distribution vs. MAS-derived durations. (b) Trains the stochastic duration predictor. (c) Without it, the model cannot predict durations at inference time; synthesis fails entirely.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 3 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">3. Inference Path Tracing</span>
                            <span class="difficulty easy">Easy</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Trace the inference path of VITS step by step, starting from input text and ending at the output waveform. For each step, name the component used and describe its input and output. Which components from training are NOT used during inference?</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>Inference path:</strong></p>
                                <ol>
                                    <li><strong>Text Processing:</strong> Input text &rarr; phoneme sequence (using G2P).</li>
                                    <li><strong>Prior Encoder:</strong> Phoneme sequence &rarr; $\mu_p, \sigma_p$ (text-conditional prior parameters).</li>
                                    <li><strong>Stochastic Duration Predictor:</strong> Text hidden states &rarr; sampled durations $d$ (one per phoneme).</li>
                                    <li><strong>Length Regulator:</strong> Expand prior parameters according to durations: $\mu_p, \sigma_p$ from phoneme-level to frame-level.</li>
                                    <li><strong>Normalizing Flow (forward):</strong> Sample $z_0 \sim \mathcal{N}(\mu_p, \sigma_p)$, then transform: $z = f_\theta(z_0)$.</li>
                                    <li><strong>HiFi-GAN Decoder:</strong> $z$ &rarr; waveform.</li>
                                </ol>
                                <p><strong>Components NOT used during inference:</strong></p>
                                <ul>
                                    <li><strong>Posterior Encoder:</strong> Only used during training (it requires the actual audio/spectrogram as input).</li>
                                    <li><strong>MAS:</strong> Only used during training to find alignments. At inference, the duration predictor provides alignments instead.</li>
                                    <li><strong>Discriminator:</strong> Only used during training for adversarial loss. Not needed for generation.</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <h3 style="margin-top: 2rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">Medium</h3>

                    <!-- Exercise 4 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">4. ELBO Derivation for VITS</span>
                            <span class="difficulty medium">Medium</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Derive the Evidence Lower Bound (ELBO) for VITS. Start from $\log p(x|c)$ where $x$ is the waveform and $c$ is the text. Introduce the posterior $q_\phi(z|x)$ and show how the ELBO decomposes into reconstruction and KL terms. Then explain how the normalizing flow modifies the KL term.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p>Start from the marginal log-likelihood:</p>
                                <p>$$\log p_\theta(x|c) = \log \int p_\theta(x|z) p_\theta(z|c) \, dz$$</p>
                                <p>Introduce the posterior $q_\phi(z|x)$ using Jensen's inequality:</p>
                                <p>$$\log p_\theta(x|c) \geq \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x) \| p_\theta(z|c))$$</p>
                                <p>This is the standard ELBO with two terms:</p>
                                <ul>
                                    <li><strong>Reconstruction term:</strong> $\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]$ &mdash; the decoder should reconstruct $x$ from $z$. In VITS, this is approximated by the mel-spectrogram L1 loss ($\mathcal{L}_{recon}$) plus the adversarial loss.</li>
                                    <li><strong>KL term:</strong> $D_{KL}(q_\phi(z|x) \| p_\theta(z|c))$ &mdash; the posterior should match the prior.</li>
                                </ul>
                                <p><strong>Flow modification of KL:</strong> Without the flow, $p_\theta(z|c) = \mathcal{N}(z; \mu_p, \sigma_p)$ is a simple diagonal Gaussian, too simple to match the complex posterior. The normalizing flow $f_\theta$ transforms this:</p>
                                <p>$$p_\theta(z|c) = p_0(f_\theta^{-1}(z|c)) \left|\det \frac{\partial f_\theta^{-1}}{\partial z}\right|$$</p>
                                <p>The KL divergence becomes:</p>
                                <p>$$D_{KL}(q_\phi(z|x) \| p_\theta(z|c)) = \mathbb{E}_{q_\phi}[\log q_\phi(z|x) - \log p_0(f_\theta^{-1}(z)) - \log|\det J_{f^{-1}}|]$$</p>
                                <p>This is tractable because both $q_\phi$ and $p_0$ are diagonal Gaussians, and the Jacobian determinant is computed efficiently through the coupling layer structure. The flow effectively makes the prior as expressive as needed, reducing the KL gap without compromising the posterior.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 5 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">5. Flow-Enhanced Prior Analysis</span>
                            <span class="difficulty medium">Medium</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Consider a VITS model with $K$ flow steps. (a) What is the minimum KL divergence achievable with $K = 0$ (no flow) if the posterior has a bimodal distribution? (b) How does increasing $K$ reduce this gap? (c) Is there a theoretical limit on $K$ beyond which adding more flow steps provides no benefit? (d) In practice, VITS uses $K = 4$. Why is this sufficient?</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> With $K = 0$, the prior is a diagonal Gaussian $\mathcal{N}(\mu_p, \sigma_p^2 I)$. If the posterior is bimodal, the best the prior can do is place its mean between the two modes with large variance. The KL divergence would be at least $\log 2$ nats for a symmetric bimodal posterior, because the Gaussian must "spread" to cover both modes, wasting probability mass in the gap between them.</p>
                                <p><strong>(b)</strong> Each flow step adds an affine coupling layer that can bend and reshape the distribution. With $K \geq 2$ coupling layers (alternating which dimensions are transformed), the flow can create multi-modal priors, asymmetric distributions, and complex correlations. The KL gap decreases as the flow's expressiveness increases.</p>
                                <p><strong>(c)</strong> Theoretically, with infinitely expressive coupling networks, even $K = 1$ can represent any distribution (a universal approximation result for flows). In practice, each flow step has finite capacity (bounded by the coupling network's architecture). The marginal benefit of additional steps decreases: going from $K = 0$ to $K = 1$ is transformative; going from $K = 4$ to $K = 5$ provides minimal improvement.</p>
                                <p><strong>(d)</strong> $K = 4$ is sufficient because: (1) the coupling network within each flow step is itself a multi-layer WaveNet with substantial capacity, (2) speech posterior distributions, while complex, have regular structure (they are not adversarially difficult to approximate), (3) empirically, ablation studies show diminishing returns beyond $K = 4$, and (4) computational cost scales linearly with $K$, so there is a practical limit.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 6 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">6. Training Stability</span>
                            <span class="difficulty medium">Medium</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>VITS training involves a generator-discriminator dynamic. (a) What happens if the discriminator becomes too strong too early? (b) What happens if the generator trains much faster than the discriminator? (c) The VITS paper uses $\mathcal{L}_{recon}$ with a weight of 45 and $\mathcal{L}_{fm}$ with a weight of 2. Why are these weights so different? (d) Propose a training schedule that would improve stability for a dataset with only 2 hours of speech.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> If the discriminator becomes too strong, it assigns near-zero probability to all generated samples. The generator receives vanishing gradients (the discriminator's "advice" becomes "everything you do is equally bad"). Training stalls. This is the classic GAN training failure mode.</p>
                                <p><strong>(b)</strong> If the generator trains faster, it finds specific patterns that fool the current discriminator without actually improving quality. The generator "cheats" by exploiting discriminator weaknesses rather than producing better speech. The discriminator then catches up and the generator must change strategy, leading to oscillation.</p>
                                <p><strong>(c)</strong> The large reconstruction weight (45) ensures that the generator first learns to produce roughly correct spectrograms before the adversarial loss refines the details. Without this, the adversarial loss would try to add fine details to a generator that hasn't learned basic structure yet. The feature matching weight (2) is lower because it serves a stabilizing role &mdash; it prevents the generator from deviating too far from the discriminator's expectations, but shouldn't dominate the objective.</p>
                                <p><strong>(d)</strong> For a 2-hour dataset: (1) Start with only $\mathcal{L}_{recon}$ + $\mathcal{L}_{kl}$ + $\mathcal{L}_{dur}$ for 50k steps (VAE pre-training, no adversarial). (2) Introduce the discriminator with reduced learning rate (0.5x of generator). (3) Increase $\mathcal{L}_{recon}$ weight to 90 (stronger reconstruction regularization). (4) Use strong data augmentation (speed, pitch, noise). (5) Apply early stopping on validation mel-cepstral distortion. (6) Reduce model capacity (fewer WaveNet layers, smaller hidden dimension).</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 7 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">7. Low-Resource Strategies</span>
                            <span class="difficulty medium">Medium</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>You have only 2 hours of Kyrgyz speech data and want to train VITS. Propose four modifications to the standard VITS training procedure that would improve quality with limited data. For each modification, explain the rationale and the trade-off.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>Modification 1: Transfer learning from a high-resource language</strong></p>
                                <p><em>Approach:</em> Pre-train VITS on a large Russian dataset (which shares the Cyrillic script with Kyrgyz), then fine-tune on 2h of Kyrgyz. Initialize all components from the pretrained model and fine-tune with a lower learning rate.</p>
                                <p><em>Rationale:</em> The decoder (HiFi-GAN), flow, and discriminator learn language-agnostic acoustic patterns. Only the prior encoder needs significant adaptation.</p>
                                <p><em>Trade-off:</em> If the source and target languages are very different acoustically, negative transfer can occur.</p>
                                <p><strong>Modification 2: Reduce model capacity</strong></p>
                                <p><em>Approach:</em> Use fewer WaveNet layers (8 instead of 16), fewer flow steps (2 instead of 4), and smaller hidden dimensions (96 instead of 192).</p>
                                <p><em>Rationale:</em> Smaller model is less prone to overfitting on limited data.</p>
                                <p><em>Trade-off:</em> Lower quality ceiling.</p>
                                <p><strong>Modification 3: Strong data augmentation</strong></p>
                                <p><em>Approach:</em> Speed perturbation (0.9x, 1.0x, 1.1x), pitch shifting, and additive noise to effectively triple the data.</p>
                                <p><em>Rationale:</em> Increases effective dataset size and forces robust representations.</p>
                                <p><em>Trade-off:</em> Excessive augmentation can introduce artifacts.</p>
                                <p><strong>Modification 4: Stronger regularization</strong></p>
                                <p><em>Approach:</em> Increase KL weight (2.0 instead of 1.0), add weight decay, use dropout, and apply early stopping.</p>
                                <p><em>Rationale:</em> Prevents overfitting and ensures well-structured latent space.</p>
                                <p><em>Trade-off:</em> Too much regularization reduces expressiveness; high KL weight risks posterior collapse.</p>
                            </div>
                        </div>
                    </div>

                    <h3 style="margin-top: 2rem; border-bottom: 1px solid var(--color-border); padding-bottom: 0.5rem;">Hard</h3>

                    <!-- Exercise 8 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">8. Stochastic Duration Predictor Analysis</span>
                            <span class="difficulty hard">Hard</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>The VITS stochastic duration predictor uses a flow-based model. (a) Explain why a flow-based model is more appropriate than a simple Gaussian for duration distributions. (b) Derive the training objective starting from the change of variables formula. (c) Describe how temperature-controlled sampling affects generated speech, addressing the interaction between duration temperature $\tau_d$ and latent temperature $\tau_z$.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> Duration distributions are complex: multimodal (the same phoneme has different durations depending on emphasis), skewed (bounded below by zero with a long right tail), and context-dependent. A flow can represent arbitrary distributions while maintaining exact likelihood, making it ideal for this task.</p>
                                <p><strong>(b)</strong> Let $d' = \log d$ be log-durations. The flow defines: $p_d(d' | h) = p_0(g^{-1}(d' | h)) \cdot |\det \frac{\partial g^{-1}}{\partial d'}|$ where $g$ is the duration flow and $p_0$ is standard Gaussian. The training objective is:</p>
                                <p>$$\mathcal{L}_{dur} = -\log p_d(d' | h) = \frac{1}{2}\|g^{-1}(d')\|^2 + \frac{T}{2}\log(2\pi) - \sum_k \log|s_k|$$</p>
                                <p>where $s_k$ are scale factors from the affine coupling layers.</p>
                                <p><strong>(c)</strong> $\tau_d$ controls temporal diversity (rhythm, rate), while $\tau_z$ controls spectral diversity (pitch, quality). High $\tau_d$ with low $\tau_z$ produces varied rhythm but consistent voice quality. Low $\tau_d$ with high $\tau_z$ gives consistent timing but varied pitch. The optimal combination is typically $\tau_z \in [0.5, 0.8]$ and $\tau_d \in [0.6, 1.0]$, since listeners are more sensitive to spectral artifacts than rhythm variation.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 9 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">9. VITS vs Glow-TTS Comparison</span>
                            <span class="difficulty hard">Hard</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Compare VITS and Glow-TTS across five dimensions: (a) generative framework, (b) output representation, (c) alignment mechanism, (d) duration modeling, (e) training objective. For each, explain the design choice in each model and argue which is superior. Conclude with a scenario where Glow-TTS might be preferred.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a) Framework:</strong> Glow-TTS uses pure flow-based maximum likelihood. VITS uses VAE + flow + GAN. VITS is superior for quality (MOS 4.43 vs 4.0) because each framework addresses a different limitation.</p>
                                <p><strong>(b) Output:</strong> Glow-TTS generates mel-spectrograms (requires separate vocoder). VITS generates waveforms directly. VITS eliminates the mel bottleneck and train-test mismatch.</p>
                                <p><strong>(c) Alignment:</strong> In Glow-TTS, MAS operates on $z = f^{-1}(\text{mel})$. In VITS, MAS operates on $z \sim q_\phi(z|\text{spec})$. VITS operates on richer latent representations jointly optimized for generation.</p>
                                <p><strong>(d) Duration:</strong> Glow-TTS uses deterministic prediction (MSE). VITS uses stochastic prediction (flow-based). VITS produces more natural prosodic variation.</p>
                                <p><strong>(e) Training:</strong> Glow-TTS uses exact ML. VITS uses ELBO + adversarial + feature matching. VITS achieves higher quality but is harder to train.</p>
                                <p><strong>When Glow-TTS is preferred:</strong> (1) Very limited compute, (2) need modular system with swappable vocoder, (3) training stability is critical, (4) need intermediate mel for analysis/debugging, (5) low-resource language where simpler ML objective is more robust.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Exercise 10 -->
                    <div class="exercise-item">
                        <div class="exercise-header">
                            <span class="exercise-title">10. Extending VITS for Bilingual TTS</span>
                            <span class="difficulty hard">Hard</span>
                            <span class="exercise-toggle">&darr;</span>
                        </div>
                        <div class="exercise-body">
                            <p>Design a bilingual VITS system for Russian and Kyrgyz. Address: (a) phoneme inventory handling, (b) prior encoder modifications, (c) flow and decoder modifications, (d) data imbalance handling (20h Russian, 3h Kyrgyz), (e) complete training procedure with curriculum.</p>
                            <button class="btn btn-sm solution-toggle">Show Solution</button>
                            <div class="solution-content">
                                <p><strong>Solution:</strong></p>
                                <p><strong>(a)</strong> Create a unified IPA phoneme set: shared phonemes use the same embedding (enabling cross-lingual transfer), language-specific phonemes get their own. Add language tokens [RU]/[KY] prepended to each utterance.</p>
                                <p><strong>(b)</strong> Add language embedding concatenated with phoneme embeddings before the transformer. Use language-adaptive layer normalization: LayerNorm scale/bias conditioned on language embedding.</p>
                                <p><strong>(c)</strong> Flow and decoder should be mostly shared (language-agnostic acoustics). Add language embedding to flow conditioning. Decoder (HiFi-GAN) can be fully shared.</p>
                                <p><strong>(d)</strong> Balanced sampling: 50% RU / 50% KY per batch (oversampling Kyrgyz ~7x). Stronger augmentation for Kyrgyz (speed, pitch). Curriculum: start with more Russian, gradually increase Kyrgyz ratio.</p>
                                <p><strong>(e)</strong> Phase 1 (0-100k): Russian only, standard VITS. Phase 2 (100k-150k): Introduce Kyrgyz gradually (10% to 50%), reduced LR, lower KL weight for KY. Phase 3 (150k-400k): Balanced training, normal weights. Monitor per-language MOS; early stop if Kyrgyz degrades.</p>
                            </div>
                        </div>
                    </div>

                </div>
            </article>

        </main>

        <!-- TOC (Right Side) -->
        <aside class="toc-container">
            <h4 class="toc-title">Contents</h4>
            <nav class="toc-list">
                <a href="#vits-breakthrough" class="toc-link">The VITS Breakthrough</a>
                <a href="#vits-architecture" class="toc-link">Architecture Deep Dive</a>
                <a href="#vits-training-losses" class="toc-link">Training Losses</a>
                <a href="#kl-flow" class="toc-link">KL Divergence with Flows</a>
                <a href="#mas-in-vits" class="toc-link">Monotonic Alignment in VITS</a>
                <a href="#end-to-end-advantage" class="toc-link">Why End-to-End Matters</a>
                <a href="#why-natural" class="toc-link">Why VITS Sounds Natural</a>
                <a href="#vits-extensions" class="toc-link">VITS Extensions</a>
            </nav>
        </aside>
    </div>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-brand">
                    <span class="logo-symbol">&nabla;</span>
                    <span>ML Fundamentals</span>
                </div>
                <p class="footer-tagline">Deep understanding through first principles.</p>
            </div>
            <div class="footer-links">
                <a href="../../../index.html">Home</a>
                <a href="https://github.com/ml-entropy/ml-entropy.github.io" target="_blank">GitHub</a>
            </div>
        </div>
    </footer>

    <script src="../../../js/main.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            if (typeof renderMathInElement === 'function') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }

            const tabs = document.querySelectorAll('.tutorial-tab');
            const articles = document.querySelectorAll('.article-content');

            function switchTab(targetId) {
                if (!targetId || targetId === '#') targetId = '#theory';
                tabs.forEach(tab => {
                    if (tab.getAttribute('href') === targetId) {
                        tab.classList.add('active');
                    } else {
                        tab.classList.remove('active');
                    }
                });
                articles.forEach(article => {
                    const articleId = '#' + article.id;
                    if (articleId === targetId) {
                        article.style.display = 'block';
                    } else {
                        article.style.display = 'none';
                    }
                });
                if (typeof renderMathInElement === 'function') {
                    renderMathInElement(document.body, {
                        delimiters: [
                            {left: '$$', right: '$$', display: true},
                            {left: '$', right: '$', display: false},
                            {left: '\\[', right: '\\]', display: true},
                            {left: '\\(', right: '\\)', display: false}
                        ],
                        throwOnError: false
                    });
                }
                const toc = document.querySelector('.toc-container');
                if (toc) {
                    if (targetId === '#theory') {
                        toc.style.display = 'block';
                        setTimeout(() => toc.classList.add('visible'), 100);
                    } else {
                        toc.classList.remove('visible');
                        setTimeout(() => toc.style.display = 'none', 300);
                    }
                }
            }

            tabs.forEach(tab => {
                tab.addEventListener('click', (e) => {
                    e.preventDefault();
                    const targetId = tab.getAttribute('href');
                    history.pushState(null, null, targetId);
                    switchTab(targetId);
                });
            });

            window.addEventListener('popstate', () => {
                switchTab(window.location.hash);
            });

            switchTab(window.location.hash);
        });
    </script>
</body>
</html>
