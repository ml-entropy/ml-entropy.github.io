<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Machine Learning Tutorials | ML Fundamentals</title>
    <meta name="description" content="Comprehensive machine learning tutorials with entropy perspective, from probability to VAEs and neural networks.">
    
    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&family=Playfair+Display:wght@400;500;600;700&display=swap" rel="stylesheet">
    
    <!-- KaTeX for Math -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}, {left: '\\\\[', right: '\\\\]', display: true}, {left: '\\\\(', right: '\\\\)', display: false}], throwOnError: false});"></script>
    
    <!-- Styles -->
    <link rel="stylesheet" href="../../css/main.css">
    <link rel="stylesheet" href="../../css/components.css">
    
    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>‚àû</text></svg>">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar" id="navbar">
        <div class="nav-container">
            <a href="../../index.html" class="nav-logo">
                <span class="logo-symbol">‚àá</span>
                <span class="logo-text">ML Fundamentals</span>
            </a>
            
            <button class="nav-toggle" id="navToggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            
            <div class="nav-menu" id="navMenu">
                <div class="nav-links">
                    <a href="../../tutorials/ml/index.html" class="nav-link active">Machine Learning</a>
                    <a href="../../tutorials/linear-algebra/index.html" class="nav-link">Linear Algebra</a>
                    <a href="../../tutorials/calculus/index.html" class="nav-link">Calculus</a>
                    <a href="../../tutorials/physics/index.html" class="nav-link">Physics</a>
                    <a href="../../index.html#philosophy" class="nav-link">Philosophy</a>
                    <a href="../../index.html#roadmap" class="nav-link">Roadmap</a>
                    <a href="https://github.com/ml-entropy/ml-entropy.github.io" class="nav-link" target="_blank">GitHub</a>
                </div>
                
                <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
                    <svg class="sun-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="5"/>
                        <path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"/>
                    </svg>
                    <svg class="moon-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                    </svg>
                </button>
            </div>
        </div>
    </nav>

    <!-- Header -->
    <header class="tutorial-header">
        <div class="container">
            <nav class="breadcrumb">
                <a href="../../index.html">Home</a>
                <span class="breadcrumb-separator">‚Üí</span>
                <span>Machine Learning</span>
            </nav>
            
            <h1>Machine Learning Tutorials</h1>
            <p class="lead">
                A comprehensive journey through machine learning from an information-theoretic perspective. 
                Every formula derived, every concept visualized.
            </p>
            
            <div class="featured-equation" style="margin-top: 2rem;">
                $$H(X) = -\sum_{x \in \mathcal{X}} p(x) \log_2 p(x)$$
            </div>
        </div>
    </header>

    <!-- Tutorial List -->
    <main class="tutorial-list">
        <div class="container">
            <!-- Part I: Mathematical Foundations -->
            <h2 style="margin-bottom: 1.5rem; font-size: 1.25rem; color: var(--color-text-secondary);">
                Part I: Mathematical Foundations
            </h2>

            <a href="00-probability/index.html" class="tutorial-item">
                <div class="tutorial-number">00</div>
                <div class="tutorial-content">
                    <h3>Probability Concepts for ML</h3>
                    <p>P(X), P(Y|X), Bayes theorem, priors, posteriors, likelihood. The foundation of all ML.</p>
                    <div class="tutorial-meta">
                        <span>üìñ Theory + Exercises</span>
                        <span>‚è± ~45 min</span>
                    </div>
                </div>
            </a>

            <a href="04-logarithms/index.html" class="tutorial-item">
                <div class="tutorial-number">01</div>
                <div class="tutorial-content">
                    <h3>Logarithms in ML</h3>
                    <p>Why we use log-likelihood. Is it just numerical stability or something fundamental?</p>
                    <div class="tutorial-meta">
                        <span>üìñ Theory + Proofs</span>
                        <span>‚è± ~45 min</span>
                    </div>
                </div>
            </a>

            <a href="05-combinatorics/index.html" class="tutorial-item">
                <div class="tutorial-number">02</div>
                <div class="tutorial-content">
                    <h3>Combinatorics</h3>
                    <p>Permutations, combinations, and why they appear everywhere in probability.</p>
                    <div class="tutorial-meta">
                        <span>üìñ Theory + Formula Derivations</span>
                        <span>‚è± ~40 min</span>
                    </div>
                </div>
            </a>

            <a href="03-distributions/index.html" class="tutorial-item">
                <div class="tutorial-number">03</div>
                <div class="tutorial-content">
                    <h3>Probability Distributions</h3>
                    <p>PDF, CDF, expectation, variance. Normal and multivariate normal distributions in depth.</p>
                    <div class="tutorial-meta">
                        <span>üìñ Theory + Visualizations + Code</span>
                        <span>‚è± ~60 min</span>
                    </div>
                </div>
            </a>

            <!-- Part II: Information Theory -->
            <h2 style="margin: 3rem 0 1.5rem; font-size: 1.25rem; color: var(--color-text-secondary);">
                Part II: Information Theory
            </h2>

            <a href="01-entropy/index.html" class="tutorial-item">
                <div class="tutorial-number">04</div>
                <div class="tutorial-content">
                    <h3>Information Entropy</h3>
                    <p>Shannon entropy, information content, optimal coding, Huffman encoding. Why entropy measures surprise.</p>
                    <div class="tutorial-meta">
                        <span>üìñ Theory + Code + Exercises</span>
                        <span>‚è± ~60 min</span>
                    </div>
                </div>
            </a>

            <a href="02-cross-entropy/index.html" class="tutorial-item">
                <div class="tutorial-number">05</div>
                <div class="tutorial-content">
                    <h3>Cross-Entropy Loss</h3>
                    <p>The heartbeat of classification. Why negative log probability is the ultimate error metric.</p>
                    <div class="tutorial-meta">
                        <span>üìñ Theory + Code + Visuals</span>
                        <span>‚è± ~50 min</span>
                    </div>
                </div>
            </a>

            <a href="02-kl-divergence/index.html" class="tutorial-item">
                <div class="tutorial-number">06</div>
                <div class="tutorial-content">
                    <h3>KL Divergence</h3>
                    <p>Relative entropy for discrete and continuous distributions. The cost of using the wrong model.</p>
                    <div class="tutorial-meta">
                        <span>üìñ Theory + Derivations + Code</span>
                        <span>‚è± ~75 min</span>
                    </div>
                </div>
            </a>

            <a href="14-entropy-connections/index.html" class="tutorial-item">
                <div class="tutorial-number">07</div>
                <div class="tutorial-content">
                    <h3>Entropy Connections</h3>
                    <p>The Holy Trinity: How Entropy, Cross-Entropy, and KL Divergence are mathematically unified.</p>
                    <div class="tutorial-meta">
                        <span>üìñ Theory + Synthesis</span>
                        <span>‚è± ~40 min</span>
                    </div>
                </div>
            </a>

            <!-- Part III: Deep Learning -->
            <h2 style="margin: 3rem 0 1.5rem; font-size: 1.25rem; color: var(--color-text-secondary);">
                Part III: Deep Learning
            </h2>

            <a href="06-backpropagation/index.html" class="tutorial-item">
                <div class="tutorial-number">08</div>
                <div class="tutorial-content">
                    <h3>Backpropagation</h3>
                    <p>Chain rule, computational graphs, gradient flow. Connection between entropy and backprop.</p>
                    <div class="tutorial-meta">
                        <span>üìñ Theory + Code + Exercises</span>
                        <span>‚è± ~90 min</span>
                    </div>
                </div>
            </a>

            <a href="07-regularization/index.html" class="tutorial-item">
                <div class="tutorial-number">09</div>
                <div class="tutorial-content">
                    <h3>Regularization</h3>
                    <p>L1, L2, dropout, weight decay. The Bayesian interpretation of regularization.</p>
                    <div class="tutorial-meta">
                        <span>üìñ Theory + Code + Exercises</span>
                        <span>‚è± ~60 min</span>
                    </div>
                </div>
            </a>

            <a href="08-batch-normalization/index.html" class="tutorial-item">
                <div class="tutorial-number">10</div>
                <div class="tutorial-content">
                    <h3>Batch Normalization</h3>
                    <p>Internal covariate shift, running statistics, layer norm variants.</p>
                    <div class="tutorial-meta">
                        <span>üìñ Theory + Code + Exercises</span>
                        <span>‚è± ~50 min</span>
                    </div>
                </div>
            </a>

            <a href="09-learning-rate/index.html" class="tutorial-item">
                <div class="tutorial-number">11</div>
                <div class="tutorial-content">
                    <h3>Learning Rate</h3>
                    <p>Gradient descent dynamics, schedulers, warmup, cyclical learning rates.</p>
                    <div class="tutorial-meta">
                        <span>üìñ Theory + Visualizations + Code</span>
                        <span>‚è± ~45 min</span>
                    </div>
                </div>
            </a>

            <a href="10-cnn/index.html" class="tutorial-item">
                <div class="tutorial-number">12</div>
                <div class="tutorial-content">
                    <h3>Convolutional Neural Networks</h3>
                    <p>Convolutions, pooling, feature maps. Why CNNs work for images.</p>
                    <div class="tutorial-meta">
                        <span>üìñ Theory + Code + Exercises</span>
                        <span>‚è± ~75 min</span>
                    </div>
                </div>
            </a>

            <a href="11-rnn/index.html" class="tutorial-item">
                <div class="tutorial-number">13</div>
                <div class="tutorial-content">
                    <h3>Recurrent Neural Networks</h3>
                    <p>Sequence modeling, BPTT, LSTMs, GRUs. The vanishing gradient problem.</p>
                    <div class="tutorial-meta">
                        <span>üìñ Theory + Code + Exercises</span>
                        <span>‚è± ~75 min</span>
                    </div>
                </div>
            </a>

            <!-- Part IV: Representation Learning & Generative Models -->
            <h2 style="margin: 3rem 0 1.5rem; font-size: 1.25rem; color: var(--color-text-secondary);">
                Part IV: Representation Learning &amp; Generative Models
            </h2>

            <a href="15-autoencoder/index.html" class="tutorial-item">
                <div class="tutorial-number">14</div>
                <div class="tutorial-content">
                    <h3>Autoencoders</h3>
                    <p>Learning compressed representations through bottlenecks. Sparse, denoising, and contractive variants. The information bottleneck perspective.</p>
                    <div class="tutorial-meta">
                        <span>üìñ Theory + Code + Exercises</span>
                        <span>‚è± ~60 min</span>
                    </div>
                </div>
            </a>

            <a href="13-variational-inference/index.html" class="tutorial-item">
                <div class="tutorial-number">15</div>
                <div class="tutorial-content">
                    <h3>Variational Inference</h3>
                    <p>Approximating intractable posteriors. Mean-field approximation. Connection to EM algorithm.</p>
                    <div class="tutorial-meta">
                        <span>üìñ Theory + Derivations</span>
                        <span>‚è± ~75 min</span>
                    </div>
                </div>
            </a>

            <a href="12-vae/index.html" class="tutorial-item">
                <div class="tutorial-number">16</div>
                <div class="tutorial-content">
                    <h3>Variational Autoencoders</h3>
                    <p>VAE architecture, ELBO derivation, reparameterization trick. Why KL loss on latent space.</p>
                    <div class="tutorial-meta">
                        <span>üìñ Theory + Full Derivation + Code</span>
                        <span>‚è± ~90 min</span>
                    </div>
                </div>
            </a>

            <!-- Part V: Inductive Bias & Architecture Design -->
            <h2 style="margin: 3rem 0 1.5rem; font-size: 1.25rem; color: var(--color-text-secondary);">
                Part V: Inductive Bias &amp; Architecture Design
            </h2>

            <a href="16-inductive-bias/index.html" class="tutorial-item">
                <div class="tutorial-number">17</div>
                <div class="tutorial-content">
                    <h3>Inductive Bias</h3>
                    <p>Why every learner must assume. No Free Lunch theorem, bias-variance revisited, taxonomy of biases in classical ML.</p>
                    <div class="tutorial-meta">
                        <span>üìñ Theory + Code + Exercises</span>
                        <span>‚è± ~60 min</span>
                    </div>
                </div>
            </a>

            <a href="17-architectural-biases/index.html" class="tutorial-item">
                <div class="tutorial-number">18</div>
                <div class="tutorial-content">
                    <h3>Architectural Biases in Deep Learning</h3>
                    <p>How CNNs, RNNs, Transformers, and GNNs encode assumptions through equivariance and weight sharing.</p>
                    <div class="tutorial-meta">
                        <span>üìñ Theory + Code + Exercises</span>
                        <span>‚è± ~75 min</span>
                    </div>
                </div>
            </a>

            <a href="18-designing-biases/index.html" class="tutorial-item">
                <div class="tutorial-number">19</div>
                <div class="tutorial-content">
                    <h3>Designing Inductive Biases</h3>
                    <p>Regularization, data augmentation, transfer learning, the Bitter Lesson, and when biases fail.</p>
                    <div class="tutorial-meta">
                        <span>üìñ Theory + Code + Exercises</span>
                        <span>‚è± ~75 min</span>
                    </div>
                </div>
            </a>

            <!-- Part VI: Structured Sequence Processing -->
            <h2 style="margin: 3rem 0 1.5rem; font-size: 1.25rem; color: var(--color-text-secondary);">
                Part VI: Structured Sequence Processing
            </h2>

            <a href="19-fst-fundamentals/index.html" class="tutorial-item">
                <div class="tutorial-number">20</div>
                <div class="tutorial-content">
                    <h3>FST Fundamentals</h3>
                    <p>Finite State Machines and Transducers: definitions, determinism, composition, regular languages, and when to use FSTs.</p>
                    <div class="tutorial-meta">
                        <span>üìñ Theory + Code + Exercises</span>
                        <span>‚è± ~60 min</span>
                    </div>
                </div>
            </a>

            <a href="20-weighted-fsts/index.html" class="tutorial-item">
                <div class="tutorial-number">21</div>
                <div class="tutorial-content">
                    <h3>Weighted FSTs</h3>
                    <p>Semirings, weighted transitions, shortest path algorithms, determinization, and weighted composition.</p>
                    <div class="tutorial-meta">
                        <span>üìñ Theory + Code + Exercises</span>
                        <span>‚è± ~75 min</span>
                    </div>
                </div>
            </a>

            <a href="21-fst-libraries/index.html" class="tutorial-item">
                <div class="tutorial-number">22</div>
                <div class="tutorial-content">
                    <h3>FST Libraries</h3>
                    <p>Practical guide to pynini, OpenFST, and HFST: installation, API usage, workflows, and performance optimization.</p>
                    <div class="tutorial-meta">
                        <span>üìñ Theory + Code + Exercises</span>
                        <span>‚è± ~75 min</span>
                    </div>
                </div>
            </a>

            <a href="22-fst-applications/index.html" class="tutorial-item">
                <div class="tutorial-number">23</div>
                <div class="tutorial-content">
                    <h3>FST Applications</h3>
                    <p>FSTs in speech recognition, machine translation, text normalization, and morphological analysis.</p>
                    <div class="tutorial-meta">
                        <span>üìñ Theory + Code + Exercises</span>
                        <span>‚è± ~75 min</span>
                    </div>
                </div>
            </a>

            <a href="23-neural-symbolic/index.html" class="tutorial-item">
                <div class="tutorial-number">24</div>
                <div class="tutorial-content">
                    <h3>Neural-Symbolic Hybrids</h3>
                    <p>FST vs neural tradeoffs, hybrid architectures, decision framework, and when to choose each approach.</p>
                    <div class="tutorial-meta">
                        <span>üìñ Theory + Code + Exercises</span>
                        <span>‚è± ~60 min</span>
                    </div>
                </div>
            </a>

            <!-- Part VII: Sequence Alignment & Speech -->
            <h2 style="margin: 3rem 0 1.5rem; font-size: 1.25rem; color: var(--color-text-secondary);">
                Part VII: Sequence Alignment &amp; Speech
            </h2>

            <a href="24-sequence-alignment/index.html" class="tutorial-item">
                <div class="tutorial-number">25</div>
                <div class="tutorial-content">
                    <h3>Sequence Alignment Fundamentals</h3>
                    <p>Monotonic alignment between text and audio. Alignment matrices, cost functions, dynamic programming formulation, and the connection to attention mechanisms.</p>
                    <div class="tutorial-meta">
                        <span>üìñ Theory + Code + Exercises</span>
                        <span>‚è± ~60 min</span>
                    </div>
                </div>
            </a>

            <a href="25-mas-algorithm/index.html" class="tutorial-item">
                <div class="tutorial-number">26</div>
                <div class="tutorial-content">
                    <h3>Monotonic Alignment Search (MAS)</h3>
                    <p>The MAS algorithm from Glow-TTS in detail. Worked examples, the critical silence edge case, Viterbi comparison, and MAS in TTS training loops.</p>
                    <div class="tutorial-meta">
                        <span>üìñ Theory + Code + Exercises</span>
                        <span>‚è± ~75 min</span>
                    </div>
                </div>
            </a>

            <a href="26-forced-alignment/index.html" class="tutorial-item">
                <div class="tutorial-number">27</div>
                <div class="tutorial-content">
                    <h3>Forced Alignment &amp; MFA</h3>
                    <p>HMM-based forced alignment, Montreal Forced Aligner, CTC alignment. How MFA handles silence natively. Practical corpus alignment workflows.</p>
                    <div class="tutorial-meta">
                        <span>üìñ Theory + Code + Exercises</span>
                        <span>‚è± ~75 min</span>
                    </div>
                </div>
            </a>
                    </div>
    </main>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-brand">
                    <span class="logo-symbol">‚àá</span>
                    <span>ML Fundamentals</span>
                </div>
                <p class="footer-tagline">Deep understanding through first principles.</p>
            </div>
            <div class="footer-links">
                <a href="../../index.html">Home</a>
                <a href="https://github.com/ml-entropy/ml-entropy.github.io" target="_blank">GitHub</a>
            </div>
        </div>
    </footer>

    <!-- Scripts -->
    <script src="../../js/main.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            if (typeof renderMathInElement === 'function') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false}
                    ],
                    throwOnError: false
                });
            }
        });
    </script>
</body>
</html>
